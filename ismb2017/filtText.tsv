#fileId	externalId	articleId	url	desc	fileType	time	mimeType	locFname	content
5028882005002	PMID28882005	5028882005	https://watermark.silverchair.com/btx277.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28882005.main.pdf	Bioinformatics, 33, 2017, i5–i12doi: 10.1093/bioinformatics/btx277ISMB/ECCB 2017CATS (Coordinates of Atoms by Taylor Series):protein design with backbone flexibility in alllocally feasible directionsMark A. Hallen1,2,* and Bruce R. Donald1,3,4,*1Department of Computer Science, Duke University, Durham, NC 27708, USA, 2Toyota Technological Institute atChicago, Chicago, IL 60637, USA, 3Department of Chemistry, Duke University, Durham, NC 27708, USA and4Department of Biochemistry, Duke University Medical Center, Durham, NC 27710, USA*To whom correspondence should be addressed.AbstractMotivation: When proteins mutate or bind to ligands, their backbones often move signiﬁcantly, especially in loop regions. Computational protein design algorithms must model these motions inorder to accurately optimize protein stability and binding afﬁnity. However, methods for backboneconformational search in design have been much more limited than for sidechain conformationalsearch. This is especially true for combinatorial protein design algorithms, which aim to search alarge sequence space efﬁciently and thus cannot rely on temporal simulation of each candidatesequence.Results: We alleviate this difﬁculty with a new parameterization of backbone conformational space,which represents all degrees of freedom of a speciﬁed segment of protein chain that maintain validbonding geometry (by maintaining the original bond lengths and angles and x dihedrals). In orderto search this space, we present an efﬁcient algorithm, CATS, for computing atomic coordinates asa function of our new continuous backbone internal coordinates. CATS generalizes the iMinDEEand EPIC protein design algorithms, which model continuous ﬂexibility in sidechain dihedrals, tomodel continuous, appropriately localized ﬂexibility in the backbone dihedrals / and w as well. Weshow using 81 test cases based on 29 different protein structures that CATS ﬁnds sequences andconformations that are signiﬁcantly lower in energy than methods with less or no backbone ﬂexibility do. In particular, we show that CATS can model the viability of an antibody mutation knownexperimentally to increase afﬁnity, but that appears sterically infeasible when modeled with less orno backbone ﬂexibility.Availability and implementation: Our code is available as free software at https://github.com/donaldlab/OSPREY_refactor.Contact: mhallen@ttic.edu or brdþismb17@cs.duke.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionProtein design algorithms (Donald, 2011; Lippow and Tidor, 2007;Regan, 1999) address the following problem: given a protein systemand a set of possible localized changes in chemical composition,choose the combination of changes that will optimize a desired functional property. Typically the chemical changes are mutations in sequence or modification of a ligand, while the functional requirementis ligand binding affinity (Floudas et al., 1999; Georgiev et al.,2008b; Karanicolas and Kuhlman, 2009; Lilien et al., 2005), proteinstability (Desmet et al., 1992; Donald, 2011; Gainza et al., 2012;Georgiev et al., 2014; Kuhlman and Baker, 2000), or some combination thereof (Hallen and Donald, 2016; Lewis et al., 2014). Solvingthis problem requires the ability to accurately model protein structure, as binding affinity is sensitive to small changes in the conformation of the protein and ligand.Two approaches are currently employed for protein structuremodeling and coupling it to sequence optimization. First, moleculardynamics can be used to simulate the behavior of a candidate designover time (Rapaport, 2004). This approach has the advantage that itcan explore all conformational degrees of freedom. However, theseCV The Author 2017. Published by Oxford University Press.i5This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i5/3953990by gueston 07 January 2018i6simulations are time consuming and must be run separately for eachcandidate, making them prohibitively expensive for large sequencespaces. For example, a molecular dynamics-based design consideringall 20 amino-acid types for each of 10 residues will require 2010¼10trillion simulations, which is clearly intractable. Indeed, accuratelycomputing the binding constant for a single sequence is relativelytime-consuming, since the timesteps are on the order of femtoseconds while the timescale of ligand binding is many orders of magnitude greater. Other loop modeling methods, such as POOL(Tripathy et al., 2012), that search extensively over the backboneconformational space of a protein loop also limit their search to asingle sequence (Donald, 2011).This brings us to the second approach, consisting of combinatorial algorithms that search a much larger sequence space without considering each sequence separately—the time cost scales sublinearlyin the number of candidate sequences. This is important because thenumber of sequences is exponential in the number of mutable residues. Several classes of methods fall under this approach, as reviewed extensively in Donald (2011) and Gainza et al. (2016).Methods based on the DEE/A* algorithm (Desmet et al., 1992;Gainza et al., 2012; Georgiev et al., 2008b; Gordon et al., 2003;Hallen et al., 2013; Leach and Lemon, 1998; Pierce et al., 2000), onbranch- (Jou et al., 2016) and tree decompositions (Xu and Berger,2006), and on algorithms from integer linear programming(Kingsford et al., 2005; Roberts et al., 2015) and weighted constraint satisfaction (Roberts et al., 2015; Traore et al., 2013, 2016)´offer provable guarantees of accuracy, while methods based onsimulated annealing (Das and Baker, 2008; Kuhlman and Baker,2000; Wang et al., 2005) and genetic algorithms (Desjarlais andHandel, 1995; Lewis et al., 2014; Leaver-Fay et al., 2011) do not.Although the technique we present in this work could be used withmost of these methods in principle, we have implemented it in aframework based on the DEE/A* algorithm, which we will now explain further. Using a provable algorithm with our new model ensures that empirical observations of accuracy precisely reflect theaccuracy of the model, rather than a convolution of modeling andalgorithm accuracy.DEE/A* was first presented as a method to optimize protein stability while modeling only sidechain flexibility (Leach and Lemon,1998). Protein sidechain flexibility is known empirically to consistalmost entirely of flexibility in sidechain dihedral angles, which arerestricted to certain regions of dihedral space. These regions, termedrotamers, have been characterized for each natural amino-acid type,(Lovell et al., 2000) by clustering of sidechain dihedral values formany residues of each type across many different high-resolutioncrystal structures. DEE/A* provided an efficient way to assign anamino acid type and rotamer to each residue in a protein to minimize energy.Initially, DEE/A* assumed every residue would only be found atthe ‘ideal’ dihedral values for its rotamer (the modal values for thatrotamer in crystal structure data). Later work helped to relax this assumption. The minDEE algorithm (Georgiev et al., 2008b; Robertset al., 2012) enabled search over sequence and conformational spacewith each sidechain dihedral restricted to a continuous range (an ideal rotameric value 69 ), instead of to an ideal rotameric valueexactly. The energy minima over this larger, more realistic sidechainconformational space have been shown to be significantly lower(Gainza et al., 2012). The iMinDEE (Gainza et al., 2012) and EPIC(Hallen et al., 2015) algorithms sped up minDEE substantially whileusing the same modeling assumptions, and other extensions addedthe capability to model sidechain conformational entropy (Chenet al., 2009; Donald, 2011; Georgiev et al., 2008b; Lilien et al.,M.A.Hallen and B.R.Donald2005; Roberts et al., 2012) and backbone motions (Georgiev andDonald, 2007; Georgiev et al., 2008a; Hallen et al., 2013), whilestill exploiting the speedups iMinDEE and EPIC offer.Previous combinatorial protein design algorithms have alsoincorporated backbone flexibility, albeit to a limited extent. The BDalgorithm (Georgiev and Donald, 2007) can allow motions in allbackbone dihedrals (/ and w), but these motions are propagateddown the entire backbone chain, which severely limits the extent towhich the backbone in the region of interest (e.g. active-site loop)˚can move without unfolding the protein (generally to (1 A).Modeling larger changes would require either handling dramaticbackbone movement elsewhere in the protein or facing the illconditioned problem of making dihedral changes in subsequent residues cancel each other’s downstream effects. The new parameterization we present here makes the latter problem well-conditioned, byusing an intrinsically local set of internal coordinates.Another previous model for backbone flexibility in protein design is the use of a restricted repertoire of motions that may movethe backbone more, but do not search all biophysically feasible motions even locally. These can be ad hoc, discrete backbone changesspecific to a particular protein system (e.g. from antibody loop libraries (Al-Lazikani et al., 1997)), transplantations of fragments ofother proteins’ backbones (Jacobs et al., 2016; Zhou and Grigoryan,2015), or backbones generated by molecular dynamics simulations(Fung et al., 2008). Alternately, the repertoire can contain motionslike the backrub (Davis et al., 2006) and shear (Hallen et al., 2013)that have been observed repeatedly in crystallographic alternates.The backrub (Davis et al., 2006) in particular has been used in bothDEE/A*-based (Georgiev et al., 2008a; Hallen et al., 2013) andsimulated annealing-based (Smith and Kortemme, 2008) protein design algorithms. The DEEPer algorithm (Hallen et al., 2013) performs a provably complete search over the space defined by a set ofpossible mutations and a predefined repertoire of backrubs, shearsand/or local discrete backbone perturbations.Indeed, some restriction on backbone flexibility is acceptable inthe protein design context, because we know from X-ray crystallography that backbone conformational changes due to mutations orligand binding are usually fairly local (Al-Lazikani et al., 1997;Wong et al., 1999). We also know that backbone motions aremostly limited to changes in the two dihedral angles / and w of eachresidue, and that these dihedrals are restricted to a small subset oftheir possible values (Lovell et al., 2003). This subset is known asthe Ramachandran-allowed region and is well-characterized foreach amino acid type (Lovell et al., 2003), analogously to how sidechains are generally restricted to rotamers. Thus, the set of feasiblebackbone conformational changes can be characterized in the spaceof / and w changes in the flexible region by imposing both inequality (Ramachandran) constraints, and holonomic (i.e. equality) constraints that ensure the non-flexible regions of the backbone do notmove. Without the latter, significant / and w changes would unfoldthe protein, because the amount of atomic motion due to a backbone dihedral change increases for atoms that are further from theaxis of the dihedral rotation. Nevertheless, previous combinatorialprotein design algorithms restrict the backbone substantially morethan these empirical limits on flexibility would require.In the present work, we use a new parameterization of backboneconformational space to obtain a much more systematic search overthe continuous space of local conformational changes. Any differential motion in a specified region of the backbone that is accessible bychanging the backbone dihedrals / and w can be accessed via ourparameterization (Fig. 1). Our parameterization is designed for usein continuous energy minimization with box constraints on allDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i5/3953990by gueston 07 January 2018CATS for protein design backbone flexibilityi7Fig. 1. Backbone degrees of freedom used by CATS. (A) A voxel used in CATS for a 7-residue loop in ponsin (PDB id 2O9S (Gehmlich et al., 2007)), projected intothe 2-D space of two of our new continuous degrees of freedom, denoted by xb;1 and xb;2 . Voxel border, blue; central conformation, black. (B) Conformations inthe voxel: black, central conformation; red and green, conformations shown as dots in A; purple, a conformation for which all 8 degrees of freedom are at the˚voxel edge. (C, D) The boundary of the 2-D-projected voxel shown in A, graphed in the space of atomic Cartesian x coordinates (in A) for the N and Ca atoms ofE856 (C) and in the space of that residue’s backbone dihedrals (in degrees, D). For this 7-residue loop, the voxel has 8 dimensions and thus forms an 8-dimensional hypersurface in the 14-dimensional backbone dihedral space. The distorted parallelogram in (C) would be exactly a parallelogram if the constraints werelineardegrees of freedom (Gainza et al., 2012; Hallen et al., 2013). Thus,we need not explicitly include holonomic constraints when usingour parameterization; our parameterization intrinsically does notmove the regions of protein backbone that need to be kept fixed.This parameterization allows us to use polynomial approximations(Taylor series) to efficiently evaluate the continuous backbonemovements around a reference backbone. We thus provide a fastmethod to compute all atomic coordinates as a function of our noveldegrees of freedom, by calculating Coordinates of Atoms by TaylorSeries (CATS). We have integrated CATS with the iMinDEE(Gainza et al., 2012) and EPIC (Hallen et al., 2015) protein designalgorithms, which call such continuous minimization as a subroutine. CATS casts the modeling of localized, continuous backbone dihedral flexibility into a form that supports all operations requiredby iMinDEE and EPIC.We have implemented CATS in the OSPREY (Gainza et al.,2013; Georgiev et al., 2008b, 2009; Ojewole et al., 2017) opensource protein design package. OSPREY has yielded many designsthat performed well experimentally—in vitro (Chen et al., 2009;Frey et al., 2010; Georgiev et al., 2012; Gorczynski et al., 2007;Roberts et al., 2012; Rudicell et al., 2014; Stevens et al., 2006) andin vivo (Frey et al., 2010; Gorczynski et al., 2007; Roberts et al.,2012; Rudicell et al., 2014) as well as in non-human primates(Rudicell et al., 2014)—and contains a wide array of flexibility modeling options and provably accurate design algorithms (Gainzaet al., 2013; Georgiev et al., 2009). These features will allow CATSto be used for many different types of designs.By presenting CATS, this paper makes the followingcontributions:1. A new, continuous parameterization of backbone conformational space that includes all degrees of freedom that respectthe backbone’s natural geometric constraints.2. An efﬁcient algorithm, CATS, for using this parameterization inprotein design.3. An implementation of CATS in our laboratory’s open-sourceOSPREY protein-design software package (Chen et al., 2009;Frey et al., 2010; Georgiev et al., 2008b, 2009; Gainza et al.,2013), conﬁgured for use with any of the protein design algorithms in OSPREY (Georgiev et al., 2008b; Gainza et al., 2012;Hallen et al., 2013, 2015, 2016; Hallen and Donald, 2016;Lilien et al., 2005; Roberts and Donald, 2015), available fordownload upon publication as free software.4. Experimental results of computational design calculations thatdemonstrate CATS ﬁnds sequences and conformations that aresigniﬁcantly lower in energy than previous algorithms, across 81test cases using 29 different crystal structures, including an antibody mutant that resisted modeling by previous algorithms. Inthe antibody study, CATS models a loop backbone motion thatis sterically crucial to the binding activity of a mutant that improves both gp120 binding and HIV-1 neutralization.2 Materials and methods2.1 Protein design with continuous flexibilityin closed loops2.1.1 FrameworkCATS builds on previous protein design algorithms that model continuous flexibility: iMinDEE (Gainza et al., 2012) and its variantsDEEPer (Hallen et al., 2013) and EPIC (Hallen et al., 2015). In thissection, we will review some aspects of the mathematical frameworkunderlying these algorithms, which will also serve as the foundationfor CATS.We assume that the conformation of the protein is a function ofthe sequence and n internal coordinates x ¼ fxi j i 2 f1; . . . ; ngg. Wethen define the conformational space of our system as the union ofvoxels (Georgiev et al., 2008b; Gainza et al., 2012; Hallen et al.,2013). Each voxel v is defined by a protein sequence and the inequality constraintsai ðvÞxibi ðvÞ;(1)for i 2 f1; . . . ; ng, where ai ðvÞ and bi ðvÞ are voxel-specific constantsdefined per our modeling assumptions. If ai ðvÞ < bi ðvÞ, coordinatexi is said to have continuous flexibility in v.The conformation of each residue j will be a function of onlythat residue’s amino-acid type and a subset of the degree-of-freedomvalues xj ¼ fxi j i 2 Sj g where Sj & f1; . . . ; ng. Thus, we can construct a very large voxel space combinatorially. The conformationspace of each residue j consists of a limited number (usually <100)of ‘residue-specific’ voxels that bound only the degrees of freedomin xj . Thus, the conformation space of the entire system consists ofall possible combinations v ¼ v1 \ v2 \ . . . of residue-specific voxels,where v1 is a voxel specific to residue 1, v2 to residue 2, etc. andthus all degrees of freedom of the system are bounded in their finiteintersection v. These residue-specific voxels are called residueDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i5/3953990by gueston 07 January 2018i8M.A.Hallen and B.R.Donaldconformations (RCs) (Hallen et al., 2013). As discussed by Hallenet al. (2013), any continuous degrees of freedom can be used in thisframework, as long as we can perform efficient and accurate energyminimizations of the formmin E0 ðxÞx2v(2)where E0 : Rn ! R is the energy as a function of the conformationaldegrees of freedom. We must be able to evaluate Eq. (2) for the entire system and for subsets of it. In the former case, the voxel v willbound all the system’s degrees of freedom and E0 will be the energyof the entire system. In the latter case, v will only restrict degrees offreedom for a subset A of the residues: v will be of the form \ vj .j2ALikewise, the energy E0 will consist only of interactions among thoseresidues, and thus only will depend on the degrees of freedomfxi j i 2 [ Sj g.j2AFollowing Georgiev et al. (2008b), Gainza et al. (2012) andHallen et al. (2013), we assume local minimization to be sufficientto find the minimum within a voxel, and we perform this minimization with the cyclic coordinate descent algorithm implemented inOSPREY (Chen et al., 2009; Frey et al., 2010; Georgiev et al.,2008b, 2009; Gainza et al., 2013). We also assume the availabilityof an energy function Ec : R3m ! R that maps the coordinates of them atoms in the system to an energy. We use the implementation ofAMBER (Cornell et al., 1995; Weiner and Kollman, 1981) withEEF1 (Lazaridis and Karplus, 1999) solvation in OSPREY for thisfor purposes of this work, but the iMinDEE framework supports awide range of energy functions (Georgiev et al., 2009; Hallen et al.,2015), and adding CATS to this framework introduces no additionalrestrictions on the energy function. Having chosen Ec, we defineE0 ðxÞ ¼ Ec ðaðxÞÞ, where a : Rn ! R3m maps internal coordinates toall-atom coordinates.As discussed by Hallen et al. (2013), the iMinDEE framework isactually agnostic to the geometric meaning of the degrees of freedomx, as long as (i) each voxel is defined by box constraints, of the formin Eq. (1), and (ii) we know how to compute the kinematic mapaðxÞ. The reason iMinDEE and its previously described variantshave limited or no backbone flexibility is that holonomic constraintson the backbone dihedrals / and w which restrict backbone motionto a specified region of protein backbone—e.g. a flexible loop region—are not box constraints. Our contribution in this paper is aparameterization of backbone conformational space that is equivalent to varying / and w subject to these holonomic constraints, butsatisfies the conditions (i) and (ii) above.2.1.2 Open and closed loopsFor internal coordinates that are sidechain dihedrals, the kinematicmap a is well known: the sidechains are just rotated to the correctangles. This is because there is no restriction on the termini of thesidechains. Likewise, defining the voxel in sidechain dihedral spaceis fairly straightforward: we assume as in Georgiev et al. (2008b),Gainza et al. (2012) and Hallen et al. (2013) that each voxel corresponds to the assignment of a sidechain rotamer (Janin et al., 1978;Lovell et al., 2000) to each residue, and each dihedral is allowed to vary by 69 about the ideal dihedral for the rotamer, which is empirically derived from a database of high-resolution crystal structures (Lovell et al., 2000). Using sidechain dihedrals as continuousdegrees of freedom allows sidechain motions in all directions thatkeep the bond lengths and angles and backbone conformation fixed.However, as mentioned in Section 1, backbone conformationalchanges associated with mutations or binding are generally fairlylocal—and indeed, complex, non-local changes are likely outside thescope of what protein design algorithms can accurately predict. Thiseffectively imposes holonomic equality constraints: we vary / and wsubject to the constraint that the (user-designated) flexible section ofbackbone matches the starting structure at both ends of the flexiblesection. Such equality constraints are incompatible with theiMinDEE framework (Gainza et al., 2012; Hallen et al., 2013). Toresolve this incompatibility, we reparameterize the backbone conformational space. Moving our new backbone degrees of freedomwill allow backbone motions in all directions that do not change thebond lengths, angles and x dihedrals, while keeping the non-flexibleparts of the backbone fixed.We will now describe the assumptions about peptide planegeometry underlying CATS (Section 2.2). We will then use these assumptions to define the new degrees of freedom x and explain howall-atom coordinates aðxÞ are computed from them (Section 2.3).2.2 Peptide-plane geometry assumptionsThe starting point for CATS is a set of assumptions about whichbackbone degrees of freedom are free to move and which are not.We will assume (iii) that peptide planes are rigid bodies, and (iv)that the N-Ca-C0 bond angle in each residue is fixed. We encodethese assumptions as equality constraints in the formcðan ðxÞÞ ¼ c0 ;(3)where an ðxÞ denotes the nitrogen and alpha-carbon coordinates ofthe flexible residues, the elements of c are quantities constrained byour geometry assumptions (iii–iv), and the corresponding elementsof c0 are the values of those quantities in the starting crystal structure. There are four constrained quantities per residue, and eachcomponent of c is a multivariate quadratic function. A detailed description of these constraints and a justification of the assumptionsare provided in Supplementary Material (SM) 1. The coordinates ofall backbone atoms besides the nitrogens and alpha carbons can becomputed from an ðxÞ and the assumption that peptide planes arerigid bodies, as described in SM 1 as well. Once the backbone conformation is determined, the sidechains and alpha hydrogens areplaced onto the backbone as in Hallen et al. (2013). These observations greatly simplify the calculation of aðxÞ from our backbone degrees of freedom x: we need only calculate an ðxÞ, and then the othercomponents of aðxÞ can be computed from an ðxÞ.2.3 New backbone parameterizationTo define a voxel in backbone conformational space, we will choosea central conformation and allow backbone motions away from thisconformation in all directions that maintain the peptide plane geometry (Fig. 1). For a flexible backbone segment of k contiguous residues with k ! 3, this space of motions has 2k À 6 dimensions: 2kfor the / and w dihedrals of each residue, and 6 constraints to ensurethat the residue at the end of the segment is continuous with thenon-flexible residues after it (since the position and orientation of arigid body each have 3 degrees of freedom). In the computationalexperiments described in this work, the central conformation foreach voxel will be the crystal structure conformation, since we knowit to be favorable and expect that local backbone adjustmentsaround it can be scored energetically more accurately than arbitrarybackbone motions can. However, in principle other central conformations could be used, to cover as much of backbone conformational space as desired (albeit at increased computational cost,which could scale up to linearly in the number of voxels in backboneconformational space).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i5/3953990by gueston 07 January 2018CATS for protein design backbone flexibilityi9Let y be the vector of nitrogen and alpha-carbon coordinates forthe k flexible residues. Let y0 be the value of y at the central conformation. Consider a vector function f : R6k ! R6k such that thefirst 4k þ 6 components of fðyÞ are the constrained quantities cðyÞ(see Section 2.2), and the remaining 2k À 6 components are affinefunctions of y, which we will call zðyÞ. In other words, f ¼ fc; zg.The components z parameterize the (2k À 6)-dimensional hypersurface of constraint-satisfying backbone conformations, and arechosen to be affine for simplicity. As long as rf is nonsingular, anydirection of motion b of the nitrogen and alpha-carbon atoms thatkeeps the constrained quantities c constant corresponds to a direction of motion rf Á b of the affine components. To put this moreformally,Theorem 1.Let Db denote the directional derivative in directionb. If zðyÞ ¼ Mz y þ vz is an affine function and c satisfiesjrðcðy0 ÞT Þ MT j 6¼ 0, then there exists an affine bijection betweenzZ ¼ fxb 2 R2kÀ6 j xb 6¼ 0g and B ¼ fb 2 R6k j b 6¼ 0; Db cðy0 Þ ¼ 0g.A proof of Theorem 1 is provided in SM 3.Thus we can use the affine components z as our continuousbackbone degrees of freedom. We will choose the constant terms ofthe affine functions so that zðy0 Þ ¼ 0. We can choose the linear coefficients defining z somewhat arbitrarily as long as rf is nonsingular;we will choose the (constant) gradient of each component of z tohave norm 1 and to be orthogonal to all other gradients of components of f (evaluated at y0 in the case of the constrained componentsc, which have non-constant gradient). In other words, we letzðyÞ ¼ Mz ðy À y0 Þ(4)where Mz is a ð2k À 6Þ Â ð6kÞ matrix whose rows are orthonormal,and also are orthogonal to the rows of the ð4k þ 6Þ Â ð6kÞ matrixrcðy0 Þ. In this sense the components zðyÞ resemble ‘normal modes’of backbone flexibility (Bahar and Rader, 2005) in the vicinity ofthe central conformation (though whether they are actual normalmodes depends on the energy landscape; our definition of z is intended to be agnostic to the energy function). They are also analogous to the user-controllable degrees of freedom in computergraphics systems that allow image manipulation while maintainingsatisfaction of a set of constraints (Gleicher, 1992; Ngo et al., 2000;Ngo and Donald, 1999).Now, let xb denote the vector of backbone degrees of freedom.To evaluate aðxb Þ, as is required by the iMinDEE framework, wemust evaluate the inverse mapping of f at the correct constrainedvalues: aðxb Þ ¼ f À1 ðfc0 ; xb gÞ. We compute this inverse functionefficiently in the form of a Taylor series, whose coefficients we canderive analytically because we can compute all derivatives of f. TheTaylor series is valid within a certain neighborhood around the central conformation y0 , and we verify its accuracy within that neighborhood by sampling. In the case where there are multiple possiblevalues of a given values of xb , we are interested in the branchdefined by the Taylor series. This way, a is a well-defined functionmapping values of our new backbone degrees of freedom xb toconstraint-satisfying atomic Cartesian coordinates (Fig. 1). A summary of the algorithm for computing a is given in SM 2 and detailsof the Taylor series computation are given in SM 5.Thus, we can use these xb as a set of continuous degrees of freedomto parameterize our backbone conformational space for use in theiMinDEE framework. Finally, we can impose bounds on xb to define avoxel, allowing motion away from the central conformation in any direction that satisfies the peptide-plane geometry constraints (Eq. 3).3 Results3.1 Energy differences and backbone shifts80 test cases using 28 different crystal structures showed CATS canmake a big difference in protein energetics (Fig. 2). Three types oftest cases were used: (a) design cases searching a large sequencespace, (b) conformational searches for the wild-type sequence and(c) single-voxel minimizations starting from the wild-type backboneand sidechain conformations. In each case, CATS was compared torigid-backbone design and to DEEPer backbone flexibility (Hallenet al., 2013). The iMinDEE (Gainza et al., 2012) and EPIC (Hallenet al., 2015) search algorithms were used throughout, which haveguarantees of accuracy, thus ensuring that energy improvements between the different models of conformational space are actually dueto changes in the backbone flexibility model and not to error in thesearch algorithm. The five to nine flexible residues in each test casewere chosen to be a contiguous segment of protein backbone.In 87% of designs, 86% of wild-type conformational searches,and 54% of minimizations, the minimum-energy conformationfound using CATS was lower than the minimum rigid-backbone energy by at least the thermal energy at room temperature (0.592 kcal/mol, calculated as the universal gas constant times a room temperature of 298 K). This is a rough measure for functional significance(Hallen et al., 2013). Indeed, in 73% of designs the gap between theCATS and DEEPer minima exceeded this thermal energy. The gapbetween DEEPer and rigid-backbone minima in designs exceededthermal energy in 67% of designs, closely matching the result inFig. 2. Seventy-nine computational experiments comparing CATS, DEEPer and rigid-backbone design. (A) Average improvement in energy (kcal/mol) in CATS(red) and DEEPer (blue) calculations compared to rigid-backbone calculations. Averages with standard error bars shown for designs, wild-type (WT) conform˚ational searches, and single-voxel minimizations starting from the wild-type conformation. (B) RMSD (A) between crystal-structure backbones and optimal backbones computed by CATS (red) and DEEPer (blue) for the same test cases as (A). CATS is able to model larger backbone changes, and the greater RMSD fordesigns compared to minimizations indicates CATS is modeling the backbone shifts induced by mutationsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i5/3953990by gueston 07 January 2018i10M.A.Hallen and B.R.DonaldFig. 3. The CATS conformational space for a mutant of the antibody VRC07 includes non-clashing conformations inaccessible to rigid-backbone design. The backbone was either held rigid (A) or allowed DEEPer (Hallen et al., 2013) (B) or CATS (C) ﬂexibility for ﬁve residues. (A–C) Steric clashes between atoms indicated inpink. (D) The three designs overlaid (rigid backbone in magenta, DEEPer in cyan, CATS in green). (E) Broader view: 15 residues (green, yellow, pink) were allowed continuous sidechain ﬂexibility, of which ten were restrained in an ð18 Þn -continuous rotamer voxel centered on the original rotamer (n ¼ number of sidechain dihedrals); the segment with backbone ﬂexibility is shown in yellow, and Trp 54 in pink. Designs were run starting from PDB id 4OLX (Rudicell et al., 2014)Hallen et al. (2013). On average, designs had 3.5 kcal/mol betterenergies with CATS than without backbone flexibility (Fig. 2A).Moreover, designs with CATS often differed in optimal sequencefrom the corresponding rigid-backbone designs, with CATS favoringlarger amino acids in all but one case. Some of these amino acidswere dramatically larger: for example, tryptophan replaced methionine 31 in a redesign of high-potential iron-sulfur protein (PDB id3A38). This reflects CATS’ ability to find space in a protein forlarger amino acids that would be sterically infeasible with the original backbone conformation. Thus, CATS greatly improves themodeling of major sequence changes.Ironically, the design with the largest backbone motion identifiedby CATS was in an 8-residue loop in the Dachshund regulatory protein (PDB id 1L8R (Kim et al., 2002)), which had backbone RMSD˚0.31 A RMSD and improved the energy by 17.1 kcal/mol comparedto the original backbone.As discussed in SM 5, voxel sizes were selected by starting with a˚2-A range (-1 to 1) for each CATS degree of freedom, and then scaling down this range (for all degrees of freedom at once) by a factorof 1.3 repeatedly until RMS constraint violations sank below 0.01˚˚A. Despite this strict threshold, a $1-A range for each CATS degreeof freedom was usually chosen (Supplementary Fig. S2). These voxels are thus centered at the original (crystal structure) backbone conformation, which by construction has a value of 0 for each CATSdegree of freedom. Sidechain dihedrals were allowed 9 degrees ofmotion in either direction from ideal rotameric values, as describedpreviously (Georgiev et al., 2008b; Gainza et al., 2012; Hallen et al.,2015). Conformational search over the space defined by these voxelswas performed using the EPIC algorithm (Hallen et al., 2015).Computation times for the CATS designs reported here ranged fromless than a minute to eleven days, with a median of 17.6 hours; forwild-type conformational searches the median was 7.9 hours.Further details of all the test cases described in this section areprovided in SM 6.3.2 Modeling of Trp 54 mutation in VRC07The homologous antibodies NIH45-46 and VRC07 both bind withhigh potency to the HIV surface glycoprotein gp120, and neutralize abroad range of strains of the virus. However, HIV is notorious formutating to resist the immune system, and thus modified antibodieswith increased potency and breadth are of great biomedical interest—both for passive immunization and as a guide for vaccine development. A mutation from glycine to tryptophan at position 54 ofNIH45-46 was found to increase breadth and potency significantly(Diskin et al., 2011). In a previous study, one of us (BRD) and colleagues showed that this mutation increases the breadth and potencyof VRC07 as well (Rudicell et al., 2014). Since then, the question ofwhether this mutation can be modeled in computational design hasbeen an open problem of considerable interest. Large changes in sizesof sidechains, as in this mutation, are more likely to induce backbonemotions and thus more difficult to model computationally.Indeed, modeling this mutation has presented a challenge forprevious protein design algorithms. A rigid-backbone conformationsearch (starting from a VRC07-gp120 complex structure with leucine at position 54 and PDB id 4OLX (Rudicell et al., 2014)) showsextensive clashes with two nearby backbone segments (Fig. 3A).Backrub perturbations (Davis et al., 2006) to the backbone, whichare often used to model previously unobserved backbone changes inextended conformations such at this loop (Georgiev et al., 2008a;Hallen et al., 2013; Smith and Kortemme, 2008), could not resolvethese clashes (Fig. 3B). The provably complete DEEPer algorithmwas used to search the space of backrubs, ensuring that a feasibleconformation was not missed in the search. Backbone conformational changes can also be modeled using loops transplanted fromother structures, and indeed antibody loops have been classified intoa list of canonical structures (Al-Lazikani et al., 1997). But the crucial backbone motion here is far more subtle than the shifts betweencanonical structures, and thus is best handled with a continuous approach. Although molecular dynamics techniques can search overDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i5/3953990by gueston 07 January 2018CATS for protein design backbone flexibilityall degrees of freedom in a protein, they are unsuitable for large design spaces because a separate simulation would be needed for eachsequence. Thus, modeling this sort of backbone motion in the combinatorial protein design context requires a technique for continuousand systematic search of backbone conformations that is compatiblewith combinatorial protein design algorithms.Indeed, CATS resolves this problem, as its conformational spaceincludes a conformation with favorable contacts all around the mutation. Allowing one of the backbone segments that clashes heavilywith Trp 54 in rigid-backbone search to relax by CATS resolves theclashes (Fig. 3C), causing a 16 kcal/mol improvement in energy relative to the rigid-backbone search (this is a 9 kcal/mol improvementrelative to the DEEPer search). This improvement results from a˚fairly modest backbone shift: 0.28 A backbone RMSD for the flex˚ible segment, with per-residue backbone RMSDs up to 0.46 A (Fig.3D). The backbone motion modeled by CATS reduces the backboneRMSD of the modeled structure compared to a crystal structure˚with Trp 54 (PDB id 4OLZ (Rudicell et al., 2014)), from 0.61 A to˚0.46 A, calculated using the method of Kromann and Bratholm(2013) and Kabsch (1976) for Trp 54 and the two gp120 residues itclashes with in the rigid-backbone model (Trp 427 and Gly 473).However, the RMSD change is somewhat difficult to interpret because independent crystal structures of the same protein would alsobe likely to exhibit RMSDs around this level.These results show the key role that local backbone flexibility, asmodeled by CATS, can play in identifying favorable conformationsand sequences. They also show that CATS can perform designs thatcould not be modeled using previous algorithms. In particular, theyshow that the level of backbone flexibility modeled by CATS isfunctionally significant, resulting in a qualitatively different conformational space. In particular, CATS reveals how a mutant thatrigid-backbone computations dismiss as sterically infeasible can actually bind its target well.4 ConclusionsCATS is a novel and systematic method to search substantial, continuous regions of backbone conformational space during proteindesign calculations. By moving away from fixed repertoires of motions and into comprehensive search of conformations with validbonding geometry, it moves closer to fully realistic modeling ofbackbone conformational changes.A key challenge as we move into these larger spaces is ensuringthat the energetic cost of the backbone conformational changes isestimated accurately enough by the energy function to yield usefulresults. But CATS can play an important role in addressing this challenge as well. CATS enables provably accurate algorithms, whichintroduce no new error beyond the error in the model, in contrast tostochastic, heuristic approaches that have been shown to drasticallyundersample the conformational space specified by the model(Gainza et al., 2016; Simoncini et al., 2015). As a result, CATS canbe used to validate energy functions in the highly backbone-flexibledesigns it enables, with the guarantee that error in design predictionsis due only to error in the energetic and geometric modeling and notto error in the algorithm (aside from CATS’ negligible andwell-controlled Taylor series error). In addition, because CATS isagnostic to the energy function, it will be useful for performing conformational searches with the more accurate energy functions of thefuture (Hallen et al., 2015, 2016).CATS is also easily generalizable to non-protein systems—whetherother macromolecules or small molecules. It is applicable in any context where local conformational perturbations are needed subject toi11bonding geometry constraints. One need only construct the appropriate multivariate quadratic cðyÞ to reflect these constraints.We believe these capabilities will make CATS useful in manykinds of designs.AcknowledgementsWe thank Dr Kyle Roberts for molecular structures and helpful comments onthe VRC07 system; and Dr Pablo Gainza, Hunter Nisonoff, Jonathan Jou,Adegoke Ojewole, Marcel Frenkel, Anna Lowegard, Siyu Wang and GrahamHolt for helpful comments on the manuscript.FundingThis work was supported by the Liebmann Foundation [to M.A.H.]; andNational Institutes of Health [R01-GM-78031 to B.R.D.].Conﬂict of Interest: none declared.ReferencesAl-Lazikani,B. et al. (1997) Standard conformations for the canonical structures of immunoglobulins. J. Mol. Biol., 273, 927–948.Bahar,I. and Rader,A.J. (2005) Coarse-grained normal mode analysis in structural biology. Curr. Opin. Struct. Biol., 15, 586–592.Chazelle,B. et al. (2004) A semideﬁnite programming approach to side chainpositioning with new rounding strategies. INFORMS J. Comput. Comput.Biol. Special Issue, 16, 380–392.Chen,C.-Y. et al. (2009) Computational structure-based redesign of enzymeactivity. Proc. Natl. Acad. Sci. U. S. A., 106, 3764–3769.Cornell,W.D. et al. (1995) A second generation force ﬁeld for the simulationof proteins, nucleic acids, and organic molecules. J. Am. Chem. Soc., 117,5179–5197.Das,R. and Baker,D. (2008) Macromolecular modeling with Rosetta. Annu.Rev. Biochem., 77, 363–382.Davis,I.W. et al. (2006) The backrub motion: how protein backbone shrugswhen a sidechain dances. Structure, 14, 265–274.Desjarlais,J.R. and Handel,T.M. (1995) De novo design of the hydrophobiccores of proteins. Protein Sci., 4, 2006–2018.Desmet,J. et al. (1992) The dead-end elimination theorem and its use in protein side-chain positioning. Nature, 356, 539–542.Desmet,J. et al. (2002) Fast and accurate side-chain topology and energy reﬁnement (FASTER) as a new method for protein structure optimization.Proteins Struct. Funct. Bioinf., 48, 31–43.Diskin,R. et al. (2011) Increasing the potency and breadth of an HIV antibodyby using structure-based rational design. Science, 334, 1289–1293.Donald,B.R. (2011). Algorithms in Structural Molecular Biology. MIT Press,Cambridge, MA.Floudas,C.A. et al. (1999) Global optimization approaches in protein foldingand peptide docking. In: Farach-Colton,M. (ed.) Mathematical Support forMolecular Biology, Volume 47 of DIMACS Series in Discrete Mathematicsand Theoretical Computer Science, pp. 141–172. American MathematicalSociety, Providence, RI.Frey,K.M. et al. (2010) Predicting resistance mutations using protein design algorithms. Proc. Natl. Acad. Sci. U. S. A., 107, 13707–13712.Fung,H.K. et al. (2008) Toward full-sequence de novo protein design withﬂexible templates for human b-defensin-2. Biophys. J., 94, 584–599.Gainza,P. et al. (2012) Protein design using continuous rotamers. PLoSComput. Biol., 8, e1002335.Gainza,P. et al. (2013) OSPREY: Protein design with ensembles, ﬂexibility,and provable algorithms. Methods Enzymol., 523, 87–107.Gainza,P. et al. (2016) Algorithms for protein design. Curr. Opin. Struct.Biol., 39, 16–26.Gehmlich,K. et al. (2007) Paxillin and ponsin interact in nascent costameres ofmuscle cells. J. Mol. Biol., 369, 665–682.Georgiev,I. and Donald,B.R. (2007) Dead-end elimination with backboneﬂexibility. Bioinformatics, 23, i185–i194.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i5/3953990by gueston 07 January 2018i12Georgiev,I. et al. (2008a) Algorithm for backrub motions in protein design.Bioinformatics, 24, i196–i204.Georgiev,I. et al. (2008b) The minimized dead-end elimination criterion andits application to protein redesign in a hybrid scoring and search algorithmfor computing partition functions over molecular ensembles. J. Comput.Chem., 29, 1527–1542.Georgiev,I. et al. (2009). OSPREY (Open Source Protein Redesign for You)user manual. Available online: www.cs.duke.edu/donaldlab/software.phpUpdated, 2015. 94 pages.Georgiev,I. et al. (2012) Design of epitope-speciﬁc probes for sera analysis andantibody isolation. Retrovirology, 9, P50.Georgiev,I.S. et al. (2014) Antibodies VRC01 and 10E8 neutralize HIV-1 withhigh breadth and potency even with Ig-framework regions substantially reverted to germline. J. Immunol., 192, 1100–1106.Gleicher,M. (1992). Integrating constraints and direct manipulation. In:Proceedings of the 1992 symposium on Interactive 3D graphics. ACM, pp.171–174.Gorczynski,M.J. et al. (2007) Allosteric inhibition of the protein-protein interaction between the leukemia-associated proteins Runx1 and CBFb. Chem.Biol., 14, 1186–1197.Gordon,D.B. et al. (2003) Exact rotamer optimization for protein design.J. Comput. Chem., 24, 232–243.Hallen,M.A. and Donald,B.R. (2016) COMETS (Constrained Optimizationof Multistate Energies by Tree Search): a provable and efﬁcient protein design algorithm to optimize binding afﬁnity and speciﬁcity with respect to sequence. J. Comput. Biol., 23, 311–321.Hallen,M.A. et al. (2013) Dead-end elimination with perturbations (DEEPer):A provable protein design algorithm with continuous sidechain and backbone ﬂexibility. Proteins Struct. Funct. Bioinf., 81, 18–39.Hallen,M.A. et al. (2015) Compact representation of continuous energy surfaces for more efﬁcient protein design. J. Chem. Theory Comput., 11, 2292–2306.Hallen,M.A. et al. (2016). LUTE (Local Unpruned Tuple Expansion):Accurate continuously ﬂexible protein design with general energy functionsand rigid-rotamer-like efﬁciency. In: International Conference on Researchin Computational Molecular Biology. Springer, pp. 122–136.Jacobs,T.M. et al. (2016) Design of structurally distinct proteins using strategies inspired by evolution. Science, 352, 687–690.Janin,J. et al. (1978) Conformation of amino acid side-chains in proteins.J. Mol. Biol., 125, 357–386.Jou,J.D. et al. (2016) BWM*: A novel, provable, ensemble-based dynamic programming algorithm for sparse approximations of computational proteindesign. J. Comput. Biol., 23, 413–424.Kabsch,W. (1976) A solution for the best rotation to relate two sets of vectors.Acta Crystallogr. Sect. A Cryst. Phys. Diffract. Theor. Gen. Crystallogr., 32,922–923.Karanicolas,J. and Kuhlman,B. (2009) Computational design of afﬁnity andspeciﬁcity at protein-protein interfaces. Curr. Opin. Struct. Biol., 19,458–463.Kim,S.-S. et al. (2002) Structure of the retinal determination proteinDachshund reveals a DNA binding motif. Structure, 10, 787–795.Kingsford,C.L. et al. (2005) Solving and analyzing side-chain positioningproblems using linear and integer programming. Bioinformatics, 21,1028–1039.Kromann,J.C. and Bratholm,L.A. (2013). Calculate RMSD for two XYZstructures. Available online: http://github.com/charnley/rmsd. Updated,2017.Kuhlman,B. and Baker,D. (2000) Native protein sequences are close to optimal for their structures. Proc. Natl. Acad. Sci. U. S. A., 97, 10383–10388.Lazaridis,T. and Karplus,M. (1999) Effective energy function for proteins insolution. Proteins Struct. Funct. Bioinf., 35, 133–152.Leach,A.R. and Lemon,A.P. (1998) Exploring the conformational space ofprotein side chains using dead-end elimination and the A* algorithm.Proteins Struct. Funct. Bioinf., 33, 227–239.Leaver-Fay,A. et al. (2011) A generic program for multistate protein design.PLoS One, 6, e20937.M.A.Hallen and B.R.DonaldLewis,S.M. et al. (2014) Generation of bispeciﬁc IgG antibodies bystructure-based design of an orthogonal Fab interface. Nat. Biotechnol.,32, 191–198.Lilien,R.H. et al. (2005) A novel ensemble-based scoring and search algorithmfor protein redesign and its application to modify the substrate speciﬁcity ofthe gramicidin synthetase A phenylalanine adenylation enzyme. J. Comput.Biol., 12, 740–761.Lippow,S.M. and Tidor,B. (2007) Progress in computational protein design.Curr. Opin. Biotechnol., 18, 305–311.Lovell,S.C. et al. (2000) The penultimate rotamer library. Proteins Struct.Funct. Genet., 40, 389–408.Lovell,S.C. et al. (2003) Structure validation by Ca geometry: /; w; and Cb deviation. Proteins Struct. Funct. Bioinf., 50, 437–450.Ngo,J.T. and Donald,B.R. (1999). System for image manipulation and animation using embedded constraint graphics. US Patent 5,933,150.Ngo,T. et al. (2000). Accessible animation and customizable graphics via simplicial conﬁguration modeling. In: Proceedings of the 27th annual conference on Computer graphics and interactive techniques. ACM Press/Addison-Wesley Publishing Co., pp. 403–410.Ojewole,A. et al. (2017) OSPREY predicts resistance mutations using positiveand negative computational protein design. Methods Mol. Biol., 1529,291–306.Pierce,N.A. and Winfree,E. (2002) Protein design is NP-hard. Protein Eng.,15, 779–782.Pierce,N.A. et al. (2000) Conformational splitting: A more powerful criterionfor dead-end elimination. J. Comput. Chem., 21, 999–1009.Rapaport,D.C. (2004). The Art of Molecular Dynamics Simulation, 2nd edn.Cambridge University Press, Cambridge, England.Regan,L. (1999) Protein redesign. Curr. Opin. Struct. Biol., 9, 494–499.Roberts,K.E. and Donald,B.R. (2015) Improved energy bound accuracy enhances the efﬁciency of continuous protein design. Proteins Struct. Funct.Bioinf., 83, 1151–1164.Roberts,K.E. et al. (2012) Computational design of a PDZ domain peptide inhibitor that rescues CFTR activity. PLoS Comput. Biol., 8, e1002477.Roberts,K.E. et al. (2015) Fast gap-free enumeration of conformations and sequences for protein design. Proteins Struct. Funct. Bioinf., 83, 1859–1877.Rudicell,R.S. et al. (2014) Enhanced potency of a broadly neutralizing HIV-1antibody in vitro improves protection against lentiviral infection in vivo.J. Virol., 88, 12669–12682.Simoncini,D. et al. (2015) Guaranteed discrete energy optimization on largeprotein design problems. J. Chem. Theory Comput., 11, 5980–5989.Smith,C. and Kortemme,T. (2008) Backrub-like backbone simulation recapitulates natural protein conformational variability and improves mutantside-chain prediction. J. Mol. Biol., 380, 742–756.Stevens,B.W. et al. (2006) Redesigning the PheA domain of gramicidin synthetase leads to a new understanding of the enzyme’s mechanism and selectivity. Biochemistry, 45, 15495–15504.Traore,S. et al. (2013) A new framework for computational protein design´through cost function network optimization. Bioinformatics, 29,2129–2136.Traore,S. et al. (2016) Fast search algorithms for computational protein de´sign. J. Comput. Chem., 37, 1048–1058.Tripathy,C. et al. (2012) Protein loop closure using orientational restraintsfrom NMR data. Proteins Struct. Funct. Bioinf., 80, 433–453.Wang,C. et al. (2005) Improved side-chain modeling for protein-protein docking. Protein Sci., 14, 1328–1339.Weiner,P.K. and Kollman,P.A. (1981) AMBER: Assisted model building andenergy reﬁnement. A general program for modeling molecules and theirinteractions. J. Comput. Chem., 2, 287–303.Wong,K.-B. et al. (1999) Hot-spot mutants of p53 core domain evince characteristic local structural changes. Proc. Natl. Acad. Sci. U. S. A., 96,8438–8442.Xu,J. and Berger,B. (2006) Fast and accurate algorithms for protein side-chainpacking. J. ACM, 53, 533–557.Zhou,J. and Grigoryan,G. (2015) Rapid search for tertiary fragments revealsprotein sequence–structure relationships. Protein Sci., 24, 508–524.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i5/3953990by gueston 07 January 2018
5028882004002	PMID28882004	5028882004	https://watermark.silverchair.com/btx272.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28882004.main.pdf	Bioinformatics, 33, 2017, i389–i398doi: 10.1093/bioinformatics/btx272ISMB/ECCB 2017When loss-of-function is loss of function:assessing mutational signatures and impact ofloss-of-function genetic variantsKymberleigh A. Pagel1, Vikas Pejaver1,†, Guan Ning Lin2,‡,Hyun-Jun Nam2, Matthew Mort3, David N. Cooper3, Jonathan Sebat2,4,Lilia M. Iakoucheva2, Sean D. Mooney5 and Predrag Radivojac1,*1Department of Computer Science and Informatics, Indiana University, Bloomington, IN, USA, 2Department ofPsychiatry, University of California San Diego, La Jolla, CA, USA, 3Institute of Medical Genetics, Cardiff University,Cardiff, UK, 4Beyster Center for Psychiatric Genomics, Department of Psychiatry, University of California SanDiego, La Jolla, CA, USA and 5Department of Biomedical Informatics and Medical Education, University ofWashington, Seattle, WA, USA*To whom correspondence should be addressed.†Present address: School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200030, P.R. China‡Present address: Shanghai Key Laboratory of Psychotic Disorders, School of Biomedical Engineering, ShanghaiJiaotong University, Shanghai 200030, P. R. ChinaAbstractMotivation: Loss-of-function genetic variants are frequently associated with severe clinical phenotypes, yet many are present in the genomes of healthy individuals. The available methods to assess the impact of these variants rely primarily upon evolutionary conservation with little to noconsideration of the structural and functional implications for the protein. They further do not provide information to the user regarding speciﬁc molecular alterations potentially causative ofdisease.Results: To address this, we investigate protein features underlying loss-of-function genetic variation and develop a machine learning method, MutPred-LOF, for the discrimination of pathogenicand tolerated variants that can also generate hypotheses on speciﬁc molecular events disrupted bythe variant. We investigate a large set of human variants derived from the Human Gene MutationDatabase, ClinVar and the Exome Aggregation Consortium. Our prediction method shows an areaunder the Receiver Operating Characteristic curve of 0.85 for all loss-of-function variants and 0.75for proteins in which both pathogenic and neutral variants have been observed. We appliedMutPred-LOF to a set of 1142 de novo vari3ants from neurodevelopmental disorders and ﬁnd enrichment of pathogenic variants in affected individuals. Overall, our results highlight the potentialof computational tools to elucidate causal mechanisms underlying loss of protein function in lossof-function variants.Availability and Implementation: http://mutpred.mutdb.orgContact: predrag@indiana.edu1 IntroductionGenetic data-driven approaches to human health have resulted inthe implication of loss-of-function (LOF) variants in phenotypesranging from complex neuropsychiatric diseases to Mendelian bloodgroups (Stenson et al., 2014). Loss-of-function variants includeframeshifting and stop variants and are of particular interest becauseof their potentially profound impact on the mRNA transcript andtranslated protein. Frameshifting variants are insertions and deletions of nucleotides (indels) not divisible by three, causing a changein the mRNA coding frame. Stop variants, on the other hand, entailthe gain or loss of stop codons in mRNA; stop-gain or nonsensevariants introduce a premature termination codon that truncates theCV The Author 2017. Published by Oxford University Press.i389This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018i390protein, whereas stop-loss or nonstop variants alter the terminationcodon and lead to elongated proteins.Altered transcripts resulting from LOF variants are eitherdegraded at the mRNA or protein levels or rendered non-functional,often leading to disease phenotypes. For example, disease-causingstop variants are significantly enriched for alterations which activatenonsense mediated decay, resulting in haploinsufficiency (Mortet al., 2008). However, LOF variants can also be functionally andphenotypically neutral; in fact, each human genome may containhundreds of frameshifting indels and dozens of stop variants withlittle or no observable impact upon phenotype (Kircher et al., 2014;MacArthur and Tyler-Smith, 2010; MacArthur et al., 2012; Sulemet al., 2015; Thousand Genomes Project Consortium, 2010).Robustness in the genome through gene duplication (Hsiao andVitkup, 2008) and compensatory mechanisms (Hu and Ng, 2012)can result in the toleration of many LOF variants. Furthermore,gene loss in regions under relaxed selection, including olfactory andtaste receptor genes, is typically tolerated (Risso et al., 2014).Recently, there has been a growing interest in the phenotypicand clinical roles of de novo LOF variants. For example, an estimated one out of every hundred de novo LOF mutations contributesto autism spectrum disorders (Ronemus et al., 2014). Data from theExome Aggregation Consortium (ExAC) found that the exomes of60 706 individuals contain nearly sixty thousand non-singleton nonsense variants while 3230 genes exhibit near-complete depletion(Lek et al., 2016); therefore, a major challenge remains in understanding the nature and quantifying the impact of LOF variants in agiven genome. The lack of annotation for previously unseen geneticvariants in many such cases further highlights the need for sophisticated prediction models specifically designed for LOF variants.Unlike single nucleotide variants (Cline and Karchin, 2011), LOFvariants are not as well-studied. Early approaches used in the work ofZia and Moses (2011), SIFT Indel (Hu and Ng, 2012) and NutVar(Rausell et al., 2014) have primarily utilized conservation features.However, these features are limited in distinguishing between LOFvariants of different classes in the same protein. Moreover, thesemethods restrict their training sets to core proteins that have highquality annotations, thereby reducing their utility on less well-studiedgenes. To circumvent this issue, CADD (Kircher et al., 2014), amethod designed to predict the impact of all classes of genetic variation, was trained on simulated de novo and ancestral mutations.DDIG-in further supplemented conservation-based features with intrinsic disorder predictions from the region affected by stop gain andframeshifting variants (Folkman et al., 2015), whereas VEST-Indelevaluates frameshifting indels via a random forest model, yet restrictsfunctional and structural features to stability, solvent accessibility andthe temperature factor (Douville et al., 2016). However, there are numerous other structural and functional properties of proteins thatcould potentially be impacted by LOF variants. The incorporation ofsuch information alongside features indicative of functional redundancy of the protein into more complex predictive models is expectedto not only increase discriminative power but also suggest the specificnature of functional impact in a principled manner.To address these challenges, we study the evolutionary, structural and functional signatures of loss-of-function genetic variants.We subsequently develop machine learning methods for the discrimination of pathogenic from tolerated frameshifting and stop gainvariants and hypothesizing specific molecular alterations. Our results provide evidence that there is significant potential to analyzethe causal mechanisms for loss of function in these classes of variants and point out potential difficulties in developing computationaltools for automated prioritization of loss-of-function variants.K.A.Pagel et al.2 Materials and methods2.1 Training data setsPathogenic (disease-causing) stop gain and frameshifting variantswere obtained from the July 2016 version of the Human GeneMutation Database (HGMD) (Stenson et al., 2014) and ClinVar(Landrum et al., 2016). The set of putatively neutral variants wascomposed of frameshifting and stop gain variants from ExAC whichwere annotated to have passed all quality filters. To remove potential biases, we did not perform filtering based upon population frequency and only retained canonical isoforms in subsequent analyses.Table 1 summarizes all data sets.2.2 Neurodevelopmental disorders datasetWe applied MutPred-LOF to a data set, curated from the publishedliterature, consisting of 970 de novo LOF variants identified throughwhole-exome or whole-genome sequencing of individuals diagnosedwith six neurodevelopmental disorders, including autism spectrumdisorder (ASD), schizophrenia, intellectual disability, bipolar disorder, developmental delay and epileptic encephalopathy (de Ligtet al., 2012; De Rubeis et al., 2014; EuroEPINOMICS-RESConsortium, Epilepsy Phenome/Genome Project and Epi4KConsortium, 2014; Epi4K Consortium and Epilepsy Phenome/Genome Project, 2013; Fromer et al., 2014; Gilissen et al., 2014;Girard et al., 2011; Guipponi et al., 2014; Gulsuner et al., 2013;Hashimoto et al., 2016; Iossifov et al., 2012, 2014; Jiang et al.,2013; Kong et al., 2012; McCarthy et al., 2014; Neale et al., 2012;O’Roak et al., 2011, 2012a, b; Rauch et al., 2012; Sanders et al.,2012; Turner et al., 2016; Xu et al., 2011; Xu et al., 2012; Yuenet al., 2015, 2016; S.Jonathan et al., unpublished data, doi: https://doi.org/10.1101/102327) and a control set of 172 de novo LOF mutations from healthy siblings (Gulsuner et al., 2013; Iossifov et al.,2012, 2014; O’Roak et al., 2011, 2012b; Rauch et al., 2012;Sanders et al., 2012; Xu et al., 2011, 2012; S.Jonathan et al., unpublished data, doi: https://doi.org/10.1101/102327).2.3 Feature engineeringWe created features for the description of each variant based uponthe properties of wildtype protein sequence, with features dividedinto those representing portions of the protein sequence affected andunaffected by the alteration. Amino acids occurring prior to the variant were considered to constitute the unaffected portion of the protein and are referred to in the text as the amino side (Fig. 1).Residues that occur after the variant are likewise denoted as thecarboxyl side, either wildtype or mutant. The amino acid sequencesof the carboxyl side in mutant variants were not reconstructed fromthe genomes for use in structural and functional property features;incorporation of features based upon predicted mutant amino acidsequence did not lead to improved performance and the exclusion ofthese features greatly simplifies predictor development and itsTable 1. Number of variants (proteins) present in each data setDiseaseFrameshiftStop gainTotalNeutral18 116 (1545)14 318 (1681)32 434 (1995)90 135 (13 427)7960 (4990)98 095 (13 605)Total108 251 (13 713)22 278 (6137)The set of canonical sequences was derived from UniProt (Suzek et al.,2007). The number of available stop-loss variants was too small to beincluded in this work.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018When loss-of-function is loss of functionapplication because the variants could be scored based on the wildtype sequence only.The feature space covers three classes: general sequence features,evolutionary features and predicted structural and functional features. Sequence-based features include the relative position of thevariant, the number of amino acids affected by the variant, binarynonsense-mediated decay prediction based upon the 50 nucleotiderule (variant is more than 50 nucleotides upstream of the final exon–exon junction (Maquat, 2004), number of amino acids from thevariant to the final exon–exon junction, and a two-element binaryvector indicating the type of LOF variant (stop gain or frameshiftinginsertion/deletion).2.3.1 Evolutionary featuresEvolutionary features involved general sequence conservationindexes for amino/carboxyl sides of the protein as well as counts ofclose homologs of the wildtype protein in the human and mousegenomes. Conservation features for the wildtype sequence were extracted from two sources. First, we generated a position-specificscoring matrix (PSSM) by running PSI-BLAST against the "nr" database (Altschul et al., 1997). Second, we used AL2CO (Pei andGrishin, 2001) to derive nine conservation indexes from the UCSCGenome Browser 46-species alignment (Karolchik et al., 2014) foreach position in the sequence. Both normalized and unnormalizedversions of these scores were calculated for the whole alignment andtwo sub-alignments (mammals only and primates only). To capturethe relative conservation of affected and unaffected regions of theprotein, we took the maximum conservation over the amino andcarboxyl sides of the protein as well as the difference betweenthese regions. This resulted in 3 Â 42 ¼ 126 PSSM features and3 Â 2 Â 3 Â 9 ¼ 162 AL2CO conservation features.2.3.2 Structural and functional property featuresThe structural and functional features included both gene-basedand residue-based features. The gene-based features contained2132-dimensional vectors of predicted Gene Ontology terms usingFANN-GO (Clark and Radivojac, 2011), where each variant inthe same protein received the same set of features. The predictedfeatures were used in order to mitigate biases that could arise dueto the fact that known disease genes are generally better studiedand contain more functional information than the remaininggenes.We extended the gene-based feature space via the prediction of avariety of residue-level structural and functional properties in thewildtype protein that convert its amino acid sequence into a series ofreal-valued vectors of the same length. We then used the methodology behind vector quantization kernels (Clark and Radivojac,2014) to encode these property vectors into a fixed-length featurerepresentation. Vector quantization features address an importantchallenge in encoding LOF variants as they facilitate encoding ofAmino side wildCarboxyl side wildWildtypevariantMutanti391variable-length amino acid sequences into a fixed-length representation, beyond simple summary statistics. At the same time, they allowfor more effective use of external biological data to be incorporatedinto method development via a series of prediction models previously constructed for given structural and functional properties.Specifically, the vector quantization procedure involved a datapreprocessing step and a feature construction step. In the data preprocessing step, we first defined the universe of human protein sequences S ¼ fs1 ; s2 ; s3 ; . . .g as the UniRef50 database (Suzek et al.,2007), where each s 2 S is a string composed of amino acids fromA ¼ fA; C; . . . ; Yg. For each s 2 S, we mapped the protein sequenceinto a real-valued property vector p ¼ ðp1 ; p2 ; . . . ; pl Þ, where l islength of the string s, using any particular feature mapping describedin Table 2. For example, predicting the helical propensity for a givensequence s results in one such vector p of numbers between 0 and 1.Next, we decomposed the property vector into n-dimensional overlapping subvectors p½1;n  ; p½2;nþ1  ; . . . ; p½lÀnþ1;l  , where p½1;n  representsthe first n elements of p. The process was repeated for each proteinsequence to create a large database of n-dimensional fragments for aparticular property. This database of fragments was then clusteredusing the K-means algorithm into m groups to generate a partitionto m regions R ¼ fR1 ; R2 ; . . . ; Rm g, represented by centroidsC ¼ fc1 ; c2 ; . . . ; cm g. Each centroid is associated with a Voronoi region, Ri, such thatRi ¼ fx : dðx; ci Þdðx; cj Þ; i 6¼ jg;(1)where d(x, c) is the Euclidean distance between fragment x and centroid c. One such clustering was created for each property and a set ofm centroids per property was stored. In the feature construction step,a particular property was first predicted for the wildtype sequence anda set of length-n segments within the amino and carboxyl regions relative to the variant position were extracted. A set of features was thencreated for each side by counting the segments closest to each of theUniRef50-derived centroids; i.e., the count at position i correspondsto the number of fragments closest to the i-th centroid. Two vectors ofcounts, one for the amino and one for the carboxyl side, were createdfor each property. Based on previous work, and with minimal experimentation, we chose n ¼ 16 and m ¼ 16 (Clark and Radivojac, 2014).This vector quantization framework was utilized on both the carboxyland amino side sequence fragments for the 57 structural and functional features to derive a total of 57 Â 16 Â 2 ¼ 1824 features.Overall, the data set contained 4 270 features.2.4 Predictor developmentAn ensemble of one hundred bagged two-layer feed-forward neuralnetworks was trained for all loss-of-function variants collectivelyutilizing the Matlab Neural Network Toolbox. The number of hidden units in each network was fixed at 10. To eliminate featuresvery likely to be uninformative, we applied a two-sample t-test witha high P-value threshold of 0.5. Furthermore, we applied principalcomponent analysis with 99% retained variance on z-score normalized data to reduce dimensionality and eliminate (near-)colinear features. The resilient propagation method was used for training with25% of the training data used as validation (Riedmiller and Braun,1993). All parameters were set prior to training and were not varied.Finally, all models were trained on balanced training sets, where themajority class was subsampled.Carboxyl side mutantFig. 1. Illustration of the impacted portions of the protein for loss-of-functionvariants. The impacted region can be shorter or longer for the mutant protein(if translated); its length is zero for the stop gain variants2.5 Predictor evaluationPerformance of MutPred-LOF is represented by the area under theROC curve (AUC) derived from scores generated in 10-fold per-Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018i392K.A.Pagel et al.Table 2. Predicted structural and functional featuresProperty categoryPredicted featuresStructure anddynamicsSignal peptide andtransmembraneregions*Enzyme activity*Regulation*MacromolecularbindingMetal-binding*Post-translationalmodiﬁcation(PTM) (Pejaveret al., 2014)MotifsThree classes*—Helix, strand, loop; Intrinsic disorder (Peng et al., 2006); B-factor (Radivojac et al., 2004); Relative solventaccessibility*; Coiled-coil region*Seven classes—N- and C-termini of signal peptide, signal helix, signal peptide cleavage site, transmembrane segment, cytoplasmic and non-cytoplasmic loopsCatalytic residuesAllosteric residuesDNA*; RNA*; Protein-protein interaction (PPI)*; PPI hotspots*; Molecular Recognition Features (MoRFs)*; Calmodulinbinding (Radivojac et al., 2006)Cd; Ca; Co; Cu; Fe; Mg; Mn; Ni; K; Na; ZnAcetylation, ADP-ribosylation, Amidation, Carboxylation, Disulﬁde linkage, Farnesylation, Geranylgeranylation,Glycosylation (C-linked, N-linked and O-linked), GPI anchor amidation, Hydroxylation, Methylation, Myristoylation,N-terminal acetylation, Palmitoylation, Phosphorylation, Proteolytic cleavage, Pyrrolidone carboxylic acid, Sulfation,SUMOylation, UbiquitylationFrom PROSITE (Sigrist et al., 2013) and ELM (Dinkel et al., 2014)*Indicates in-house predictors.protein cross-validation. All pre-processing steps (normalization,dimensionality reduction) were carried out on the training partitiononly and applied on the test partition. To ensure that the model didnot suffer from over-reliance on protein-based features, we performedper-protein cross-validation such that for each fold all variants in aprotein were either entirely in the training or test set. We illustratethe influence of gene-based features on estimated performanceof MutPred-LOF using three types of cross-validation protocols:(1) per-variant, (2) per-protein and (3) per-cluster. Per-variant crossvalidation considers all variants as independent data points and partitions variants into ten-folds without consideration of the proteinsfrom which the set is derived. In contrast, per-protein cross-validationensures in each fold that all variants from the same protein are eitherin the training or in the test partition. This evaluation protocol ensuresbetter performance estimation when the model is presented with avariant in protein that was not in the training set or that containedonly one type of variant (e.g., disease-associated). In per-cluster crossvalidation, variants within proteins with at least 50% sequence identity were included in either training or test sets, and can be used toestimate performance when the model is presented with a protein forwhich no protein within 50% sequence identity was available in thetraining set. We also used this evaluation protocol to assess the overfitting potential of the per-protein performance assessment.Next, we compared the performance of MutPred-LOF againstthree currently available methods, each of which was evaluated usinga set of frameshifting and stop gain variants from the MutPred-LOFtraining set. Deleterious variants from HGMD professional version2016 that were not present in HGMD 2015 were extracted to represent a set of deleterious variants that are unlikely to be included in thetraining data of these methods (2,409 variants from 745 genes). Theneutral test set consisted of variants from the ExAC training data(43,534 variants from 11,098 genes). The test set was filtered to remove the training data from DDIG-in and Vest-Indel; this procedure,however, could not be replicated for the CADD training data.Further, we created a for-comparison-only version of MutPred-LOF,MutPred-LOF0 , with these test set variants removed from the trainingdata, to ensure that for every model in this comparison the test variants were not included in the training set.2.6 Assessing significance of functional alterationsTo identify loss-of-function variants with significant functional impact, we defined a P-value for each feature listed in Table 2 to assignsignificance and rank prospective mechanisms. In the developmentof MutPred (Li et al., 2009), a method that predicts the impact ofsingle amino acid substitutions, we constructed empirical P-valuesas follows. The null distribution was first defined using the scores offunctional disruption for all variants present in the set of putativelyneutral substitutions. Then, given a score of functional disruptionfor a new variant, the P-value is determined as the fraction of neutralsubstitutions with disruption scores at least as high as the observedvalue. This method relies on two strong assumptions: (1) that eachfunctional mechanism is equally disrupted in the set of neutral substitutions, and (2) that each functional mechanism is equally likely.In this work, we rank functional disruption scores by modifyingMutPred’s approach so as to mitigate the latter assumption.Specifically, we rank the molecular mechanisms by adjusting theP-values asP0 ¼ ð1 À aÞ Á P;(2)where P is the P-value assigned in a way identical to MutPred’s approach and a is the frequency of a particular functional mechanism.We refer to this quantity as prior-corrected P-value. The rationalefor this adjustment comes from the definition of the false discoveryrate (FDR); i.e.,FDR ¼ð1 À aÞ Á FPR;a Á TPR þ ð1 À aÞ Á FPR(3)where TPR is the true positive rate and FPR is the false positive rate.Here, we use the P-value as an approximation of the false positiverate and ignore the denominator. Ideally, molecular alterationswould be prioritized based on the posterior probability that a particular mechanism (e.g., DNA binding, or protein binding) is disrupted. However, this step either requires a data set of disrupted andnon-disrupted mechanisms that can be used to estimate true positiveand false positive rates or further assumptions on how to probabilistically reason on the disruption of structural/functional propensityfor entire protein regions based on such propensities on a singleresidue basis.The scoring function that was used to determine the empiricalnull distribution and assign P-values was the number of residueswith high structural and functional propensities. The thresholds forthese high propensity scores were determined separately for each individual model described in Table 2 during the training phase (lowfalse positive rates; here, 10%). On the other hand, the priorDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018When loss-of-function is loss of functionprobabilities that a particular residue has a specific property wereestimated using the AlphaMax algorithm (Jain et al., 2016).i393Amino side3PathogenicNeutral22.7 MutPred-LOF outputFor every mutation input into MutPred-LOF, the model returns ascore between zero and one, where variants with higher scores aremore likely to be pathogenic. In addition, MutPred-LOF returns upto five structural and functional mechanisms that are impacted inthe affected region of the protein, and have significant priorcorrected P-values (less than 0.05). For the purposes of classification, we provide three score thresholds to aid users in discriminatingbetween pathogenic and neutral variation, at different levels of falsepositive rate (FPR): 0.40 (10% FPR), 0.50 (5% FPR, recommended), 0.70 (1% FPR).3 Results3.1 Evolutionary conservation of loss-of-functionvariantsConservation has consistently been shown to be a key signature ofpathogenic variants (Ng and Henikoff, 2003). Unsurprisingly, thesefeatures were also shown to be informative across all variant typesin our data. To highlight the differences in conservation betweenpathogenic and neutral loss-of-function variants, Figure 2 shows arepresentative plot created from one of the conservation metrics.The plots show the approximate probability density function ofaverage unnormalized unweighted entropy in the vertebrate alignment for amino and carboxyl sides of the wildtype sequences. Whilethe affected region in disease-associated variants is more conservedthan that of the neutral variants, the similarity between distributionsof conservation between amino and carboxyl sides indicates that theconservation of the protein as a whole plays a dominant role.Therefore, for such a feature to be sufficiently effective in discriminating between pathogenic and neutral variants, the training setwould need to contain both types of variants for most proteins,which is currently not the case.An alternative metric to analyze sequence conservation betweenvariation is the number of closely related proteins. Functional redundancy in the genome, as measured by the number of closely relatedparalogs has been shown to be associated with neutral loss-offunction variants (MacArthur et al., 2012) and genes with homologsthat have 90% sequence identity are three times less likely to harbordisease variants (Hsiao and Vitkup, 2008). To test if our data wereconsistent with this trend, we counted sequences with high sequenceidentity in human and mouse proteomes. The average number ofhomologs in overlapping sequence identity regions for each class ofloss-of-function variation are shown in Figure 3. In the humanhuman comparison, proteins containing pathogenic variants tend tohave fewer similar proteins across all levels of sequence identity thanproteins containing neutral variants. Broadly speaking, this suggestsincreased robustness of the system through functional redundancy;i.e., it allows very similar proteins to compensate for one another.On the other hand, human-mouse protein comparisons show the opposite trend. Proteins with disease-associated variants tend to havemore homologs in the mouse genome than proteins with neutralvariants. Although trends remain consistent, the magnitude betweenthe average homolog count differs between the types of variant. Thisdiscrepancy may partially be due to biases in the data sets but mayalso reflect evolutionary constraints underlying sequences susceptible to each variant type.10-1.6-1.4-1.2-1-0.8-0.6-0.4-0.20-0.4-0.20Carboxyl side3210-1.6-1.4-1.2-1-0.8-0.6Conservation MeasureFig. 2. Approximate probability density functions of an average conservationmeasure of the wildtype protein (A) at the amino side of the variant and (B) atthe carboxyl side of the variant for pathogenic (blue) and neutral (orange)variants. To ensure clarity, we omit proteins that contain both pathogenic andneutral variants from this ﬁgure. Proteins harboring disease variants are generally more conserved at both sides of the variant3Human, stop gain3Human, frameshiftPathogenicNeutral221103(0.5,1](0.6,1](0.7,1](0.8,1](0.9,1]Mouse, stop gain03(0.5,1](0.6,1](0.7,1](0.8,1](0.9,1]Mouse, frameshiftPathogenicNeutral22110(0.5,1](0.6,1](0.7,1](0.8,1](0.9,1]Sequence Identity0(0.5,1](0.6,1](0.7,1](0.8,1](0.9,1]Sequence IdentityFig. 3. Homology proﬁles for the types of loss-of-function variants. Each plotshows the average number of sequences affected by pathogenic and putatively neutral variants, within a particular range of global sequence identityagainst human and mouse genomesGenerally, our results agree with previous observations suggesting the importance of evolutionary conservation for identifyingdisease-associated variants. However, we also find that disease variants are often located in more evolutionarily conserved proteinscompared to the neutral variants suggesting limitations in using suchinformation as a sole discriminator of pathogenicity of variants. Thehomology profiles from Figure 3 also support previous observationsthat proteins harboring disease variants tend to have fewer homologs in the human genome but more homologs in the genomes ofmodel organisms compared to other proteins (Hsiao and Vitkup,2008; Mushegian et al., 1997).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018i3943.2 Prediction evaluationClassifier performance is reported as the area under the ROC curve(AUC) and shown in Figure 4. In panel A, we show the AUC of alternative models based upon per-variant, per-protein and per-clustercross-validation. The per-variant version of MutPred-LOF outperforms per-gene and per-cluster methods as a result of the model exhibiting over reliance on gene-specific features. The per-gene andper-cluster versions show comparable performance, and thus furtheranalyses are carried out using the per-gene evaluation. Additionally,we assessed the performance of an alternative model utilizing percluster cross-validation with 25% sequence similarity and foundsimilar performance to the 50% sequence identity threshold (datanot shown). Figure 4B shows the performance of MutPred-LOF onthe two types of loss-of-function variants separately. We see that theperformance of stop gain variants is significantly lower than frameshifting variants, which have higher collective performance. Weobserved that models developed specifically for each variant typeshow similar, but worse collective performance than a single unifiedmodel (data not shown). Finally, we compared the performance ofMutPred-LOF against three currently available tools in Figure 4C.We define a set of proteins which contain both neutral and pathogenic variants as bi-class proteins, and similarly derive a set of variants contained in those proteins as the bi-class subset. We observethat MutPred-LOF and CADD show reduced performance on thebi-class subset compared to the full set of variants whereas the othermethods show significantly degraded performance on subset of variants in bi-class proteins. This suggests that MutPred-LOF is less dependent on the global protein-specific attributes that may simply bereflective of the signatures of disease genes. Although we have carried out as stringent a comparison as possible, community-wide assessments such as the Critical Assessment of Genome Interpretation(CAGI) will be able to further establish performance of all availablemethods on a common set of variants in the future.K.A.Pagel et al.3.4.1 pcdh19Several mutations in Protocadherin-19 (PCDH19) are included inthe training data, including two putatively neutral variants fromExAC and dozens of pathogenic variants from HGMD, all of whichhave been predicted accurately in cross-validation. Mutations inPCDH19 have been identified as causative for early infantile epileptic encephalopathy 9, a disease shown to exhibit variable expressivity (Depienne and LeGuern, 2012). In this case, the two neutralvariants occur towards the carboxyl-terminus representing an idealuse case of MutPred-LOF on a bi-class protein. In particular, theneutral variants occur in the final 50 amino acids of the protein anddo not directly impact the primary Protocadherin-19 domain in theprotein. Additionally, MutPred-LOF uncovers several molecularmechanisms significantly disrupted by these pathogenic variantsincluding calcium binding, phosphorylation and palmitoylation,that are not identified in the neutral variants.3.4.2 sim1The Single-minded homolog 1 (SIM1) protein similarly containsboth a pathogenic variant and several putatively neutral variants.Previously discovered loss-of-function mutations in SIM1 result inhaploinsuffciency and subsequent hypodevelopment of paraventricular nuclei in the hypothalamus, and have been associated withsevere early-onset obesity and Prader-Willi-like syndrome features(Bonnefond et al., 2013). The pathogenic variant abolishes themajority of a PAS domain, including a ubiquitination site(P0 ¼ 0:0265), whereas the neutral variants impact a portion of theC-terminal single-minded domain. These putatively neutral variantsmay still be associated with obesity if derived from obese ExAC participants, depending upon the inclusion criteria for particular studies. The neutral variants impact the C-terminal Single Mindeddomain, which has proposed relationship to transcriptional regulation of SIM1 and therefore may still have some clinical relevance(Ramachandrappa et al., 2013).3.3 Performance of feature setsWe investigated the predictive capacity of each individual featuresubset on the performance of MutPred-LOF for frameshifting andstop-gain variants. To accomplish this, we generated a neural network model with the same parameters as the full MutPred-LOF butwith a reduced feature set, shown in Table 3. Values in Table 3 correspond to features discussed in Section 2.3. Here we also observemetal binding and predicted GO terms show performance on parwith conservation-based features. Any individual feature subset,particularly vector quantized functional features, are not sufficientto discriminate between pathogenic and neutral variants. However,if any feature set is removed from the training data then the AUC ofthe full model drops by several points (data not shown).3.4 Phenotype-specific impact on structural andfunctional featuresWe identified structural and functional features that exhibit differences between pathogenic variants and the neutral background, reflected in a P-value with respect to a given feature. To ascertain thediscriminative capacity of these P-values for individual proteins, weidentified and analyzed several proteins that can represent typicaluse cases. For these examples, we selected proteins which containboth pathogenic and putatively neutral variants, to highlight functional differences underlying proteins with both disease-causing andneutral loss-of-function variants.3.5 Performance on de novo variants inneurodevelopmental disordersWe applied MutPred-LOF to de novo loss-of-function variants thathave been observed in whole exome and whole genome sequencingof families affected by neurodevlopmental disorder (de novo variantset and MutPred-LOF scores are available on the website). In thissetting, the case variants may include a large fraction of nonpathogenic variants and so the performance of MutPred-LOF on thisset cannot be accurately assessed in a binary classification framework (Jain et al., 2017). To this end, we utilized a Fisher’s exact testto determine if there is a significantly higher proportion of LOF variants predicted to be pathogenic in the case samples than in the control samples, shown in Figure 4D. For the threshold associated with5% false positive rate, we find that 56% (547/970) of the case LOFvariants are scored with high confidence to be pathogenic comparedto only 46% (79/172) of the control variants (P ¼ 0.0071). For thethreshold associated with 10% false positive rate, we find that 86%(839/970) of the case variants are scored with high confidence to bepathogenic compared to only 80% (137/172) of the control variants(P ¼ 0.0152). The excess of LOF variants has been previouslyobserved in the patients with autism compared to their healthy siblings (Iossifov et al., 2012). The fact that we observe a significantlyhigher fraction of predicted pathogenic LOFs in the patients compared to controls suggests that LOF variants may have an importantrole in neurodevelopmental diseases.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018When loss-of-function is loss of functionAi395B110.80.70.70.60.6Sensitivity0.90.8Sensitivity0.90.50.40.30.10C00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.40.3Per-cluster (0.835)Per-cluster, bi-class (0.732)Per-gene (0.846)Per-gene, bi-class (0.745)Per-variant (0.979)Per-variant, bi-class (0.919)0.20.50.1011D00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9110.90.90.80.60.5P = 0.01520.70.6CaseControl0.80.7SensitivityNonsense (0.751)Nonsense, bi-class (0.621)Frameshift (0.811)Frameshift, bi-class (0.683)0.20.50.40.4MutPred-LOF (0.892)MutPred-LOF, bi-class (0.710)Vest (0.795)Vest, bi-class (0.561)DDIG-IN (0.670)DDIG-IN, bi-class (0.582)CADD (0.749)CADD, bi-class (0.693)0.30.20.100P = 0.00710.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.30.20.1015% FPR10% FPRFig. 4. Receiver operating characteristic (ROC) curves and Areas Under the ROC Curves (AUC). (A) Cross-validation performance of MutPred-LOF with per-variant,per-protein, and per-cluster cross-validation; (B) Cross-validation performance of MutPred-LOF for frameshifting and stop gain variants separately; (C) The performance for other methods based upon the testing set. Black curves represent the performance of MutPred-LOF0 . The dotted line represents performance ofeach model on the subset of variants from bi-class proteins; (D) Proportion of high-scoring de novo variants implicated in neurodevelopmental disorders in thecase and control datasets based upon 5 and 10% false positive rate thresholds. The P-value derived from Fishers exact test is shown aboveTable 3. Per-feature evaluation: top ten performing feature setsFeature setPredicted GO TermsMaximum conservationMetal bindingStructure and dynamicsEnzyme activityRegulationMacromolecular bindingHomology countsPost-translational modiﬁcationSignal peptide and transmembraneFull model0.7290.7070.6600.6520.6450.6410.6330.6140.6110.610For each set of features we train ensembles of neural networks with thesame parameters in all models. The performance (AUC) of a model trained ona feature set is used to estimate the performance of each feature separately.4 DiscussionUnderstanding the repertoire of molecular alterations consequent togenetic variation is essential to advancing personalized medicine(Rost et al., 2016). Severe alterations in mRNA transcripts resultingfrom stop variants and frameshifting indels are particularly challenging because of their potentially major impact on the sequence oftranslated proteins as well as on their structure and function. To address these challenges, we assembled a large data set of human genetic variants and analyzed specific types of molecular alterationsthat could potentially be causative of underlying diseases. Using insights from this analysis, we developed a computational modelMutPred-LOF, an extension to our variant predictor MutPred (Liet al., 2009), to discriminate between pathogenic and tolerated putatively loss-of-function variants. MutPred-LOF exploits detailed evolutionary and functional information to classify LOF variationDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018i396K.A.Pagel et al.from different and disparate contexts including severely pathogenicvariation, pathogenic recessive variants in a heterozygous state, variants tolerated due to gene redundancy and sequencing errors(MacArthur et al., 2012). In addition to providing a classificationscore for pathogenic vs. tolerated variants, MutPred-LOF provideshypotheses regarding affected molecular events that might be responsible for pathogenicity of the variant.result in pathogenicity. One of the objectives behind the development of MutPred-LOF is to clarify the relationship between molecular function and pathogenicity by allowing potential users to make amore informed assessment based upon both pathogenicity prediction and impacted molecular function. To this end, we provided twoseparate scoring mechanisms, and embrace the complexity underlying loss-of-function variation.4.1 Evolutionary conservation vs. structure and function4.4 LimitationsIn the course of our study, we were particularly interested in understanding the influence of evolutionary conservation as well asstructural and functional impact of the variant. Because pathogenicloss-of-function variants cover a relatively small subset of humangenes, we found that a straightforward analysis may be misleadingand is likely to recover signatures of known disease genes (Dalkilicet al., 2008), instead of properly accounting for the combined influence of the gene and variant features. To effectively include proteinstructure and function, we incorporated over 50 computational models that output positional propensities for several major types of structural and functional features. These models were integrated via avector quantization-like approach into a rich feature representation.The estimated accuracy of MutPred-LOF suggests that the inclusion ofadditional features was beneficial in the modeling process.4.2 Positive-unlabeled learning frameworkTechnically, our classification setting falls under the category ofpositive-unlabeled learning (Denis et al., 2005), with a further caveat that a fraction of positive data might be incorrectly labeled.Recent advances in machine learning suggest that, under mild assumptions, the traditional supervised models trained on positive vs.negative examples provide the same ranking as the (non-traditional)models trained on positive vs. unlabeled examples (Blanchard et al.,2010; Elkan and Noto, 2008; Menon et al., 2015). Moreover, if theclass prior probability is known or can be estimated, one may further exploit the following: (1) there exists a monotonic relationshipbetween the traditional and non-traditional posterior class distributions (Jain et al., 2016) and (2) the performance accuracy in thenon-traditional setting can be corrected to reflect the performanceaccuracy in the traditional setting (Jain et al., 2017). Given thesetheoretical results, we decided to use the entire ExAC database as aset of ‘negative’ examples in our training procedure, with the reasoning that filtering out a subset of variants (e.g., rare variants) is morelikely to be harmful by biasing the sample than class label noise.4.3 A note on terminologyLoss-of-function variants are defined here as frameshifting and stopgain variants. The term can be considered a misnomer, due to thepotential for the interpretation that ‘loss of function’ implies axiomatic loss of functional activity. Instead, we use the term to refer toa class of variants that are likely to result in profound impact on theprotein, similar to the term ‘protein disrupting variant’. Loss-offunction variants, although frequently causing disease, are likelypresent in every human genome. Misinterpretation of the impact ofa variant that would appear to result in loss of molecular functioncan lead to attributing phenotype to the wrong root molecularcause.By using this terminology, we sought to emphasize that the extent of impact on function lies on a spectrum from the absolute abolition of function, to reduction of functional capacity and, finally,no phenotypic consequence. Adding an additional layer of complexity, we allowed for the fact that functional impact may or may notWhile MutPred-LOF showed good performance, shortcomings inthe training data and method development may be a limitation.Context-based genetic information such as zygosity or haplotype aretypically not known since publicly available databases are commonly stripped of this information to maintain participant anonymity. The mutational context including rescuing frameshiftingmutations and relevant variation within the gene or pathway is important to consider in causative variant discovery. Population biasand undiscovered pathogenic variation in the neutral data set mayalso have unintended impact on the final model. Finally, in themethod development step, we do not encode properties of the mutant protein sequence, thereby excluding outcomes such as splicesite disruptions that may not damage the entire protein sequencedownstream of the variant. Reduced performance on bi-class variants highlights difficulties in discrimination between variants in biclass genes will continue to be of particular difficulty and should befurther emphasized.4.5 Final thoughtsWe believe that our analysis provides new insights into the understanding of loss-of-function variants, especially the interplay between protein-specific and variant-specific features. MutPred-LOFencodes both types of features and shows the ability to differentiatebetween disease-causing and tolerated loss-of-function mutations,especially those occurring in the bi-class proteins. As such, MutPredLOF allows for the specialized interpretation of one of the mostimpactful forms of genetic variation to facilitate variant and genomeinterpretation.FundingThis work has been supported by the National Institutes of Health throughthe awards R01LM009722 (SDM), R01MH105524 (LMI and PR),R21MH104766 (LMI), R01MH109885 (LMI), RO1MH076431 (JS) and theIndiana University Precision Health Initiative. MM and DNC received financial support from Qiagen Inc. through a License Agreement with CardiffUniversity.Conflict of Interest: none declared.ReferencesAltschul,S.F. et al. (1997) Gapped BLAST and PSI-BLAST: a new generationof protein database search programs. Nucleic Acids Res., 25, 3389–3402.Blanchard,G. et al. (2010) Semi-supervised novelty detection. J. Mach. Learn.Res., 11, 2973–3009.Bonnefond,A. et al. (2013) Loss-of-function mutations in sim1 contribute toobesity and prader-willi-like features. J. Clin. Invest., 123, 3037–3041.Clark,W.T. and Radivojac,P. (2011) Analysis of protein function and its prediction from amino acid sequence. Proteins, 79, 2086–2096.Clark,W.T. and Radivojac,P. (2014) Vector quantization kernels for the classiﬁcation of protein sequences and structures. Pac. Symp. Biocomput., 19,316–327.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018When loss-of-function is loss of functionCline,M. and Karchin,R. (2011) Using bioinformatics to predict the functionalimpact of snvs. Bioinformatics, 27, 441–448.Dalkilic,M. et al. (2008) From protein-disease associations to disease informatics. Front Biosci., 13, 3391–3407.de Ligt,J. et al. (2012) Diagnostic exome sequencing in persons with severe intellectual disability. N. Engl. J. Med., 367, 1921–1929.De Rubeis,S. et al. (2014) Synaptic, transcriptional and chromatin genes disrupted in autism. Nature, 515, 209–215.Denis,F. et al. (2005) Learning from positive and unlabeled examples. TheorComput. Sci., 348, 70–83.Depienne,C. and LeGuern,E. (2012) PCDH19-related infantile epileptic encephalopathy: an unusual X-linked inheritance disorder. Hum. Mutat., 33,627–634.Dinkel,H. et al. (2014) The eukaryotic linear motif resource ELM: 10 yearsand counting. Nucleic Acids Res., 42, D259–D266.Douville,C. et al. (2016) Assessing the pathogenicity of insertion and deletionvariants with the Variant Effect Scoring Tool (VEST-Indel). Hum. Mutat.,37, 28–35.Elkan,C. and Noto,K. (2008) Learning classiﬁers from only positive and unlabeled data. In Proceedings of the 14th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining, KDD 2008, pages213–220, New York, NY, USA. ACM.Epi4K Consortium and Epilepsy Phenome/Genome Project (2013) De novomutations in epileptic encephalopathies. Nature, 501, 217–221.EuroEPINOMICS-RES Consortium, Epilepsy Phenome/Genome Project andEpi4K Consortium (2014) De novo mutations in synaptic transmissiongenes including DNM1 cause epileptic encephalopathies. Am. J. Hum.Genet., 95, 360–370.Folkman,L. et al. (2015) DDIG-in: detecting disease-causing genetic variationsdue to frameshifting indels and nonsense mutations employing sequence andstructural properties at nucleotide and protein levels. Bioinformatics, 31,1599–1606.Fromer,M. et al. (2014) De novo mutations in schizophrenia implicate synaptic networks. Nature, 506, 179–184.Gilissen,C. et al. (2014) Genome sequencing identiﬁes major causes of severeintellectual disability. Nature, 511, 344–347.Girard,S.L. et al. (2011) Increased exonic de novo mutation rate in individualswith schizophrenia. Nat. Genet., 43, 860–863.Guipponi,M. et al. (2014) Exome sequencing in 53 sporadic cases of schizophrenia identiﬁes 18 putative candidate genes. PLoS One, 9, e112745.Gulsuner,S. et al. (2013) Spatial and temporal mapping of de novo mutationsin schizophrenia to a fetal prefrontal cortical network. Cell, 154, 518–529.Hashimoto,R. et al. (2016) Whole-exome sequencing and neurite outgrowthanalysis in autism spectrum disorder. J. Hum. Genet., 61, 199–206.Hsiao,T.L. and Vitkup,D. (2008) Role of duplicate genes in robustness againstdeleterious human mutations. PLoS Genet., 4, e1000014.Hu,J. and Ng,P.C. (2012) Predicting the effects of frameshifting indels.Genome Biol., 13, R9.Iossifov,I. et al. (2012) De novo gene disruptions in children on the autisticspectrum. Neuron, 74, 285–299.Iossifov,I. et al. (2014) The contribution of de novo coding mutations to autism spectrum disorder. Nature, 515, 216–221.Jain,S. et al. (2016). Estimating the class prior and posterior from noisy positives and unlabeled data. In Proceedings of the 30th Advances in NeuralInformation Processing Systems, NIPS 2016, pages 2693–2701. CurranAssociates, Inc.Jain,S. et al. (2017). Recovering true classiﬁer performance in positiveunlabeled learning. In Proceedings of the 31st AAAI Conference onArtiﬁcial Intelligence, AAAI 2017, pages 2066–2072. AAAI.Jiang,Y.H. et al. (2013) Detection of clinically relevant genetic variants in autism spectrum disorder by whole-genome sequencing. Am. J. Hum. Genet.,93, 249–263.Karolchik,D. et al. (2014) The UCSC Genome Browser database: 2014 update. Nucleic Acids Res., 42, D764–D770.Kircher,M. et al. (2014) A general framework for estimating the relativepathogenicity of human genetic variants. Nat. Genet., 46, 310–315.Kong,A. et al. (2012) Rate of de novo mutations and the importance of father’sage to disease risk. Nature, 488, 471–475.i397Landrum,M.J. et al. (2016) ClinVar: public archive of interpretations of clinically relevant variants. Nucleic Acids Res., 44, D862–D868.Lek,M. et al. (2016) Analysis of protein-coding genetic variation in 60,706humans. Nature, 536, 285–291.Li,B. et al. (2009) Automated inference of molecular mechanisms of diseasefrom amino acid substitutions. Bioinformatics, 25, 2744–2750.MacArthur,D.G. et al. (2012) A systematic survey of loss-of-function variantsin human protein-coding genes. Science, 335, 823–828.MacArthur,D.G. and Tyler-Smith,C. (2010) Loss-of-function variants in thegenomes of healthy humans. Hum. Mol. Genet., 19, R125–R130.Maquat,L.E. (2004) Nonsense-mediated mRNA decay: splicing, translationand mRNP dynamics. Nat. Rev. Mol. Cell Biol., 5, 89–99.McCarthy,S.E. et al. (2014) De novo mutations in schizophrenia implicatechromatin remodeling and support a genetic overlap with autism and intellectual disability. Mol. Psychiatry, 19, 652–658.Menon,A.K. et al. (2015) Learning from corrupted binary labels via classprobability estimation. In Proceedings of the 32nd International Conferenceon Machine Learning, ICML 2015, pages 125–134.Mort,M. et al. (2008) A meta-analysis of nonsense mutations causing humangenetic disease. Hum. Mutat., 29, 1037–1047.Mushegian,A.R. et al. (1997) Positionally cloned human disease genes: patterns of evolutionary conservation and functional motifs. Proc. Natl. Acad.Sci. USA, 94, 5831–5836.Neale,B.M. et al. (2012) Patterns and rates of exonic de novo mutations in autism spectrum disorders. Nature, 485, 242–245.Ng,P.C. and Henikoff,S. (2003) SIFT: Predicting amino acid changes that affect protein function. Nucleic Acids Res., 31, 3812–3814.O’Roak,B.J. et al. (2011) Exome sequencing in sporadic autism spectrum disorders identiﬁes severe de novo mutations. Nat. Genet., 43, 585–589.O’Roak,B.J. et al. (2012a) Multiplex targeted sequencing identiﬁes recurrently mutated genes in autism spectrum disorders. Science, 338,1619–1622.O’Roak,B.J. et al. (2012b) Sporadic autism exomes reveal a highly interconnected protein network of de novo mutations. Nature, 485, 246–250.Pei,J. and Grishin,N.V. (2001) AL2CO: calculation of positional conservationin a protein sequence alignment. Bioinformatics, 17, 700–712.Pejaver,V. et al. (2014) The structural and functional signatures of proteinsthat undergo multiple events of post-translational modiﬁcation. ProteinSci., 23, 1077–1093.Peng,K. et al. (2006) Length-dependent prediction of protein intrinsic disorder. BMC Bioinformatics, 7, 208.Radivojac,P. et al. (2004) Protein ﬂexibility and intrinsic disorder. ProteinSci., 13, 71–80.Radivojac,P. et al. (2006) Calmodulin signaling: analysis and prediction of adisorder-dependent molecular recognition. Proteins, 63, 398–410.Ramachandrappa,S. et al. (2013) Rare variants in single-minded 1 (sim1) areassociated with severe obesity. J. Clin. Invest., 123, 3042–3050.Rauch,A. et al. (2012) Range of genetic mutations associated with severe nonsyndromic sporadic intellectual disability: an exome sequencing study.Lancet, 380, 1674–1682.Rausell,A. et al. (2014) Analysis of stop-gain and frameshift variants in humaninnate immunity genes. PLoS Comput. Biol., 10, e1003757.Riedmiller,M. and Braun,H. (1993) A direct adaptive method for faster backpropagation learning: the RPROP algorithm. Proc. IEEE Internat’l. Conf.Neural Netw, 1, 586–591.Risso,D. et al. (2014) Genetic variation in taste receptor pseudogenes provides evidence for a dynamic role in human evolution. BMC Evol. Biol.,14, (198.Ronemus,M. et al. (2014) The role of de novo mutations in the genetics of autism spectrum disorders. Nat. Rev. Genet., 15, 133–141.Rost,B. et al. (2016) Protein function in precision medicine: deep understanding with machine learning. FEBS Lett., 590, 2327–2341.Sanders,S.J. et al. (2012) De novo mutations revealed by whole-exomesequencing are strongly associated with autism. Nature, 485, 237–241.Sigrist,C.J. et al. (2013) New and continuing developments at PROSITE.Nucleic Acids Res., 41, D344–D347.Stenson,P.D. et al. (2014) The Human Gene Mutation Database: towards acomprehensive repository of inherited mutation data for medical research,Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018i398genetic diagnosis and next-generation sequencing studies. Hum. Genet.Mar. 27. doi: 10.1007/s00439-017-1779-6.Sulem,P. et al. (2015) Identiﬁcation of a large set of rare complete humanknockouts. Nat. Genet., 47, 448–452.Suzek,B. et al. (2007) Uniref: comprehensive and non-redundant uniprot reference clusters. Bioinformatics, 23, 1282–1288.Thousand Genomes Project Consortium (2010) A map of human genome variation from population-scale sequencing. Nature, 467, 1061–1073.Turner,T.N. et al. (2016) Genome sequencing of autism-affected families reveals disruption of putative noncoding regulatory DNA. Am. J. Hum.Genet., 98, 58–74.K.A.Pagel et al.Xu,B. et al. (2011) Exome sequencing supports a de novo mutational paradigm for schizophrenia. Nat. Genet., 43, 864–868.Xu,B. et al. (2012) De novo gene mutations highlight patterns of genetic andneural complexity in schizophrenia. Nat. Genet., 44, 1365–1369.Yuen,R.K. et al. (2015) Whole-genome sequencing of quartet families withautism spectrum disorder. Nat. Med., 21, 185–191.Yuen,R.K. et al. (2016) Genome-wide characteristics of de novo mutations inautism. NPJ Genom. Med., 1, 160271–1602710.Zia,A. and Moses,A.M. (2011) Ranking insertion, deletion and nonsense mutations based on their effect on genetic information. BMC Bioinformatics,12, 299.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i389/3953989by gueston 07 January 2018
5028882003002	PMID28882003	5028882003	https://watermark.silverchair.com/btx271.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28882003.main.pdf	Bioinformatics, 33, 2017, i261–i266doi: 10.1093/bioinformatics/btx271ISMB/ECCB 2017miniMDS: 3D structural inference fromhigh-resolution Hi-C dataLila Rieber and Shaun Mahony*Department of Biochemistry and Molecular Biology and Center for Eukaryotic Gene Regulation, The PennsylvaniaState University, University Park, PA 16802, USA*To whom correspondence should be addressed.AbstractMotivation: Recent experiments have provided Hi-C data at resolution as high as 1 kbp. However,3D structural inference from high-resolution Hi-C datasets is often computationally unfeasibleusing existing methods.Results: We have developed miniMDS, an approximation of multidimensional scaling (MDS) thatpartitions a Hi-C dataset, performs high-resolution MDS separately on each partition, and thenreassembles the partitions using low-resolution MDS. miniMDS is faster, more accurate, and usesless memory than existing methods for inferring the human genome at high resolution (10 kbp).Availability and implementation: A Python implementation of miniMDS is available on GitHub:https://github.com/seqcode/miniMDS.Contact: mahony@psu.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionHi-C is a high-throughput method for genome-wide analysis ofchromosome conformation. The Hi-C protocol begins by crosslinkingprotein–DNA interactions, with the goal of crosslinking together pairsof chromosomal loci that are proximal to one another in 3D space.Crosslinked chromatin is then fragmented (typically using restrictionenzymes), and the ends of the resulting fragments are marked withbiotin. A random ligation reaction results either in intermolecular ligations that join two distinct DNA molecules together (also referredto as a contact) or self-ligations, which join the two ends of a singleDNA molecule. After further shearing, ligation products are immunoprecipitated via the biotin label and paired-end sequenced.The number of contacts between two loci observed in a population of cells is referred to as the contact frequency, which is inverselyproportional to the average in vivo 3D distance between the loci inthe cell population (Lieberman-Aiden et al., 2009). Contact frequency data are typically presented as a matrix, which can be analyzed using methods such as eigenvector decomposition (Imakaevet al., 2012). However, for the purposes of structural comparisonand visualization, it is often useful to convert the matrix to a 3Dstructure or ensemble of structures.There are two types of methods for 3D structural inference fromHi-C data (Park and Lin, 2016). Modeling-based methods assumethat contact frequencies are related to distances via a probabilisticfunction, such as a Poisson distribution. These methods aim tomaximize the likelihood of the inferred 3D structure using algorithms such as Markov Chain Monte Carlo (Hu et al., 2013; Parkand Lin, 2016; Rousseau et al., 2011; Varoquaux et al., 2014; Zouet al., 2016). Optimization-based methods infer a function to convert the contact frequency matrix to a distance matrix. A 3D structure is initialized (typically at random), and an objective function isused to quantify the difference between the inferred 3D structureand the distance matrix. The 3D structure is iteratively updated tominimize the objective function, using multidimensional scaling(MDS) (Adhikari et al., 2016; Ba  and Marti-Renom, 2012; Duanuet al., 2010; Lesne et al., 2014; Szałaj et al., 2016; Zhang et al.,2013).Recent experiments have provided Hi-C data at resolutions ashigh as 1–5 kbp for several human cell lines (Rao et al., 2014).However, existing structural inference methods have not beenapplied to mammalian whole-genome Hi-C data with a resolutiongreater than 40 kbp (Hu et al., 2013). High resolution createsunique challenges for structural inference, because computationalrequirements increase exponentially with the number of loci inthe dataset. The sparsity of the contact matrix also increases withresolution (Park and Lin, 2016). Long-range contacts are particularly sparse, because the contact frequency between two locidecreases exponentially with the linear separation between the loci(Lieberman-Aiden et al., 2009). The excess of zeroes can be corrected using interpolation (Lesne et al., 2014) or a statistical adjustment (Park and Lin, 2016). On the other hand, zeroes provide littleor no information for structural inference. Removing zeroes wouldthus have little informational cost and would reduce the memoryand computations associated with large datasets.CV The Author 2017. Published by Oxford University Press.i261This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i261/3953988by gueston 07 January 2018i262We propose a structural inference method for high-resolutionHi-C data that partitions the contact matrix using a method similarto topologically associating domain (TAD) identification and performs MDS individually on each partition. This algorithm is somewhat similar to the subsampling used by other MDS approximationalgorithms (Platt, 2005). Because loci preferentially interact withloci within the same TAD (Dixon et al., 2012), this approach minimizes the number of zeroes in the data. It also reduces the amountof data that must be stored in memory at any given time and allowsfor parallelization of analysis. The partitions are then assembledinto a global structure using low-resolution data. Our method isfaster, more memory-efficient and more accurate than alternativemethods and can solve a 3D structure for the human genome atkilobase-resolution in <5 h.L.Rieber and S.MahonyFig. 1. Partitions identiﬁed by the simpliﬁed algorithm with a particularsmoothing factor approximately correspond to the previously identiﬁedTADs. (A) TADs identiﬁed by Dixon et al. for mouse embryonic stem cell chr6.(B) Partitions created by the partitioning algorithm applied to the same data2 Materials and methods2.1 Data sourceWe used GM12878 Hi-C count matrices (MAPQ !30) from Raoet al. (2014) for all analyses, which were normalized using theKnight–Ruiz normalization (Knight and Ruiz, 2013) factors provided (GEO accession number: GSE63525).2.2 AlgorithmminiMDS infers detailed whole-genome 3D structures by progressively solving and integrating structures at three resolution levels.High-resolution local structures are first solved within each partition. Lower-resolution structures are then inferred for each wholechromosome, and the high-resolution partitioned structures areoverlaid onto these chromosomal structures after solving theoptimal transformations. For inter-chromosomal analysis, a lowresolution inter-chromosomal structure is inferred, and the highresolution intra-chromosomal structures are overlaid onto this.2.2.1 PartitioningTo solve local high-resolution structures, the contact matrix is firstpartitioned, with the average size of partitions determined by a userdefined smoothing parameter. Larger partitions may produce moreaccurate results but have greater computational requirements.Our algorithm to partition the genome was derived from amethod for identifying TADs, which used a directionality index thatdescribes whether a locus preferentially interacts with loci upstreamor downstream (Dixon et al., 2012). Loci near the beginning of aTAD preferentially interact downstream, and those near the endpreferentially interact upstream. The existing method uses a hiddenMarkov model to identify large regions of upstream or downstreambias in Hi-C data. However, this method produces only one partitioning of the genome. To partition the genome at different lengthscales, we propose a modification of the directionality indexmethod, in which directionality indices of individual loci are binnedto identify large regions of bias. Partition boundaries are defined bythe occurrence of a downstream-biased bin, followed by anupstream-biased bin. Partitions are then defined as the regions between boundaries. An adjustable smoothing factor determines thesize of the bins (the smoothing factor multiplied by the total numberof loci on a chromosome equals the bin size used in partitioning).Larger smoothing factors result in larger partitions. Given a certainsmoothing factor, our method produces partitions that approximately correspond to the TADs identified by Dixon (Fig. 1). By adjusting the smoothing factor, we observed a hierarchy of genomeorganization (Fig. 2), suggesting that there may not be a single TADFig. 2. Partitions created using various smoothing factors applied to datafrom Figure 1definition for a chromosome. This is consistent with recent resultsdemonstrating a hierarchy of genome folding, in which TADs havefunctional importance but no particular structural importance(Zhan et al., 2017).2.2.2 Assembling the global structureThe contact matrix for each partition is converted to a distance matrix using the following equation (zero-distances are ignored by theMDS algorithm):( acij ; cij > 0dij ¼(1)0; cij ¼ 0We used a ¼ À0.25, which was found to best fit Hi-C data in results from fluorescence in situ hybridization (FISH) experiments(Wang et al., 2016).Inference of each partition’s structure is performed independently of every other partition, and thus is trivially parallelizable bysplitting partition computations across processors. Threedimensional metric MDS (mMDS) is performed on each partition’sdistance matrix in parallel, producing a high-resolution structure foreach partition (Fig. 3A). To infer the spatial relationships betweenpartitions, a global structure is inferred from low-resolution data,using mMDS without partitioning (Fig. 3C). High-resolution partitions are then assembled in parallel, using the low-resolution structure as a guide. By taking the average coordinates of multiple loci,each high-resolution partition structure is approximated at the sameresolution as the global low-resolution structure (Fig. 3B). As a result, each low-resolution locus in a partition structure has an analogin the global low-resolution structure. The optimal transformation(translation, rotation and reflection) to align each low-resolutionDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i261/3953988by gueston 07 January 20183D structural inferencei2632016), ChromSDE (Zhang et al., 2013), tREX (Park and Lin, 2016),MCMC5 (Rousseau et al., 2011), PASTIS (Varoquaux et al., 2014),TAD-bit (Ba  and Marti-Renom, 2012), HSA (Zou et al., 2016),uChromosome3D (Adhikari et al., 2016) and InfMod3DGen (Wanget al., 2015) (Table 1). We also tested classical MDS (cMDS), whichis used by ShRec3D (Lesne et al., 2014), and standard mMDS.We were unable to install PASTIS, because it requires the IPOPTpackage, which must be built from source and has many additionaldependencies. We were unable to test MCMC5 because it requiresthe standard deviation of contact frequencies, which was not available for the datasets we used. We were unable to test tREX becauseit requires fragment length, which was not available for the datasetswe used. PASTIS, MCMC5 and tREX are modeling-based methods.Because modeling-based methods must probabilistically explore parameter space, they have large computational requirements. Thus, itis unlikely that they would have been able to analyze high-resolutiondata if we had been able to run them. BACH and InfMod3DGenwere excluded from further testing due to errors. BACH producedan error for chr22 100-kbp-resolution data. InfMod3DGen produced an error when tested on its sample data for yeast chrXVI at100-kbp resolution. A quadratic solver was used for ChromSDE.Defaults were used for all other parameters.Table 1 summarizes the results of Hi-C structural inferencemethods applied to chr22 kilobase-resolution data. The only algorithms that were able to infer structures at kilobase-resolution wereminiMDS, 3D-GNOME and ShrRec3D. We were unable to evaluatethe accuracy of 3D-GNOME because it provides only graphical output, rather than 3D coordinates, so it was excluded from furthertesting. ShRec3D is represented in further testing by cMDS.Fig. 3. Overview of the miniMDS algorithm, demonstrated on GM12878 chr22at 10-kbp-resolution. MDS is applied to each partition individually, creatinghigh-resolution local structures, such as the structure shown in (A). The structure is approximated at low-resolution (B). MDS is performed on a low-resolution dataset, creating a global low-resolution structure (C). The low-resolutionpartition has an analog in the global structure (D). The optimal transformationto align the low-resolution approximation with its analog is calculated. For illustration, (E) shows the transformation applied to the low-resolution approximation. The transformation is applied to the high-resolution partition (F). Whenthis process is repeated for all partitions, a global high-resolution structure iscreated (G)partition to its analog in the global structure (Fig. 3D) is calculatedusing the Kabsch algorithm (Kabsch, 1976). Each transformation isthen applied to the corresponding high-resolution partition structure(Fig. 3F) to create a global high-resolution intra-chromosomal structure (Fig. 3G). To infer inter-chromosomal structures, a global interchromosomal structure is inferred at very low resolution, analogousto Figure 3C. miniMDS is applied to each chromosome in the structure individually. Then the high-resolution miniMDS intrachromosomal structures are aligned to the inter-chromosomalstructure.2.3 ImplementationThe miniMDS algorithm is implemented in Python. MDS steps areperformed using scikit-learn with default parameters (Pedregosaet al., 2011). Parallelization was performed using pymp. miniMDSis released under an MIT open source license. miniMDS source codeis available on GitHub: https://github.com/seqcode/miniMDS.2.4 Comparison with alternative approachesWe attempted to test the performance of BACH (Hu et al., 2013),MOGEN (Trieu and Cheng, 2014), 3D-GNOME (Szałaj et al.,3 Results3.1 miniMDS is more efficient than other structuralinference methodsWe tested the computational time required for miniMDS, standardmMDS, cMDS, TAD-bit, MOGEN, Chromosome3D andChromSDE to analyze chr22 Hi-C data at 10-kbp resolution (Fig. 4)and 100-kbp resolution (Supplementary Fig. S1). TAD-bit did notproduce output in a reasonable amount of time (i.e. several days)and thus was excluded from further analysis. HSA was excludedfrom the 10-kbp analysis because of the large amount of timerequired for the 100-kbp resolution (over 60 h), which we assumedwould be significantly greater for 10-kbp resolution. miniMDS hadthe lowest time requirements of all remaining methods at bothresolutions.We also tested the amount of RAM required for miniMDS,standard mMDS, cMDS, MOGEN, Chromosome3D andChromSDE to analyze chr22 data at 100-kbp resolution(Supplementary Fig. S2). Though the full benefits of miniMDS arenot demonstrated at low resolution, miniMDS had the lowest memory requirements. MOGEN, Chromosome3D and ChromSDErequired orders of magnitude more memory.3.2 miniMDS is robustWe performed two iterations of Chromosome3D, mMDS,miniMDS, MOGEN, HSA and ChromSDE on chr22 at 100-kbpresolution. cMDS was excluded because it does not use a randomseed, so it produces the same output from every iteration. We created a distance matrix from each set of output coordinates and calculated the Pearson correlation between the distance matrices foreach pair of iterations (Fig. 5). MDS-based methods,Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i261/3953988by gueston 07 January 2018i264L.Rieber and S.MahonyTable 1. Comparison of Hi-C structural inference methodsNameminiMDSBACHShRec3DMOGEN (Trieu and Cheng, 2014)3D-GNOME (Szałaj et al., 2016)ChromSDE (Zhang et al., 2013)tRex (Park and Lin, 2016)MCMC5 (Rousseau et al., 2011)PASTIS (Varoquaux et al., 2014)TAD-bit (Ba  and Marti-Renom, 2012)uHSA (Zou et al., 2016)Chromosome3D (Adhikari et al., 2016)InfMod3DGen (Wang et al., 2015)TypeResults from 1-kbp-resolution dataOptimizationModelingOptimizationOptimizationOptimizationOptimizationModelingModelingModelingOptimizationModelingOptimizationModelingSuccessBugSuccessExceeds maximum input sizeSuccess (graphical output only)Failed to produce output in reasonable timeRequires fragment lengthRequires standard deviationFailed to install IPOPTFailed to produce output in reasonable timeMemory errorFailed to produce output in reasonable timeBug (error was corrected while the current manuscript was in press)Fig. 4. Time required to analyze chr22 data at 10-kbp resolutionInter-chromosomal?YesNoNoNoYesNoNoNoYesYesNoNoNoFig. 5. Correlation between the output of two iterations of the same methodapplied to the same chr22 100-kbp-resolution datasetChromosome3D and ChromSDE produced the same output fromboth iterations, resulting in correlations of 1. The correlation between different iterations was lower for HSA and MOGEN.3.3 Performance on 10-kbp-resolution data3.3.1. Time and memory requirementsTo determine how computational requirements changed with thenumber of loci, we tested MOGEN, miniMDS, mMDS and cMDSon all chromosomes at 10-kbp resolution. Chromosome3D,ChromSDE and HSA were excluded from these analyses because thetime costs were prohibitive. MOGEN was the fastest on average,with miniMDS performing almost as well (Fig. 6). However, giventhat MOGEN is not robust (see Section 3.2), its speed may be because the algorithm does not converge. Both methods were significantly faster than standard mMDS and cMDS, with a lower rate ofincrease of time requirements with the number of loci. miniMDShad the lowest memory requirements, with minimal increase inmemory requirements for increasing number of loci (Fig. 7).3.3.2 Correlation between input and outputWe calculated the correlation between input distances and distancesinferred from the output 3D structure for MOGEN, miniMDS,mMDS and cMDS applied to each chromosome at 10-kbp resolution. For the MDS methods, accuracy was calculated as the correlation between the input distance matrix and the distances calculatedfrom output coordinates. For MOGEN, we inferred a by fitting anexponential curve to distances, which were calculated from outputcoordinates, as a function of contact frequencies. We thenFig. 6. Time required to analyze each chromosome at 10-kbp resolutiontransformed contact frequencies to inferred distances using theinferred value of a and calculated the correlation between inferreddistances and output distances.The correlation was highest for miniMDS for every chromosome(Fig. 8), demonstrating that miniMDS infers 3D structures that aremore consistent with the underlying Hi-C data. The structuresinferred for chr22 using mMDS and cMDS are collapsed globuleswith few discernable features (Fig. 9A and B). MOGEN produces afairly unfolded structure dominated by outliers, with little evidenceof TAD structure or the hierarchical organization hypothesized byDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i261/3953988by gueston 07 January 20183D structural inferencei265Fig. 7. Memory required to analyze each chromosome at 10-kbp resolutionFig. 10. GM12878 chromosome conformation at 10-kbp resolution(Fig. 10). We used a smoothing factor of 0.05 and a minimum partition size of 1% of the matrix. Intermediate low-resolution structuresfor each whole chromosome were inferred from 100-kbp-resolutiondata. Inference of the entire whole-genome structure required 4 h21 min.4 DiscussionFig. 8. Accuracy of methods for each chromosome at 10-kbp resolutionFig. 9. Chr22 10-kbp-resolution structures produced by mMDS (A), cMDS (B),miniMDS (C) and MOGEN (D)the fractal globule model (Rao et al., 2014) (Fig. 9D). miniMDS produces a structure that is densely clustered but still displays discernable structures (Fig. 9C).MOGEN and MDS-based methods are optimization-based. Wewere interested in whether a modeling-based method would offeradvantages for accuracy. HSA was the only modeling-based methodthat we were able to test (see Section 2.4), so we evaluated its performance on 100-kbp-resolution chr22 data, even though it is notrobust (see Section 3.2). We calculated its correlation between inputand output using the analysis described for MOGEN. The correlation was lower for HSA than for MDS-based methods applied tothe same data (Supplementary Fig. S3).3.4 miniMDS performs whole-genome structural inference at high resolutionWe used miniMDS to infer a global conformation for all GM12878chromosomes using 10-kbp-resolution Hi-C data (Rao et al., 2014)We have presented miniMDS, a method for inferring 3D structuresfrom Hi-C experiments that is suitable for high-resolution data. Ituses genome partitioning and parallelization to achieve greaterspeed and lower memory requirements compared with alternativemethods. Theoretically, the memory requirements of miniMDScould always be maintained below a certain threshold by increasingthe number of partitions, regardless of the size of the dataset. Asmeasured by the correlation between input distances and output distances, miniMDS is more accurate than other methods that are ableto efficiently analyze large, high-resolution datasets. However, wenote that Chromosome3D and ChromSDE, which do not scale wellto large datasets, perform better on this metric for smaller datasets.These algorithms use different optimization algorithms, which couldbe made more efficient using a partitioning strategy similar tominiMDS.Comprehensive identification of chromatin loops, which play animportant role in gene regulation, requires high-resolution Hi-Cdata (Rao et al., 2014). Detection of co-localization of DNAbinding proteins would also be improved by high resolution.Though Hi-C data can be analyzed in two dimensions, the dynamicsof chromosome conformation are easiest to understand in three dimensions. For example, 3D structures from multiple time points orcell types could be compared using a structural alignment algorithm(Hasegawa and Holm, 2009) to determine how the localization ofgenomic regions changes. It is possible that dynamic genomic regions are correlated with changes in gene regulation. Highresolution 3D chromosome conformation inference will thus contribute to exploration of the “4D nucleome” (Chen et al., 2015).miniMDS is the only method currently available that can accuratelysolve whole-genome 3D structures from the highest-resolution Hi-Cdatasets.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i261/3953988by gueston 07 January 2018i266AcknowledgementsThe authors thank the members of the Center for Eukaryotic GeneRegulation at Penn State for helpful feedback and discussions.FundingThis material is based upon work supported by the National ScienceFoundation Graduate Research Fellowship Program under Grant No.DGE1255832 (to L.R.) and by the National Science Foundation ABIInnovation Grant No. DBI1564466 (to S.M.). Any opinions, ﬁndings andconclusions or recommendations expressed in this material are those of theauthor(s) and do not necessarily reﬂect the views of the National ScienceFoundation.Conﬂict of Interest: none declared.ReferencesAdhikari,B. et al. (2016) Chromosome3D: reconstructing three-dimensionalchromosomal structures from Hi-C interaction frequency data using distance geometry simulated annealing. BMC Genomics, 17, 886.Ba ,D., and Marti-Renom,M.A. (2012) Genome structure determination viau3C-based data integration by the Integrative Modeling Platform. Methods58, 300–306.Chen,H. et al. (2015) Functional organization of the human 4D Nucleome.Proc. Natl Acad. Sci., 112, 8002–8007.Dixon,J.R. et al. (2012) Topological domains in mammalian genomes identiﬁed by analysis of chromatin interactions. Nature, 485, 376–380.Duan,Z. et al. (2010) A three-dimensional model of the yeast genome. Nature,465, 363–367.Hasegawa,H., and Holm,L. (2009) Advances and pitfalls of protein structuralalignment. Curr. Opin. Struct. Biol., 19, 341–348.Hu,M. et al. (2013) Bayesian inference of spatial organizations of chromosomes. PLoS Comput. Biol., 9, e1002893.Imakaev,M. et al. (2012) Iterative correction of Hi-C data reveals hallmarks ofchromosome organization. Nat. Methods, 9, 999–1003.Kabsch,W. (1976) A solution for the best rotation to relate two sets of vectors.Acta Crystallogr. A, 32, 922–923.L.Rieber and S.MahonyKnight,P.A., and Ruiz,D. (2013) A fast algorithm for matrix balancing. IMAJ. Numer. Anal., 33, 1029–1047.Lesne,A. et al. (2014) 3D genome reconstruction from chromosomal contacts.Nat. Methods, 11, 1141–1143.Lieberman-Aiden,E. et al. (2009) Comprehensive mapping of long-range interactions reveals folding principles of the human genome. Science, 326, 289–293.Park,J., and Lin,S. (2016) Impact of data resolution on three-dimensionalstructure inference methods. BMC Bioinformatics, 17, 70.Pedregosa,F. et al. (2011) Scikit-learn: machine learning in Python. J. Mach.Learn. Res., 12, 2825–2830.Platt,J.C. (2005). FastMap, MetricMap, and Landmark MDS are all NystromAlgorithms. In 10th International Workshop on Artiﬁcial Intelligence andStatistics, pp. 261–268.Rao,S.S.P. et al. (2014) A 3D map of the human genome at kilobase resolutionreveals principles of chromatin looping. Cell, 159, 1665–1680.Rousseau,M. et al. (2011) Three-dimensional modeling of chromatin structurefrom interaction frequency data using Markov chain Monte Carlo sampling.BMC Bioinformatics, 12, 414.Szałaj,P. et al. (2016) An integrated 3-dimensional genome modeling enginefor data-driven simulation of spatial genome organization. Genome Res.,26, 1697–1709.Trieu,T., and Cheng,J. (2014) Large-scale reconstruction of 3D structures ofhuman chromosomes from chromosomal contact data. Nucleic Acids Res.,42, e52–e52.Varoquaux,N. et al. (2014) A statistical approach for inferring the 3D structure of the genome. Bioinformatics, 30, i26–i33.Wang,S. et al. (2015) Inferential modeling of 3D chromatin structure. NucleicAcids Res., 43, e54–e54.Wang,S. et al. (2016) Spatial organization of chromatin domains and compartments in single chromosomes. Science, 353, 598–602.Zhan,Y. et al. (2017) Reciprocal insulation analysis of Hi-C data shows thatTADs represent a functionally but not structurally privileged scale in thehierarchical folding of chromosomes. Genome Res., 27, 479–490.Zhang,Z. et al. (2013) 3D chromosome modeling with semi-deﬁnite programming and Hi-C data. J. Comput. Biol., 20, 831–846.Zou,C. et al. (2016) HSA: integrating multi-track Hi-C data for genome-scalereconstruction of 3D chromatin structure. Genome Biol., 17, 40.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i261/3953988by gueston 07 January 2018
5028882002002	PMID28882002	5028882002	https://watermark.silverchair.com/btx270.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28882002.main.pdf	Tumor phylogeny inference usingtree-constrained importance samplingGryte Satas1 and Benjamin J. Raphael2,*1Department of Computer Science, Brown University, Providence, 02912 RI, USA and 2Department of ComputerScience, Princeton University, Princeton, NJ, 08544 USA*To whom correspondence should be addressed.AbstractMotivation: A tumor arises from an evolutionary process that can be modeled as a phylogenetictree. However, reconstructing this tree is challenging as most cancer sequencing uses bulk tumortissue containing heterogeneous mixtures of cells.Results: We introduce Probabilistic Algorithm for Somatic Tree Inference (PASTRI), a new algo-rithm for bulk-tumor sequencing data that clusters somatic mutations into clones and infers aphylogenetic tree that describes the evolutionary history of the tumor. PASTRI uses an importancesampling algorithm that combines a probabilistic model of DNA sequencing data with a enumer-ation algorithm based on the combinatorial constraints defined by the underlying phylogenetictree. As a result, tree inference is fast, accurate and robust to noise. We demonstrate on simulateddata that PASTRI outperforms other cancer phylogeny algorithms in terms of runtime and accur-acy. On real data from a chronic lymphocytic leukemia (CLL) patient, we show that a simple linearphylogeny better explains the data the complex branching phylogeny that was previously reported.PASTRI provides a robust approach for phylogenetic tree inference from mixed samples.Availability and Implementation: Software is available at compbio.cs.brown.edu/software.Contact: braphael@princeton.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionTumors develop through the accumulation of somatic mutationsduring the lifetime of an individual in a process called clonal evolu-tion (Nowell, 1976). Thus, many tumors are heterogeneous, con-taining multiple populations of cells (or clones), each with its ownunique combination of somatic mutations. This intra-tumor hetero-geneity complicates the diagnosis and treatment of cancer. Accuratecharacterization of the process of clonal evolution, modeled by aphylogenetic tree, is crucial to understanding cancer development,and also important for comprehensive treatments that target mul-tiple clones within a tumor. Recent studies have shown that metasta-sis often occurs from clones present at minor proportion in thetumor cell population; moreover, at time of diagnosis patients mayalready have clones within their tumor that already possess resist-ance to the therapy (Schmitt et al., 2016). For example, in a recentstudy of an acute myeloid leukemia (AML) patient (Griffith et al.,2015), a clone with a driver mutation in IDH2, present in less than2% of the pre-treatment sample, was found to be the dominantclone in the subsequent relapse.The vast majority of cancer sequencing performed to date, includ-ing in large scale projects such as The Cancer Genome Atlas (TCGA)and the International Cancer Genome Consortium (ICGC), issequencing of bulk-tumor tissue, where each sequenced sample iscomposed of a mixture of thousands-millions of tumor cells. Thiscomplicates analysis of tumors, as we expect a high level of heterogen-eity amongst individual tumor cells, and we do not observe the muta-tional profiles of component clones directly. Instead, we observe amixed signal of all the genetic material present in the sample. Single-cell sequencing presents an alternative approach to characterize tumorevolution and there has been promising work in this direction (Jahnet al., 2016; Wang et al., 2014). However, single-cell sequencing re-mains error-prone and expensive (Navin, 2015). Thus, characterizingintra-tumor heterogeneity and reconstructing tumor evolution frombulk-sequencing data is an area of active development.Like any evolutionary process, the somatic mutational processgiving rise to a tumor can be described by a phylogenetic tree, whoseleaves correspond to present clones, and whose edges describe theancestral relationships between clones. In classic phylogeny, we dir-ectly observe the contents of the leaves, and use this information toreconstruct the ancestral relationships between species. However,with bulk-sequencing data, we do not directly observe the contentsof the leaves, but rather we observe mixtures of genetic material.VC The Author 2017. Published by Oxford University Press. i152This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i152–i160doi: 10.1093/bioinformatics/btx270ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018In particular, for single-nucleotide variants (SNVs), the fraction ofreads covering the nucleotide containing the mutation allele provideas estimate of the cell fraction, or fraction of cells in the mixturecontaining the mutation. As such, specialized algorithms that decon-volve bulk-sequencing data are needed to accurately characterizetumor composition and reconstruct the process of clonal evolution.We divide the task of characterizing the clonal structure of thetumor from bulk-sequencing data into two problems: (1) clusteringmutations into clones, or groups of cells that have the same set ofsomatic mutations, and (2) identifying the tree that relates clones.Methods such as PyClone (Roth et al., 2014), SciClone (Miller et al.,2014), and Clomial (Zare et al., 2014) focus on the first problem andcluster mutations, without requiring that these clusters are generatedby a tree. These algorithms use a probabilistic model for the sequenc-ing data to estimate the number of clones, the assignments of muta-tions into clones, and the cell fraction of clusters of mutations. For thesecond problem, a number of algorithms, including TrAP (Strinoet al., 2013), Rec-BTP (Hajirasouliha et al., 2014), LICHeE (Popicet al., 2015), AncesTree (El-Kebir et al., 2015), CITUP (Donmezet al., 2016; Malikic et al., 2015) and SPRUCE (El-Kebir et al., 2016)use a combinatorial approach that relies on constraints that the under-lying phylogenetic tree imposes on the cell fractions. Because these al-gorithms exploit the combinatorial structure given by the tree, theytend to be fast and also perform well when the clustering of mutationsinto clones is straightforward. However, these algorithms may strug-gle in more challenging cases of moderate-to-low coverage data dueto simplistic error models for allele frequencies, or reliance on muta-tion clusters being given as input.There is, however, a circular dependence between clustering andtree inference. The cell fractions of clusters are used to construct thetree, but the underlying phylogenetic tree constrains allowed cellfractions. If a tree constraint is not accounted for in a clustering al-gorithm, then the clustering algorithm may yield clones whose cellfractions do not permit a tree. Thus, treating the problems of muta-tion clustering and tree inference independently may produce poorresults, especially when there is high uncertainty in the cell fractions.A few methods, including PhyloSub (Jiao et al., 2014),PhyloWGS (Deshwar et al., 2015) and Canopy (Jiang et al., 2016),cluster mutations and infer the tree simultaneously. These methodscombine a robust error model for sequencing data with a tree con-straint on the clusters in the generative model. Thus, the resultingclusters necessarily respect the tree constraint. These algorithms useMarkov Chain Monte Carlo (MCMC) to sample trees, cluster cellfractions and cluster assignments in order to estimate the posteriordistribution over clusters and trees. However, in practice, oninstances of realistic size, the sample space is large and complex andthe sampling procedure may become stuck in local minima and failto converge in reasonable time. Thus, while the generative modelused by these methods effectively describes the data, the solutionsfound by the algorithms may be suboptimal. Table 1 summarizesthe approaches cited above.1.1 ContributionsIn this article, we introduce Probabilistic Algorithm for SomaticTree Inference (PASTRI), an algorithm that uses importance sam-pling to simultaneously cluster mutations into clones and infer aphylogenetic tree that relates the clones. PASTRI exploits the condi-tional independence of the observed read counts from the latentphylogenetic tree given the cluster cell fractions, thus separating in-ference into two parts. PASTRI first samples likely cluster cell frac-tions from an informed proposal distribution determined by aclustering algorithm without the tree constraint (e.g. (Miller et al.,2014; Roth et al., 2014; Zare et al., 2014)), and calculates the datalikelihood given these cell fractions. Second, PASTRI uses a com-binatorial algorithm described in Popic et al. (2015) and El-Kebiret al. (2016) to enumerate exactly the set of possible trees for a givenset of cluster cell fractions. This procedure allows us to efficientlycompute the likelihood of all trees that respect the tree constraint,under a realistic noise model for the data. Moreover, by samplingfrom clusters obtained without a tree constraint, PASTRI focuses onhigher probability regions of the sample space and thus samplesmore efficiently than MCMC approaches. Moreover, PASTRI sam-ples only cell fractions, and forgoes sampling from the space of treesand cluster assignments. As a result, PASTRI is faster and has betterconvergence properties than previous MCMC approaches.We show on simulated data that PASTRI outperforms both com-binatorial and probabilistic methods in accuracy and runtime. Wethen examine data from a chronic lymphocytic leukemia (CLL) pa-tient from Rose-Zerilli et al. (2016). This patient was classified ashaving a complex branching phylogeny, based on analysis byPhyloSub. In contrast, PASTRI finds a higher likelihood tree with alinear, rather than branching topology, suggesting that the clonalevolution process in this patient was simpler than previouslydescribed.2 Materials and methodsWe describe our PASTRI algorithm in the following three sections.In Section 2.1, we introduce our model for tumor evolution andTable 1. Methods for characterizing tumor heterogeneity from bulk-sequencing dataProblem ApproachMethod Clustering Tree Inference Combinatorial ProbabilisticPyClone (Roth et al., 2014), SciClone (Miller et al., 2014), Clomial (Zare et al.,2014)Y YTrAP (Strino et al., 2013), LICHeE (Popic et al., 2015), AncesTree (El-Kebiret al., 2015), SPRUCE*(El-Kebir et al., 2016), CITUP (Malikic et al., 2015;Donmez et al., 2016)Y YPhyloSub (Jiao et al., 2014), PhyloWGS* (Deshwar et al., 2015), Canopy* (Jianget al., 2016)Y Y YPASTRI Y Y Y YWe categorize a subset of previous work according to two problems: (1) clustering mutations into clones according to inferred cell fractions, and (2) tree infer-ence. These methods take one of two approaches: a combinatorial model and algorithm, or a probabilistic model and inference. PASTRI performs both clusteringand tree inference. It uses a probabilistic model for observed allele counts, and integrates the combinatorial framework into inference. (*) indicates methodaccounts for copy-number aberrations.Tumor phylogenies i153Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018sequencing mixtures. We conclude this section by describing the treeconstraint on cell fractions (Fig. 1). In Section 2.2, we describe agenerative probabilistic model for trees, clusters of mutations, andobserved read counts (Fig. 2). Finally, in Section 2.3, we describe theimportance sampling approach that we use to compute the posteriordistribution over phylogenetic trees (Fig. 3).2.1 ModelWe model tumor evolution using SNVs as phylogenetic characters,leaving extension to other types of genomic aberrations (e.g. copy-number aberrations) as future work. Following previous work(Deshwar et al., 2015; El-Kebir et al., 2015; Hajirasouliha et al.,2014; Jiang et al., 2016; Jiao et al., 2014; Malikic et al., 2015; Popicet al., 2015; Strino et al., 2013), we assume that each locus mutatesat most once during the lifetime of the tumor, an assumption knownas the infinite-sites assumption (ISA). As such, we encode the stateof a locus in a cell as a binary character, with a 0 indicating thegermline state and a 1 indicating a somatic mutation. We model can-cer evolution as a clone tree T ¼ ðVðTÞ;EðTÞÞ, a directed tree with jVðTÞj ¼ k vertices (Fig. 1a). Vertex vi corresponds to a clone i in thetumor, and a directed edge (vi, vj) encodes the evolutionary relation-ship that clone j is a direct descendant of clone i. Equivalently, wecan represent a tree T as a k k perfect-phylogeny matrix BT ¼ ½bij (Gusfield, 1991). Column i of matrix BT corresponds to the genomeof vertex vi, such that bij¼1 if vertex vj is on the unique path fromvi to the root, and 0 otherwise.We use cluster to refer to the set of mutations that first occur in aparticular clone. We define the cluster assignment vector ~c ¼ ðc1; . . . ;cnÞ to be the vector mapping each mutation j to a clone v, such thatcj¼ v indicates that v is the first clone in which mutation j occurs. Bythe ISA, the genomes of all descendant clones of vi also contain muta-tion j. The genome of a clone vi is then defined by the set of mutationsassigned to vertices on the path from the root of the tree to vi.We obtain DNA sequencing data from one or more samplesfrom a tumor, separated spatially or temporally (Fig. 1b). Each ofthese samples contain mixtures of the k clones in the tumor, possiblywith varying proportions across the multiple samples. Let U ¼ ½ui;p be a k m usage matrix of clone proportions, such that ui;p is theproportion of cells in sample p belonging to clone i. For all clones iand samples p, the entries of U are non-negative, i.e. ui;p   0 for alli, p, and the columns of U are on the ðk  1Þ-simplex,Pki¼1 ui;p ¼ 1for all p. Let clone 1 correspond to normal (non-tumor) cells, suchthat u1;p is the proportion of normal (non-cancerous) cells in sample(a) (c)(b)Fig. 1. Tree constraint on cluster cell fractions. (a) We model the evolution of a tumor as a clone tree T, with k vertices corresponding to clones in the tumor. A mu-tation (denoted here by a star) is assigned to the clone in which it originates. Under the infinite sites assumption, a mutation occurs once, and is never lost. Thus,if a mutation occurs in clone vi, all descendent clones of vi will also contain that mutation. A clone tree T can be described by a binary perfect phylogeny matrixBT. (b) We measure m samples from a heterogeneous tumor, each sample containing a mixture of clones. The usage matrix U describes the proportion of eachclone in each sample. The cell fraction matrix F describes the proportion of cells that contain a given cluster of mutations. For example, here the clone v1, contain-ing the red mutation, occurs in u1;1 ¼ 40% of sample S1, but has a cell fraction of f1;1 ¼ 1, as the red mutation is present in all cells. (c) As the usage matrix U de-scribes mixture proportions, the columns of U are constrained to be on the ðk   1Þ-simplex. For a tree T, F and U are related by F¼BTU. Thus, the set of allowedcell fractions for a tree T is a linear transformation of the ðk   1Þ-simplex and is unique for every distinct tree T. This set can be described by the Sum Condition,where dðvi Þ denotes the set of children of vi in TFig. 2. Generative model for variant allele counts A from DNA sequencingdata of a tumor. A latent (unobserved) clone tree T generates m samples,each consisting of mixtures of cells with different mutations. Each mutation isassigned to a cluster Cj . A cluster i of mutations occur in fraction Fi;p of cellsin sample p. Variant read counts A are generated for each mutation with a bi-nomial likelihood model, given an observed total read counts Di154 G.Satas and B.J.RaphaelDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018p. For a vertex vi and sample p, the cell fraction fi;p is the proportionof the cells in the sample that contain the mutations assigned to vi.As a result of the infinite sites assumption, all descendants of vi willalso contain any mutation that occurred at vi. Thus, we relate theclone usage matrix U to the cluster cell fraction matrix F ¼ fi;p asfollows. Let dðviÞ be the set of children of vi in T, and let DðviÞ bethe set of all descendants of vi. Then, in sample p, fi;p ¼ ui;pþPvj2DðviÞ uj;p ¼ ui;p þPvj2dðviÞ fj;p. Thus, we have the following SumCondition that constrains the cell fractions given a tree T, as notedin previous works (El-Kebir et al., 2015; Jiao et al., 2014; Malikicet al., 2015; Popic et al., 2015; Strino et al., 2013).fi;p  Xv‘2dðviÞf‘;p for all vertices i and samplesp: (1)As clone 1 corresponds to normal cells and all tumor cells are des-cendants of a normal cell, we also have the constraint that f1;p ¼ 1for all samples p.Equivalently, (as described in El-Kebir et al. (2015)), the cellfraction matrix F is related to the tree T and usage matrix U accord-ing to F¼BTU, where BT is the square binary perfect phylogeny ma-trix corresponding to T. As BT is invertible, we have that U ¼ B 1T F.Allowed cell fractions F for a tree T are then those for whichU ¼ B 1T F is a valid usage matrix, i.e. the entries are non-negativeand the columns are on the ðk  1Þ-simplex. Figure 1c shows thatthis constraint on the usage matrix U corresponds to the SumCondition, and the additional constraint that the cell fraction f1;p ¼ 1in all samples p for germline variants in normal cells. As described inEl-Kebir et al. (2015), these constraints provide a necessary and suffi-cient condition for a valid usage matrix (Fig. 1c).For each mutation j identified in sample p, we measure the num-ber aj;p of variant reads—reads that contain the somatic mutation—and the total number dj;p of reads that align to the locus. Supposewe observe data for n mutations across all samples. Let A and D ben m matrices corresponding to the observed number of variantand total reads for each mutation. The variant-allele frequency aj;p=dj;p of a mutation j in sample p is proportional to the fraction of cellscontaining the variant in the mixture. Under the infinite sites as-sumption with a diploid genome, this fraction is 12 fcj ;p, as each cellcontaining the mutation has one mutated and one unmutated copy.2.2 Probabilistic modelWe model the observed data using a finite tree-constrained mixturemodel (Fig. 2). We divide the model into three components: the treeand cell fraction model (highlighted in blue), the cluster assignmentmodel (orange), and the observed data model (green).We first describe the tree and cell fraction model. Let T be therandom variable corresponding to the latent unobserved clone tree.We assume that T follows a categorical distribution which selectstree T with weight cT. For the results in this paper, we set c such thatthe probability of all trees is uniform. As the columns of a usage ma-trix lie on the ðk  1Þ-simplex, i.e. all entries are non-negative andPki¼1 ui;p ¼ 1 for all samples p, we model the usages for sample pusing a Dirichlet distribution, Up   Dirðl1; . . . ;lkÞ with vector ofhyperparameters l of length k.Under this model, any matrix U whose columns are not on theðk  1Þ-simplex will have a probability PrðU ¼ UjlÞ ¼ 0. Thisimplies that the cell fractions Fp for sample p are distributed asFp   BT  Dirðl1; . . . ;lkÞ, where BT is the perfect phylogeny matrixcorresponding to tree T. As described in Section 2.1, for a validusage matrix U, F ¼ BTU respects the Sum Condition for tree T.Thus, a cell fraction matrix F will have non-zero probability PrðF¼ FjT ¼ TÞ if and only if it respects the Sum Condition across allsamples.As in a standard mixture model, the observed data is composedof mixtures of k clusters. Let C ¼ ðC1; . . . ;CnÞ be the random vari-able corresponding to cluster assignments where Cj follows a cat-egorical distribution parameterized by weights x. Under this model,the cluster assignments are conditionally independent given the fixedhyperparameter x. This choice allows us to easily marginalize overpossible cluster assignments during inference, described below. Notethat this model differs from a Dirichlet process mixture models,(a)(c)(b)Fig. 3. Overview of PASTRI algorithm. (a) We observe variant-allele read counts A and the total number of reads D that align to the locus for n mutations across msamples of the tumor. (b) A clustering algorithm that does not require that the data is generated by a phylogenetic tree gives an estimate QðFÞ of the posterior dis-tribution over cluster cell fractions F. (c) PASTRI draws samples F from QðFÞ. For each sample F , PASTRI enumerates the set T F of trees T and assignments p ofcell fractions to vertices of of T that satisfy the Sum Condition. All trees/vertex-assignment pairs not in T F have a probability of 0. Algorithm estimates the poster-ior probability of each tree using importance samplingTumor phylogenies i155Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018where the number of clusters is not fixed, and the cluster assign-ments have a complex dependence on each other.Let A and D be the random variables corresponding to the num-ber of variant and total reads across n mutations and p samples.Under the infinite sites assumption, we expect the variant-allele fre-quency Aj;p=Dj;p of a mutation j in sample p to be proportional tothe cell fraction FCi ;p of the cluster containing the mutation. That isE½Aj;p=Dj;p  ¼ 12 FCj ;p in a diploid genome. We will use a binomialmodel for allele counts in the present work, so thatAj;p   BinðDj;p; 12 Fcj ;pÞ. However, the model described above allowsfor more sophisticated models of read counts that involve additionalparameters H that can model sequencing error, over-dispersion,copy-number aberrations, or other features. One example of such amodel, which includes probabilities of false positive and false nega-tive mutations, is used in Section 3.2, and described inSupplementary Section SB.3. A key feature of these models is thatthe observed variant allele counts do not depend directly on the treeT, only on the cell fractions F. This allows us to sample cell fractionsand compute the data likelihood and model parameter likelihoodsseparately.In summary, the complete data likelihood of our model isPrðA;C; F;TjD;x; c;lÞ¼ PrðAjC;F;DÞPrðFjT;lÞPrðTjcÞPrðCjxÞ¼Ymp¼1½DirðB 1T FpjlÞYnj¼1BinðAj;pjFcj ;p;Dj;pÞ cTYnj¼1xCj :(2)2.3 Tree inferenceGiven variant-allele counts A ¼ A and total read counts D ¼ D, wewant to compute the posterior probability of a tree T ¼ T given theobserved data and hyperparameters,PrðT ¼ TjA ¼ A;D ¼ D;x; c;lÞ/ PrðA ¼ AjT ¼ T;D ¼ D;x;lÞPrðT ¼ TjcÞ(3)In order to calculate this posterior probability from the completedata likelihood given in Equation 2, we marginalize over latent clus-ter assignments C and cluster cell fractions F,PrðA ¼ AjT ¼ T;D ¼ D;x; lÞPrðT ¼ T; cÞ¼ÐFP~c PrðA ¼ AjC ¼~c; F ¼ F;D ¼ DÞ PrðC ¼~cjxÞPrðF ¼ FjT ¼ T; lÞPrðT ¼ TjcÞdF(4)We first show how we marginalize over cluster assignments C, com-puting the inner summand above. Then in Section 2.3.1, we will useimportance sampling to numerically integrate over cell fractions.In Equation 2, the terms that depend on C arePrðAjC; F;DÞPrðCjxÞ. By marginalizing over all vector assignmentsC ¼~c such that ci 2 f1; . . . ; kg, we obtain a variant-allele count like-lihood PrðAjF;D;xÞ that does not depend on C. Because the clusterassignments are conditionally independent given x;A; F, we canmarginalize them independently for each mutation. Thus we obtainthe following.Pr A ¼ AjF ¼ F;D ¼ D;xð Þ¼X~cPr A ¼ AjC ¼~c;F ¼ F;D ¼ Dð ÞPr C ¼~cjxð Þ¼X~cPr C ¼~cjxð ÞYnj¼1Ymp¼1Bin aj;pjfcj ;p; dj;p   (5)¼Ynj¼1Xki¼1Pr Cj ¼ ijx   Ymp¼1Bin dj;pjfi;p;dj;p    !¼Ynj¼1Xki¼1xjYmp¼1Bin aj;pjfi;p; dj;p    !We refer to this term PrðA ¼ AjF ¼ F;D ¼ D;xÞ as the uncon-strained data likelihood, as it does not depend on the tree T. Thuswe havePrðT ¼ TjA ¼ A;D ¼ D;x; c;lÞ/ cTÐFPrðA ¼ AjF ¼ F;D ¼ D;xÞPrðF ¼ FjT ¼ T; lÞdF:(6)2.3.1 Importance samplingWe now describe how we compute the integral over cell fractions Fin Equation 6. Integrating, or marginalizing, over cell fractions ismore complicated that marginalizing over cluster assignments as F iscontinuous, high-dimensional, and the entries of matrix F are not in-dependent. Thus, we cannot analytically calculate this integral. Inthis section, we describe how to calculate this integral using import-ance sampling (Tokdar and Kass, 2010), using input from a cluster-ing approach without a tree constraint, and a combinatorial treeenumeration algorithm. Figure 3 shows an overview the PASTRIalgorithm.Importance sampling uses a proposal distribution QðXÞ that ap-proximates the distribution of interest PðXÞ over a random variableor set of random variables X. We numerically calculate an integralÐPðXÞdX as follows. Let X1; . . . ;XN be samples from QðXÞ. Then,ðPðXÞdX ¼ðPðXÞQðXÞQðXÞdX ¼ EQPðXÞQðXÞ     1NXNi¼1PðXiÞQðXiÞ: (7)In our case, the distribution of interest isPðFÞ ¼ PrðA ¼ AjD ¼ D; F ¼ F;xÞPrðF ¼ FjT ¼ T;lÞ. We use aclustering method, such as SciClone (Miller et al., 2014) or PyClone(Roth et al., 2014), which gives an estimate of the posterior prob-ability over F, without the tree constraint, QðFÞ ¼ PrQðF ¼ FjD ¼ D;A ¼ A;  Þ / PrQðA ¼ AjD ¼ D; F ¼ F;  Þ PrQðF ¼ Fj Þ where weuse PrQðXÞ to denote the probability under the model used by theclustering method and where   indicates the distribution may de-pend on a number of hyperparameters. Thus, QðFÞ and PðFÞ differprimarily in the generative model for cell fractions: a tree constraint,PrðF ¼ FjT ¼ T; lÞ versus no constraint PrQðF ¼ Fj Þ. As there is anunderlying tree T that generated the data, the true F respects a treeconstraint on cell fractions, and thus, we expect a significant portionof posterior probability mass respects the tree constraint for T.Thus, using unconstrained QðFÞ is an effective approximation ofconstrained PðFÞ.When sampling from the unconstrained posterior QðFÞ, cell frac-tions do not yet correspond to vertices of the tree. A permutation pof the rows of a sampled cell fraction F corresponds to assignmentsof cell fractions to vertices of the tree. We denote by p‘ F the ‘th per-mutation of the rows of F. All permutations result in the same datalikelihood PrðA ¼ AjF ¼ F;D ¼ D;xÞ. However, not all permuta-tions satisfy the Sum Condition (Fig. 4). For example, consider a lin-ear tree T, as in Figure 4. There is a single permutation that satisfiesthe Sum Condition: the frequencies must be in descending order. Ingeneral, for a tree on k vertices, a relatively small fraction of the k!total permutations will satisfy the Sum Condition. Since, cell frac-tions F that do not respect the Sum Condition for a tree T have prob-ability PrðF ¼ FjT ¼ T; lÞ ¼ 0, most of the k! permutations of asampled cell fraction F will have a probability of 0.i156 G.Satas and B.J.RaphaelDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018Using insights from combinatorial approaches (as described inSection 2.1), we can enumerate exactly the set of trees T and permu-tations p for which p F satisfy the Sum Condition. Indeed, given cellfractions F, the problem of finding a tree T and an assignment p ofcell fractions to vertices of T satisfying the Sum Condition is theproblem investigated in several previous works (El-Kebir et al.,2015, 2016; Malikic et al., 2015; Popic et al., 2015). El-Kebir et al.(2015) and Popic et al. (2015) describe the solutions to this problemas finding a constrained set of spanning trees of a particular graph.Popic et al. (2015) and El-Kebir et al. (2016) use a specialized ver-sion of the Gabow-Myers algorithm (Gabow and Myers, 1978) toenumerate this specific set of trees.We combine the above ideas into the PASTRI algorithmwhose steps are the following. (1) Generate N samplesFð2Þ; . . . ;FðNÞ   QðFÞ. (2) For each sample FðiÞ, enumerate the setTFðiÞ of tree/permutation pairs ðT;p‘Þ that respect the SumCondition across all vertices. (3) Calculate the posterior probabilityof a tree T as follows:PrðT ¼ TjA ¼ A;D ¼ D;x; c; lÞ / cTðFPrðA ¼ AjF ¼ F;D¼ D;xÞPrðF ¼ FjT ¼ T;lÞdF (8)  cTN  k!XNi¼1Xk!‘¼1PrðA ¼ AjD ¼ D; F ¼ FðiÞ;xÞPrðF ¼ p‘ FðiÞjT ¼ T; lÞQðFðiÞÞ(9)¼ cTN k!XNi¼1XðT;p‘Þ2TFðiÞPrðA¼AjD¼D;F¼FðiÞ;xÞPrðF¼p‘ FðiÞjT¼T;lÞQðFðiÞÞ:(10)Note that any pair ðT; p‘Þ 62 T FðiÞ will have a probabilityPrðTjF ¼ p‘ FðiÞ; lÞ ¼ 0. Thus, summing over just ðT; p‘Þ 2 T FðiÞ(Equation 10) is equivalent to summing over all permutations(Equation 9). In practice, instead of calculating the likelihood foreach tree T separately, we use the same set of samples for all trees.Thus, for each sample FðiÞ, we only need to enumerate TFðiÞ once.In the results below, we report a single optimal cluster cell frac-tions F and cluster assignments ~c. We compute these optima asfollows. Let T be the optimal tree, the one with the highest poster-ior probability. Then F is obtained asF ¼ argmaxF maxðT ;pÞ2T F PrðA ¼ AjD ¼ D;F ¼ F;xÞ PrðF ¼ p FjT ¼ T; lÞ:(11)As cluster assignments are conditionally independent given x; Fand read counts A and D, we optimize each independently,obtainingcj ¼ argmaxi2f1;...;kgxi  PrðA ¼ AjD ¼ D;F ¼ FÞ: (12)In Supplementary Methods SA.1, we describe how we generalize im-portance sampling to use multiple proposal distributions. Thisallows us to sequentially adjust the proposal distribution to findmore samples that respect the Sum Condition, using the uncon-strained posterior as a starting point.3 Results3.1 Benchmarking on simulated dataWe compare PASTRI to three other methods for constructing tumorphylogenies: PhyloSub (Jiao et al., 2014), Canopy (Jiang et al.,2016) and AncesTree (El-Kebir et al., 2015). PhyloSub and Canopyboth employ a Bayesian non-parametric model to simultaneouslyinfer the number of clones, the most likely clusters and cluster cellfractions, and the phylogeny. AncesTree is a combinatorial methodwhich takes as input clusters of SNVs and infers the largest tree withthese clusters. We used SciClone (Miller et al., 2014) to generateclusters as input for both PASTRI and AncesTree.We generate 50 instances each of 3, 4 and 5 vertex trees with 20SNVs. All simulated instances contain 5 sequenced samples, eachwith 200X coverage. PhyloSub and Canopy were both run using de-fault parameters. PASTRI was run for 10, 000 iterations, with uni-form priors over U, C and T. We compare the methods using threemetrics: accuracy in recovering ancestral relationships, accuracy incluster cell fractions, and runtime. Further details of the simulationsare in Supplementary Section SB.1.3.1.1 Recovering the phylogenetic treeTo assess the ability of each method to recover the true phylogenetictree, we measured the proportion of ancestral relationships betweenSNVs that were correctly reported in the best reported tree by eachmethod. A pair of SNVs c and d can have one of four relationshipsin a tree: c and d may be in the same cluster, c may be ancestral to d,d may be ancestral to c, or c and d may be on distinct branches ofthe tree. For all pairs of distinct SNVs in each sample, we measurewhether the reported relationship matched the relationship in thetrue tree. The results for 5 vertex trees are shown in Figure 5a, withresults for 3 and 4 vertex trees in Supplementary Figure S3. We seethat PASTRI outperforms the other three methods. On 5 vertextrees, PASTRI correctly infers all ancestral relationships on 46% ofinstances, while neither PhyloSub nor Canopy have a single instanceon which this happens.3.1.2 Recovering cluster cell fractionsTo assess each method’s ability to recover true cluster cell fractions,we compare the true cluster cell fractions F to the reported clustercell fractions FPT ; FPS and FCP, for PASTRI, PhyloSub and Canopyrespectively. We do not compare to AncesTree in this section sincewe used SciClone clusters and cell fractions as input to AncesTree.Fig. 4. Importance sampling. The data likelihood PrðA ¼ AjD ¼ D; F ¼ pi  F ;xÞis the same for all permutations p of F for this tree T. However, p1 F satisfiesthe Sum Condition, and p2 F does not. Thus p2 F has a probabilityPrðF ¼ p2 F jT ¼ T ; lÞ ¼ 0Tumor phylogenies i157Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018Since an algorithm may return a different number of clusters thanthe true number of clones, we use two different metrics to measureaccuracy Let dðfi; fjÞ ¼ 1‘Pm‘¼1 jfi;‘   fj;‘j be the average per-entry dis-tance between two rows. Let k be the inferred number of clusters.1. Metric 1 – A measure of sensitivity, matches the true clones tothe nearest reported clusters.M1ðF; FÞ ¼Xki¼1argminj2½1...k dðfi; fj Þ (13)2. Metric 2 – A measure of specificity, matches the reported clus-ters to the nearest true clones.M2ðF;FÞ ¼Xkj¼1argminj2½1...k dðfi; fj Þ (14)Results using both metrics for 5 vertex trees are shown in Figure 5b.Results with 3 and 4 vertex trees are in Supplementary Figure S4.PASTRI consistently outperforms both PhyloSub and Canopy. Notethat in all cases, the median distance for PASTRI is less than the firstquartile of distances for both PhyloSub and Canopy.3.1.3 RuntimeWe compare the runtime of the four algorithms, using the combinedruntime of SciClone clustering and tree inference for PASTRI andAncesTree. Figure 5c shows the results for 5 vertex trees. Note thatthese results are shown on a log scale. Supplementary Figure S2shows the results for 3 and 4 vertex trees. Note that both PhyloSuband Canopy are sampling methods and the runtime is determined byhow many samples each method uses within their default param-eters. As such, we expect increasing the number of samples improvesthe accuracy of the methods. However, we see that PASTRI achieveshigher accuracy with significantly lower runtimes than eitherPhyloSub or Canopy. Overall, runtimes ranged from on the order ofseconds for SciClone and AncesTree, minutes for PASTRI, andhours for PhyloSub and Canopy.3.1.4 Comparison to AncesTreeAncesTree and PASTRI rely on the same combinatorial structure fortree inference, and differ primarily in the way they handleuncertainty in variant allele frequencies. In particular, PASTRI usesa probabilistic model for observed read counts, while AncesTreerelies on confidence intervals for cell fractions. Because of these sim-ilarities, we performed additional comparisons to illustrate the dif-ferences between these approaches. Since PASTRI uses a moresophisticated error model, we expect that it would perform as goodas or better than AncesTree in recovering the true tree.We generated trees with varying number of vertices (6–10), sam-ples (2–10) and coverage (50X–500X). For these experiments, wegenerated 50 trees, and the parameters that were not being variedwere fixed to k¼5 vertices, m¼5 samples, and coverage r ¼ 100X.Each tree contained n¼50 mutations. Here we report the propor-tion of ancestral relationships correctly inferred by AncesTree andby the maximum likelihood tree found by PASTRI.Figure 6a shows that as coverage increases, the performance ofboth methods improves. Overall, PASTRI outperforms AncesTreeacross all coverages, but we see the largest effect at high coverage.At low coverage, there is considerable uncertainty in variant allelefrequencies, and thus, many trees are indistinguishable by likeli-hood. While AncesTree reports a single tree, PASTRI provides a pos-terior over all trees, reflecting the level of uncertainty in thereconstruction. As the coverage increases, PASTRI sees larger gainsin performance. Interestingly AncesTree performance declines in thehighest coverage. This effect may be due to the error model forAncesTree. As the coverage increases, confidence intervals over clus-ter cell fractions become narrower. Thus small errors in the initialclustering by SciClone may result in violations of the Sum Condition(described in Section 2.1 and Fig. 1).As the number of samples increases, the Sum Condition becomesa stronger constraint. Thus, we expect a corresponding increase inaccuracy. Figure 6b shows that this is indeed the case for PASTRI.However, for AncesTree, we see that after 4 samples, performancebegins to decline. This too is likely attributed to the simpler modelof uncertainty in cell fractions employed by AncesTree. For a clus-ter, if the Sum Condition is violated in any sample, then that clusteris not included in the tree. Thus, increasing the number of samplesresults in a decline in performance for AncesTree.To evaluate the performance of AncesTree and PASTRI on dif-ferent size trees, we used parameters r¼100 and m¼5 where thetwo algorithms showed similar performance for smaller trees. Wesee a moderate decline in performance of both algorithms as the(a) (b) (c)Fig. 5. Comparison of phylogenetic reconstruction algorithms. We simulate 50 trees with 5 vertices, 5 samples and 20 mutations. (a) To quantify the accuracy ofthe tree inference, we measure the proportion of ancestral relationships that the algorithms correctly recover. A pair of mutations c and d has four possible ances-tral relationships: c is ancestral to d, d is ancestral to c, c and d are in the same cluster, or c and d are on separate branches. (b) To measure how accurately the al-gorithms recover the true cell fractions, we report two metrics, given by Equations 13 and 14. Both metrics measure the average distance between the truecluster cell fraction matrix F and the reported cluster cell fractions. Metric 1 penalizes for overestimating the number of clusters, and Metric 2 penalizes for under-estimating. (c) We report the runtime in seconds of each method, where the x-axis is a logarithmic scalei158 G.Satas and B.J.RaphaelDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018number of vertices grow. This is not surprising since the number ofpossible trees grows exponentially with the number of vertices, butthe amount of observed data A and D remains fixed. PASTRI con-sistently outperforms AncesTree on both average and worse case,for nearly all size trees.3.2 Chronic lymphocytic leukemiaRose-Zerilli et al. (2016) sequenced 13 patients with chroniclymphocytic leukemia (CLL). Each patient had 2–5 samples takenlongitudinally over the course of their disease and these were sub-jected to a number of analyses including targeted deep sequencing.The authors classify the patients into those with linear phylogenies(4/13 patients) and those with complex branching phylogenies (9/13patients) on the basis of PhyloSub (Jiao et al., 2014) analysis.Branching phylogenies are used as evidence of subclonal competi-tion prior to therapy.Here we investigate Patient 5 in the study. This patient was clas-sified as having a complex branching phylogeny (Fig. 7) usingPhyloSub analysis. For this patient, SciClone inferred 8 clusters ofmutations. Running PASTRI results in an optimal tree with 8 verti-ces that is mostly linear. The fully linear tree was ranked third out of115 trees. We calculate the likelihood of the data under both thePASTRI read count observation model, and the PhyloSub read countmodel, which allows for sequencing error (i.e. false positives andnegatives) in observations. Details on these models can be found inSupplementary Section SB.3. Under both models, the likelihood forboth trees found by PASTRI are higher than the branching phyl-ogeny reported in Rose-Zerilli et al. (2016). There are two possibleexplanations for the discrepancy. First, it is possible that PhyloSub’ssampling procedure never found either the optimal tree or the lineartree. Second, the tree-structured stick breaking prior over trees usedby PhyloSub has hyperparameters that influence the width and thedepth of the trees. As a result, the branching phylogeny presentedmay have been preferred to a linear phylogeny. However, if an in-tention of a study is to classify patients by phylogenetic tree struc-ture, it makes sense to use non-informative priors over trees.We analyzed this same patient using AncesTree. AncesTree wasnot able to find a tree relating all 8 clusters. Supplementary FigureS6 shows the largest tree found by AncesTree, containing 6 clusters,and 18/20 mutations.4 DiscussionWe introduced PASTRI, a new method that simultaneously clustersmutations and infers tumor phylogenies from bulk-sequencing data.PASTRI exploits the conditional independence of the observed readcounts from the latent phylogenetic tree given the cluster cell frac-tions. Because of this conditional independence, we are able to ex-ploit combinatorial constraints (El-Kebir et al., 2015; Jiao et al.,2014; Malikic et al., 2015; Popic et al., 2015; Strino et al., 2013) toFig. 7. Patient 5 from Rose-Zerilli et al. (2016). This patient was classified as having a complex branching phylogeny using PhyloSub analysis (right). RunningPASTRI finds an optimal phylogeny that is mostly linear (left). Restricting to linear phylogenies results in the center tree, which was the third most likely phyl-ogeny out of 115 possible. We calculate the likelihood of the data under both the PASTRI read count observation model, and the PhyloSub observation modelthat also models sequencing error in observations. Under both models, the likelihood for both trees found by PASTRI are better than the reported branchingphylogeny(a) (b) (c)Fig. 6. The effect of coverage, number of samples, and number of vertices on performance of AncesTree and PASTRI. For every combination of parameters, wegenerate 50 trees. Here we report the proportion of ancestral relationships that were correctly recovered. Overall, PASTRI has higher average performance for allsets of parameters, but the magnitude of this effect differs. (a) As the coverage increases, and uncertainty in variant allele frequencies decreases, PASTRI’s accur-acy increases. AncesTree’s accuracy increases initially, but declines at the highest coverages. (b) Similarly, as the number of samples increases and the problembecomes more constrained, PASTRI’s accuracy increases, while AncesTree’s accuracy peaks with four samples, and then declines. (c) As the number of verticesin the tree increase, both AncesTree and PASTRI see a similar moderate decline in performance, although PASTRI consistently outperforms AncesTreeTumor phylogenies i159Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018efficiently marginalize over cluster cell fractions, using a combina-torial tree enumeration algorithm. At the same time, we utilize aprobabilistic model for the observed read counts that models errorsand uncertainty in sequencing data. By leveraging combinatorialstructure into the probabilistic inference, we obtain improved accur-acy over prior combinatorial algorithms—due to better modeling ofuncertainty in the sequence data—and improved runtime over prob-abilistic methods—due to more efficient inference.By using importance sampling, we direct the computation towardregions of the sample space with highest probability. As a result,PASTRI will calculate more precisely the posterior probability of thetrees that have the higher probability versus trees that have low prob-ability. In our application, this is an acceptable tradeoff, as we aremost interested in recovering the highest probability trees, and gener-ally are not concerned with ranking highly unlikely trees. Note thatour importance sampling approach can use any algorithm that com-putes a posterior distribution over clusters; we have used SciClone(Miller et al., 2014) in this work, but PyClone (Roth et al., 2014),Clomial (Zare et al., 2014) or other algorithms can also be used.In Section 3.2, we examined data from a study that aimed to clas-sify patients as having either a linear of a complex branching phyl-ogeny. We showed a case where an existing algorithm failed to find alinear phylogeny that had higher likelihood than the reported branch-ing phylogeny. In cases with few samples or low coverage, there is sig-nificant ambiguity in the tree structure, and many trees may havesimilar likelihood. Reporting the single solution of highest likelihoodmay not accurately recover the underlying phylogeny. In particular,when distinguishing between linear and branching phylogenies, theremay be many branching trees, but only a single linear tree. Thus, it isimportant to consider the posterior distribution over tree topologies,particularly when correlating topology to other clinical features.There are a number of directions for future work. First is to im-prove the model selection problem of choosing the number of clones/clusters. In this work, we relied on the model selection procedure per-formed by the clustering algorithm. However, it is plausible that insome scenarios the phylogenetic tree constraint would shift cluster cellfractions in such a way to affect the choice of the number of clones.Second, while we have demonstrated PASTRI using single-nucleotidemutations, the hybrid combinatorial-probabilistic approach generalizesto the analysis of copy-number aberrations (CNAs), which are wide-spread in solid tumors. As CNAs affect the observed allele counts ofsingle-nucleotide mutations in a predictable way (as described inDeshwar et al. 2015; El-Kebir et al. 2016), the interaction betweenobserved CNAs and allele counts can be modeled. Finally, the hybridinference algorithm presented here could generalize to other problemswhere there is conditional independence between a combinatorialstructure (here a tree) and a probabilistic model.FundingThis work is supported by a US National Science Foundation (NSF) CAREERAward (CCF-1053753) and US National Institutes of Health (NIH) grantsR01HG005690, R01HG007069, and R01CA180776 to BJR. BJR issupported by a Career Award at the Scientific Interface from the BurroughsWellcome Fund, an Alfred P. Sloan Research Fellowship.Conflict of Interest: B. Raphael is a founder and consultant of MedleyGenomics.ReferencesDeshwar,A.G. et al. (2015) PhyloWGS: Reconstructing subclonal composition andevolution from whole-genome sequencing of tumors. Genome Biol., 16, 35.Donmez,N. et al. (2016). Clonality inference from single tumor samples usinglow coverage sequence data. In International Conference on Research inComputational Molecular Biology, pages 83–94. Springer.El-Kebir,M. et al. (2015) Reconstruction of clonal trees and tumor compos-ition from multi-sample sequencing data. Bioinformatics, 31, i62–i70.El-Kebir,M. et al. (2016) Inferring the mutational history of a tumor usingmulti-state perfect phylogeny mixtures. Cell Syst., 3, 43–53.Gabow,H.N. and Myers,E.W. (1978) Finding all spanning trees of directedand undirected graphs. SIAM J. Comput., 7, 280–287.Griffith,M. et al. (2015) Optimizing cancer genome sequencing and analysis.Cell Syst., 1, 210–223.Gusfield,D. (1991) Efficient algorithms for inferring evolutionary trees.Networks, 21, 19–28.Hajirasouliha,I. et al. (2014) A combinatorial approach for analyzing intra-tumor heterogeneity from high-throughput sequencing data. Bioinformatics,30, i78–i86.Jahn,K. et al. (2016) Tree inference for single-cell data. Genome Biol., 17, 86.Jiang,Y. et al. (2016) Assessing intratumor heterogeneity and tracking longitu-dinal and spatial clonal evolutionary history by next-generation sequencing.Proc. Natl. Acad. Sci. USA, 113, E5528–E5537.Jiao,W. et al. (2014) Inferring clonal evolution of tumors from single nucleo-tide somatic mutations. BMC Bioinformatics, 15, 35.Malikic,S. et al. (2015) Clonality inference in multiple tumor samples usingphylogeny. Bioinformatics.Miller,C.A. et al. (2014) Sciclone: inferring clonal architecture and trackingthe spatial and temporal patterns of tumor evolution. PLoS Comput. Biol.,10, e1003665.Navin,N.E. (2015) The first five years of single-cell cancer genomics and be-yond. Genome Res., 25, 1499–1507.Nowell,P.C. (1976) The clonal evolution of tumor cell populations. Science,194, 23–28.Popic,V. et al. (2015) Fast and scalable inference of multi-sample cancer lin-eages. Genome Biol., 16, 91.Rose-Zerilli,M. et al. (2016) Longitudinal copy number, whole exome and tar-geted deep sequencing of ‘good risk’ IGHV-mutated CLL patients with pro-gressive disease. Leukemia.Roth,A. et al. (2014) Pyclone: statistical inference of clonal population struc-ture in cancer. Nat. Methods, 11, 396–398.Schmitt,M.W. et al. (2016) The influence of subclonal resistance mutations ontargeted cancer therapy. Nat. Rev. Clin. Oncol., 13, 335–347.Strino,F. et al. (2013) TrAp: a tree approach for fingerprinting subclonaltumor composition. Nucleic Acids Res., 41, e165.Tokdar,S.T., and Kass,R.E. (2010) Importance sampling: a review. WileyInterdisciplinary Rev. Comput. Stat., 2, 54–60.Wang,Y. et al. (2014) Clonal evolution in breast cancer revealed by single nu-cleus genome sequencing. Nature, 512, 155–160.Zare,H. et al. (2014) Inferring clonal composition from multiple sections of abreast cancer. PLoS Comput Biol, 10, e1003703.i160 G.Satas and B.J.RaphaelDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i152/3953987by gueston 07 January 2018
5028882001002	PMID28882001	5028882001	https://watermark.silverchair.com/btx269.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28882001.main.pdf	Efficient approximations of RNA kineticslandscape using non-redundant samplingJuraj Mich alik1,2, Hélène Touzet3,4 and Yann Ponty1,2,*1AMIB project, Inria Saclay, 91120 Palaiseau, France, 2LIX CNRS UMR 7161, Ecole Polytechnique, 91120 Palaiseau,France, 3CNRS, CRIStAL (UMR 9189, University of Lille), 59655 Villeneuve d’Ascq, France and 4Bonsai project, InriaLille-Nord Europe, 59655 Villeneuve d’Ascq, France*To whom correspondence should be addressed.AbstractMotivation: Kinetics is key to understand many phenomena involving RNAs, such as co-transcriptional folding and riboswitches. Exact out-of-equilibrium studies induce extreme computa-tional demands, leading state-of-the-art methods to rely on approximated kinetics landscapes, ob-tained using sampling strategies that strive to generate the key landmarks of the landscape top-ology. However, such methods are impeded by a large level of redundancy within sampled sets.Such a redundancy is uninformative, and obfuscates important intermediate states, leading to anincomplete vision of RNA dynamics.Results: We introduce RNANR, a new set of algorithms for the exploration of RNA kinetics land-scapes at the secondary structure level. RNANR considers locally optimal structures, a reduced setof RNA conformations, in order to focus its sampling on basins in the kinetic landscape. Along withan exhaustive enumeration, RNANR implements a novel non-redundant stochastic sampling, andoffers a rich array of structural parameters. Our tests on both real and random RNAs reveal thatRNANR allows to generate more unique structures in a given time than its competitors, and allowsa deeper exploration of kinetics landscapes.Availability and implementation: RNANR is freely available at https://project.inria.fr/rnalands/rnanr.Contact: yann.ponty@lix.polytechnique.fr1 IntroductionRiboNucleic Acids (RNAs) are fascinating biopolymers. Beyondtheir coding capacities, they can serve as a medium for the transmis-sion of genetic information, as in the case of highly structured RNAviruses such as Ebola or HIV (Wilkinson et al., 2008). They can alsoperform a large diversity of catalytic and regulatory functions, asdemonstrated by the 2474 functional families found in the currentrelease of the RFAM database (Nawrocki et al., 2015). This versatil-ity is such that RNA is currently considered by a whole scientificcommunity as the most parsimonious explanation for the molecularbasis of the origin of life (Cech, 2015). This versatility, coupled withthe combinatorial specificity of its interactions with other nucleicacids, makes RNA a tool of choice for designing nanoarchitecturesthrough programmable self-assembly (Li et al., 2011), or in theblooming field of synthetic biology (Kushwaha et al., 2016).A substantial proportion of the functions performed by RNAscritically relies on the adoption of a stable 3D structure through apairing of its nucleotides, mediated by hydrogen bonds. A precisestructural modeling of RNA structure, possibly in interaction withother molecules, is thus required to identify binding and catalyticsites, and more generally formulate functional hypotheses Cruz andWesthof (2011). Despite recent progress, such as SHAPE chemistry(Smola et al., 2015), experimental techniques for RNA structureresolution are still lagging behind high-throughput sequencing tech-niques, leading to a striking asymmetry between the amount ofavailable structure and sequence data. It is a current challenge ofRNA bioinformatics, and the object of ongoing efforts for a wholecommunity, to accurately predict the structure of RNA from its se-quence by integrating data of various origins (Miao et al., 2015).RNA folding is inherently stochastic, and governed by the lawsof statistical physics (McCaskill, 1990). It is generally believed to behierarchical (Tinoco and Bustamante, 1999) which, in conjunctionwith intrinsic computational limitations (Akutsu, 2000; Sheikhet al., 2012), has led to an initial dismissal of complex topologicalmotifs such as pseudoknots within computational methods(Isambert, 2009). The seminal work of McCaskill (1990) hasVC The Author 2017. Published by Oxford University Press. i283This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i283–i292doi: 10.1093/bioinformatics/btx269ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 2018demonstrated the computability in polynomial-time of the partitionfunction and the subsequent derivation of base-pairing probabilitieswhich provide realistic notion of supports for predicted base pairs(Mathews, 2004).However the assumption of a thermodynamic equilibrium failsto account for the observed behavior of certain RNAs, whichstrongly suggests the prevalence of kinetics effects in their foldingprocess. Perhaps the most prominent example can be found in ribos-witches (Baumstark et al., 1997; Schultes and Bartel, 2000), RNAsthat have been found to adopt different conformations dependingon the presence/absence of a ligand, despite a significant differencein free energies between the two conformers. This is hardly compat-ible with the assumption of a thermodynamic equilibrium, whichwould dictate the main adoption of the Minimal Free Energy (MFE)structure regardless of the presence of the ligand. This is howeverconsistent with a kinetics-inspired model, where the ligand modu-lates an energy barrier separating the two conformers in the foldinglandscape, modifying the convergence speed towards the thermo-dynamic equilibrium (Badelt et al., 2015). The prevalence of kineticseffects can also be suspected in instances of co-transcriptional fold-ing (Watters et al., 2016), or when transcripts undergo a fast degrad-ation and the half-life of some transcript are much shorter than thetime taken to converge towards the thermodynamic equilibrium(Sharova et al., 2009).Computational methods for the study of RNA kinetics essen-tially fall into two categories. A first category of methods, dubbedsimulation methods, perform a stochastic simulation of the foldingprocess at the base-pair (Flamm et al., 2000, Kinfold) or helix(Danilova et al., 2006, RNAKinetics) step resolution, possiblyallowing for the presence of pseudoknots (Xayaphoummine et al.,2007, kinefold). Sets of generated folding trajectories are analyzedand main conformers, along with the evolution of their concentra-tions through time, is easily obtained.However, as noted in Flamm and Hofacker (2008), the numberof trajectories required to obtain reproducible results quickly be-comes prohibitively large as the size of RNAs increases. For this rea-son, a second type of computational methods analyze RNA kineticsas a continuous Markov process, adopting a general four-steps pro-gram: Generation of a representative subset of conformations;Embedding of representative conformations into adjacency struc-ture, whose main alternatives are barrier trees used by barrier(Flamm et al., 2002), and the basin hopping graphs of BHGBuilder(Kucharik et al., 2014); Estimation of transition rates from (ap-proximate) energy barriers. The exact computation of this quantityrequires solving an NP-hard problem (Ma nuch et al., 2011), andavailable methods rely on direct path heuristics (Morgan and Higgs,1998), or on upper bounds based on 2D projections of folding land-scapes (Lorenz et al., 2009; Senter et al., 2015); Analysis of the evo-lution of concentrations through time, typically through numericalintegration as provided by treekin (Wolfinger et al., 2004).The present work pertains to the generation step, arguably themost critical aspect of kinetics analysis. A first category ofapproaches, such that RNASLOpt (Li and Zhang, 2011) andRNAsubopt (Wuchty et al., 1999), rely on an exhaustive enumer-ation of suboptimal structures within some predefined energy dis-tance of the MFE. Popular alternatives, such as RNALocmin(Kucharik et al., 2014) and RNALocopt (Lorenz and Clote, 2011),rely on some variation on Boltzmann-Gibbs sampling. Kuchariket al. (2014) have noted the difficulties of existing approaches rely-ing on sampling to generate unique conformations, leading to theadoption by RNALocmin of an adaptive heuristics similar tosimulated-annealing called n-scheduling to increase the samplediversity as the Bolzmann ensemble of low-energy becomes satu-rated. Despite such specific efforts, as shown in Figure 1, the numberof distinct structures decreases as the number of sample increases,making it hard to reach alternative structures beyond a few kcal.mol 1of the MFE structure.2 ApproachWe introduce the concept of non-redundant sampling to study RNAkinetics, using locally optimal secondary structures as representativestructures. Working with a reduced conformation space allows tomitigate the limitations of existing (redundant) samplingapproaches. The problem of constructing all locally optimal second-ary structures was addressed in Saffarian et al. (2012). Here, we de-scribe an alternative generation algorithm which is efficient, andallows the specification of comprehensive structural restrictions(Section 2.2). We then define the first non-redundant sampling algo-rithm for locally optimal RNA structures (Section 2.3), allowing forthe exploration of RNA folding landscape. The two algorithms areimplemented within the standalone software RNANR.2.1 DefinitionsAn RNA sequence w is a nucleotide sequence of length n over the al-phabet fA; C; G; Ug. The symbol at position i is denoted by w½i . Asecondary structure S is set of pairs of positions in w, called basepairs, that are pairwise juxtaposed or nested. Specifically, if twobase pairs (i, j) and ðk; ‘Þ are such that i   k, then either i < j < k< ‘ or i < k < ‘ < j. This definition implies that a secondarystructure is non-crossing, or pseudoknot-free, and each position isinvolved in at most one base pair. As a consequence, it can beencoded by a dot-parenthesis expression, where each base pair is apair of matching brackets and unpaired positions are reprented by adot. We also require that each pair (i, j) in S is valid, i.e. fw½i ; w½j gis in ffA; Ug; fC; Gg; fU; Ggg. Given a base pair (x, y), we denote hpðx; yÞ the helix of length p stemming from (x, y): this is the set ofbase pairs fðx; yÞ; . . . ; ðx þ p   1; y   p þ 1Þg.2.1.1 Structural restrictionsThe set of secondary structures on a given sequence can be furtherrestricted by enforcing additional constraints, giving rise to morerealistic structures. Those restrictions include: a) the minimum helixFig. 1. Comparison of local minima production speed for the SV11 RNAswitch L07337_1 (115 nt). Experiment reproduced from Kucharik et al. (2014)i284 J.Mich alik et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 2018length a. In particular, isolated base pairs are forbidden for anyvalue a > 1; b) the maximum number of consecutive unpaired basesin the structure b; c) the maximum number of branches within amultiloop c   1, which defines the maximum allowed number ofoutermost base pairs within another base pair; d) the minimumlength of a hairpin loop h. These parameters are illustrated onFigure 2. The special case a¼1, b ¼ n; c ¼ n and h¼1 correspondsto the whole set of secondary structures.Subsequently, we will use S as a shorthand for this search space,i.e. the restriction of secondary structures that respect those param-eters. Within this search space, we define the neighborhood of a sec-ondary structure S as the subset of secondary structures on w thatcan be obtained by adding or removing a single base pair in S.2.1.2 Energy modelsFor a given secondary structure S on w, we associate a free energyES, computed with respect to a specific energy model. In this work,we consider two energy models. The first model is a simple base-pairing model where the energy is the number of base pairs. We callit the Nussinov model in the spirit of (Nussinov and Jacobson,1980), even if structural restrictions introduced in the precedingparagraph significantly reduce the set of secondary structures. Thesecond model is the 2004 version of the Turner thermodynamicmodel (Turner et al., 1988; Turner and Mathews, 2010). For a givenfree energy model, we say that a secondary structure is locally opti-mal if, and only if, it has minimal free energy within its neighbor-hood. We respectively denote by ‘N and ‘T the sets of locallyoptimal secondary structures (LOSSes) with respect to the Nussinovand Turner energy models, and will respectively refer to them asNussinov LOSSes and Turner LOSSes in the following.2.1.3 Thermodynamic conceptsFrom a free-energy model, one computes the Boltzmann factor Bs ofa secondary structure as Zs ¼ e EskT where Es is the free energy of s, kis the Boltzmann constant and T is the temperature in Kelvin. Thepartition function Z is obtained by summing the Boltzmann factorsof all the conformations in a set S:Z ¼Xs2SBs (1)The Boltzmann probability of a given structure s in S is then simplydefined as PðsÞ ¼ BsZ . Finally, given a set of structures T   S, we de-fine the coverage cðT Þ as the accumulated Boltzmann probability inthe set, also expressed ascðT Þ ¼Pt2T BtZ (2)2.2 Building LOSSes in the restrained Nussinov modelIn Saffarian et al. (2012), it is shown that locally optimal secondarystructures without structural restrictions can be built from substruc-tures that are maximal by juxtaposition, called here flat structures inshort. We elaborate on this idea in order to account for the expres-sive set of structural restrictions introduced in Section 2.1.2.2.1 Flat structuresFor any interval ½i; j  in w, a flat structure f is a sequence of juxta-posed (non-nested) helices which is maximal, meaning that it cannotbe completed by a valid base pair between the positions left access-ible by the helices. In other words, let x1; y1; . . . ;x‘; y‘ be positionsin w, such that i   x1 < y1 < . . . < x‘ < y‘   j and such that haðxk; ykÞ is a valid helix for each k, 1   k   ‘. This sequence of heli-ces is a flat structure f on ½i; j  if, and only if: for each secondarystructure S on ½i; j  containing f, if (x, y) is in S and not in f, then (x,y) is nested in (xk, yk) for some 1   k   ‘. We further assume that‘   c; yk   xk   2a   h and xkþ1   yk   b to meet the require-ments on b, d and h. We denote by  ‘k¼1haðxk; ykÞ such a flat struc-ture, assuming that x1 < . . . < x‘. We denote by F i;j the completeset of flat structures associated with the region ½i; j  in w, as illus-trated by Figure 3. When an interval ½i; j  is not associated with anyflat structure, we have F i;j ¼ feg (e is the empty flat structure) when-ever j   i þ 1   h, and F i;j ¼1 (empty set) otherwise. F i;j can becomputed using a dynamic programming scheme adapted fromSection 3.1.2, Theorem 2 in Saffarian et al. (2012).2.2.2 A grammar for ‘NNow comes the crucial observation that any locally optimal second-ary structure of ‘N can be built up from a set of flat structures, com-pleted with helix extensions as illustrated by Figure 4. Given a helixhpðx; yÞ of length p, an extension is the addition of the valid basepair ðx þ p; y   pÞ to form hpþ1ðx; yÞ. The dot-parenthesis notationsfor the set of all structures of ‘N associated with an RNA w can bemodeled as a context-free language generated by the grammarGw ¼ ðN; T; R; YÞ, where• N :¼ fAji;Hjj ; 1   i < j   ng is the set of non-terminal sym-bols. Aji represents all locally optimal substructures within theFig. 2. Graphical representation of the structural restrictions supported byRNANR. a is the lower bound on the size of helices. h is the minimum lengthof a hairpin. c limits the maximum number of branches within multiloops.Finally, b is the maximum number of nucleotides in unpaired regionsFig. 3. Examples of flat structures. For the structural parameters a¼2, d¼ 3,h¼1 and c¼ 3, the set F 1;18 of flat structures associated with the interval½1; 18  consists of the five flat structures f1    f5. Note that h2ð1; 7Þ h2ð8; 14Þdoes not meet the condition on c, as positions 15 and 18 are left unpaired,and thus is not a valid flat structureNon-redundant sampling for RNA kinetics i285Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 2018interval ½i; j , and Hji represents the choice between a helix exten-sion with the base pair (i, j) or starting a new substructure on thesame interval;• T :¼ fð; Þ; :g is the set of terminal symbols;• R is the set of production rules:Hji ! ðHj 1iþ1 Þ if ði; jÞ valid for w and j   i þ 1   h (P1)Hji ! Aji (P2)Aji ! :x1 iY1 k  ‘ða Hyk axkþa Þa :xkþ1 yk 1for each  ‘k¼1haðxk; ykÞ 2 F i;j such thatðx1 ¼ iÞ _ ðy1 ¼ jÞ whenever ði; jÞ ¼ ð1; nÞ;and where x‘þ1 ¼ j þ 1(P3)Using classic language notations,Yis the concatenation operatorand li denotes i   0 copies of the letter l. Aji represents all locally op-timal substructures within the interval ½i; j  (P3), and Hji representsthe choice between a helix extension with the base pair (i, j) (P1), orstarting a new substructure on the same interval (P2);• Y :¼ An1 is the start symbol.This grammar has Hðn2Þ non-terminal symbols and Hðn2 þPi;jjF i;jjÞ productions. The proof of its completeness with respect to ‘Ncan be adapted from the proof of Theorem 1 in Saffarian et al. (2012).The condition ðx1 ¼ iÞ _ ðy1 ¼ jÞ in P3 ensures that Gw is unam-biguous, and can be used as a conceptual template to derive other al-gorithms, e.g. to compute the partition function (Waldispühl andClote, 2007) or base-pairing probabilities. Here, we use it on tworelated applications: exhaustive enumeration of structures and non-redundant statistical sampling of structures. While the former can beobtained in a straightforward fashion, being the language of thegrammar, the latter is more involved and is the object of the nextsection.2.3 Non-redundant sampling algorithmThe large redundancy of stochastic sampling methods has been iden-tified by previous studies as one of the major shortcomings of exist-ing methods. Ah hoc heuristics, such as the n-scheduling techniqueinspired by simulated annealing, have been introduced to circum-vent such limitations, sometimes at the cost of a control over thesampled distribution (Kucharik et al., 2014). Here, we propose an-other approach based on an explicit avoidance of redundancy withinthe sampling, adapting principles introduced by Lorenz and Ponty(2013).For the sake of simplicity, we illustrate those ideas by describinga uniform sampling algorithm for structures of ‘N that are compat-ible with a given RNA sequence. Starting from the precomputed setsF i;j of flat structures, the algorithm computes the number of locallyoptimal structures for each interval ½i; j  using DP equations. Thoseequations are isomorphic to the productions of grammar Gw.hði; jÞ ¼P(hði þ 1; j   1Þ if ði; jÞ is valid;aði; jÞaði; jÞ ¼Qhaðxk ;ykÞ2f hðxk þ a; yk   aÞ;for all f 2 F i;j and when j   i þ 1   h:A stochastic backtrack, a concept independently introduced in enu-merative combinatorics (Denise et al., 2010; Flajolet et al., 1994)and RNA bioinformatics (Ding and Lawrence, 2003), can then beused to generate elements of ‘N uniformly. Such a procedure choo-ses at every step one of the possible productions of the grammar,with probability proportional to its contribution to the overall num-ber/weights of words. It considers a triplet (i, j, m), where ½i; j  is thecurrent interval and m is the current matrix, initially starting fromð1;n; aÞ. At each step, it proceeds as follows, depending on the valueof m:• m ¼ h: Choose aði; jÞ with probability aði; jÞ=hði; jÞ and backtrackover the triplet ði; j; aÞ, otherwise append the base-pair (i, j) to theoutput, and backtrack over ði þ 1; j   1; hÞ;• m ¼ a: Choose a flat structure f ¼ ðhaðxk; ykÞÞ‘k¼1 2 F i;j withprobabilitypf ¼Qhaðxk ;ykÞ2f hðxk þ a; yk   aÞaði; jÞ ;append all the helices in the chosen f to the output, and back-track over the triplets ðxk þ a; yk   a; hÞ (if any).Note that the sets F i;j are explicitly computed, and so are the proba-bilities pf of choosing any flat structure f during the backtrack. It isthus possible to order the flat structures in F i;j by decreasing prob-ability, leading to a substantial speed-up during the backtrack.2.3.1 Non-redundant samplingThe stochastic backtrack becomes much more involved in the pres-ence of a predefined set of forbidden structures, e.g. singled out toavoid redundancy, as the probabilities of the backtrack on disjointintervals can no longer be considered independent.As a minimal illustration, consider two intervals I1 and I2 wherelocal sets of substructures fS1; S01g and fS2; S02; S0 02g can be respect-ively chosen, leading to the generation of 6 different structures.Clearly, in the absence of forbidden sets, one simply needs to chooseuniformly within each set to draw each structure with probability1/6, i.e. in the uniform distribution. However, if a given combin-ation of structures has to be avoided, say S01S2, then the choice overI1 now influences the valid combinations, and thus the probabilities,of choosing a structure over I2. Namely, choosing S01 for I1 onlyallows access to 2 viable alternatives for I2, while choosing S1 for I1enables 3 alternatives over I2. In order to be uniform, a stochasticbacktrack must therefore choose S1 with probability 3/5, and S01with probability 2/5. Once chosen, the remaining choice is uniformover fS2; S02; S0 02g if S1 is chosen (prob.¼ 2=5   1=2 ¼ 1=5), or overfS02; S0 02g (prob.¼ 3=5   1=3 ¼ 1=5) if S01 is chosen.More generally, in order to pick local alternatives in a way that isconsistent with a predetermined distribution, one needs to access (orcompute) the overall mass of forbidden structures that can be gener-ated before and after the choice. The idea of Lorenz and Ponty (2013)consists in maintaining a dedicated data structure l, which enablesFig. 4. All locally optimal secondary structures of ‘N generated by the gram-mar Gw for the sequence, and using structural restrictions, described inFigure 3i286 J.Mich alik et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 2018efficient access to the overall count/weight lðTÞ of accessible forbid-den structures from the current state f the backtracking stack T.The modified non-redundant backtrack is similar in structure tothe classic one, but uses different derivation probabilities, startingfrom a stack T ¼ fð1; n; aÞg. Let NðTÞ :¼Qði;j;mÞ2T mði; jÞ denotethe overall number/weight of structures accessible from a givenstate. At each iteration, it extracts a triplet (i, j, m) from T:• If m ¼ h, choose aði; jÞ with probabilityaði; jÞ   NðT0Þ   lðT 0 [ fði; j; aÞgÞNðTÞ   lðTÞwhere T 0 :¼ T   fði; j; mÞgg and backtrack over T 0 [ fði; j; aÞg,or otherwise backtrack over T0 [ fði þ 1; j   1;hÞg, after adding(i, j) to the output;• If m ¼ a, choose a flat structure f ¼ ðhaðxk; ykÞÞ‘k¼1 2 F ai; j withprobabilitypf ¼Yjkjk¼1hðxk þ a; yk   aÞ   NðT 0Þ lðT 0 [ fðxl þ a; yl   a;hÞglÞNðTÞ   lðTÞwhere T0 :¼ T   fði; j;mÞgg, and backtrack over the stackT 0 [ fðxk þ a; yk   a;hÞgk after adding the chosen f to the output.In practice, both the data structure l and the N(T) can be updatedon-the-fly during the backtrack, so that the non-redundancy retainsthe same asymptotic complexity as the redundant one.2.3.2 Turner energy model and expressive structural restrictionsThe various contributions of the loops in the Turner energy modelcan easily be identified in the grammar. Namely, stacking pairs aregenerated by rule (P1), while rule (P2) generates terminal loops(hairpins) when f ¼1, internal loops and bulges when jf j ¼ 1, ormultiple loops when jf j > 1. Finally, the exterior loop correspondsto ði; jÞ :¼ ð1;nÞ.This enables the incorporation of Boltzmann weights, based on theTurner energy model as weights in each of the DP equations. The in-corporation of such weights during the stochastic (non-redundant)backtrack leads to an algorithm for Boltzmann sampling, whose detailsand (sketch of) proof of correctness are provided in Supp. Mat. 1.3 Materials and MethodsAll experiments were run on a laptop with Intel Core i7 5600 CPUequipped with a quad core at 2.6 GHz with 16 GB of RAM underUbuntu 16.04 LTS.RNANR was implemented in C, and is freely available. It inter-faces the RNALib, using the CþþAPI provided in the Vienna pack-age (Lorenz et al., 2011), to access the individual contributions ofthe 2004 version of the Turner energy model. The tests were doneon a version compiled with gcc using GNU99 standard.Gradient walks were performed used the move_gradient functionof the ViennaRNA package (Lorenz et al., 2011), which performs agradient descent in the Turner energy landscape and return one ofthe closest local minima.3.1 DatasetsThree datasets were considered in our validation effort. The uniformdataset consists in random, uniformly distributed, RNA sequencesof length from 10 to 140 nt increasing by 10 nt, with 50 samples perlength.In order to assess the characteristics of real RNAs, we also gath-ered the RNAStrand dataset, which consists of the 154 RNA se-quences of length between 120 and 170 nt downloaded fromRNAStrand database (Andronescu et al., 2008), filtering out un-defined symbols.Finally, since currently available kinetics data is too scarce toallow for a quantitative comparison of tools, we created a dataset of250 bistable sequences of length 100 nt. A bistable RNA sequencepresents two stable conformations differing by a sufficient number ofbase pairs. We generated random uniform sequences of length 100 nt,and retained only those whose most stable LOSS (MFE) had free-energy lower than -30 kcal.mol–1 according to RNAEval (Lorenzet al., 2011). Remaining sequences were then subjected to non-redundant sampling of 1000 LOSSes using RNANR. We finally keptthe sequences which, within the sampled set, featured an alternativemetastable LOSS, differing by  20 base-pairs from the MFE struc-ture, and having free-energy   5 kcal.mol–1 higher than the MFE.3.2 Program parametersUnless noted otherwise, the settings of the different programs usedin our comparisons are those described in this section.For RNANR, the default mode is that of non-redundant sam-pling, with 20 samples. The structural restrictions include a min-imum helix length a¼3, a minimal base pair distance h¼3, amaximal unpaired region length d¼7, and a maximal number ofhelices in multiloop b¼4.For RNALocopt the number of returned samples is the same andthe temperature was set to 310.15K.For RNASLOpt, the suboptimality percentage was set to 100%,meaning that all LOSSes with energy ranging between the MFE and0 are returned. Since we are interested in LOSSes independently oftheir stability, we set the barrier_cutoff to an arbitrarily large valueof 30 kcal.mol–1 to speed up the computations by avoiding the com-putation of the energy barriers. Likewise, the number of top stableLOSSes can be chosen arbitrarily, here its value is set to 3.RNALocmin was run using the second version of the built-inadaptative search script, referred to as asearch and coded in Python(Kucharik et al., 2014). The parameters used are those by default,i.e. 10 000, 10 and 0.1 respectively for the number of samples per it-eration, number of iterations and convergence parameter.3.3 Theoretical speed-up of non-redundant samplingWe propose a closed-form formula to quantify the speed-up factorinduced by non-redundant sampling, i.e. the average number of oc-currence of each unique samples. Let us consider a fixed sequence ofi unique structures, and let Ri be the number of structures, generatedby a redundant Boltzmann sampling algorithm, before returning anovel iþ1-th structure.It is easy to show (du Boisberranger et al., 2012) thatEðRiÞ ¼ 1  Xij¼1e EjkTZ ! ! 1(3)where Ej is the energy of j-th secondary structure, k the Boltzmannconstant and T the temperature in Kelvin. Since, for a given samplessequence, the Rj are independent, then the overall number of gener-ations needed to obtain k distinct elements via redundant samplingis given by TðkÞ :¼ k þPki¼0 EðRiÞ and the speed-up factor is simplyTðkÞ=k:Non-redundant sampling for RNA kinetics i287Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 20183.4 Time benchmarking RNANR against its competitorsWe reproduced, and report in Figure 1, the benchmark of Kuchariket al. (2014) which compares the rates at which different samplingmethods produce unique local minima. It focuses on the SV11L07337_1 RNA switch, a challenging 115 nt RNA whose landscapeis particularly deep and steep. For RNANR, the total number of gen-erated samples was 4000. RNALocmin was run using both first andsecond versions of asearch for maximum of 30 iterations. For bothtools, the sampled structures are not necessary Turner LOSSes, so agradient walk is performed, and duplicates were removed. The timespent by the gradient walk is added to the generation time in thebenchmark. Finally, the total number of samples generated byRNALocopt was set to 6 000 000. Other parameters were the sameas those specified above.3.5 Comparison of folding landscape analysis efficiencyTo compare the quality of sampled sets of structures, we consideredan artificial bistable dataset described in Section 3.1, and producedrepresentative sets of structures using the four programs mentionedin Section 3.2. We analyzed sampled sets using a standard kineticanalysis pipeline based on an estimation of energy barriers for eachpair of structures, followed by a numerical integration using treekin.For each sequence in the bistable dataset, each program wasused to generate nsam ¼50, 75 and 100 samples (output truncated ifnecessary). Gradient walks were performed to each sample set, andduplicated Turner LOSSes were removed. As a control, we alsoincluded the results of RNAsubopt -e, adjusting DE to return at leastnsam LOSSes. Next, we estimated the energy barriers using the singlepath heuristics implemented by the findpath tool (Flamm et al.,2001). Due to the high computational cost of this operation, we re-stricted it to pairs of states having base-pair distance at most bplim.For each sequence, the value of bplim was set in such a way thatlandscapes sampled by all tools were connected (with the possibleexception of RNAsubopt).We then used Arrhenius rule to estimate the transition rate ki!jfrom a structure i to a structure j aski!j ¼ e ðEBi!j  Ei ÞkT ;where EBi!j is the free energy of the barrier, Ei the free-energy ofstate i, k the Boltzmann constant and T the absolute temperature.The rate between unconnected LOSSes was set to be 0. These rateswere used to generate transition matrix. For each sampled set, weidentified a minimal free-energy (MFE) and a metastable LOSSes asthe most similar structures to the reference ones (see Section 3.1) forthe sequence.Finally, we considered a scenario where the starting concentra-tion of the metastable structure (or its closest neighbor in thesampled set) is set to 1. We used treekin to determine the evolutionof the concentration of all LOSSes in the sampled set. Finally, we re-port the switching time, i.e. the time at which the MFE structureeventually achieves higher concentration becomes more frequentthan the metastable structure.4 Discussion4.1 Validating Nussinov LOSSes as key landmarks ofkinetic landscapes4.1.1 Nussinov LOSSes are less numerous than Turner LOSSesWe compared the numbers of LOSSes returned by RNANR,RNALocopt and RNASLOpt. For each sequence in the uniformdataset, these sequences were subjected to runs of each software.For RNANR, we performed three runs, each with different value ofc (no limit, 4 and 3 respectively). For RNALocopt and RNASLOptwe performed one run for each with parameters as specified in 3.2.The values of each run were then aggregated by the sequence length.The results are shown on Figure 5.Figure 5A shows the evolution of number of LOSSes for differentprograms in function of sequence length. We observe thatRNASLOpt returns the smallest number of results. This is consistentwith the primary objective of Li and Zhang (2011) to reduce thenumber of structures as a way to reduce the complexity. Besides ag-gressive structural constraints, RNASLOpt returns only LOSSes thathave negative free energy, which is not the case for both RNANRand RNALocopt. On the other hand, RNANR presents a lowernumber of solutions than RNALocopt. This stems both from the re-duction of search space by RNANR due to the structural restric-tions, and to our focus on Turner LOSSes, while RNALocoptarguably considers a larger neighborhood (Lorenz and Clote, 2011).Our search space reduction, while not as aggressive as that ofRNASLOpt, leads to a substantial reduction of the complexity, boththeoretically and practically, as shown in Figure 5.Of interesting note is the comparison between the numbers offlat structures returned by RNANR for different values of c(Fig. 5B). While the number of flat structures noticeably decreasesfor lower values of c, the number of LOSSes does not seem to be par-ticularly affected (Fig. 5A). This could be explained by the fact theexcluded flat structures participate in few of the complete LOSSes,as substantiated by the fact that, for shorter sequences, the forma-tion of multiloops with high number of branches is improbable.This interpretation is consistent with an analysis of RNAStrandstructures (Fig. 6), which shows that very few (0.1%) RNAs oflength under 140 nt features multiloops of degree greater than 4branches. For shorter sequences, it thus seems reasonable to limit c,which results in lowered time complexity. Naturally, as shown byFigure 6, this ceases to be the case for longer sequences. While for se-quences shorter 300 nt the number of ignored structures for c¼4 isA BCFig. 5. Comparison of RNANR with RNALocopt and RNASLOpt. (A) Numberof structures returned by each program and in case of RNANR, for differentupper limits of c. (B) Number of flat structures returned by RNANR for differ-ent values of c. (C) Benchmark for different programs, for the same cases as(A)i288 J.Mich alik et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 2018still low (1.92%), for sequences shorter than 400 nt the number ofmultiloops with at least 5 branches accounts for 15.82% of all struc-tures. Overall, we found that setting c to 5 constitutes a reasonabletradeoff, reducing the computation time while keeping the numberof undetected LOSSes reasonable.4.1.2 Nussinov LOSSes are very close to turner LOSSesWe used gradient walks to determine how distant are NussinovLOSSes, output by RNANR, to their counterpart in the Turner en-ergy model. For each sequence in the RNAStrand dataset, a non-redundant sampling of 1000 distinct structures was performed byRNANR. These structures were then subject to a gradient descent,resulting in a Turner LOSS. We tracked the modifications, both interm of energy and base pairs, induced by the walk. Our results aresummarized in Table 1.Overall, 52.49% of the Nussinov LOSSes were already local min-ima with respect to the Turner energy model. Moreover, our analysisshows that, from a Nussinov LOSS, it is sufficient to add or remove anaverage of 0.703 base pairs, contributing an average 0.547kcal.mol–1,to reach a Turner LOSS. A further analysis reveals differing behaviorsof whether or not the Turner LOSS, obtained as the outcome of thegradient descent, belongs to the restricted search space.Finally, it is worth stressing that the structures obtained after thegradient walk are overwhelmingly unique (99.1%), suggesting ahomogenous coverage of the Turner LOSSes by the NussinovLOSSes. We thus conclude that the structures generated by RNANRcan reliably used as representatives for Turner LOSSes within thefolding landscape.4.2 Efficiency of sampling methods for LOSSesA first time benchmark, whose results are given on Figure 5C was per-formed on our uniform dataset. We observe that, without limiting c,RNANR is fastest for sequences of length under 80 nt. Around thislength, it is briefly matched by RNASLOpt, whose execution time in-creases spectacularly around 110 nt. RNALocopt is faster thanRNASLOpt or RNANR (no limitation on c) for longer sequences(more than 120 nt) due to the polynomial nature of the underlying al-gorithm. However, for values of c set to 3 or 4, the execution time ofRNANR becomes polynomial, and RNANR becomes considerablyfaster in practice than its competitors. Of course, one needs to exercisecaution before setting restrictions that could lead to the omission ofimportant conformations, and it would probably not be wise to setc¼3 for RNAs beyond 80 nt. However, setting c¼4 makes RNANRfaster than any other tested software, while only missing a negligibleproportion of existing conformations (cf Fig. 6, for multiloop branchnumber equal to 6) for sequences beyond 300 nt.A second basic time benchmark, described in Section 3.4 and inFigure 1, measures the rate of production of distinct Turner LOSSesgenerated using different software. We observe that RNANR returnsmore unique LOSSes in a given time, even when including the timefor precomputations and gradient descents, than both versions ofRNALocmin and RNALocopt (note that RNASLOpt does not per-form sampling). This is mainly due to the redundancy within the re-turned samples, as indicated by the diminishing production speedsfor both methods. The second version of asearch uses an alternativestrategy for n-scheduling which increases its number of samples lin-early between iterations, and seems to eventually perform betterthan its initial version. However it starts slower, proving that the re-dundancy is still an issue.4.3 Non-redundant sampling allows a deeperexploration of kinetic landscapesThe main new contribution of RNANR is its non-redundant sam-pling algorithm, which allows to obtain a set of locally optimalA BFig. 6. Rationale for our restricted search space. Effect of structural limitationson structure counts obtained from RNAStrand database (Andronescu et al.,2008). (A) Proportion of structures having maximal multi loop branchesbelow a given threshold. A branch number within multi loop is the number ofstems sorting from a multiloop and is equal to cþ1. (B) Proportion of heliceshaving at least given lengthTable 1. Discrepancy between Nussinov and Turner LOSSesSamples% DDG Base pair dist.Avg (SD) Avg (SD) Avg (SD)Within search space 59.57% (21.00) 0.071 (0.309) 0.129 (0.289)Outside search space 40.42% (21.00) 1.248 (0.925) 1.550 (0.619)Global average 100.00% (—) 0.547 (0.817) 0.703 (0.757)For our RNAStrand dataset, 1000 Nussinov LOSSes were generated. Foreach structure, one of the closest Turner LOSS was determined using a gradi-ent descent. On average, a Nussinov LOSS is distant by  0:547 kcal.mol–1,and by 0.7 base pairs, from its closest Turner LOSS.A BC DFig. 7. Comparison of classical and non-redundant sampling. (A) Theoreticalspeed-up T(k)=k using non-redundant sampling when compared to redundantsampling for a 5S ribosomal RNA of Thermoplasma acidophilum (123 nt). (B)Same test for a telomerase RNA of Tetrahymena silvana (154 nt). Purplepoints indicate the coverage of 1   e, for e ¼ 5%; 1% and 0.1% respectively. (C)Number of unique LOSSes and coverage c from 1000 samples generated byRNALocopt on our uniform artificial dataset. The number of unique LOSSes,generated using RNANR in order to achieve a coverage c, is plotted in (D)Non-redundant sampling for RNA kinetics i289Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 2018secondary structures, each of whom appears at most once. Thismethod of sampling has its advantages which will be discussed inthis and next subsection. The more obvious one, discussed here, isthe fact it allows to obtain higher number of different samples faster.Instead of sampling same structures with free energy close to theminimal free-energy over and over, it consecutively picks up thestructures with higher energies which were not sampled previously.To demonstrate this point, we first computed the value of speed-up T(k)=k as defined in Section 3.3 on two sequences: a 123 nt 5Sribosomal RNA of Thermoplasma acidophilum, and a 154 nt tel-omerase RNA of Tetrahymena silvana. A non-redundant samplingwas performed, until a coverage of 0.99% was achieved, and theevolution of the speed-up was calculated. The results are shown onFigure 7A and B. The purple dots mark the points with c ¼ 1   e fore ¼ 5%, 1% and 0.1% for the final coverage.The value of T(k)=k increases with k, which is expected since theprobability of generating a novel structure decreases with each iter-ation. The speed-up thus becomes more important for higher valuesof c (or lower values of  ), where the probability of a new uniquestructure almost vanishes. This means that the structures with higherfree energies are considerably easier to generate by non-redundantsampling than by their redundant counterpart, since their initialprobability increases along with the sampling. Moreover, differentsequences exhibit different evolutions for c, as shown on Figure 7Aand B, depending on the concentration of the Boltzmann distribu-tion around a few conformations. This means that higher values of cwill be more difficult to attain for some sequences than for others,and for these cases the usage of non-redundant sampling mightprove more advantageous.Our second test consisted in creating the samples using redun-dant generation, and comparing these results with the non-redundant sampler of RNANR. For this purpose, we usedRNALocopt to generate 1000 LOSSes for each sequence of our uni-form dataset. For each set of structures, the number of unique sam-ples and the coverage c were computed. RNANR was then used togenerate a set of structures achieving the same coverage c. The aver-aged number of structures for each length was reported, leading tothe values shown in Figure 7C and D.While the number of unique structures returned by RNALocoptincreases with the sequence length, the coverage value c diminishes.This is caused by the increasing number of local minima for longersequences, which results in lower Boltzmann probabilities for indi-vidual structures. This means that, to attain a given coverage c, asharply increasing number of LOSSes must be generated for longersequences, leading to a higher number of repeats. Figure 7D showsthat the number of samples necessary to achieve identical c byRNANR as the one achieved by RNALocopt is considerably lower.This partially stems from the fact that while RNALocopt encom-passes the entirety of Turner LOSSes, RNANR explores a muchmore drastically reduced search space, and thus requires less samplesto achieve a given coverage. On the other hand, this also means thata target coverage is achieved faster while generating most of interest-ing structures with the correct parameter settings, meaning thatRNANR can considerably speed up and simplify the analysis of thefolding landscape.A BFig. 8. Efficiency of analysis performed by existing sampling software on bi-stable structures. (A) Proportion of artificial bistable sequences for whichstructures close to the reference states are generated. (B) Distribution of best-performing software, as assessed by the lowest predicted switch time, for dif-ferent values of nsamABCDEFig. 9. Illustration of bistable RNA analysis. Starting from an artificial 100 nt bistable RNA (A), metastable (B) and MFE (D) states are identified, along with keylandmarks of the kinetic landscapes, using the non-redundant sampling of RNANR (C, left) and RNASLOpt (C, right). Due to the stochastic nature of the landscapereconstruction methods, the metastable structure identified by RNANR has 3 extra base pairs (B, colored in red). Numerical integration of the master equationusing Treekin (D) predicts a shorter switching time for RNANR kinetics landscape than its competitors, suggesting a better coverage of important kinetic inter-mediate structuresi290 J.Mich alik et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 20184.4 Non-redundant sampling of Turner LOSS enables afaster, more accurate analysis of RNA kineticsThe main interest of non-redundant sampling is, besides its inherentspeed-up, its capacity to dig deeper within the space of suboptimalstructures when approximating the folding landscape of a givenRNA. In this final validation, we tested whether this increased diver-sity translates into sampled sets of higher quality, leading to moreaccurate kinetics analysis.More specifically, we evaluated the capacity of a simple, real-life, analysis pipeline to estimate the switching time of bistable artifi-cial RNAs from samples of small size, generated using RNANR,RNALocmin, RNASLOpt, RNALocopt and RNAsubopt. Detailsare described in Section 3.5, and the results are summarized byFigure 8, and illustrated by Figure 9.First, we observe that RNANR detects structures that are similarto both the MFE and metastable LOSSes more consistently than itscompetitors. This is not overly surprising, since these two structureswere initially identified from an independent execution of RNANR(albeit from a much larger sampled set, see Section 3.1). This howevermeans that RNANR generates sets of structures that, while notstrictly overlapping, represent the main dominant conformations evenfor small sampled sets, and may be used for reproducible further ana-lysis. RNALocmin and RNALocopt both suffered from redundancy,and generally failed to identify the two dominant conformation forabout  80% of the bistable RNAs. RNASLOpt exhibit the sametend, probably due to an aggressive filtering of LOSSes.Then we compared the switching time, defined in Section 3.5, aspredicted by our pipeline from the different sampled sets. We rea-soned that, since both undersampled landscapes and single-pathheuristics lead to an overestimation of energy barriers, a goodsampled set, by populating important energy basins, would be asso-ciated to a fast perceived kinetics, i.e. a fast switching time. On theother hand, a set of scattered, or highly similar, structures wouldpractically disconnect the landscape, leading to slow predicted kin-etics. Faster predicted switching times therefore indicate better ap-proximate folding landscapes.Our results, summarized in Figure 8, show that using RNANRleads to the shortest switching time  70% of the time, irrespec-tively of the number of samples. RNASLOpt is a clear second, anddominates other tools with respect to the switching time in about20% of the sequences. While high computational demands, onboth the design and analysis tasks, disallowed us to repeat this onlarger bistable sequences, we expect this trend to carry for largersequences and sets.AcknowledgementsThe authors wish to thank Ronny Lorenz and Gregor Entzian for their invalu-able help in interfacing the versatile Vienna RNAlib, and Ivo Hofacker for hisfeedback and suggestions at an earlier stage of this work.FundingAll authors acknowledge support from the RNALands project, jointly fundedby the French Agence Nationale de la Recherche (ANR-14-CE34-0011) andthe Austrian Fonds zur Förderung der wissenschaftlichen Forschung (FWF-I-1804-N28).Conflict of Interest: none declared.ReferencesAkutsu,T. (2000) Dynamic programming algorithms for RNA secondarystructure prediction with pseudoknots. Discrete Appl. Math., 104, 45–62.Andronescu,M. et al. (2008) RNA strand: the RNA secondary structure andstatistical analysis database. BMC Bioinf., 9, 340.Badelt,S. et al. (2015) Chapter eight – thermodynamic and kinetic folding ofriboswitches. In: Chen,S.-J. and Burke-Aguero,D.H. (eds.) ComputationalMethods for Understanding Riboswitches, Volume 553 of Methods inEnzymology. Academic Press, Cambridge, MA, USA, pp. 193–213.Baumstark,T. et al. (1997) Viroid processing: switch from cleavage to ligationis driven by a change from a tetraloop to a loop e conformation. EMBO J.,16, 599–610.Cech,T.R. (2015) RNA world research-still evolving. RNA (New York, N.Y.),21, 474–475.Cruz,J.A. and Westhof,E. (2011) Sequence-based identification of 3d struc-tural modules in RNA with rmdetect. Nat. Methods, 8, 513–521.Danilova,L.V. et al. (2006) RNAkinetics: a web server that models secondarystructure kinetics of an elongating RNA. J. Bioinf. Comput. Biol., 4,589–596.Denise,A. et al. (2010) Controlled non-uniform random generation of decom-posable structures. Theoret. Comput. Sci., 411, 3527–3552.Ding,Y. and Lawrence,E. (2003) A statistical sampling algorithm for RNA sec-ondary structure prediction. Nucleic Acids Res., 31, 7280–7301.du Boisberranger,J. et al. (2012). The weighted words collector. In: 23rdIntern. Meeting on Probabilistic, Combinatorial, and Asymptotic Methodsin the Analysis of Algorithms, AofA, volume AQ of DMTCS Proceedings,pp. 243–264.Flajolet,P. et al. (1994) Calculus for the random generation of labelled com-binatorial structures. Theoret. Comput. Sci., 132, 1–35. A preliminary ver-sion is available in INRIA Research Report RR-1830.Flamm,C. and Hofacker,I.L. (2008) Beyond energy minimization: approachesto the kinetic folding of RNA. Monatshefte Für Chemie Chem. Mon., 139,447–457.Flamm,C. et al. (2000) RNA folding at elementary step resolution. RNA (NewYork, N.Y.), 6, 325–338.Flamm,C. et al. (2001) Design of multistable RNA molecules. RNA (NewYork, N.Y.), 7, 254–265.Flamm,C. et al. (2002) Barrier trees of degenerate landscapes. Zeitschrift FürPhysikalische Chemie, 216, 155.Isambert,H. (2009) The jerky and knotty dynamics of RNA. Methods (SanDiego, Calif.), 49, 189–196.Kucharik,M. et al. (2014) Basin hopping graph: a computational frameworkto characterize RNA folding landscapes. Bioinformatics (Oxford, England),30, 2009–2017.Kushwaha,M. et al. (2016) Using RNA as molecular code for programmingcellular function. ACS Synth. Biol., 5, 795–809.Li,H. et al. (2011) Nucleic acid-based nanoengineering: novel structures forbiomedical applications. Interface Focus, 1, 702–724.Li,Y. and Zhang,S. (2011) Finding stable local optimal RNA secondary struc-tures. Bioinformatics (Oxford, England), 27, 2994–3001.Lorenz,R. et al. (2009). 2d projections of RNA folding landscapes. InGrosse,I. et al. (eds.) German Conference on Bioinformatics 2009, Volume157 of Lecture Notes in Informatics. Gesellschaft f. Informatik, Bonn, pp.11–20.Lorenz,R. et al. (2011) ViennaRNA package 2.0. Algorithms Mol. Biol. AMB,6, 26.Lorenz,W.A. and Clote,P. (2011) Computing the partition function for kinet-ically trapped RNA secondary structures. PLoS One, 6, e16178.Lorenz,W.A. and Ponty,Y. (2013) Non-redundant random generation algo-rithms for weighted context-free grammars. Theoret. Comput. Sci., 502,177–194.Ma nuch,J. et al. (2011) Np-completeness of the energy barrier problem with-out pseudoknots and temporary arcs. Nat. Comput., 10, 391–405.Mathews,D.H. (2004) Using an RNA secondary structure partition functionto determine confidence in base pairs predicted by free energy minimization.RNA (New York, N.Y.), 10, 1178–1190.Non-redundant sampling for RNA kinetics i291Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 2018McCaskill,J.S. (1990) The equilibrium partition function and base pair bindingprobabilities for RNA secondary structure. Biopolymers, 29, 1105–1119.Miao,Z. et al. (2015) RNA-puzzles round ii: assessment of RNA structure pre-diction programs applied to three large RNA structures. RNA (New York,N.Y.), 21, 1066–1084.Morgan,S.R. and Higgs,P.G. (1998) Barrier heights between ground states in amodel of RNA secondary structure. J. Phys. A Math. Gen., 31, 3153.Nawrocki,E.P. et al. (2015) Rfam 12.0: updates to the RNA families database.Nucleic Acids Res., 43, D130–D137.Nussinov,R. and Jacobson,A.B. (1980) Fast algorithm for predicting the sec-ondary structure of single-stranded RNA. Proc. Natl. Acad. Sci. U. S. A.,77, 6309–6313.Saffarian,A. et al. (2012) RNA locally optimal secondary structures.J. Comput. Biol. J. Comput. Mol. Cell Biol., 19, 1120–1133.Schultes,E.A. and Bartel,D.P. (2000) One sequence, two ribozymes: implica-tions for the emergence of new ribozyme folds. Science (New York, N.Y.),289, 448–452.Senter,E. et al. (2015) RNA folding pathways and kinetics using 2d energylandscapes. J. Math. Biol., 70, 173–196.Sharova,L.V. et al. (2009) Database for mRNA half-life of 19977 genes obtainedby DNA microarray analysis of pluripotent and differentiating mouse embryonicstem cells. DNA Res. Int. J. Rapid Publ. Rep. Genes Genomes, 16, 45–58.Sheikh,S. et al. (2012). Impact of the energy model on the complexity of RNAfolding with pseudoknots. In: K€arkk€ainen,J. and Stoye,J. (eds.)Combinatorial Pattern Matching, Volume 7354 of Lecture Notes inComputer Science. Springer, Berlin, Heidelberg, pp. 321–333.Smola,M.J. et al. (2015) Selective 2’-hydroxyl acylation analyzed by primerextension and mutational profiling (shape-map) for direct, versatile and ac-curate RNA structure analysis. Nat. Protoc., 10, 1643–1669.Tinoco,I. and Bustamante,C. (1999) How RNA folds. J. Mol. Biol., 293,271–281.Turner,D.H. and Mathews,D.H. (2010) Nndb: the nearest neighbor param-eter database for predicting stability of nucleic acid secondary structure.Nucleic Acids Res., 38, D280–D282.Turner,D.H. et al. (1988) RNA structure prediction. Annu. Rev. Biophys.Biophys. Chem., 17, 167–192.Waldispühl,J. and Clote,P. (2007) Computing the partition function andsampling for saturated secondary structures of RNA, with respect to theturner energy model. J. Comput. Biol. J. Comput. Mol. Cell Biol., 14,190–215.Watters,K.E. et al. (2016) Cotranscriptional folding of a riboswitch at nucleo-tide resolution. Nat. Struct. Mol. Biol., 23, 1124–1131.Wilkinson,K.A. et al. (2008) High-throughput shape analysis reveals struc-tures in hiv-1 genomic RNA strongly conserved across distinct biologicalstates. PLoS Biol., 6, e96.Wolfinger,M.T. et al. (2004) Efficient computation of RNA folding dynamics.J. Phys. A Math., 37,Wuchty,S. et al. (1999) Complete suboptimal folding of RNA and the stabilityof secondary structures. Biopol, 49, 145–164.Xayaphoummine,A. et al. (2007) Encoding folding paths of RNA switches.Nucleic Acids Res., 35, 614–622.i292 J.Mich alik et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i283/3953986by gueston 07 January 2018
5028882000002	PMID28882000	5028882000	https://watermark.silverchair.com/btx268.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28882000.main.pdf	Bioinformatics, 33, 2017, i274–i282doi: 10.1093/bioinformatics/btx268ISMB/ECCB 2017Integrative deep models for alternative splicingAnupama Jha1, Matthew R. Gazzara1,2,3 and Yoseph Barash1,2,*1Department of Computer and Information Science, School of Engineering, 2Department of Genetics andDepartment of Biochemistry and Biophysics, Perelman School of Medicine, University of Pennsylvania,Philadelphia, PA 19104, USA3*To whom correspondence should be addressed.AbstractMotivation: Advancements in sequencing technologies have highlighted the role of alternativesplicing (AS) in increasing transcriptome complexity. This role of AS, combined with the relation ofaberrant splicing to malignant states, motivated two streams of research, experimental and computational. The ﬁrst involves a myriad of techniques such as RNA-Seq and CLIP-Seq to identifysplicing regulators and their putative targets. The second involves probabilistic models, alsoknown as splicing codes, which infer regulatory mechanisms and predict splicing outcome directlyfrom genomic sequence. To date, these models have utilized only expression data. In this work, weaddress two related challenges: Can we improve on previous models for AS outcome predictionand can we integrate additional sources of data to improve predictions for AS regulatory factors.Results: We perform a detailed comparison of two previous modeling approaches, Bayesian andDeep Neural networks, dissecting the confounding effects of datasets and target functions. Wethen develop a new target function for AS prediction in exon skipping events and show it signiﬁcantly improves model accuracy. Next, we develop a modeling framework that leverages transferlearning to incorporate CLIP-Seq, knockdown and over expression experiments, which are inherently noisy and suffer from missing values. Using several datasets involving key splice factors inmouse brain, muscle and heart we demonstrate both the prediction improvements and biologicalinsights offered by our new models. Overall, the framework we propose offers a scalable integrative solution to improve splicing code modeling as vast amounts of relevant genomic data becomeavailable.Availability and implementation: Code and data available at: majiq.biociphers.org/jha_et_al_2017/Contact: yosephb@upenn.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionA key contributor to transcriptome complexity is alternative splicing(AS): the joining together of different exonic segments of a premRNA to yield different gene isoforms. The most common type ofAS event in human and mouse is exon skipping where a fraction ofthe mRNA produced include an exon while others skip it.Thousands of such variations were found to be highly conserved andcommon between tissues. Overall, more than 90% of human multiexon genes are alternatively spliced (Pan et al., 2008; Wang et al.,2008) and splicing defects have been associated with numerous diseases. This has motivated detailed studies of AS variations across tissues, development stages and malignant states (Scotti and Swanson,2016). These studies monitor mRNA expression at exonic resolutionusing RNA-Seq in a variety of experimental conditions, includingknockdown (KD), knockout (KO) or over-expression (OE) of condition specific splicing factors (SF). Other experiments monitorbinding affinity of splice factors using several similar protocolsinvolving UV cross-linking of the factor to the RNA, followed byimmunoprecipitation and sequencing of the bound RNA fragments(CLIP-Seq).In parallel, the fact that splicing outcome is highly condition specific and regulated by many factors led to an effort to computationally derive predictive ‘splicing codes’: models that use putativeregulatory features (e.g. sequence motifs, secondary structure) topredict splicing outcome in a condition specific manner (e.g. braintissue; Barash et al., 2010a; Barash et al., 2010b). Concentrating oncassette exons, the most common form of AS in mammals, thesemodels aimed to predict percent splicing inclusion (PSI, W) of the alternative exon, or changes of its inclusion (dPSI, DW). Such modelshave been used successfully to identify novel regulators of key genesin disease associated genes, and predict the effect of genetic variations on splicing outcome (Gazzara et al., 2014; Xiong et al., 2015;CV The Author 2017. Published by Oxford University Press.i274This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018Integrative deep models for alternative splicingVariance explained in PSIBNN-UDC% Variance explained by PSI prediction (R2)Sotillo et al., 2015). However, given the sharp growth in sequencingdata, two main questions are: Can we leverage the new CLIP-Seqand splice factors KD/OE experiments and more generally, can weimprove on current splicing code models?Previous work has shown that Bayesian Neural Networks compare favorably to a plethora of other modeling approaches includingK-Nearest Neighbors, Support Vector Machine, Naive Bayes andDeep Neural Networks with dropouts (Xiong et al., 2011;Srivastava et al., 2014). Specifically, (Srivastava et al., 2014)described dropout as performing an approximation to the BNNBayesian model averaging, and pointed to the latter as being advantageous for smaller datasets. However, later work using a DeepNeural Network with an autoencoder demonstrated improved performance compared to a BNN model (Leung et al., 2014). Notably,these different works used different datasets and mixed the effect ofmodeling framework (BNN versus DNN) with changes of the targetfunction. Thus, in this work we reconstructed previous BNN andDNN models on the original dataset from (Leung et al., 2014) to establish a baseline. Afterwards, we monitored the effect of a new target function, of increasing dataset size by exploiting improvementsin RNA-Seq quantification algorithms (Vaquero-Garcia et al.,2016), and adding new types of experimental data.The first contribution of this work is in developing a new targetfunction for splicing code models. Due to limitations of both available data and algorithms, previous works were unable to predict Wor DW directly. Instead, they formulated a three way predictionPtask fps j0ps1; s ps ¼ 1g for any exon e in each condit;et;et;etion t. In the original formulation, s represented the chances forincreased inclusion, exclusion or no change for exon e in conditiont, compared to a hidden baseline of inclusion inferred from a set of27 tissues (Barash et al., 2010a). This formulation allowed thelearned model to concentrate its predictive power on tissue regulated exons, using a dedicated sparse factor analysis model to identify those exons from noisy micro-array data (Barash et al.,2010b). Subsequently, the same target function formulation wasused, but instead of inferring splicing changes, s now representedbinning of W values into three levels: ‘Low’ (0W < 33%),‘Medium’ (33%W < 66%) and ‘High’ (66%W100%).While useful, these target functions are inherently unsatisfying asan approximation to the underlying biological variability. Here,we develop a new target function which models W directly, anddemonstrate its improved accuracy compared to previousapproaches. Serving as a baseline, Figure 1 depicts the improvement in percent variance explained in W by the new model compared to previous BNN and DNN on the original dataset used by(Leung et al., 2014).The second contribution of this work is developing a framework to integrate additional types of experimental data into thesplicing code models. Specifically, CLIP-Seq based measurementsof in vivo splice factors binding are turned into an additional set ofinput features while knockdown and over-expression experimentsare added with binary vectors coding the tissue and splice factor(if any) measured. A graphical representation of the old and thenew model architectures is given in Figure 2. We demonstrate theeffect of the new integrative modeling approach using a set ofCLIP-Seq, knockdown and overexpression experiments for members of the RBFOX, CELF and MBNL family of splicing factorsin mouse heart, muscle and brain. Finally, we showcase some ofthe possible biological usage cases for these splicing code modelsfor accurate in silico prediction of splice factor KO effect, and foridentifying novel regulatory interplay between different splicefactors.i275DNN-LMHDNN-PSIBrainHeartKidneyLiverTestisTissueFig. 1. Improvement in percent variance explained by the new target function(green bars) compared to previous BNN (blue bars) and DNN (red bars)models on the tissue data used by (Leung et al., 2014)2 Materials and methods2.1 DatasetsTwo RNA-Seq datasets were processed for this work. One, denotedFive Tissue Data, is the RNA-Seq data from five mouse tissues(brain, heart, kidney, liver and testis) produced by (Brawand et al.,2011). This dataset was used in the (Leung et al., 2014) paper andthus to ensure that we can accurately reconstruct their models, weuse it to compare the old and new models. We generated genomicfeatures and PSI quantification for $12 000 cassette exons used in(Barash et al., 2013) for this dataset for the five tissues using MAJIQ(Vaquero-Garcia et al., 2016) and AVISPA (Barash et al., 2013).The second dataset, denoted MGP Data, was prepared by (Keaneet al., 2011) and it contains RNA-Seq data from six tissues (heart,hippocampus, liver, lung, spleen and thymus) with average readcoverage of 60 million reads. To this data we added 15 CLIP-Seq experiments (see Supplementary Table S10). Together, these datasetshighlight some of the challenges involved in utilizing such diverseexperiments. First, CLIP-Seq experiments give noisy measurementof where a splice factor binds. The measurements are noisy sincebinding signal (reads aligning to a certain area) may be false positives, may not indicate active regulation and may suffer from falsenegatives due to low coverage, indirect binding, antibody sensitivity,etc. Moreover, these experiments are typically executed by differentlabs, in different conditions and at varying levels of coverage. Thus,it is crucial that any learning framework that we develop should beable to handle missing and noisy measurements.In our learning setting, the CLIP-Seq data is turned to input features indicating possible binding in a region proximal to the alternative exon (e.g. upstream intron). Since CLIP-Seq measurements areinherently noisy and suffer from different coverage levels we abstractthem as binary indicators of binding in the various regions of interestaround our alternative cassette exons. The target in our problem formulation is the relative exon inclusion level in a given experiment, expressed as percent spliced in (PSI, W 2 ½0; 1 ). W serves to capture theproportion of isoforms that include the alternative cassette exon versus those that skip it. But since these are not observed directly, theshort sequencing reads are used to estimate these values. Specifically,we apply MAJIQ (Vaquero-Garcia et al., 2016) to derive posteriorDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018i276A.Jha et al.Bayesian Neural NetworkGenomicFeatures HiddenLayerGenomicFeaturesAutoencoderLayerHiddenLayer 1HiddenLayer 2OutputLayerLowMediumHighLowMediumHighInclusionExclusionNo ChangeOutputLayerLowMediumHighLowMediumHighInclusionExclusionNo ChangeTissue 1Tissue 2Tissue 1Tissue 2Tissue 1Tissue 2ChangeTissue 1Tissue 2ChangeIndex fortissue 1Index fortissue 2New Deep Neural NetworkNew Model For Knockdown DataGenomicFeaturesAutoencoder HiddenLayerLayer 1GenomicFeaturesAutoencoder HiddenLayerLayer 1HiddenLayer 2HiddenLayer 2OutputLayerOutputLayerTCLIPFeaturesTe,ce,cCLIPFeaturesIndex fortissue 1Index fortissue 2Index for KD/OEIndex for tissue 1Index for tissue 2Index for KD/OEFig. 2. Architecture of the Bayesian Neural Network, Deep Neural Network used by (Leung et al., 2014) referred as Leung’s Deep Neural Network, new DeepNeural Network models for tissue data and for splice factor Knockdown/Overexpression data. Green represents new features added to the existing modelsdistribution over W 2 ½0; 1  using the short sequencing reads.Similarly, when comparing two conditions the short reads are used toconstruct a posterior distribution over dPSI DW 2 ½À1; 1  for the inclusion of the alternative exon. In practice many alternative exonstend to be either highly included or highly excluded in any given condition, but around 20% of the measurements in our dataset have 0:1< E½W  < 0:9 and the concentration of the posterior W or DW distribution around that mean value depends on the total number of readshitting that region and how these are distributed across the transcriptome (Vaquero-Garcia et al., 2016).Except the additional CLIP based features described above, wederived a feature set similar to previous works, to enable comparison. The 1357 non CLIP-Seq features are comprised of binary, integer and real valued features. These features have vastly differentdistributions with some being highly sparse, and some features beinghighly correlated (e.g. alternative representations of a splice factorbinding motif). Finally, in any given condition only a small subset ofthose features are expected to represent relevant regulatory features.Since many splicing changes occur in complex/non-binary splicing events, limiting the splicing code model to the original predefined 12 000 cassette events means that we may lose manyimportant splicing variations. To capture additional cassette orcassette-like splicing variations we develop a pipeline that parsesgene splice graphs constructed by MAJIQ to find additional trainingsamples in the dataset. This process allowed us to find 2876 moreevents changing in at least one tissue in the MGP data.Next, we process seven splice factor knockdown, knockout andover-expression RNA-Seq datasets for four key splicing factorsCELF1/2, MBNL1 and RBFOX2 (for details of the datasets, seeSupplementary Table S11). These datasets pose a challenge for anyintegrative learning framework since they are low coverage, noisyand are processed by different labs.We divided our datasets into 5-folds. 3-folds were used for training,one was used for validation and the last one was used for testing. We repeated the modeling tasks three times, permuting the dataset each timeto produce standard deviation estimates for performance evaluation.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018Integrative deep models for alternative splicingi2772.2 Likelihood target functionMotivated by the high noise in microarrays and later applied toRNA-Seq data, previous works translated the measurements of exoninclusion levels into a posterior distribution over random variableqe for each exon e and condition c with three possible assignmentsc;sPfqe g where qe ! 0 8e; c; s and s qe ¼ 1. For PSI prediction, sc;sc;sc;s2 fL; M; Hg represent chances of 0W < 0:33; 0:33W< 0:66 and 0:66W1, respectively. For changes in PSI,s 2 finc; exc; ncg, represent chances of increased inclusion, exclusionor no change. Consequently, an information theoretic code qualitymeasure ðQc Þ was used to score the predictions made by the splicingcode. Qc is expressed as the difference in the Kullback–Leibler (KL)divergence between each target and predicted distribution:Qc ¼EXDKL ðqe jj  Þ À DKL ðqe jjpe Þc qcce¼1¼XEXqec;se¼1 s2finc;exc;ncg  e  pc;slog; qs(1)where c is the splicing condition (e.g. CNS), E is the number ofexons and pe and qe are the predicted and target probabilities.c;sc;sAlternatively, Qc can be interpreted as the log-likelihood of the predictions minus the log-likelihood of a naive predictor based on themarginal distribution only.Although useful, this target function suffers from several deficiencies when applied to RNA-Seq data. First, the binning results ina rudimentary estimation of W and DW. Second, the optimizationonly aims to bring pe and qe closer, without any relation to orderc;sc;sor meaning. For example, if a cassette event has low inclusion(qc;s¼L $ 1) then predicting pc;s¼M $ 1 or pc;s¼H $ 1 are just as bad.Moreover, in cases where an event suffers from insufficient or highlyvariable read coverage we may have qc;s¼L $ qc;s¼M $ qc;s¼H . Insuch cases, a model with prediction pc;s¼H $ 1 based on sequencefeatures will be penalized, even though there was no substantial evidence against it.In order to overcome the above limitations, for every pair of conditions c and c0 , we define three target variables as:TWe;c ¼ E½We;c  TDWinc;c;c0 ¼ jmaxð ; E½DWc;c0  Þj(2)TDWexc;c;c0 ¼ jminð ; E½DWc;c0  Þjwhere TWe;c is the expected PSI value of the event e in condition c,TDWinc;c;c0 captures the dPSI for events with increased inclusion between condition c and c0 and TDWexc;c;c0 captures the dPSI for eventswith increased exclusion between condition c and c0 .   is a uniformrandom variable with values between 0.01 and 0.03, it is used toprovide very low dPSI values for non-changing events. E½Wc   and E½DWc;c0   were computed from the raw RNA-Seq data from conditionc and c0 using MAJIQ (Vaquero-Garcia et al., 2016). Given theabove target variable definitions, we define the new likelihood targetfunction as:L¼EXXXkc;e wc;eLt;c;ecLt;c;eet¼ t log b þ ð1 À tÞ log ð1 À bÞttwc;e ¼E½We;c  þDX(3)PðWÞW¼E½We;c  ÀDwhere t 2 fTWe;c ; TDWinc;c;c0 ; TDWexc;c;c0 g and kc;e ¼ 1 if exon e is quantifiable in condition c. The weight wc;e is defined by the probabilitymass in an area 6D around the expected Wc as defined by MAJIQ.This definition carries several benefits. First, it allows us to combinemany different datasets, where the same event may or may not bequantifiable. Second, even when an event is deemed quantifiable(kc;e ¼ 1), the model can take into account the confidence of MAJIQin the W inferred from the RNA-Seq experiment based on the number of reads encountered for the event.2.3 Models2.3.1 ArchitectureThe BNN model was described in detail in (Xiong et al., 2011;Leung et al., 2014). Briefly, the network consists of one hidden layerwith varying number of sigmoidal hidden units. Network weightsare random variables with a Gaussian distribution and a spike andslab prior which encourages sparsity. Figure 2 shows the networkarchitecture of the BNN used in this work. Notably, (Leung et al.,2014) only used the Low, Medium and High PSI variables for theBNN, which may limit the model’s ability to learn splicing changebetween tissues. For example, if we have an alternative cassetteexon e in conditions c and c0 s.t. We;c $ 0:1 and We;c0 $ 0:3 withhigh confidence, the splicing change for exon e between condition cand c0 ; DWe;c;c0 $ À0:20, is considered significant in the field.However, if we translate these PSI values to LMH variables, bothWe;c and We;c0 will be in the Low category (0W < 0:33) and themodel will not be able to learn the splicing change. We thereforesupplemented the LMH variables with UDC variables for inclusionlevel going up, down or not changing. This addition made the BNNtargets equivalent to those of the DNN architecture used in thatwork, leading to improved performance for the BNN model (seeSupplementary Table S4).The original DNN model shown in Figure 2 included an autoencoder layer with tanh activation and two hidden layers with ReLUactivation units. Additionally tissue type was input as two hot vectors of length equal to the number of tissues in the dataset whereeach bit represents a tissue and is active when the network is inputan event comparing that tissue with another. For example, if the tissue order is [brain, heart, kidney, liver and testis] and the currentcomparison is brain versus heart, then the two tissue type hot vectors will be [10000] and [01000]. Dropout with probability 0.5 wasused in each layer except the autoencoder layer. The hyperparameters are described in Supplementary Table S12. We experimentedwith different types of network architectures with different numberof hidden layers and hidden units, different activation units andbatch normalization. Since none of those architectures performedsignificantly better (data not shown) we decided to maintain the original DNN architecture for the purpose of this work.The new DNN model architecture shown in Figure 2 includes thefollowing additions. First, the target function has been changed asdescribed in Section 2. We also added 874 CLIP features to the inputdataset. We maintained the three layer structure of the original DNNmodels since we observed that adding additional layers did not improve performance. Dropout with probability 0.5 was applied to thesecond and third layers. We noticed that adding L1/L2-regularizationdid not have any impact on the model performance and we decided toexclude it from the final model. We allowed the learning rates of thethree target variables to vary to capture optimal model performance.As shown in Figure 2, for splice factor modeling, we modifiedthe tissue type input to include the splice factor knockdown/knockout or overexpression data. We used two hot vectors with lengthequal to the number of tissues to represent the tissues and two hotvectors with length equal to the number of splice factors to representDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018i278the splice factors. Since the datasets for this model were lower coverage and more noisy than the previous models, this model was moresensitive to different hyperparameter values during the tuning phase.Three hidden layers were found to be optimal and L1-regularizationwas performed on the autoencoder layer. Dropout of 0.5 was usedfor the second and third hidden layers.2.3.2 LearningFollowing the procedure suggested by (Leung et al., 2014), wetrained the first layer of the model as an autoencoder for dimensionality reduction. This procedure proved beneficial for the new modelsas well. Next, the set of weights from the first layer were fixed andthe tissue input was added. In the second stage, the two layered feedforward neural network was trained using SGD with momentum,and weights were fine tuned by backpropagation. Each sample inputto the network consists of 1357 genomic (and 874 CLIP) featuresand has three target variables, TWe;c , TDWinc;c;c0 and TDWexc;c;c0 . Trainingbatches are biased to prioritize changing events. Early stopping anddropout layers prevent the network from overfitting.Our target variables capture different aspects of splicing change,one learns the baseline PSI for an event in a condition and the othertwo learn the inclusion and exclusion dPSI of that event between conditions. Therefore varying their learning rates improved overall modelperformance. The autoencoder network was trained for 300–500epochs and the feed-forward neural network was trained for 1000–1500 epochs. Validation data was used for the hyperparameter tuning,and once the set of hyper parameters were fixed, the final model wastrained with the training and the validation data. 15 models weretrained with the 5-folds and three permutations of the whole datasets.The performance evaluation is on the concatenated predictions of thetest set from the 5-folds and error bars are computed using the threepermutations. Tensorflow was used to develop the deep model andGPUs were used to accelerate the training process.For the BNN model, each tissue pair was trained as an independentmodel. Spike and slab prior was used to enforce sparsity and theweights were assumed to have an Gaussian distribution. 950 samplesfrom the posterior distribution of weights were generated using 1000MCMC training iterations with Gibbs sampling. Initial 50 sampleswere discarded as burn-in. The final predictions are generated by averaging over the predictions from the 950 sampled weights. 15 modelswere trained per tissue comparison with 5-fold cross validation andthree data permutations. After fixing the model hyperparameters, thevalidation data was included in the training of the final model.3 ResultsFor assessing the prediction accuracy, two types of measures havebbeen used in this work. The predicted E½W c;e   is compared to theestimated E½Wc;e   from the RNA-Seq experiments to compute thefraction of variance explained (R2). Area under the ROC curve(AUC) was computed for the prediction of exons that were differentially excluded/included (jDWe;c;c0 j ! 0:15) or not changing(jDWe;c;c0 j0:05).We aim to measure the effect of each new element on the prediction accuracy. As a baseline, Figure 1 shows the effect of new targetfunction on prediction accuracy when the models are trained on theoriginal dataset used by (Leung et al., 2014) and no other modelingadditions are made. We see significant improvement (5–22.5%) inPSI estimation and in splicing target prediction (see SupplementaryTable S6) by the new model (DNN-PSI) when compared to theDNN (DNN-LMH) and BNN (BNN-UDC) with the old target function. We note that the results for the previous models are notA.Jha et al.directly extracted from (Leung et al., 2014), but rather reconstructedto produce similar performance since both code and data were notavailable in the original publication. Supplementary Figure S2 showsa scatter-plot comparing the performance (AUC-ROC) reported in(Leung et al., 2014) on x-axis and our reconstruction on y-axis. Thisfigure shows that despite different RNA-quantification proceduresand data selection criteria, our reconstruction has comparable performance to the (Leung et al., 2014) model. In fact, our model performs better on the Medium class (0:33W < 0:66) of PSIquantification where generally a higher proportion of differentialsplicing events are located. We added inclusion, exclusion and nochange output variables to the Bayesian Neural Network since it improved splicing target prediction performance when compared tothe BNN without these labels [BNN-MLR, Leung et al. (2014); seeSupplementary Table S4]. DNN-LMH was designed according tothe architecture and hyperparameters described in Leung et al.(2014). Also, since the DNN-LMH does not predict PSI directly, wecomputed the E½W  as the weighted average of the {L, M, H} classprediction probabilities, following (Xiong et al., 2015).As noted earlier, previous works (Barash et al., 2013; Leunget al., 2014; Xiong et al., 2015) were performed on a predefined setof $12 000 alternative cassette exons. This approach of using onlypredefined cassette exons can limit the performance of the learnedmodels, especially those involving deep neural networks which requirelarge datasets. Thus, we developed a process termed cassettization(see Section 2.1) to detect and quantify additional cassette and cassettelike alternative exons from RNA-Seq data. Additionally, due to thelimited coverage of (Brawand et al., 2011), we performed subsequentanalysis on the MGP data described in Section 2.1. To assess the effectof cassettization on performance, we used two identically configuredBNN models and trained one on the original 12 000 cassette exons(BNN-UDC) while the second (BNN-CAS) got additional trainingdata with cassettized events not present in the original dataset.Figure 3a shows that cassettization caused a substantial improvementin PSI estimation and splicing target prediction (see SupplementaryTable S7) with all other factors being constant.Our next goal was to measure the effect of CLIP-Seq data onPSI estimation. Using the same setup described above, we trainedtwo BNNs identical in every aspect except that one was given theCLIP data as input features (BNN-CAS-CLIP) and the other(BNN-CAS) was not. Introducing the CLIP features added a modest improvement to the PSI estimation as seen in Figure 3b. Onepossible explanation for the modest improvement could be underfitting of BNN-CAS-CLIP since CLIP was introduced as new features to the model but the model’s hidden layer size and otherhyperparameters were fixed.In order to test the combined effect of the new target function,CLIP data and cassettization on the model’s performance and tocompare BNN and DNN frameworks for the task of PSI estimation,we trained a BNN model with the old target function, cassettizationand CLIP (BNN-CAS-CLIP) and a DNN model with the new targetfunction, cassettization and CLIP (DNN-PSI-CAS-CLIP). Figure 3cand Table 1 summarize the results for the two models for both PSIestimation and splicing target prediction. Figure 3c shows large performance improvement by the new model for PSI estimation whencompared to the BNN. This improvement carries over to the task ofsplicing target prediction for every tissue pair seen in Table 1.Next, we turned to the new integrative framework that incorporates knockdown/knockout and over-expression experiments (seeSection 2.3.1 and 2.1). Figure 4a shows that the new integrativedeep model generalizes well for to this new type of KD/KO/OE data,offering large performance improvement for PSI estimation. OneDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018Integrative deep models for alternative splicing(b)(c)Variance explained in PSIBNN-CAS vs. BNN-CAS-CLIP% Variance explained by PSI prediction (R2)% Variance explained by PSI prediction (R2)Variance explained in PSIBNN-UDC vs. BNN CASHeartHippLiverLungSpleenThymusVariance explained in PSIBNN-CAS-CLIP vs. DNN-PSI-CAS-CLIP% Variance explained by PSI prediction (R2)(a)i279HeartHippLiverTissueLungSpleenThymusHeartHippTissueLiverLungSpleenThymusTissueFig. 3. (a) Effect of different elements on PSI estimation (% variance explained) (a) Increasing original dataset size by using cassettization (see Section 2.1,BNN-UDC (blue) versus BNN-CAS (brown). BNN-UDC: Bayesian Neural Network with Up, down, no change target variables and old target function; BNN-CAS:BNN-UDC with cassettized data) (b) Adding CLIP data (BNN-CAS (brown) versus BNN-CAS-CLIP (purple). BNN-CAS-CLIP: BNN-CAS with CLIP data). (c) Overalleffect of new target function and Deep Neural Network on PSI estimation measured by comparing BNN-CAS-CLIP (purple, BNN with cassettization, CLIP data andthe old target function) and DNN-PSI-CAS-CLIP (green, DNN with cassettization, CLIP data and the new target function)Table 1. Comparison of splicing target prediction of DNN-PSI-CAS-CLIP versus BNN-CAS-CLIP in terms of AUC of Inclusion versus notInclusion, Exclusion versus not-Exclusion and Change versus not-ChangeTissue pairModelInclusionExclusionNo changeHeart-HippBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIPBNN-CAS-CLIPDNN-PSI-CAS-CLIP92.97 6 0.1295.70 6 0.0678.09 6 0.4992.15 6 0.6082.52 6 0.6792.15 6 0.8079.37 6 0.2193.18 6 0.2282.01 6 0.6492.76 6 0.3683.33 6 0.0894.36 6 0.4184.19 6 0.2393.32 6 0.3383.84 6 0.3493.77 6 0.0983.10 6 0.3691.77 6 0.2784.60 6 0.3698.14 6 0.5485.41 6 0.4097.01 6 0.6184.25 6 1.1096.80 6 0.7679.82 6 0.3296.83 6 0.7579.97 6 0.4194.98 6 1.0570.55 6 1.3197.91 6 0.4788.22 6 0.1694.09 6 0.3489.38 6 0.2496.26 6 0.1889.77 6 0.1895.42 6 0.3091.03 6 0.1396.98 6 0.4786.20 6 0.2495.83 6 0.1593.16 6 0.0297.33 6 0.2492.71 6 0.0595.92 6 0.1193.36 6 0.0696.86 6 0.1388.63 6 0.1595.64 6 0.1081.73 6 0.3794.23 6 0.1587.66 6 0.1594.46 6 0.2974.23 6 0.0393.27 6 0.3880.71 6 0.4996.03 6 1.1378.41 6 0.4496.51 6 0.4770.73 6 1.1491.86 6 1.2392.26 6 0.0694.72 6 0.0685.13 6 0.1594.11 6 0.2687.94 6 0.1893.60 6 0.2687.45 6 0.0895.22 6 0.3385.91 6 0.2394.06 6 0.3290.32 6 0.0795.60 6 0.0790.61 6 0.0494.47 6 0.1690.75 6 0.1095.51 6 0.1087.83 6 0.1894.46 6 0.0583.07 6 0.4295.71 6 0.2887.59 6 0.2196.04 6 0.6377.03 6 0.1493.44 6 0.2080.71 6 0.0996.91 6 0.3979.57 6 0.3095.91 6 0.1670.99 6 0.7692.21 6 0.85Heart-LiverHeart-LungHeart-SpleenHeart-ThymusHipp-LiverHipp-LungHipp-SpleenHipp-ThymusLiver-LungLiver-SpleenLiver-ThymusLung-SpleenLung-ThymusSpleen-ThymusNote: Each table entry represents AUC 6 standard deviation, where AUC is computed by concatenating predictions from all the 5 test sets and the standarddeviations are calculated by permuting the dataset three times. Numbers in bold indicate statistically signiﬁcant performance improvement for the DNN-PSICAS-CLIP model over the BNN-CAS-CLIP model.exception is the model performance on RBFOX2 KD in C2C12cells. This may be due to the different experimental condition(C2C12 cells) or the number of samples, which require specific adjustments of the model’s training parameters.3.1 Regulatory modeling with new splicing codesIn order to demonstrate the usefulness of the new splicing codes forsplicing regulatory analysis we tested how well the model predicts theeffect of splice factor knockdowns on unseen test cases with orDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018i280A.Jha et al.(b)Correlation between CELF1-heartexperimental and model predictedOE dPSIModel Predicted Overexpression dPSI% Variance explained by PSI prediction (R2)Variance explained in PSICELF1heart-OECELF2MBNL1MBNL1CELF1MBNL1RBFOX2heart-OE heart-KO muscle-KOmuscle-OE brain-KO C2C12diff-KDTissueCorrelation between CELF1-heartexperimental OE andin silico KD dPSI1.01.0R=0.41R=-0.35In Silico Knockdown dPSI(a)0.50.0-0.5-1.0-1.0-0.5v0.0v0.5RNA-Seq Overexpression dPSI1.00.50.0-0.5-1.0-1.0-0.50.00.51.0RNA-Seq Overexpression dPSIFig. 4. (a) Predicting the effect of splice factors. (a) Improvement in PSI prediction (% variance explained) for conditions involving splice factor KD/KO/OE, comparing BNN-UDC model with old target function (purple) to the new model (green). (b) Correlation scatter plots between predicted (y-axis) and measured (x-axis)dPSI for CELF1 Overexpression(OE) in mouse heart. Left, showing correlation to model predicted Overexpression dPSI. Right, showing correlation to in silico KDof CELF. (R: Pearson’s correlation coefﬁcient, n ¼ 2134 in both scatter plots)without the available KD data. It is important to note that since thenew models predict dPSI directly, we were able to evaluate dPSI between conditions such as WT versus KD. In contrast, there is no directway to extract dPSI from previous splicing code target functions. Weattempted to extract dPSI from the old DNN model by computing theE½W  as the weighted average of the {L, M, H} class prediction probabilities for the two conditions and subtracting them but this dPSI estimate had extremely poor correlation with the true dPSI (data notshown). Figure 4b (left) shows the correlation between the experimental (RNA-Seq) overexpression dPSI and the new model’s predictionsin CELF1 heart OE experiment. Good correlation (R ¼ 0.41) indicatesthat the model learns the effects of overexpression of the splice factorwell. Figure 4b (right) shows the correlation plot when the model performs in silico knockdown of CELF by zeroing out the featuresrelated to CELF versus the experimental CELF1 overexpression dPSI.Negative correlation (R ¼ À0:35) even without KD or overexpressiondata demonstrates how the splicing codes can now accurately predictchanges in dPSI with in silico knockdowns (for similar plots with correlations ranging from 0.13 to 0.6 for the other KO/KD/OE datasets,see Supplementary Fig. S1).Finally, we wished to see if we could gain mechanistic insightinto the regulation of physiologically relevant targets in thesesystems. Specifically, exons correctly predicted to have reducedinclusion upon CELF1 over-expression but are not affected byCELF-related features (Fig. 5a, left) are of particular interest interms of alternative mechanisms of regulation. Two such cases inkey genes are shown in Figure 5, for the myofibrillar protein Nrap(Pedrotti et al., 2015) in muscle (top) and for the beta microexon inthe key myogenic transcription factor Mef2d (Singh et al., 2014) inheart (bottom). Quantification using RNA-Seq data from these contexts confirmed the accuracy of the model in predicting CELF1 regulation in both cases (Fig. 5a, compare bars 1 and 4 from the left).However, in silico removal of CELF-related features did not lead tosignificant changes in exon inclusion in either case (Fig. 5a, comparebars 1 and 2 from the left), suggesting indirect regulation could becausing repression upon CELF1 over-expression. In line with this,no CELF1 CLIP peaks were found upstream of these regulatedexons (Fig. 5b) where CELF proteins have been found to repressexon inclusion (Ajith et al., 2016). Strikingly, in silico removal offeatures related to the RBFOX family recapitulated the predictedsplicing change upon CELF1 overexpression (Fig. 5a, compare bars1 and 3 from the left). Analysis of RBFOX2 knockdown data frommyotubes (Singh et al., 2014) (Fig. 5a, bar 5 from the left) orRBFOX1 muscle-specific knockout mice (Pedrotti et al., 2015) supports that the RBFOX family typically enhances inclusion of theseexons. Additionally, a number of RBFOX binding motifs (GCAUG)and CLIP peaks are located just downstream of these exons(Fig. 5b), where these proteins enhance inclusion (Singh et al.,2014). These observations motivated additional study in human Tcells where we found CELF2 is a potent repressor of RBFOX2(Gazzara et al., 2017), suggesting that a similar indirect mechanismmay be at play in murine muscle and heart where CELF overexpression represses RBFOX proteins to drive splicing changes in theseand other targets (Fig. 5c).4 DiscussionIn this study, we offered a new formulation for the task of learningcondition specific splicing codes from a compendium of RNA features. We defined a new target function which enabled us to avoidbinning exon inclusion levels into discrete categories of low, medium and high (LMH). Similarly, for predicting differential splicing the new target function predicts dPSI directly rather thancategories of up, down or no change (UDC) in inclusion levels.The new target function allowed us to gain significant accuracyboost for predicting PSI, tissue specific variations of it, and splicefactors target prediction (dPSI). Moreover, the new target functionallowed us to incorporate samples with missing quantification values or with different degrees of quantification accuracy, leveragingrecent advances in RNA-Seq quantification (Vaquero-Garciaet al., 2016).We also showed how new sources of data for splice factors binding affinity (CLIP-Seq) and regulation (KD/OE experiments) can beintegrated for modeling splicing outcome. Such data by itself isproblematic for splicing model training given its noisy nature andlimited amount of splice factor targets. Here, however, we combineit with many other relevant datasets, leveraging transfer learning toimprove overall model performance. Thus, the gain offered by thiswork is not only measured by the significant improvement in prediction accuracy, but also by the ability to combine many differentDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018Integrative deep models for alternative splicing(b)X2Muscle control RNA-SeqCELFBFO1OEKD.RNA-SeqRre BFm Oov Xa l fe atELF0.05CNrap,muscle1OECre ELm Fo v fea l a t.ModelR(a)i2810.0Muscle CELF1 OE RNA-Seq-0.1CELF1 CLIPGCAUGE(ΔΨ)E(ΔΨ)Nrap-0.2RBFOX2 CLIPHeart control RNA-Seq-0.3Heart CELF1 OE RNA-Seq0.0KDBFOX2CELF1 CLIPMef2dGCAUGRCELF1OERre BFm Oov Xal feaELCMef2d,heartF1OECre ELm Fov fea l at.t.-0.4RBFOX2 CLIP(c)-0.2-0.3E(ΔΨ)E(ΔΨ)-0.1?CELFRBFOX-0.4-0.5Fig. 5. Splicing code models suggest indirect mechanism of key CELF1 regulatory targets. (a) Model predicted changes in exon inclusion for Nrap in muscle (top) andMef2d in heart (bottom) upon CELF1 overexpression, removal of features related to the CELF family, or removal of features related to the RBFOX family (bars 1–3 fromthe left) as well as quantiﬁcation of change in inclusion from RNA-Seq upon over expression of CELF1 or knockdown of RBFOX2 in myotubes (bars 4 and 5 from theleft). (b) UCSC Genome Browser view of regulated cassette exons in Nrap (top) and Mef2d (bottom) showing locations of RNA-Seq reads in given conditions, CELF1 andRBFOX2 peaks, and the RBFOX family binding motif GCAUG. (c) Schematic representation of suggested regulatory relationship between RBFOX and CELFsources of RNA regulatory data which is now massively producedby many labs and large projects such as ENCODE.A known issue with deep model applications for biomedical studies is their often cryptic nature. However, we were able to demonstrate here how the integrative deep models we developed can be usedto gain biological insights for splicing regulation. This included highaccuracy of splice factor target prediction with or without availableKD/KO experiments, identifying putative novel regulatory interdependence between splice factors, and the affected targets. We believe the usage of splicing codes demonstrated here represents only asmall portion of the potential of this new class of models. Futurework includes predicting non-cassette splicing variations, robust automated extraction of biological hypotheses from code models, and scaling up to create regulatory codes for many conditions and datasets.AcknowledgementsSpecial thanks to Jorge Vaquero-Garcia for support and advice throughoutthis project. We would also like to thank NVIDIA Corporation for the kinddonation of a Titan X GPU used for this research.FundingThis work has been supported by R01 AG046544 to Y.B.Conﬂict of Interest: none declared.ReferencesAjith,S. et al. (2016) Position-dependent activity of celf2 in the regulation ofsplicing and implications for signal-responsive regulation in t cells. RNABiol., 13, 569–581.Barash,Y. et al. (2010a) Deciphering the splicing code. Nature, 465, 53–59.Barash,Y. et al. (2010b) Model-based detection of alternative splicing signals.Bioinformatics, 26, i325–i333.Barash,Y. et al. (2013) Avispa: a web tool for the prediction and analysis of alternative splicing. Genome Biol., 14, R114.Brawand,D. et al. (2011) The evolution of gene expression levels in mammalian organs. Nature, 478, 343–348.Gazzara,M.R. et al. (2014) In silico to in vivo splicing analysis using splicingcode models. Methods, 67, 3–12.Gazzara,M.R. et al. (2017) Ancient antagonism between Celf and Rbfox families tunes mRNA splicing outcomes. Genome Res., in press.Keane,T.M. et al. (2011) Mouse genomic variation and its effect on phenotypes and gene regulation. Nature, 477, 289–294.Leung,M.K.K. et al. (2014) Deep learning of the tissue-regulated splicingcode. Bioinformatics, 30, i121–i129.Pan,Q. et al. (2008) Deep surveying of alternative splicing complexity in thehuman transcriptome by high-throughput sequencing. Nat. Genet., 40,1413–1415.Pedrotti,S. et al. (2015) The RNA-binding protein rbfox1 regulates splicingrequired for skeletal muscle structure and function. Hum. Mol. Genet., 24,2360–2374.Scotti,M.M. and Swanson,M.S. (2016) RNA mis-splicing in disease. Nat. Rev.Genet., 17, 19–32.Singh,R.K. et al. (2014) Rbfox2-coordinated alternative splicing of mef2d androck2 controls myoblast fusion during myogenesis. Mol. Cell, 55, 592–603.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018i282Sotillo,E. et al. (2015) Convergence of acquired mutations and alternative splicing of CD19 enables resistance to CART-19 immunotherapy. Cancer Discov.Srivastava,N. et al. (2014) Dropout: a simple way to prevent neural networksfrom overﬁtting. J. Mach. Learn. Res., 15, 1929–1958.Vaquero-Garcia,J. et al. (2016) A new view of transcriptome complexity andregulation through the lens of local splicing variations. eLife, 5, e11752.A.Jha et al.Wang,E.T. et al. (2008) Alternative isoform regulation in human tissue transcriptomes. Nature, 456, 470–476.Xiong,H. et al. (2011) Bayesian prediction of tissue-regulated splicing usingRNA sequence and cellular context. Bioinformatics, 27, 2554–2562.Xiong,H.Y. et al. (2015) The human splicing code reveals new insights intothe genetic determinants of disease. Science, 347, 1254806.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i274/3953982by gueston 07 January 2018
5028881999002	PMID28881999	5028881999	https://watermark.silverchair.com/btx267.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881999.main.pdf	Bioinformatics, 33, 2017, i267–i273doi: 10.1093/bioinformatics/btx267ISMB/ECCB 2017DeepBound: accurate identification of transcriptboundaries via deep convolutional neural fieldsMingfu Shao1,*,†, Jianzhu Ma2,† and Sheng Wang3,*,†1Department of Computational Biology, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA15213, USA, 2School of Medicine, University of California San Diego, La Jolla, CA 92093, USA and 3ComputationalBioscience Research Center (CBRC), Computer, Electrical and Mathematical Sciences and Engineering (CEMSE)Division, King Abdullah University of Science and Technology (KAUST), Thuwal 23955-6900, Saudi Arabia*To whom correspondence should be addressed.†The authors wish it to be known that these authors contributed equally.AbstractMotivation: Reconstructing the full-length expressed transcripts (a.k.a. the transcript assemblyproblem) from the short sequencing reads produced by RNA-seq protocol plays a central role inidentifying novel genes and transcripts as well as in studying gene expressions and gene functions.A crucial step in transcript assembly is to accurately determine the splicing junctions and boundaries of the expressed transcripts from the reads alignment. In contrast to the splicing junctions thatcan be efﬁciently detected from spliced reads, the problem of identifying boundaries remains openand challenging, due to the fact that the signal related to boundaries is noisy and weak.Results: We present DeepBound, an effective approach to identify boundaries of expressed transcripts from RNA-seq reads alignment. In its core DeepBound employs deep convolutional neuralﬁelds to learn the hidden distributions and patterns of boundaries. To accurately model the transition probabilities and to solve the label-imbalance problem, we novelly incorporate the AUC (areaunder the curve) score into the optimizing objective function. To address the issue that deep probabilistic graphical models requires large number of labeled training samples, we propose to usesimulated RNA-seq datasets to train our model. Through extensive experimental studies on bothsimulation datasets of two species and biological datasets, we show that DeepBound consistentlyand signiﬁcantly outperforms the two existing methods.Availability and implementation: DeepBound is freely available at https://github.com/realbigws/DeepBound.Contact: mingfu.shao@cs.cmu.edu or realbigws@gmail.com1 IntroductionRNA-sequencing (RNA-seq) is a well-established and widely usedtechnology that enables sensitive identification of novel transcriptsand accurate measurement of gene expressions (Wang et al., 2009).Current high-throughput RNA-seq protocols usually produce shortsequencing reads of the expressed transcripts in a given sample.Therefore, a fundamental computational problem is to reconstructthe full-length expressed transcripts from such short sequencing reads,which is usually referred to as the transcript assembly problem.Transcript assembly is very challenging, not only because of the difficulties shared with genome assembly problem, such as sequencingerrors, homologous repeats and coverage variations, but more importantly, due to the existence of alternative splicing (in eukaryotes),which drastically increase the complexity of transcript assembly.Existing assembly methods are either reference-based [for example, Cufflinks (Trapnell et al., 2010), Scripture (Guttman et al.,2010), IsoLasso (Li et al., 2011b), SLIDE (Li et al., 2011a), CLIIQ(Lin et al., 2012), CEM (Li and Jiang, 2012), MITIE (Behr et al.,2013), Traph (Tomescu et al., 2013), StringTie (Pertea et al., 2015)and TransComb (Liu et al., 2016b)], or de novo [for example,TransABySS (Robertson et al., 2010), Rnnotator (Martin et al.,2010), Trinity (Grabherr et al., 2011), SOAPdenovo-Trans (Xieet al., 2014), Velvet (Zerbino and Birney, 2008), Oases (Schulzet al., 2012), IDBA-Tran (Peng et al., 2013) and BinPacker (Liuet al., 2016a)], depending on whether a reference genome is assumedavailable and being used. Reference-based methods usually givehigher accuracy comparing to de novo methods. In contrast to thede novo methods that gives the nucleotide sequences of theCV The Author 2017. Published by Oxford University Press.i267This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i267/3953980by gueston 07 January 2018i268M.Shao et al.expressed transcripts, reference based methods infer the structuresof expressed transcripts, i.e. the coordinates of the splicing junctionsand boundaries (i.e. 50 and 30 ends) w.r.t. the reference genome foreach expressed transcript. To achieve this, reference-based methodsfirst align all the reads to the reference genome using RNA-seq aligners [for example, TopHat2 (Kim et al., 2013), STAR (Dobin et al.,2013) and HISAT (Kim et al., 2015)]. Then the reads are clusteredinto different gene loci based on the alignment, and the coordinatesof splicing junctions and boundaries of all expressed isoforms areinferred. Finally, within each gene loci, these coordinates are organized in a so-called splice graph, and the expressed transcripts areassembled by computing a set of paths so as to mostly fit the splicegraph.Hence, accurate identification of the splicing junctions andboundaries from the reads alignment is crucial, since they providebuilding-blocks for transcript assembly. The signal for splicing junctions usually can be clearly reflected in the reads alignment, makingthe identification of splicing junctions a relatively easy task.Specifically, a splicing junction can be inferred by a group of reads(called spliced reads) which are all separately aligned to the reference genome with identical separating coordinates (see Fig. 1a). Incontrast, such strong signal does not exist for transcript boundaries.The intuition that can be used to identify boundaries is that the readcoverage tends to increase (resp. decrease) when a transcript starts(resp. terminates). However, such signal is noisy and weak, makingthe task of identification of boundaries very challenging.Limited efforts have been focused on inferring boundaries of expressed transcripts from RNA-seq reads alignment. Currentreference-based transcript assemblers usually use a simple rule toidentify boundaries: the coordinates where reads appear and disappear are identified as start and end boundaries, respectively (hereinafter we call it as TypicalRule). TypicalRule suffers from bothhigh false-positive rate and false-negative rate for the following reasons (Fig. 1b–d). First, some boundaries are inside exons, and forsuch cases TypicalRule is impossible to identify them (Fig. 1b).Second, a region within an exon might not be covered by any readsdue to low sequencing depth, high coverage variation and alignmenterrors; such gap shall result in TypicalRule reporting two falsepositive boundaries (Fig. 1c). Third, TypicalRule shall also miss twoboundaries if contaminations of DNA sequences or misalignmentsthat bridge two transcripts (Fig. 1d). A recent study on metaassembly employs a more sophisticated method to infer boundariesof the expressed transcripts (Niknafs et al., 2016). This methodACBDFig. 1. Illustration of identiﬁcation of splicing junctions and challenging ofidentifying boundaries. Exons and introns are represented as thick orangebars and thin green bars, respectively. Reads are represented as blue bars,where spliced reads are connected with dotted lines. (a) Splicing junction canbe inferred from a group of aligned spliced reads. (b) A transcript starts in themiddle of an exon. In this case, we can observe an increasing of read coverage. TypicalRule shall fail to identify this boundary. (c) A gap appears in themiddle of an exon. TypicalRule shall report two false-positive boundaries. (d)Two transcripts are bridges by reads. TypicalRule shall miss these twoboundariesemploys Mann–Whitney U test and fold change comparison to identify significant coverage change (hereinafter we call it MWUTest).However, as we will show in this paper, the accuracy of MWUTestis still very low, due to its limited capability to model transitions andto handle noise.In this work, we propose a novel algorithmic framework,DeepBound, to identify boundaries of expressed transcripts fromreads alignment by using deep convolutional neural fields(DeepCNF) (Wang et al., 2015, 2016c). Different from previousstatistical methods, DeepBound can integrate a variety of information and automatically determine their quantity under different circumstances. Our model and algorithm are particularly designed soas to take advantage of the properties and to address the challengings of boundary detection. Specifically, first, DeepBound modelsthe boundary detection problem as a sequential labeling problemusing a conditional probabilistic graphical model (Lafferty et al.,2001). The key idea is to quantify the probability of observing theboundaries on position i given the features and states around i.Second, given billion level of potential boundaries, patterns of boundaries are complicated. Previous sequential graphical model such asConditional Random Field and Hidden Markov Model cannotmodel such complicated scenarios. In contrast, DeepCNF can naturally model complex pattern hidden in the boundaries. Third, as theboundaries are extremely sparse comparing to non-boundary coordinates, previous machine learning models will all be biased towardspredicting the input samples as non-boundaries. The inherent reasonis that it is very hard to discriminate the true signal with the outliersfor sparse observations. To handle this highly label-imbalancedproblem, we introduce a novel objective function which directlymodels the AUC score between the predicted and actual labels(Wang et al., 2016a,b). The price we pay is that we get a new objective function which is neither smooth nor convex. However, byapplying the Chebyshev approximation (Calders and Jaroszewicz,2007) on this objective function, we can still be able to optimize itefficiently.To address the issue that DeepCNF requires large volume oflabeled training samples, we novelly propose to use simulationRNA-seq data to train our model. Ideally, realistic parametric distribution of boundaries can be learned more accurately by such asupervised learning way. To valid our assumption and also to compare the performance of our method with that of TypicalRule andMWUTest, we test these methods on simulation datasets from bothhuman and mouse species, as well as on biological RNA-seq datasets. The results demonstrate that the model trained with simulationdatasets on one species can also perform well on another species andon biological datasets. On all testing datasets, our methods significantly outperforms other two methods.2 Experimental resultsIn this section, we first introduce the experimental pipeline and theevaluation measurements in Section 2.1. After that we then illustratethe performance of all three methods on simulation datasets inSection 2.2, and that on biological datasets in Section 2.3.2.1 Experimental settingsTo identify boundaries of the expressed transcripts in a given RNAseq dataset, we first align all the reads to the reference genome usingthe recent RNA-seq aligner HISAT2 (Kim et al., 2015). We thencluster reads into different gene loci according to their aligned coordinates such that the distance between adjacent gene loci is at leastDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i267/3953980by gueston 07 January 2018DeepBoundAi269I1BCI2DEI3FGHI4IJI5KI6L2.2 Results on simulation datasetsWe use Flux-Simulator (Griebel et al., 2012) to simulate the RNAseq datasets. Flux-Simulator utilizes the known transcriptome8060Recall (%)405040300102020Precision (%)TypicalRuleMWUTestDeepCNF050 bp. Within each gene loci, splicing junctions are identified,defined as two or more spliced reads that share the same splicing coordinates. We use all these splicing junctions as well as the start andend boundaries of the gene loci to partition the whole gene loci intoseparate partial exons (see Fig. 2). Each partial exon serves as an independent instance, i.e. each method will take a partial exon (thefeatures of this partial exon, such as coverage profile, sequence information, etc.) as input and identify boundaries of the expressedtranscripts within this partial exon. For each dataset and eachmethod, the predicted boundaries by this method for this datasetshall be the union of the identified boundaries for all partial exonsin this dataset.We apply three methods to identify boundaries, TypicalRule,MWUTest and DeepBound (see Section 3 for details about them).To measure the accuracy of these methods, we first define theground-truth boundaries. For simulation datasets, we know exactlythe expressed transcripts from the simulation process, and theirboundaries are served as the ground-truth. For biological datasets,we use the boundaries of all known transcripts in the annotationdatabase, and keep those within a distance of 50 bp for some partialexon (in the studied RNA-seq dataset) as ground-truth. Notice thatusually for each RNA-seq dataset, only a subset of them gets expressed, and it could be also that some novel transcripts are expressed but not in the current annotation database. Nevertheless,this way of comparison is still fare enough and commonly used to illustrate the comparative performance of different methods.Given the ground-truth boundaries, we use precision and recallto measure the identified boundaries by each method. We evaluatestart boundaries and end boundaries separately. For each dataset,we compute a one-to-one matching between the predicted start(resp. end) boundaries and the ground-truth start (resp. end) boundaries, such that each pair of matched boundaries are adjacent according to their coordinates. We say an identified boundary iscorrect if it is matched to some ground-truth boundary and the distance between them is <50 bp. With this definition, we can thencompute precision, which is defined as the ratio between the numberof correct boundaries and the total number of identified boundaries,and recall, which is defined as the ratio between the number of correct boundaries and the number of ground-truth boundaries. Noticethat for our boundary prediction problem it is not appropriate touse true positive and false positive rates. This is because most genomic positions are not boundaries, making the false positive rate always a tiny number. Thus, it is much more meaningful to useprecision and recall to perform evaluation.60Fig. 2. Illustration of partial exons within a gene loci. Thick orange bars (representing exons) and thin green bars (representing introns) show the (unknown) structures of the expressed transcripts. Green bars represent the observed reads that aligned to the reference genome. Notice that the three expressed transcriptscome from two genes, while we identify all the reads as a single gene loci (because the single read in the middle of I4 bridges the two genes). Six splice junctions,(B, C), (D, E), (F, G), (D, G), (H, I) and (J, K), are identiﬁed, which, together with the gene loci boundaries, A and L, partition this gene loci into six partial exons, Ik,1 k 6. Each partial exon becomes an independent instance. Notice that in partial exon I3, there exists a gap <50 bp, thus the two parts are regarded within asingle gene lociSTARTENDSTARTENDFig. 3. Comparison of the three methods on human simulations with eachdataset containing 150 M reads. The height of each bar shows the averageprecision (left part) and recall (right part) over the 10 datasets, while the thinbar on top illustrates the standard deviationannotation database (in our experiments, we use ENSEMBL release87 annotation) and reference genome to simulate the expressedtranscripts and the sequenced reads, following certain empirical distributions. To compare the performance of methods on datasetswith different sequencing depths, we use Flux-Simulation to generate two types of datasets, one containing 150 M paired-end reads oflength 75 bp, while the other containing 15 M 75 bp paired-endreads. To test the methods on different species, we choose twomodel species, human and mouse, and for each species, we independently simulate 10 datasets for each type of dataset. We independently simulate another two human datasets (with 150 M readsin each dataset), and use them to train our DeepCNF model, as wellas to determine the parameters in TypicalRule and MWUTest.The comparison of the three methods on 10 human simulationdatasets with 150 M reads is illustrated in Figure 3. First, we can observe that DeepBound significantly outperforms both TypicalRuleand MWUTest in terms of both precision and recall. Specifically,DeepBound obtains precision of 44% and 59% for start and endboundaries, while TypicalRule (better than MWUTest) gets 27%and 41%, respectively. The advantage of DeepBound is even morepronounced in terms of recall: DeepBound obtains 87% and 81%,while TypicalRule (better than MWUTest) gets 42% and 61%, forstart and end boundaries, respectively. Second, MWUTest is betterthan TypicalRule in terms of recall (about 8.1% and 5.9%, for startand end boundaries, respectively), but slightly worse in terms of precision (about 0.93% and 2.4%). Third, notice that all three methodshave a small variation over the 10 datasets, indicating the consistency of the methods on simulation datasets. Fourth, all the threemethods behave quite divergently on start and end boundaries. Thisis probably because the sequencing protocols have different distributions and patterns on start and end boundaries.The comparison of the three methods on 10 human simulationdatasets with 15 M reads is illustrated in Figure 4. Again,Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i267/3953980by gueston 07 January 2018i270M.Shao et al.200100STARTENDSTARTENDGEO Acc.#SpotsCell LineLocalizationLengthSRR534319SRR534291SRR545695SRR387661SRR307911SRR545723SRR315323SRR307903SRR315334SRR534307GSM981256GSM981244GSM984609GSM840137GSM758566GSM984621GSM765399GSM758562GSM765404GSM98125225M114M40M125M41M147M30M36M39M167MCD20þIMR90CD14þK562H1-hESCHMEpCNHEKBJHeLa-S3MCF-7cellcytosolcellcytosolcellcellnucleuscellcytosolcytosol76101767676101767676101TypicalRuleMWUTestDeepCNF1010202030Recall (%)4030Precision (%)80Recall (%)30404060TypicalRuleMWUTestDeepCNF20START0100STARTENDSTARTEND8060Recall (%)40403000102020Precision (%)506070Fig. 5. Comparison of the three methods on mouse simulations with eachdataset containing 150 M reads. The height of each bar shows the averageprecision (left part) and recall (right part) over the 10 datasets, while the thinbar on top illustrates the standard deviationTypicalRuleMWUTestDeepCNFSTARTEND0020Precision (%)5060405060Fig. 4. Comparison of the three methods on human simulations with eachdataset containing 15 M reads. The height of each bar shows the average precision (left part) and recall (right part) over the 10 datasets, while the thin baron top illustrates the standard deviation50Recall (%)40306080SRA Acc.40TypicalRuleMWUTestDeepCNF20Precision (%)506070Table 1. Summary of the 10 biological datasetsSTARTENDSTARTENDFig. 7. Comparison of the three methods on biological datasets. The height ofeach bar shows the average precision (left part) and recall (right part) overthe 10 datasets, while the thin bar on top illustrates the standard deviationdatasets, respectively. We can observe that our method trained onhuman datasets works surprisingly well on mouse, again outperforms TypicalRule and MWUTest by a significant margin. Thefact that human-trained model still performs well for mouse tellsthat the statistical pattern of boundaries is more heavily relatedwith sequencing and alignment errors, and reads coverage bias,rather than different species (notice that although the sequencingreads are simulated by Flux-Simulator, the reads alignmentare generated by ‘real’ aligner, HISAT). This is crucial for transcript boundary detection, since, as we illustrate here, we canuse deep learning model to capture the hidden patterns relatedwith boundaries from human data, and the trained model shall begeneral enough to be applied to infer boundaries for otherspecies.ENDFig. 6. Comparison of the three methods on mouse simulations with eachdataset containing 15 M reads. The height of each bar shows the average precision (left part) and recall (right part) over the 10 datasets, while the thin baron top illustrates the standard deviationDeepBound significantly outperforms both TypicalRule andMWUTest, especially in terms of recall. Comparing with the datasets with higher sequencing depth (Fig. 3), we can see that precisionof all three methods get improved, while recall keeps more or lessthe same. This is probably because higher sequencing depth alsointroduces more noise for identifying boundaries. Notice that ourmodel is trained on datasets with 150 M reads, thus the results heredemonstrate that our method does not suffer from overfitting withthe change of sequencing depths.The comparison of the three methods on 10 mouse datasetsare illustrated in Figures 5 and 6 for 150 M datasets and 15 M2.3 Results on biological datasetsTo compare the performance of the three methods on biologicaldatasets, especially to illustrate whether DeepBound trained onsimulation datasets can still obtain high accuracy on biological datasets, we choose 10 RNA-seq datasets from ENCODE project (summarized in Table 1). All these 10 datasets are from human speciesand employ paired-end and strand-specific sequencing protocols.They have various sequencing depths, read lengths and come fromdivergent cell lines.The comparison of the three methods on these biological datasets are illustrated in Figure 7. Again, our method outperformsTypicalRule and MWUTest by a significant margin in terms of bothrecall and precision for both start and end boundaries. These resultsalso demonstrate that our model trained on simulation datasets arenot overfitted towards simulations, but still be capable of applyingto biological datasets.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i267/3953980by gueston 07 January 2018DeepBound3 Materials and methodsIn this Section, we fist describe our method, DeepBound, in Section3.1. We then give further information about the parameters and implementing details of methodsTypicalRule and MWUTest in Section3.2 and Section 3.3, respectively.3.1 DeepBoundDeepBound involves three major steps (see Fig. 8). Given an instance(i.e. a partial exon), DeepBound first computes features for eachposition within this partial exon based on the coverage profile, andsequence information around the neighborhood of this position.Second, DeepCNF is applied to learn robust logic that can translatethese position-specific features to position-specific probabilities, i.e.the probabilities of being a boundary for each position. Third, withthese probabilities, we devise an efficient algorithm to infer actualboundaries, through iteratively identify regions that yield high average probables.3.1.1 Position-specific featuresFor each position i within a partial exon, we compute 17 features.The first feature we add is the reads coverage for position i. We thencompute features about the context of position i by considering different size of windows around position i. Specifically, for a windowsize of k, we compute nL and nR, which are the number of readsaligned within the region [i– k,i) and (i,i þ k], respectively. Byassuming a null hypothesis of uniform distribution, i.e. each of these(nL þ nR) reads has an equal chance to be in either region, we cancompute the P-value of that left (resp. right) region contains significantly more reads than right (resp. left) region:!nXRL þnnL þ nRPL ¼Á 0:5nL þnR ;nn¼nL!nXRL þnnL þ nRÁ 0:5nL þnR :PR ¼nn¼nRWe choose three window sizes, k ¼ 20, 50, 100 and for each k,we add the two P-values PL and PR and the two standard deviationsof the read coverage in the two regions as features. We also includethe sequence information as features. Specifically, for position k, weadd four binary features, indicating whether the nucleotide at thisposition is A, C, G or T. Finally, we incorporate another two binaryfeatures, indicating whether position i is the start or end boundaryof the whole gene loci (the boundaries of gene loci are very likely tobe boundaries of expressed transcripts).i2713.1.2 AUC-maximized deep convolutional neural fieldsWe formalize the problem of transcript boundary detection as asequential labeling problem. There are three possible labels, start,middle, end, representing whether a position is a start boundary,non-boundary, or end boundary, respectively. For position i withinthe partial exon, its predicted label is determined based on both thefeatures for position i and the predicted labels of position (i – 1) and(i þ 1). Our DeepCNF model has two major components: theConditional Neural Fields (CNF) module (Peng et al., 2009), andthe Deep Convolutional Neural Network (DCNN) module (Leeet al., 2009). The CNF module is a linear chain probabilistic graphical model which explicitly models conditional probability ofobserving the labeling sequences given the corresponding features.The binary potential of CNF model describes the dependency between the labels of adjacent positions. We substitute the original linear function with the DCNN model to capture the complex patternsin the training data. In particular, given the input features fetchedfrom two adjacent positions, there are nine different DCNN modelsquantifying the likelihood of observing nine pairs of states (for example, start to middle, etc.), respectively. The weights of all theDCNN models will be learned directly from training data and thenine DCNN models will compete with each other to determine thefinal assignments for each adjacent positions. Integrating DCNNmodel with CNF enables us to capture the complicated underlyingpredicting logic buried in the millions of labeled instances. To control model complexity and to avoid over-fitting, we add an L2-normpenalty term as the regularization factor and perform 10 fold crossvalidation to determine the hyper-parameter.One of the major challenges for the transcript boundary detection is the number of training samples for different labels areseverely imbalanced (Galar et al., 2012). Comparing with the nonboundary positions, the boundary positions are much rare, whichimplies that the empirical estimation of transition probabilities between non-boundary positions and boundary positions will be allnearly zero. This is a common problem for the genome and proteinsequence annotation where important nucleotides/genes/residues arerare. Traditional strategies usually solve this problem by downsampling the training samples with dominating labels. However, inour scenario, down-sampling will break the linear chain model intopieces and lead to a bias estimation of the transition probability.The computational challenge here is to solve the data imbalanceproblem while still keep the estimation of transition probabilityunbiased.To tackle this problem, in this work we replace the widely usedlog-likelihood objective function of probabilistic graphical modelwith an estimated AUC objective function during training processFig. 8. Illustration of the pipeline of DeepBound. Left: Features are extracted from reads alignment. The main feature we are using are the coverage informationover a window. Middle: Illustration of the DeepCNF model. DeepCNF model takes all features as input, and predicts the probability of being start, middle, end foreach position. Right: The ﬁnal boundaries are inferred using a greedy algorithm by taking the probabilities from the DeepCNF as inputDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i267/3953980by gueston 07 January 2018i272(Wang et al., 2016a). Note that this objective function is chosen independently to the structure and the potential functions of probabilistic graphical model. AUC can be derived from the ranking ofsamples by their predicted confidence of different labels (Cortes andMohri, 2003). The intuition of using AUC objective function is totransform the original classification problem using collection of individual samples to a new pairwise samples ranking problem. Thatis, instead of trying to assign a particular label l to a sample s, we tryto predict whether sample s has a larger marginal probability to belabeled as l comparing with sample t. It maximizes the AUC scorebecause as long as the partial order of the confidence between anypair of samples maintain the same, the AUC score will be the same.In this application, for a position in a partial exon with label start,its predicted marginal probability should be larger than the predicted marginal probability of middle and end generated by theDeepCNF model. The occurring frequency for each label is then become equal in all the new pairwise ranking samples and hence solvethe data imbalance problem.However, the above AUC-based objective function introduceslots of non-smooth indicator functions in order to indicate the partial order between two samples, making it not concave and thusvery hard to optimize. Another obstacle of the AUC function is thepairwise summation which requires O(n2) computing time. Totackle these problems, we first use a 10 order polynomial Chebyshevfunction to approximate the AUC function as a smooth and differential function (Calders and Jaroszewicz, 2007), which can be computed in linear time. To tradeoff between performance and trainingtime, here we use the L-BFGS (Liu and Nocedal, 1989) algorithm instead of stochastic gradient descent to optimize it.We emphasize that the AUC-maximized DeepCNF is a betterway to handle data imbalance issue comparing with other methodssuch as up-sampling, down-sampling, or weighting samples. This isbecause these sampling based methods are designed for independentdata points, but not suitable for sequence labeling since performingsampling might break the linear chain structure of the sequence. Infact, we have tried down-sampling on middle label positions; the result measured by AUC is only around 0.62, compared to AUCmaximized DeepCNF at 0.93, with the same model architecture. Wealso performed another experiment, in which we assumed each datais independent and trained a model to maximize likelihood. The resulting AUC was around 0.74, which is still not compatible to 0.93.We use a three layer neural network with 100 hidden neurons ineach layer, and set the window size of features as 75. By trying alternative settings, we found that there is no improvement by usingmore hidden neurons, more layers, or larger window size regardingthe predicting performance. The total 105423 instances (i.e. partialexons) in the two human simulation datasets (with 150 M pairedend reads in each dataset) are used to train our model.3.1.3 Predict boundariesFor each position within a given partial exon, the above DeepCNFmodel gives the probability of labeling this position as each of thethree labels. With these probabilities, we design a greedy algorithmto predict the actual start and end boundaries. Our algorithm iteratively compute the region of length 10 bp, which has the largestaverage probability of label start (resp. end) boundary. If this average probability is larger than a threshold P (we use 10-fold cross validation to fix this parameter as P ¼ 0.25), then the middle positionof the region is determined as a start (resp. end) boundary. Once anew boundary i is determined, the region (i – 50, i þ 50) shall be exempt from identifying the same type of boundary, and the algorithmM.Shao et al.continues to iteratively process the two sub-partial exons, one fromthe leftmost position to (i À 50), and the other from (i þ 50) to therightmost position of the current sub-problem. The algorithm terminates when no new boundary can be found.3.2 TypicalRuleGiven a partial exon, TypicalRule first partitions it into sub-partialexons by identifying gaps (i.e. regions with read coverage of 0) oflength larger than a threshold g. Then for each such gap,TypicalRule reports two boundaries, the left position of the gap asthe end boundary, and the right position of the gap as the startboundary. In addition to that, if the leftmost (resp. rightmost) position of the given partial exon is the start (resp. end) boundary of thewhole gene loci, then this position shall be reported as a start (resp.end) boundary. We have tried different value of g 2 f5 Á kj1k9g on the two training human simulation datasets, and chooseg ¼ 10.3.3 MWUTestMWUTest is a subroutine used in the TACO package to detectchange point [see (Niknafs et al., 2016) Online Methods], which isessentially the same problem we study here to identify transcriptboundaries. Given a partial exon, MWUTest iteratively identifiesthe potential change point, defined as the position i that minimizesthe following mean square error (MSE):MSEðiÞ ¼iX ðXk À X1 Þ2 þk¼1nX ðXk À X2 Þ2 ;k¼iþ1where Xk is the reads coverage at position k, n is the total length ofthis partial exon, and X1 (resp. X2) is the average reads coverage forthe region [1, i] (resp. (i, n]). MWUTest then tests whether this potential change point i meets both the following two significance criteria: the P-value under the Mann–Whitney U test shall be less thana threshold (we use the two training datasets to determine this parameter as 0.00001), and fold change of the average coverage between the two sides shall be larger than a threshold (we fix it as 3.0through trying different values on the training datasets). If the potential change point satisfies these two criteria, then MWUTest shallreport this position as a transcript boundary, and iteratively processthe two sub-partial exons. The algorithm terminates if no potentialchange point meets these two criteria.4 Conclusion and discussionIn this paper, we propose a new approach, DeepBound, to identifyboundaries of expressed transcripts from RNA-seq reads alignment byusing deep convolutional neural fields. The performance of our methodhas been extensively evaluated on both simulation datasets and biological datasets, and the results demonstrate that our method significantlyoutperforms two existing methods for boundary detection.We emphasize that our model is trained on the reads alignmentgenerated by simulated RNA-seq reads (using Flux-Simulation) followed by aligning them with ‘real’ aligner (HISAT). We show thatthis training process is successful by illustrating that this model canalso achieve high accuracy when applied to other species and to biological datasets. This opens a new way to employ the power of deeplearning technologies to solve challenging sequencing problemswhen large-scale labeled training samples are not available.Our learning framework is general and can be easily adapted toadd other features and trained and applied to other species. We haveDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i267/3953980by gueston 07 January 2018DeepBoundprovided the training and testing software package and users canuse their own gene boundary datasets to train their own model. Inthis work, we only apply RNA-seq because it is the most generaland popular protocol.A natural extension of our method is to incorporate it into thecomplete pipeline of transcript assembly. As a stand-alone softwarepackage, DeepBound can be used to detect transcript boundaries inthe RNA-seq experiments, and the results can be further applied tocorrect transcript abundance estimation and to study gene alternative splicing. For example, recent genomic analyses have indicatedthat many eukaryotic genes can have quite complex alternative splicing products, such as transcript embedded within another transcript(Kumar, 2009), transcripts from reverse strand (Adelman et al.,1987), intron retention (Ner-Gaon et al., 2004) and polyadenylationsites (Tian and Manley, 2013). We shall apply our tool to analyzethese cases in future.AcknowledgementsM.S. was supported in part by the Gordon and Betty Moore Foundation’sData-Driven Discovery Initiative Grant GBMF4554, the US National ScienceFoundation Grants CCF-1256087 and CCF-1319998, and the US NationalInstitutes of Health Grant R01HG007104 to Carl Kinsford. S.W. was supported in part by the US National Institutes of Health Grant R01GM089753and the US National Science Foundation Grant DBI-1564955 to Jinbo Xu.Conﬂict of Interest: none declared.ReferencesAdelman,J. et al. (1987) Two mammalian genes transcribed from oppositestrands of the same DNA locus. Science, 235, 1514–1518.Behr,J. et al. (2013) MITIE: simultaneous RNA-Seq-based transcript identiﬁcation and quantiﬁcation in multiple samples. Bioinformatics, 29, 2529–2538.Calders,T., and Jaroszewicz,S. (2007) Efﬁcient AUC optimization for classiﬁcation. In: Proceedings of European Conference on Machine Learning andPrinciples and Practice of Knowledge Discovery in Databases (ECMLPKDD’07), Volume 4702 of Lecture Notes in Computer Science, p.42–53.Cortes,C., and Mohri,M. (2003) AUC optimization vs. error rate minimization. In: Proceedings of Neural Information Processing. Systems (NIPS’03),vol. 9, p.10.Dobin,A. et al. (2013) STAR: ultrafast universal RNA-seq aligner.Bioinformatics, 29, 15–21.Galar,M. et al. (2012) A review on ensembles for the class imbalance problem:bagging-, boosting-, and hybrid-based approaches. IEEE Trans. Syst. ManCybern. C, 42, 463–484.Grabherr,M. et al. (2011) Trinity: reconstructing a full-length transcriptomewithout a genome from RNA-Seq data. Nat. Biotechnol., 29, 644.Griebel,T. et al. (2012) Modelling and simulating generic RNA-seq experiments with the ﬂux simulator. Nucleic Acids Res., 40, 10073–10083.Guttman,M. et al. (2010) Ab initio reconstruction of cell type-speciﬁc transcriptomes in mouse reveals the conserved multi-exonic structure of lincrnas. Nat. Biotechnol., 28, 503–510.Kim,D. et al. (2013) TopHat2: accurate alignment of transcriptomes in thepresence of insertions, deletions and gene fusions. Genome Biol., 14, R36.Kim,D. et al. (2015) HISAT: a fast spliced aligner with low memory requirements. Nat. Methods, 12, 357–360.Kumar,A. (2009) An overview of nested genes in eukaryotic genomes.Eukaryotic Cell, 8, 1321–1329.Lafferty,J. et al. (2001) Conditional random ﬁelds: probabilistic models for segmenting and labeling sequence data. In: Proceedings of 18th InternationalConference on Machine Learning (ICML’01), vol. 1, p.282–289.i273Lee,H. et al. (2009) Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In: Proceedings of. 26thInternational Conference on. Machine Learning (ICML’09), p.609–616.Li,J. et al. (2011a) Sparse linear modeling of next-generation mRNA sequencing (RNA-Seq) data for isoform discovery and abundance estimation. Proc.Natl. Acad. Sci. USA, 108, 19867–19872.Li,W., and Jiang,T. (2012) Transcriptome assembly and isoform expressionlevel estimation from biased RNA-Seq reads. Bioinformatics, 28,2914–2921.Li,W. et al. (2011b) IsoLasso: a LASSO regression approach to RNA-Seqbased transcriptome assembly. J. Comput. Biol., 18, 1693–1707.Lin,Y.-Y. et al. (2012) CLIIQ: accurate comparative detection and quantiﬁcation of expressed isoforms in a population. In: Proceedings of. 12thWorkshop on Algorithms in Bioinformatics. (WABI’12), Volume 7534 ofLecture Notes in Computer Science, p.178–189.Liu,D., and Nocedal,J. (1989) On the limited memory BFGS method for largescale optimization. Math. Program., 45, 503–528.Liu,J. et al. (2016a) BinPacker: packing-based de novo transcriptome assemblyfrom RNA-seq data. PLoS Comput. Biol., 12, e1004772.Liu,J. et al. (2016b) TransComb: genome-guided transcriptome assembly viacombing junctions in splicing graphs. Genome Biol., 17, 213.Martin,J. et al. (2010) Rnnotator: an automated de novo transcriptome assembly pipeline from stranded RNA-Seq reads. BMC Genomics, 11, 1.Ner-Gaon,H. et al. (2004) Intron retention is a major phenomenon in alternative splicing in arabidopsis. The Plant J., 39, 877–885.Niknafs,Y. et al. (2016) TACO produces robust multisample transcriptomeassemblies from RNA-seq. Nat. Methods, 14, 68–70Peng,J. et al. (2009) Conditional neural ﬁelds. In: Proceedings of. NeuralInformation Processing. Systems (NIPS’09), p.1419–1427.Peng,Y. et al. (2013) IDBA-tran: a more robust de novo de Bruijn graph assembler for transcriptomes with uneven expression levels. Bioinformatics, 29,i326–i334.Pertea,M. et al. (2015) StringTie enables improved reconstruction of a transcriptome from RNA-seq reads. Nat. Biotechnol., 33, 290–295.Robertson,G. et al. (2010) De novo assembly and analysis of RNA-seq data.Nat. Methods, 7, 909–912.Schulz,M. et al. (2012) Oases: robust de novo RNA-seq assembly across thedynamic range of expression levels. Bioinformatics, 28, 1086–1092.Tian,B., and Manley,J. (2013) Alternative cleavage and polyadenylation: thelong and short of it. Trends Biochem. Sci., 38, 312–320.Tomescu,A. et al. (2013) A novel min-cost ﬂow method for estimating transcript expression with RNA-Seq. BMC Bioinformatics, 14, 1.Trapnell,C. et al. (2010) Transcript assembly and quantiﬁcation by RNA-Seqreveals unannotated transcripts and isoform switching during cell differentiation. Nat. Biotechnol., 28, 511–515.Wang,S. et al. (2015) DeepCNF-D: predicting protein order/disorder regionsby weighted deep convolutional neural ﬁelds. Int. J. Mol. Sci., 16,17315–17330.Wang,S. et al. (2016a) AUC-Maximized deep convolutional neural ﬁelds forprotein sequence labeling. In: Proceedings of 15th European. ConferenceMachine Learning and Principles and Practice of Knowledge Discovery inDatabases (ECML PKDD’16), volume 9852 of Lecture Notes in ComputerScience, p.1–16. Springer.Wang,S. et al. (2016b) AUCpreD: proteome-level protein disorder predictionby AUC-maximized deep convolutional neural ﬁelds. Bioinformatics, 32,i672–i679.Wang,S. et al. (2016c) Protein secondary structure prediction using deep convolutional neural ﬁelds. Sci. Rep., 6, 18962.Wang,Z. et al. (2009) RNA-Seq: a revolutionary tool for transcriptomics. Nat.Rev. Genet., 10, 57–63.Xie,Y. et al. (2014) SOAPdenovo-Trans: de novo transcriptome assembly withshort RNA-Seq reads. Bioinformatics, 30, 1660–1666.Zerbino,D., and Birney,E. (2008) Velvet: algorithms for de novo short read assembly using de Bruijn graphs. Genome Res., 18, 821–829.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i267/3953980by gueston 07 January 2018
5028881998002	PMID28881998	5028881998	https://watermark.silverchair.com/btx266.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881998.main.pdf	Bioinformatics, 33, 2017, i359–i368doi: 10.1093/bioinformatics/btx266ISMB/ECCB 2017Systematic identification of featurecombinations for predicting drug response withBayesian multi-view multi-task linear regressionMuhammad Ammad-ud-din1,2,*,†, Suleiman A. Khan1,2,*,†,Krister Wennerberg1 and Tero Aittokallio1,2,31Institute for Molecular Medicine Finland FIMM, University of Helsinki, 00014 Helsinki, Finland, 2Department ofComputer Science, Helsinki Institute for Information Technology HIIT, Aalto University, 02150 Espoo, Finland and3Department of Mathematics and Statistics, University of Turku, 20014 Turku, Finland*To whom correspondence should be addressed.†The authors wish it to be known that, in their opinion, the ﬁrst two authors should be regarded as Joint First Authors.AbstractMotivation: A prime challenge in precision cancer medicine is to identify genomic and molecularfeatures that are predictive of drug treatment responses in cancer cells. Although there are severalcomputational models for accurate drug response prediction, these often lack the ability to inferwhich feature combinations are the most predictive, particularly for high-dimensional moleculardatasets. As increasing amounts of diverse genome-wide data sources are becoming available,there is a need to build new computational models that can effectively combine these data sourcesand identify maximally predictive feature combinations.Results: We present a novel approach that leverages on systematic integration of data sources toidentify response predictive features of multiple drugs. To solve the modeling task we implement aBayesian linear regression method. To further improve the usefulness of the proposed model, weexploit the known human cancer kinome for identifying biologically relevant feature combinations.In case studies with a synthetic dataset and two publicly available cancer cell line datasets, wedemonstrate the improved accuracy of our method compared to the widely used approaches indrug response analysis. As key examples, our model identiﬁes meaningful combinations of features for the well known EGFR, ALK, PLK and PDGFR inhibitors.Availability and Implementation: The source code of the method is available at https://github.com/suleimank/mvlr.Contact: muhammad.ammad-ud-din@helsinki.ﬁ or suleiman.khan@helsinki.ﬁSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionIdentifying the genomic and molecular features predictive of drug response in cancer cells is one of the prime aims of computational precision medicine. The identified features may help the clinician tochoose therapies tailored to an individual cancer patient and mayalso reveal mechanisms of drug actions. Recent large scale highthroughput screening experiments have opened new opportunitiesto build computational models of drug response predictions, by providing genomic and molecular profiles and drug response measurements on several hundreds of human cancer cell lines (Barretinaet al., 2012; Basu et al., 2013; Garnett et al., 2012; Iorio et al.,2016). Furthermore, the potential of genomic and molecularfeatures to predict drug responses in cell lines has been demonstratedin many recent studies (Costello et al., 2014; Cortes-Ciriano et al.,´2015; De Niz et al., 2016; Jang et al., 2014; Zhang et al., 2015).However, the small sample size in most of drug response studiesposes a challenging prediction task with a limited statistical strengthresulting into uncertain predictions.A promising direction is to help the learning process by formulating the problem as integrating multiple data sources, which are either readily available from high-throughput experiments orextracted from external sources as known prior knowledge. Theunderlying assumption is that some or all of the data sources mayexhibit a signal (set of features) predictive of the response variable.CV The Author 2017. Published by Oxford University Press.i359This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018i360For example, the expression patterns in only a small subset of thepathways may be linked to drug response, or a mutated gene presentin one data source may show up- or down-regulation of its expression in the other data source. Modeling the combination of sharedsignals from multiple sources may reveal hidden statistical relationships which may not be obvious from the data itself and are relevantfor the drug response prediction task. There is a need to developcomputational methods, commonly referred to as multi-view learning, that can effectively infer these signals from the data sources.Here, a key methodological challenge involves determining what isthe ‘useful signal’ (combination of predictive features) to extract.Another, closely-related problem of multi-task allows learning atask from other related tasks. For example, predicting one drug response alone can be considered as an individual task, whereas twodrugs whose responses are highly correlated can provide statisticalboost when learned together. This is especially beneficial when thenumber of samples are small, or when the samples come from a diverse collection such as in the pan-cancer scenario.A naive approach comprises of combining the different datasources into one data source and the use of a discriminative modelto learn a set of potentially predictive features by explicitly optimizing a cost function. However, such a discriminative model may result in too simple approach requiring strong regularization toeliminate the false positives, and it may be difficult to fully exploitthe multi-view nature of the data to extract the relevant signal.Kernel-based multi-view and multi-task predictive models haveshown to provide effective learning among distinct data sources anddrug classes (Ammad-ud din et al., 2016, 2014; Cichonska et al.,2015; Costello et al., 2014). Although these models can result inhighly accurate response predictions, they are less powerful in theircapability to identify the most predictive features (e.g. genes or mutations), making their practical usefulness quite limited for translational applications. While modeling the non-linear interactions ofthe signaling network in an interpretable fashion is an ongoing challenge, a simple formulation would be to model the linear combination of features and their networks that are relevant for drugresponse prediction.In this study, we present a Bayesian multi-view multi-task sparselinear regression model for cellular drug response prediction (illustrated in Fig. 1). The method solves the prediction problem by learning a model from multiple input data sources (here groups ofmolecular features) and output variables (here groups of drug responses). The model additionally identifies feature combinationsfrom the relevant data sources by assuming structured sparsity. Theproposed formulation assumes that only a few of the input datasources and features are predictive of a particular group of drugs,which share highly correlated response patterns. Hence addressingthe small sample size and high-dimensionality problem in drug response prediction.To capitalize on the proposed assumption, multiple input datasources are generated based on prior biological knowledge; here weextracted Functional-Linked-Networks of genes (FLNs) of the recently studied human cancer protein kinomes (Fleuren et al., 2016).M.Ammad-ud-din et al.2. We introduce a way for incorporating prior biological knowledge, in the form of Functional-Linked-Networks (FLNs) fordrug response modeling. Instead of using a single data sourcecomprising the genome-wide features, we treat FLN-basedgroups of features as multiple input data sources. Here the keyassumption is that biologically meaningful grouping of the features introduces additional structure that is valuable for prediction of drug responses.We first demonstrate the model’s predictive power on a syntheticdataset. We then show the significantly better performance of ourapproach on predicting drug responses in two publicly availablecancer datasets. Finally, we examine the inferred relationships between drug responses, FLNs and molecular features in the largerdataset, elucidating drug action mechanisms.2 Computational models in drug responsepredictionThe main idea of the computational models is very simple: givengenome-wide features of the cell lines as input (also known as independent variables or covariates) and drug responses as target (outputor dependent variables), learn a regression model of the drug sensitivity. The regression model can predict responses to new cell linesand can help interpreting features relevant to the response predictiontask.Nonlinear regression models such as kernel methods, supportvector regression, neural networks and random forests have beenwell-studied for drug response prediction on new cancer cell lines(Ammad-ud din et al., 2014; Ammad-ud din et al., 2016; Cichonskaet al., 2015; Costello et al., 2014; Dong et al., 2015; Menden et al.,2013; Ospina et al., 2014; Riddick et al., 2011; Zhang et al., 2015).Kernel-based methods have shown better predictive accuracy butlacks the ability to infer what genes are predictive of drug responses.Similarly, the random forest regression is built on the ensemble approach and is expected to provide high prediction accuracy, however its interpretability at the level of FLNs is currently limited.Although the method can handle a large number of features, thenumber of regression trees needed would also be very high raisingpotential complexity issues.On the contrary, in most translational applications, the objectiveis to identify features and networks that are relevant to the drug response prediction, linear models become a natural choice. A convenient aspect of the linear models is that they are easier to interpretand provide a straightforward analysis on the relationship betweenthe genomic and molecular features and drug responses.2.1 Linear regressionConsider X 2 RNÂD a matrix of genome-wide features and~ 2 RNÂ1 , the vector of drug responses. Here N denotes the numberyof samples (cell lines) and D represents the number of features(genes). Linear regression models the drug responses ~ as a linearycombination of unknown weight vector b 2 R1ÂD and the featuresX as~ $ XbTy1.1 ContributionsSpecifically, our contributions are 2-fold:1. We propose a novel formulation of Bayesian multi-view multitask linear regression. The method is simple to use and it provides straightforward means to identify feature combinationsthat are most predictive of drug responses.The machine learning goal is then to learn the optimal b to gain insights into important features. In genomic and molecular data, sincethe number of features is often much higher than the number of samples, the inference becomes ill-posed and suffers from over-fitting.A frequent solution is to introduce regularization that penalizes theDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018Multi-view multi-task linear regressioni361Fig. 1. Flow chart of the Bayesian multi-view multi-task linear regression approach. Left: The learning data consists of multiple data sources (here FLNs) extractedusing prior knowledge and denoted by Xð1Þ ::XðV Þ . Right: The model combines multi-view and multi-task learning to systematically identify feature combinationsð1Þð1Þð2Þð3Þðv Þ(b1 ; b2 ; b1 ; b1 ; bDv ) predictive of drug responses. The view-weights h(v) control the view-speciﬁc feature weights bðv Þ which are predictive of the drug responses and are shared across all the drugs. This structured formulation allows identiﬁcation of predictive views as well as features. The responses of multipledrugs are modeled by drug speciﬁc weights wtcomplexity of the model. The widely used elastic net regularizationby Zou and Hastie (2005), is represented as:b ¼ argmin jjY À Xbjj2 þ ðajjbjj þ ð1 À aÞ jjbjj2 Þ Â kb2212bHere k > 0 is the penalty parameter that controls the amount ofregularization and shrinking of the weight vector b. The penalty reduces to the ridge regression (Hoerl and Kennard, 1988) when a ¼ 0,and the lasso regression (Tibshirani, 1996) when a ¼ 1. For alla 2 ð0; 1Þ, it is the combination of the ridge and lasso regression.To identify genomic and molecular features predictive of drug responses in cancer cell lines, linear regression models employingridge, lasso and elastic net regularizations have been used in numerous benchmark experimental studies (Barretina et al., 2012; Basuet al., 2013; Garnett et al., 2012; Iorio et al., 2016) and they haveserved as popular comparison models in the context of drug response prediction in various applications (Chen et al., 2015; Cortes´Ciriano et al., 2015; Costello et al., 2014; De Niz et al., 2016; Janget al., 2014), as well as in this paper.Additionally, several extensions of linear models have also beenstudied for modeling drug responses including sparse partial leastsquares (sPLS) and sparse group lasso (SGL). In particular, sPLS isused for simultaneous dimension reduction and feature selection(Chun and Keles, 2010), while SGL extends lasso regression to¸groups of features (Chun and Keles, 2010). For drug response data¸sets of higher order, joint tensor models can be useful to analyse feature relationships (Khan and Kaski, 2014; Khan et al., 2016).regression task while pruning out any excessive views. This isachieved by incorporating two characteristics, i) controlling eachview’s (XðvÞ ) activation through a view-specific parameter for multiview learning; ii) performing simultaneous regression sharing information from multiple tasks (T) in Y. Here, a view is said to be activewhen (at least) some of its features are predictive of the outcome.Figure 1 illustrates MVLR model for the joint regression problem from the multiple views XðvÞ 2 RNÂDv , each representing anFLN of genes. More formally, the bðvÞ are feature-level coefficientsthat regress each of the corresponding views. Here, the view-specificweights hðvÞ controls the activation at the view-level, effectively limiting the search space to the predictive views (FLNs). The regressionfor multiple tasks is modeled through the wt weights that spanacross the set of drugs. The subscripts v, t and i index views, tasks,and training samples, while the total numbers of input views, tasks,and training samples are denoted by V, T and N, respectively.We next formulate a Bayesian treatment of the MVLR by complementing it with priors for model parameters. The distributionalassumptions for multi-view learning combined with multiple tasklearning for Y are as follows,!VXyt $ NðXðvÞ bðvÞ Þwt ; stv¼1ðvÞbdvðvÞ$ Cauchyð0; kdv hðvÞ ÞhðvÞ $ Dirðah ÞðvÞkdv $ Cauchyþ ð0; bk Þwt $ Dirðaw Þst $ Cauchyþ ð0; bs Þ ;3 Materials and methods3.1 Bayesian multi-view multi-task linear regressionWe formulate the multi-view multi-task linear regression (MVLR)problem for a collection of v ¼ 1,. . ., V input matrices (views or datasources) XðvÞ 2 RNÂDv and outcome matrix Y 2 RNÂT , as a joint regression that learns each views contribution to the multiplewhere Cauchy(a, b) is the Cauchy distribution parameterized by location a, scale b, and Dir is a Dirichlet prior with concentration parameter a. The bðvÞ coefficients are modeled using a Cauchy priorcentered at zero to induce regularization. The multi-view learning isachieved through the view-level parameters hðvÞ , which control theDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018i362variance for all coefficients bðvÞ in the corresponding view. AshðvÞ $ 0, all bðvÞ for the view v approach zero, while as hðvÞ increasesðvÞeach bdv is then controlled primarily by the corresponding featureðvÞlevel variance kdv . This structured-sparsity formulation allows themodel to identify the relevant views as well as the predictive features, while pruning out the excessive views. The hðvÞ are modeledusing a Dirichlet prior to induce view-level regularization, whichmatches our application assumption that only a subset of the viewsare relevant for the task. The multi-task parameters wt model the regression for multiple joint tasks.On the distributional choices, the Cauchy is a long tailed priorthat concentrates most of the mass around the area where values areexpected, though also leaves a considerable mass in the tails. It’s usefulness has been demonstrated previously in regression settings(Gelman et al., 2008). Our Dirichlet-Cauchy formulation, may alsobe seen as an extension of a global-local shrinkage construction,where the local shrinkage is enforced by the Cauchy while Dirichletcontrols the view-level shrinkage. For single input, global-localshrinkage priors have shown robust performance when the featuresare sparse, with the normal-Cauchy based horseshoe prior outperforming the laplace (Carvalho et al., 2010). For the variance parameter st we use the half-Cauchy prior of Gelman et al. (2006). Ourformulation can also be seen as an extension of the sparse group regularizer (Simon et al., 2013) in a Bayesian hierarchical formulationwith joint multi-task learning. The model is implemented in STAN(Carpenter et al., 2017) and inference is performed via variationalapproximation.3.2 Publicly available datasets and preprocessingIn this study, we used two publicly available cancer datasets to analyze cellular drug response predictions.3.3 Genomics of drug sensitivity in cancerThe first drug response data originated from Genomics of DrugSensitivity in Cancer (GDSC) project by Wellcome Trust SangerInstitute (Yang et al., 2013). For our analysis, we used data from124 human cancer cell lines and 47 anti-cancer drugs (belonging tothe class of kinase inhibitors), for which complete measurementswere available, and the drug response range was consistent with earlier publications (Garnett et al., 2012; Menden et al., 2013). Drugresponse measurements were summarized as log IC50 values, denoting the concentration of a drug required to inhibit the cell line’sgrowth by half. Additional information about drugs were also available, for instance, their primary therapeutic targets.The drugs were grouped into 16 classes based on their primarytarget, sample size and batch information. Specifically, drugs belonging to each target class and batch effect were considered an independent group. For example, two EGFR inhibitors, Erlotinib andLapatinib were profiled in a single batch and show comparable response, while other two Gefitinib and BIBW2992 were profiled inthe second batch showing correlated response; and were thereforeconsidered as independent groups for the modeling. SupplementaryMaterial Information section on "Cancer Data Sets" describes thebatch identification procedures and the groups in detail.M.Ammad-ud-din et al.study, we focused on the set of 14 drugs belonging to the class ofkinase inhibitors and 17 cell lines whose gene expression measurements were available from the GDSC project (Iorio et al., 2016).The drugs were grouped into 6 classes based on their primary targetinformation (Supplementary Material Section on "Cancer DataSets").In this article, we used gene expression profiles to represent thecell lines. Several studies including the benchmark drug sensitivityprediction challenge showed that the gene expression was the mostpredictive "omic" data source amongst others (Costello et al.,2014).3.5 Functional-linked-networksTo incorporate prior biological knowledge, we extracted FLNs ofknown human cancer protein kinases. This was done as a three-stepprocess. First, we obtained the set of 45 kinase families representedby 91 driver kinases in human cancers from (Fleuren et al., 2016).Fleuren et al. (2016) demonstrated that members of these kinasefamilies are commonly dysregulated in cancer.In the second step, we exploited the knowledge of kinase familiesin a biologically meaningful way to build functional linked networks. Specifically, for each of the 45 families, we used genes corresponding to the set of driver proteins to extract FLNs from theGeneMANIA prediction server (Warde-Farley et al., 2010).GeneMANIA takes in the list of genes and returns an extended listof genes, that are predicted to be functionally related using a largeset of association data, such as protein and genetic interactions,pathways, co-expression, co-localization and protein domain similarity. The recommended default settings of the GeneMANIA serverwere used to extract the FLNs.Finally, we take the genes participating in the FLNs as featuresand split the gene expression data into ‘views’, as shown in Figure 1.More specifically, a view comprises expression profiles of the genesthat belong to an FLN, and thereby represent a kinase family. As anexample, the EGFR kinase family contains EGFR, ERBB2, ERBB3and ERBB4 driver kinases, and is represented by an FLN of 24genes. The total number of genes in the 45 FLNs are listed inTable 1, while the description of FLNs along with the number ofgenes in each FLN, and the drug groups are provided in theSupplementary Material Information.3.6 Experimental setupWe compared the performance of our multi-view model with themost widely used linear regression models in drug response prediction problems over a grid of modeling choices, as shown in Table 2.Particularly, we learned regression models by varying the amount ofinput data (i.e. AllGenes, FLNsGenes, L1000Genes) and regularization parameters (i.e. ridge, elastic net and lasso). FLNsGenesdenotes the setting when the linear regression is learned using thenon-redundant set of genes derived from FLNs. We also used the setof 1000 genes (L1000Genes) provided by the LINCS project as abenchmark, denoting a common set of genes that are widelyTable 1. Multi-view data used in the drug response predictions3.4 Triple negative breast cancerThe second data contained responses of 301 approved and investigational anti-cancer drugs measured on 19 triple negative breast cancer (TNBC) cell lines at Institute for Molecular Medicine FinlandFIMM (Gautam et al., 2016). The response data were summarizedwith a drug sensitivity score (DSS) (Yadav et al., 2014). For our caseDatasetsGDSCFIMMCell linesDrugsAll genesFLNs (genes)124174714133211742045 (816)45 (935)Note: In parenthesis, the number of genes found in FLNs.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018Multi-view multi-task linear regressioni363expressed in diverse cellular processes and are representative of thegenome.We performed a leave-one-out cross validation (LOOCV) procedure, where in each fold one cell line is completely held-out (as atest cell line) and models were trained on the remaining cell lines(training data). The gene expression and drug response measurements were normalized to have zero mean and unit variance. An independent model was learned for each of the drug groups.We used sparse linear regression model implemented in theglmnet R-package (Friedman et al., 2010). The sparse linear regression has two parameters to be optimized: a (elastic net mixing parameter) and k (the penalty parameter), as discussed in section 2. Forelastic net predictions, we performed a nested 10-fold cross validation procedure on the training data, to choose optimal values for a2 ½0:1; 0:9  with an increment of 0.1 and k (from 100 values). We finally selected a combination of a and k values that gave minimumaverage error over the internal 10 fold cross-validation on trainingdata. To obtain the ridge and lasso predictions on the test cell line,we fixed a ¼ 0 and a ¼ 1 and choose k analogously.With MVLR, we can encode our prior belief through a strongerfeature-level sparsity when the number of views are small(ah ; aw ; bk ; bs set to 1,1,0.1,1) in the synthetic data case, and stronger view-level sparsity when the number of views is large in the twodrug response applications (ah ; aw ; bk ; bs set to 0.1,0.1,1,1).However, in the absence of prior belief’s the hyper-parameters canalso be learned using cross-validation. We evaluated the predictiveperformance of the methods in the unnormalized space usingTable 2. Computational models of drug response predictionsMethodRegularizationDataBayesian multi-view Structured priors FLNsMulti-taskLinear regressionRidgeAll genesFLNs genesL1000 genesElastic NetAll genesFLNs genesL1000 genesLassoAll genesFLNs genesL1000 genesView 1View 2AbbreviationMVLRR:AllGenesR:FLNsGenesR:L1000GenesEN:AllGenesEN:FLNsGenesEN:L1000GenesL:AllGenesL:FLNsGenesL:L1000GenesView 3correlations (Spearman and Pearson) and root-mean-squarred error(RMSE) averaged over all the drugs in each drug-group. The RMSEwas normalized to compute NRMSE such that the baseline (meanprediction) NRMSE is 1. The run time of MVLR and elastic net algorithms were less than 60 seconds for one cross-validation fold onthe larger dataset (GDSC) using a Mac Book Pro (2.9Ghz, IntelCore i7, 16 GB RAM; MVLR: 43 sec, Elastic Net: 11 sec).4 Results and discussion4.1 Synthetic datasetWe first demonstrate in a simulated example the model’s ability tocorrectly prune out the excessive views, as well as precisely identifythe sparse feature weights. We plot the behaviour of elastic net regression simultaneously, for illustration purpose.To demonstrate the ability of our method in a multi-view examplecase, four views were generated XðvÞ 2 RNÂDv , for v ¼ 1: 4, N ¼ 40and Dv ¼ 45 6 5 dimensions in each view v, such that the first twoviews were embedded with 10% features whose combinations werepredictive of the response variables (Y 2 RNÂT , T ¼ 6), while the remaining two views were composed of random features. Figure 2, leftshows the loadings used to create the response variables in all the fourviews, and the corresponding estimates of the model parameters byour method. Our model correctly segregates the views relevant for thepredictions from the excessive ones, as well as correctly identifies thefeature weights for the predictive features. The feature weightslearned by the elastic net by concatenation of the features from multiple views is also plotted. While the embedded features are correctlyidentified by both methods, deviations in the excessive views werepruned out only by the multi-view formulation.We next evaluate the models performance over the spectrum ofsmall sample and high-dimensional settings (Fig. 2, right).Specifically, we generate data analogous to the above settings v ¼ 1: 4; Dv ¼ 4565 while varying the number of samples on x-axis. Werepeat each experiment 50 times with noise varying between 1-25%of the variation of data, to obtain robust estimates; and plot theaverage LOOCV performance, using correlation and NRMSE. Ourmulti-view regression performs consistently well, and is especiallybeneficial when the sample sizes are small.We also validate our model on single-view datasets, confirmingthat it performs comparably to the existing methods in identifyingView 4MVLRElastic Net 1.0True0.6CorrelationNRMSE0.4feature weights0.80.2●●●●●●0.00.6●−0.20.4−0.4●●−0.601030010203040010 20 30 40010 20 30 4020feature index3040number of samples (N)●5060Fig. 2. Performance of the method on synthetic dataset. Left: The ﬁgure demonstrates the models functionality by effectively shutting down excessive views toprune the search space, and its ability to identify the features weights correctly. The true weights corresponding to the four views are shown along with theweights learned by our model and elastic net regression. The view-sparsity in MVLR shuts down the irrelevant views. Right: Prediction performance of our modeland the comparison approach when the number of sample size is varied. Each point represents the average prediction performance over 50 experiments witherror bars indicating one standard error over the mean. The structured sparsity assumptions of our model are especially beneﬁcial when the sample sizes aresmall in comparison to the number of dimensionsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018i364M.Ammad-ud-din et al.analogous and correct set of features in the synthetic data(Supplementary Fig. S1).4.2 Cancer datasetsWe next compare the MVLR method with the alternatives on twocase studies GDSC (pan-cancer) and FIMM (TNBC), and report theirpredictive performances in the LOOCV procedure for the differentdrug groups. Figure 3 shows the predictive performances of all themethods on the GDSC (left) and FIMM (right) datasets. MVLR outperformed its competitor in both case studies. The predictive performance obtained by MVLR was found to be significantly higher than theothers (p <0.01; one-sided paired Wilcoxon signed-rank test, corrected for multiple testing, Supplementary Table S5) in GDSC dataset.Also in the smaller FIMM dataset, the predictive performance obtained by MVLR was also found to be significantly higher than theothers (P <0.05; one-sided paired Wilcoxon signed-rank test, corrected for multiple testing, Supplementary Table S9).Figure 3 demonstrates the predictive performances over thedrugs groups for each method. As the key observation, multi-viewregression combined with prior biological knowledge improves thedrug response predictions. MVLR supplemented with FLNs outperformed ridge, elastic net and lasso regressions either supplementedwith or without FLNs/L1000 information. This result confirms thatstandard linear regression model does not seem to greatly benefitfrom the prior knowledge mainly due to the lack of systematicmulti-view modeling approach. When using FLNs information, theperformance was not better than using the full set of genes (i.e.AllGenes), except for the R:FLNsGenes scenario in FIMM dataset.Moreover, we also observed in our feasibility tests that the prediction performance did not improve even if the linear regression wasapplied in a heuristic multi-view setting (for instance concatenatingthe FLNs with duplicates to make one big input data matrix X).Secondly, the biological knowledge (FLNs) and the molecular features showed response predictive signal, outperforming the baselineperformance. The baseline prediction for the test sample is obtainedas the mean of the training drug response data. Notably, the meanprediction of uncentered data yields a correlation of –1, when usingLOOCV (See Empirical evidence of mean prediction correlation’ inthe Supplementary Material Information); implying that negativecorrelations in Figure 3 represent lower prediction performancesthat are closer to the baseline.In GDSC dataset, when predicting responses to EGFR inhibitors(Erlotinib and Lapatinib, see methods for drug groups), MVLRdemonstrated better performance then linear regression. WhereasEN:FLNsGenes (Spearman correlation ¼ 0.272), L:FLNsGenes(0.233), EN:FLNs (0.272) and L:FLNs (0.233) gave slightly betterpredictions than MVLR (0.218) for the Gefitinib and BIBW2992(EGFR inhibitors). As expected, MEK inhibitors were predictedwith high accuracy with all the methods. Most of the drug groupswere consistently predicted better by MVLR than with any variantof the standard linear regression. While predicting GW843682 andBI-2536 (PLK inhibitors), Sunitinib and Sorafenib (PDGFRA,PDGFRB, KDR, KIT, FLT3 inhibitors) and TW-37 and ObatoclaxMesylate (BCL inhibitors) MVLR gave correlation values of 0.151,0.214 and 0.173 compared to –0.367, –0.425 and –0.881, respectively. Similar trends in predictive performances can be observed inPearson correlation and NRMSE from the Supplementary MaterialInformation (Supplementary Fig. S8 and S2–S4).Likewise, in FIMM (TNBC) dataset, MVLR shows robust predictions for PI3K, PLK and ABL inhibitors, compared to other methods. Linear regression also performs well in predicting responses toMTOR and PI3K/MTOR inhibitors, nevertheless does not outperform the MVLR method. On the other hand, in case of CDK inhibitors, linear regression gave slightly improved predictions withEN:AllGenes (0.206), R:FLNsGenes (0.246), EN:FLNsGenes(0.227) and L:FLNsGenes (0.227) compared to MVLR (0.202).Supplementary Material Information (Supplementary Fig. S9 andTables S6–S8) demonstrates the comparison results in the form ofPearson correlation and NRMSE.GDSCR:AllGenes●●● ●●●●●●●●●●●● ● ●●● ●●●●●●●●●●●●● ● ●●● ●●●0.5−0.5●●●●●●L:FLNsGenes●●● ●● ●●●●●●●●●●●●●●●● ●● ●●● ●●●−1.0R:L1000Genes●●●●●●●●●●●●●L:L1000Genes●●● ● ●●● ●●●●●●●●●●●● ● ●●●●●0.5●● ●0.0●●●●● ●●●●●●L:FLNsGenes●●● ●●●●● ● ●−0.5−1.0R:L1000Genes0.5EN:L1000Genes●●●● ● ●0.0−0.5EN:FLNsGenes●●●●●●●●●L:L1000Genes●●●●●●−0.5TargetEGFRMEKBRAF●●●PLKABLSRC●●●CDKPDGFRBCL●0.50.0−0.5−1.00.50.0−1.0LR●●●−0.5−1.00.50.50.0−0.5−1.0MTORPI3KALK0.5−0.5●●●0.0−1.00.50.0−0.5−1.0−1.00.00.0●● ● ●●● ● ●●●EN:L1000GenesR:FLNsGenesMVLRMVLR●●● ● ●●● ● ●● ●EN:FLNsGenes−0.50.5●●−1.0R:FLNsGenes0.0●L:AllGenes−0.5−1.00.5EN:AllGenes●●●●●●0.0−0.50.0●● ● ●●●●●●R:AllGenesL:AllGenes−1.00.5FIMMEN:AllGenesLRAKTTarget●●MTORPI3K/MTOR●●PI3KPLK●●ABLCDKFig. 3. Spearman correlations on individual drug groups colored according to their primary target, computed across cell lines. Left: GDSC dataset, Right: FIMMdataset. Table 2 explains the method abbreviations. The predictive performance obtained by MVLR (shown on y-axis) for both datasets is found to be signiﬁcantlyhigher than the others shown on x-axis (P <0.05; one-sided paired Wilcoxon signed-rank test corrected for multiple testing). Here, negative correlations correspond to poor performance as the baseline performance is –1, which is obtained using the mean of the training drug response data as predictions for the testsampleDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018Multi-view multi-task linear regressioni365In the case of a single cancer subtype when the number of samples is often quite limited, evaluating the predictions becomes a challenging task. We therefore investigate the reproducibility of thepredictions on FIMM TNBC dataset (n ¼ 17), and compute the variance of the performance scores across ten model runs. The resultsshow that the prediction performance of our model is similar withstandard deviations between 0.05 and 0.12 for different drug groups(Supplementary Table S10).In addition to the widely used linear approaches in drug responsemodeling, we also investigate MVLR in comparison to other computational methods. Specifically, we compare the model’s performanceto sparse partial least squares (sPLS; Chun and Keles, 2010), sparse¸group lasso (SGL; Simon et al., 2013), random forest (RF; Ishwaranet al., 2008) and support vector machine (SVM; Tuia et al., 2011),using comparable multi-task variants where available. Table 3Table 3. Prediction performance measured as the Spearman correlation averaged over the drug groupsGDSCFIMMsPLSSGLRFSVM0.3750.3360.3300.2730.3380.3000.3590.2950.3630.349PLKEGFRHPEFGFRPKCKSGKRSTKALDBRSTEP4.3 Inferring gene-FLNs-drug response relationshipsThe use of multi-view data and model not only improves the prediction performance, but also helps to infer gene-FLNs-drug responserelationships. We further analyze the FLNs-drug response relationship in the GDSC dataset, followed by the subsequent analysis forthe well known EGFR, ALK, PLK and PDGFRA, PDGFRB, KDR,KIT, FLT3 inhibitors. To focus on the most predictive FLNs we consider the top-3 FLNs identified by the model for each drug group, inthe subsequent analysis.Figure 4 illustrates the FLNs-drug response relationships in theform of an eye diagram. A striking characteristic of the model is evident from the findings. In the case of four different inhibitor classes,MVLR identifies top predictive FLNs correctly. For the remainingErlotinibLapatinibGefitinibBIBW2992NVPPF -TAE684GW 02341084BI663Su -253 6826nitinibMVLRshows the prediction correlation averaged over all the drug groups,while individual performances can be found in SupplementaryTables S11–S13. MVLR demonstrates better mean prediction correlation in comparison to all the methods (except SVM in FIMM dataset); and significantly outperforms linear methods sPLS and SGLwith p¡0.05, one-sided paired Wilcoxon signed-rank test in theGDSC dataset. Notably, our method provides mechanistic interpretations at the level of both, FLNs and genes, in contrast to SVM andRF.E7KEGFRALKPLKAKTPDGFRCAMKLMEKBRAFPDGFRMTORRAFPI3KABLSRCCDKBCLAKTTAF1L1PIKKMAPKG1ABRKLRRapamycinJW-7-52-1GDC0941AZD6482AP2Nilo 4534tiniDabA- satinib77W0041AZ H-4D- -02305304479660P- 829CG P-0 itineCG scov06Ro-331RO 33299-0PD37TWesylateclax-MObatoitor-VIIIAKT-inhibA-443654TRIOAURKDYRKLAMDCKTRNKWKAJAibenibSo itin 06xA G-7 9AM EA11RD 0401CI- 325901D-0P6244AZDPL4720SB590885fraFig. 4. FLNs-drug response relationships in the GDSC dataset, visualized as an "eye diagram". For each primary target group (middle) and their correspondingdrugs (right), and the top three predictive FLNs (left) are shown. (a) EGFR Inhibitors (Erlotinib and Lapatinib). (b) EGFR Inhibitors (Geﬁtinib and BIBW2992). (c)ALK Inhibitors. (d) PLK Inhibitors. (e) PDGFRA, PDGFRB, KDR, KIT and FLT3 InhibitorsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018i366M.Ammad-ud-din et al.the inhibitors are related to these FLNs, making it possible to inhibitthe corresponding gene activities in the respective FLNs.We next analyze the combination of features from these FLNsand the drug responses, visualized as heatmaps (drawn using theComplexheatmap package in R programming language (Gu et al.,2016)).classes either the direct FLN is not available or MVLR has identifieda downstream FLN which may require further lab validation (for instance MAPK for BCL inhibitors, PEK for CDK, PI3K, MTOR andPLK target groups). Nevertheless, the identified FLNs already serveas proof-of-concept positive controls for the validation of the model.These FLNs include EGFR, FGFR, PKC, SGK and STKR predictiveof EGFR inhibitors (erlotinib, lapatinib, gefitinib and BIBW2992),ALK, BRD and STK7 predict ALK inhibitors (NVP-TAE684 andPF-02341066), PLK, PEK and EPH are found predictive of PLK inhibitors (GW843682X and BI-2536) and PDGFR, CAMKL andAKT predict PDGFRA, PDGFRB, KDR, KIT, FLT3 inhibitors (sunitinib and sorafenib) respectively. It is biologically meaningful that4.4 EGFR inhibitorsFigure 5a and b illustrates multiple known features as top predictorsof responses to EFGR inhibitors. For example EGFR, ERBB2 andNRG1 were among the top predictive features identified by theMVLR method. The over expression of these genes links to theDrugs(b)Drugs(a)ErlotinibLapatinibEGFR*NRG1ERBB2VAV3ERBB2ANKS1AFGFRAREGPIGREGFR Inhibitors (Erlotinib and Lapatinib)TK10EC−GI−10TE−6GCIYTE−12NB5HCC2157TE−1HCC2218CW−2MN−60MFM−223NCI−SNU−5weightCell linesSF2680 0.2 0.4 0.6NCI−H345ENPP1LU−65*TE−12TK10TE−1TE−6EW−24HCC2218GCIYDJM−1NB5TE−10IM−9EW−18HL−60MN−60HTGOTOES5U−698−MTURGDF5NCI−H446BMP7*KAL1NCI−H2141*NB6TFF1SK−MM−2FRS2FGFR3SGKPKCERBB4PRKCZSTKR*NRG1ANKS1BP31−FUJ*EGFR*EGFREGFRGefitinibBIBW2992*0 0.1 0.2 0.3 0.4 0.5 0.6weightCell linesEGFR Inhibitors (Geﬁtinib and BIBW2992)NVPTAE684PF02341066ALK*DrugsDrugs(d)ALK(c)GW843682BI2536EPHA5PTNPPFIA4EPHCLDN1*EPHB1BRDAKT1PPFIBP2**GNL3LPPM1APEKCCNT2NFE2L1*MAPK14VPS33APLK3PLKSTE7KSR1MAPK10SPRY2Cell linesLS−411N697KM12TE−1GT3TKBTE−100 0.1 0.2 0.3 0.4 0.5weightPLK InhibitorsAKT DrugsALK Inhibitors(e)NCI−H82SK−NEP−1ST486IM−9MS−1RLTURLNCaP−FGCCW−2ES5WSU−NHLweightEW−110 0.1 0.2 0.3 0.4 0.5P31−FUJPLK2EW−18SRKARPAS−299KM12NB14EW−1KINGS−1NCI−SNU−5697ST486CA46LAN−6ES3SHP−77HCC2218NCI−H446MFM−223EB2IM−9EC−GI−10P31−FUJMAP3K4Cell lines**SunitinibSorafenibAKT2*CAMKLADIPOR2STK11SMARCD3CAB39L**FLT3PDGFRCAV3FLT3LGCSF1KM12MONOMAC6SF126TE−12SR697HL−60SK−NEP−1ML−2Cell linesNCI−SNU−16BeckerCW−2GI−ME−NLNCaP−FGCNCI−SNU−5SW684MFM−223TURP31−FUJIM−9PTPRO00.1 0.2 0.3 0.4weightPDGFRA, PDGFRB, KDR, KIT and FLT3 InhibitorsFig. 5. Heatmaps of feature combinations predictive of drug responses (log IC50) for the 10 most sensitive (shown in violet) and resistant (shown in orange) cancercell lines from the GDSC dataset. For each cell line, gene expression features are shown (blue corresponds to lower expression, red to higher expression). On theright side of each feature is a bar indicating the absolute value of the weight (b of the MVLR model). Bars in violet are negative weights, indicating features associated with sensitivity, and bars in orange are positive weight, indicating features associated with resistance. For clarity, only the top ten features having largestweights from the top three FLNs are shown. Feature weights marked with an asterisk (*) are statistically signiﬁcant (p¡0.05, permutation test). The gene expression features are grouped based on the FLNs information, denoted on the left of the heatmapsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018Multi-view multi-task linear regressionresponse of EGFR inhbitors. As a sanity check, we also validatedthese results from the findings reported in the benchmark study thatpublished the original data (Garnett et al., 2012). Our results wereconsistent with their findings. ERBB2 (also known as HER2) overexpression was associated with sensitivity to EGFR-family inhibitorsincluding lapatinib and BIBW2992. Interestingly, the second mostpredictive feature predicting responses to gefitinib and BIBW2992 isFGFR3 which is a part of FGFR FLN. The role of FGFR signalingpathway in cancer is highly studied, however its downstream effectson the EGFR signaling for all types of cancers is not fully understood (Turner and Grose, 2010). Early studies have reported thatover expression of FGFR2 and FGFR3 can mediate resistance toEGFR inhibitor therapy in lung cancer (Ware et al., 2010) and resistance to HER2 inhibitors in HER-positive breast cancer(Koziczak et al., 2004). In Figure 5b, the identification of FGFR3may generate novel hypothesis on the effect of FGFR3 gene on theEGFR/ERBB2 gene family in oesophagus, stomach and kidney cancers (cell line names TE-6, TE-12, EC-GI-10, GCIY, TK10). Thesefindings either suggests that FGFR3 is involved in EGFR/ERBB2 signaling or it can feed the same type of oncogenic signals as EGFR.For these cancers, especially FGFR3 may play a kind of co-factorrole with EGFR and may be investigated to design therapeuticallytargetted drugs in future.i367cells. In experiments with a synthetic as well as two publicly available cancer datasets, the proposed method showed improved predictive accuracy compared to state of the art linear regression modelin drug response prediction. We also demonstrated the usefulness ofour model, combined with prior knowledge for inferring the relationships between FLNs and drug responses. The results showedthat the proposed model identified robust and biologically meaningful feature combinations for predicting sensitivity to the well knownEGFR, ALK, PLK and PDGFR inhibitors. This way of identifyingpredictive feature combinations using groups of genes (encoded inthe form of FLNs) may enhance our understanding of the actionmechanism of drugs and can potentially be used to identify novelcombination of predictive biomarkers for designing personalizedtherapies for cancer patients.FundingThis work was financially supported by the Academy of Finland (grants296516 to S.A.K.; grants 272577, 277293 to K.W.; grants 269862, 272437,279163, 292611 and 295504 to T.A.), Cancer Society of Finland (T.A. andK.W.) and the Sigrid Juselius Foundation (K.W.). The authors wish to ac´knowledge Aalto Science-IT project and CSC-IT Center for Science, Finland,for computational resources.Conflict of Interest: none declared.4.5 ALK inhibitorsFigure 5c shows the predicted features associated with sensitivity toALK inhibitors. Among others ALK, CLDN1 and MAPK14 as themost predictive features identified by MVLR. ALK inhibitors connected to the high expression of ALK gene is biologically plausible.4.6 PLK inhibitorsFigure 5d represents the top features predictive of responses to PLKinhibitors. For example PPM1A, VPS33A and PLK3 were amongthe top predictors identified by the MVLR method. High expressionof PLK3 and VPS33A genes are positively associated with the sensitivity to PLK inhibitors.4.7 PDGFRA, PDGFRB, KDR, KIT, FLT3 inhibitorsFigure 5e illustrates the top predictors of responses to sunitinib andsorafenib, which are essentially the multi-target inhibitors (PDGFRA,PDGFRB, KDR, KIT, FLT3). MVLR found FLT3 from the PDGFRFLN as one of the features postively associated with their responses inblood cell line (MONOMAC6). It is also known that PDGFR inhibition leads to AKT activation (Zhang et al., 2007), supporting theidentification of the AKT-related FLN in the analysis.The analysis demonstrated that gene-FLNs-drug response relationships provide biologically meaningful insights. These are wellstudied examples serving as proof-of-concept positive controls, forthe proposed MVLR method. As demonstrated, MVLR was successfully able to identify predictive feature combinations within a singleFLN and from across multiple FLNs. This systematic identificationof feature combinations is made possible with a multi-view learningapproach defined with structured sparse priors. The priors allowedMVLR to choose first the correct FLNs and secondly identify thefeature combinations maximally predictive of drug responses, fromthe chosen FLNs.5 ConclusionWe presented a new Bayesian multi-view multi-task linear regressionmodel for identifying features predictive of drug responses in cancerReferencesAmmad-Ud Din,M. et al. (2014) Integrative and personalized QSAR analysisin cancer by Kernelized Bayesian matrix factorization. J. Chem. Inf. Model,54, 2347–2359.Ammad-Ud Din,M. et al. (2016) Drug response prediction by inferringpathway-response associations with Kernelized Bayesian matrix factorization. Bioinformatics, 32, i455–i463.Barretina,J. et al. (2012) The cancer cell line encyclopedia enables predictivemodelling of anticancer drug sensitivity. Nature, 483, 603–607.Basu,A. et al. (2013) An interactive resource to identify cancer genetic and lineage dependencies targeted by small molecules. Cell, 154, 1151–1161.Carpenter,B. et al. (2017) Stan: a probabilistic programming language. J. Stat.Software, 76, 1–32.Carvalho,C.M. et al. (2010) The horseshoe estimator for sparse signals.Biometrika, 97, 465–480.Chen,B.J. et al. (2015) Context sensitive modeling of cancer drug sensitivity.PloS One, 10, e0133850.Chun,H. and Keles (2010) Sparse partial least squares regression for simul¸,S.taneous dimension reduction and variable selection. J. R Stat. Soc. Ser. B(Statistical Methodology), 72, 3–25.Cichonska,A. et al. (2015) Identiﬁcation of drug candidates and repurposingopportunities through compound–target interaction networks. ExpertOpin. Drug Discov., 10, 1–13.Cortes-Ciriano,I. et al. (2015) Improved large-scale prediction of growth in´hibition patterns using the NCI60 panel. Bioinformatics, 31, btv529.Costello,J.C. et al. (2014) A community effort to assess and improve drug sensitivity prediction algorithms. Nat. Biotechnol., 32, 1202–1212.De Niz,C. et al. (2016) Algorithms for drug sensitivity prediction. Algorithms,9, 77.Dong,Z. et al. (2015) Anticancer drug sensitivity prediction in cell lines from baseline gene expression through recursive feature selection. BMC Cancer, 15, 489.Fleuren,E.D. et al. (2016) The kinome’at large’in cancer. Nat. Rev. Cancer,16, 83–98.Friedman,J. et al. (2010) Regularization paths for generalized linear modelsvia coordinate descent. J. Stat. Software, 33, 1.Garnett,M.J. et al. (2012) Systematic identiﬁcation of genomic markers ofdrug sensitivity in cancer cells. Nature, 483, 570–575.Gautam,P. et al. (2016) Identiﬁcation of selective cytotoxic and synthetic lethal drug responses in triple negative breast cancer cells. Mol. Cancer, 15, 1.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018i368Gelman,A. et al. (2006) Prior distributions for variance parameters in hierarchical models (comment on article by browne and draper). BayesianAnal., 1, 515–534.Gelman,A. et al. (2008) A weakly informative default prior distribution for logistic and other regression models. Ann. Appl. Stat., 2, 1360–1383.Gu,Z. et al. (2016) Complex heatmaps reveal patterns and correlations inmultidimensional genomic data. Bioinformatics, 32, 2847–2849.Hoerl,A. and Kennard,R. (1988). Ridge regression, in Encyclopedia ofStatistical Sciences, vol. 8.Iorio,F. et al. (2016) A landscape of pharmacogenomic interactions in cancer.Cell, 166, 740–754.Ishwaran,H. et al. (2008) Random survival forests. Ann. Appl. Stat., 2,841–860.Jang,I.S. et al. (2014) Systematic assessment of analytical methods for drug sensitivity prediction from cancer cell line data. In: Proceedings of the PaciﬁcSymposium. pp. 63–74. Kohala Coast, Hawaii, USA.Khan,S.A. and Kaski,S. (2014) Bayesian multi-view tensor factorization. In:Joint European Conference on Machine Learning and KnowledgeDiscovery in Databases. pp. 656–671. Springer Berlin Heidelberg.Khan,S.A. et al. (2016) Bayesian multi-tensor factorization. Machine Learn.,105, 233–253.Koziczak,M. et al. (2004) Blocking of fgfr signaling inhibits breast cancer cellproliferation through downregulation of d-type cyclins. Oncogene, 23,3501–3508.Menden,M.P. et al. (2013) Machine learning prediction of cancer cell sensitivityto drugs based on genomic and chemical properties. PLoS One, 8, e61318.Ospina,J.D. et al. (2014) Random forests to predict rectal toxicity followingprostate cancer radiation therapy. Int. J. Radiat. Oncol.* Biol.* Phys., 89,1024–1031.M.Ammad-ud-din et al.Riddick,G. et al. (2011) Predicting in vitro drug sensitivity using random forests. Bioinformatics, 27, 220–224.Simon,N. et al. (2013) A sparse-group lasso. J. Comput. Graph. Stat., 22,231–245.Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. RStat. Soc. Ser. B Methodol., 58, 267–288.Tuia,D. et al. (2011) Multioutput support vector regression for remote sensingbiophysical parameter estimation. IEEE Geosci. Remote Sensing Lett., 8,804–808.Turner,N. and Grose,R. (2010) Fibroblast growth factor signalling: from development to cancer. Nat. Rev. Cancer, 10, 116–129.Warde-Farley,D. et al. (2010) The genemania prediction server: biological network integration for gene prioritization and predicting gene function. Nucl.Acids Res., 38 (2), W214–W220.Ware,K.E. et al. (2010) Rapidly acquired resistance to egfr tyrosine kinase inhibitors in nsclc cell lines through de-repression of fgfr2 and fgfr3 expression. PloS One, 5, e14117.Yadav,B. et al. (2014) Quantitative scoring of differential drug sensitivity forindividually optimized anticancer therapies. Sci. Rep., 4, 5193.Yang,W. et al. (2013) Genomics of drug sensitivity in cancer (GDSC): a resource for therapeutic biomarker discovery in cancer cells. Nucl. Acids Res.,41, D955–D961.Zhang,H. et al. (2007) Pdgfrs are critical for pi3k/akt activation and negativelyregulated by mtor. J. Clin. Invest., 117, 730–738.Zhang,N. et al. (2015) Predicting anticancer drug responses using a dual-layerintegrated cell line-drug network model. PLoS Comput. Biol., 11,e1004498.Zou,H. and Hastie,T. (2005) Regularization and variable selection via theelastic net. J. R Stat. Soc. Ser. B (Statistical Methodology), 67, 301–320.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i359/3953979by gueston 07 January 2018
5028881997002	PMID28881997	5028881997	https://watermark.silverchair.com/btx265.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881997.main.pdf	Bioinformatics, 33, 2017, i350–i358doi: 10.1093/bioinformatics/btx265ISMB/ECCB 2017Predicting phenotypes from microarrays usingamplified, initially marginal, eigenvectorregressionLei Ding and Daniel J. McDonald*Department of Statistics, Indiana University, Bloomington, IN 47405, USA*To whom correspondence should be addressed.AbstractMotivation: The discovery of relationships between gene expression measurements and phenotypic responses is hampered by both computational and statistical impediments. Conventionalstatistical methods are less than ideal because they either fail to select relevant genes, predictpoorly, ignore the unknown interaction structure between genes, or are computationally intractable. Thus, the creation of new methods which can handle many expression measurements onrelatively small numbers of patients while also uncovering gene–gene relationships and predictingwell is desirable.Results: We develop a new technique for using the marginal relationship between gene expressionmeasurements and patient survival outcomes to identify a small subset of genes which appearhighly relevant for predicting survival, produce a low-dimensional embedding based on this smallsubset, and amplify this embedding with information from the remaining genes. We motivate ourmethodology by using gene expression measurements to predict survival time for patients withdiffuse large B-cell lymphoma, illustrate the behavior of our methodology on carefully constructedsynthetic examples, and test it on a number of other gene expression datasets. Our technique iscomputationally tractable, generally outperforms other methods, is extensible to other phenotypes, and also identiﬁes different genes (relative to existing methods) for possible future study.Availability and Implementation: All of the code and data are available at http://mypage.iu.edu/$dajmcdon/research/.Contact: dajmcdon@indiana.eduSupplementary information: Supplementary material is available at Bioinformatics online.1 IntroductionA typical scenario in genomics is to obtain expression measurementsfor thousands of genes from microarrays or RNA-Seq which may berelevant for predicting a particular phenotype. Such studies havebeen useful in relating specific genetic variations to a wide variety ofoutcomes such as disease specific indicators (Lesage and Brice,2009; Barrett et al., 2008; Burton et al., 2007; Sladek et al., 2007);drug or vaccine response (Saito et al., 2016; Kennedy et al., 2012);and individual traits like motion sickness (Hromatka et al., 2015) orage at menarche (Elks et al., 2010; Perry et al., 2014). In these scenarios, researchers are interested in the accurate prediction of thephenotype and the identification of a handful of relevant genes witha reasonable computational expense. With these goals in mind,supervised linear regression techniques such as ridge regression(Hoerl and Kennard, 1970), the lasso (Tibshirani, 1996), theDantzig selector (Candes and Tao, 2007) or other penalized methods are often employed.However, because phenotypes tend to be the result of groups ofgenes, which perhaps together describe more complicated biomechanical processes, rather than individual polymorphisms, recentapproaches have tried to account for this group structure.Techniques such as the group lasso (Yuan and Lin, 2006) can predict the response with sparse groupings of coefficients as long as thegroups are partially understood ahead of time. In contrast, unsupervised methods such as principal components analysis (Hotelling,1957; Jolliffe, 2002; Pearson, 1901) are often used directly on thegenes when no phenotype is being examined (Alter et al., 2000;Sladek et al., 2007; Wall et al., 2003). Finally, modern approachesdeveloped specifically for the genomics context such as supervisedgene shaving (Hastie et al., 2000), tree harvesting (Hastie et al.,CV The Author 2017. Published by Oxford University Press.i350This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018AIMER for prediction with RNA-Seq2001), and supervised principal components (Bair and Tibshirani,2004; Bair et al., 2006) have sought to combine the presence of a response with the structure estimation properties of eigendecompositions from unsupervised techniques to obtain the best of both. It isthis last set of techniques that most closely resemble the approachwe present here. We give a more detailed discussion of supervisedprincipal components next, before motivating our method with anexample.Notation: We will use bolded letters M to indicate matrices, capital letters to denote column vectors, such that Mj is the jth columnof the matrix M, and lower case letters mi to denote row vectors(a single subscript) or scalars (mij being the i, j element of M). Wewill use the notation MA to mean the columns of M whose indicesare in the set A and ½k  ¼ f1; . . . ; kg. Finally, for a matrix M, wewrite the singular value decomposition (SVD) of M ¼ UðMÞKðMÞV†ðMÞ> and define M to be the Moore-Penrose inverse of M. In thecase only of the design matrix X discussed below, we will use themore compact decomposition X ¼ UKV> .1.1 Supervised eigenstructure techniquesThe first technique for extending unsupervised principal components analysis to the case where a response is available is principalcomponents regression (PCR, Hotelling, 1957; Kendall, 1965).Instead of regressing the response on all the available covariates asin ordinary least squares (OLS), PCR first performs an eigendecomposition of the empirical covariance matrix and then regresses theresponse on the subset of principal components corresponding tothe largest variances. Defining Y 2 Rn to be the centered responsevector, and X to be the n Â p centered design matrix, write the(reduced) SVD of X as X ¼ UKV> : For some integer dp, theprincipal components regression estimator is given as the solution tobC PCR ¼ argmin jjY À U½d  K½d  Cjj2 ;2Cwhich has the closed form representationbC PCR ¼ ððU½d  K½d  Þ> U½d  K½d  ÞÀ1 ðU½d  K½d  Þ> Y ¼ KÀ1 UT Y:½d  ½d Since this solution is in the space spanned by the principal components, it is easy to rotate the estimate back onto the span of X:bbb PCR :¼ V½d  C PCR ¼ V½d  KÀ1 UT Y. Then any elements of b PCR whichb½d  ½d are identically zero imply the irrelevance of those genes for predicting the phenotype while the columns of V> can be interpreted as½d indicating groupings of individual genes.Principal components regression performs well under certainconditions when we believe that there are natural groupings of covariates (linear combinations) which are useful for predicting the response. However, Lu (2002) and Johnstone and Lu (2009) showthat the empirical singular vectors U½d  are poor estimates of theassociated population quantity (the left singular vectors of the expected value of X) unless p=n ! 0 as n ! 1. In particular, whenp ) n, as is common in genomics where the number of gene expression measurements is much larger than the number of patients, PCRwill suffer.To avoid this flaw in PCR, various approaches have been proposed. Hastie et al. (2000) proposed a method called ‘gene shaving’that is applicable to both supervised (given a phenotype) and unsupervised (only gene expressions) settings. In the supervised setting,it works by computing the first principal component and rankingthe genes using a combined measure that balances the principal component scores and the marginal relationship with the response.Those genes with lowest combined scores are removed and thei351process is repeated until only one gene remains, resulting in a nestedsequence of clusters containing fewer and fewer genes. Then onechooses a cluster along this sequence, orthogonalizes the data withrespect to the genes in that cluster, and repeats the entire processagain, iterating until the desired number of clusters has been recovered. This procedure is somewhat computationally expensive aswell as requiring both the cluster sizes and the number of clusters tobe chosen.An alternative with somewhat similar behavior is supervisedprincipal components (SPC, Bair and Tibshirani, 2004; Bair et al.,2006). SPC avoids the high-dimensional regression problem by firstselecting a much smaller subset of useful genes which have high marginal correlation with the phenotype (in contrast to gene shaving,which uses the marginal correlation and the covariance betweengenes). By screening out most of the hopefully irrelevant genes, wecan return to the scenario where p < n. In follow-up work, Paulet al. (2008) show that, if a small marginal correlation with the response implies irrelevance for prediction, then SPC will find anytruly relevant genes and predict the phenotype accurately. They alsosuggest using lasso or forward stepwise selection after SPC to furtherreduce the number of genes. However, if some genes have smallmarginal relationship with the response but large conditional relationship, they will be erroneously ignored by SPC. It is this last property that our method attempts to correct. We now illustrate that thescreening step of SPC is likely to remove important genes in typicalapplications before discussing how our procedure avoids sufferingthe same fate.1.2 A motivating exampleTo motivate our methodology in relation to previous approaches,we examine a dataset consisting of 240 patients with diffuse largeB-cell lymphoma (DLBCL, Rosenwald et al., 2002) in some detail.Each patient is measured on 7399 genes, and her survival time is recorded. Previous approaches rely on the assumption that a smallmarginal correlation between the response variable, in this case patient survival time, and the vector of expression measurements for aparticular gene is sufficient for guaranteeing the irrelevance of thatparticular gene for prediction. To make this assumption mathematically precise, suppose y ¼ x> b þ  ; where y is the response, x is avector of gene expression measurements, and   is a mean-zero error.Then, the assumption can be stated mathematically asCovðxj ; yÞ ¼ 0 ) bj ¼ 0. While reasonable under some conditions,this assumption is perhaps too strong for many gene expressiondatasets. Very often, individual gene expressions are only predictiveof phenotype in the presence of other genes. We can rewrite this assumption using the population covariance matrix between genes,Covðx; xÞ ¼ Rxx , and the vector-valued covariance between gene expressions and phenotype, Covðx; yÞ ¼ Rxy . Then, using the population equation for b allows us to rewrite the assumption asðRxy Þj ¼ 0 ) bj ¼ ðRÀ1 Rxy Þj ¼ 0:xx(1)In words, we are assuming that the dot product of the jth row of theinverse covariance matrix with the covariance between x and y iszero whenever the jth element of Rxy is zero.To examine whether this assumption holds, we can estimateboth RÀ1 and Rxy using the DLBCL data and imagine that these estixxmates are the population quantities for illustration. To estimate Rxy,we use the standard covariance estimate, but set all but the largest120 values equal to zero, corresponding to a sparse solution. For thecase of RÀ1 , estimating large inverse covariance matrices accuratelyxxis impossible when p ) n unless we assume some additionalDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018i352L.Ding and D.J.McDonaldstructure. If most of the entries are 0 [a necessary condition for (1)to hold], methods like the graphical lasso (glasso, Friedman et al.,2008) or graph estimation (Meinshausen and Buhlmann, 2006) have¨been shown to work well. We use the graph estimation techniquefor all 7399 genes in the dataset at 10 different sparsity levels ranging from 100% to 99.2%. For visualization purposes, Figure 1shows the first 250 genes for one estimate of the inverse covariancethat is 97.5% sparse.To assess the validity of (1), Table 1 shows the sparsity of thefull inverse covariance matrix, the percentage of non-zero regressioncoefficients, and the percentage of non-zero regression coefficientswhich are incorrectly ignored by the assumption (the false negativerate). In all cases, Rxy is about 98% sparse. Even with an extremelysparse inverse covariance matrix, the false negative rate is at least25% meaning that 25% of possibly relevant genes are ignored bythe analysis. If the sparsity of RÀ1 is allowed to increase onlyxxslightly, the false negative rate increases to over 95%.sketching matrix S 2 RpÂ‘ . Then, we report the following approxi†mation: M % ðMSÞðS> MSÞ ðMSÞ> . The details behind the formation of the matrix S control the type of approximation.In the simplest case, which we employ here, we take S ¼ ps;where p 2 RpÂp is a permutation of the identity matrix ands ¼ ½I‘ ; 0 > 2 RpÂ‘ is a truncation matrix. While many alternativesketching matrices, mostly based on random projections, have beenproposed, this method is the only one necessary to develop our results. Without loss of generality, divide the matrix M into blocks"#M11 M>21M¼M21 M22so that we can (implicitly) construct the matrix FðMÞ 2 RpÂ‘ as"#M11FðMÞ :¼ MS ¼:M21Because1.3 Our contribution†For a similar computational budget, our method outperforms existing approaches by taking advantage of all the data. Our methoddoes not require that the set of non-zero regression coefficients be asubset of the non-zero marginal correlations.Suppose that M 2 RpÂp is a symmetric, non-negative definite matrix; that is, for all vectors a 2 Rp , a> Ma ! 0 and M> ¼ M. To approximate the matrix M, we fix an integer ‘ ( p and form aFig. 1. A sparse estimate of the inverse covariance of gene expression measurements for the ﬁrst 250 genes from the DLBCL dataset. The estimate has97.5% of the off-diagonal elements equal to 0. Darker colors represent inverseco-variances of larger magnitude†M % ðMSÞðS> MSÞ ðMSÞ> ¼ FðMÞðS> MSÞ FðMÞ> ;we can approximate the eigendecomposition of M using the SVD ofFðMÞ. If we decompose F ¼ UðFÞKðFÞVðFÞ> ; where we have suppressed the dependence of F on M when F is an argument for clarity,then the resulting approximation to the eigenvectors of M is VðMÞ†% FVðFÞKðFÞ ¼ UðFÞ: Likewise, the approximate eigenvalues of Mare given the singular values KðFÞ.Homrighausen and McDonald (2016) show that this approximation is more accurate than the one based on M11 for performing aprincipal components analysis. As previous techniques for principalcomponents regression (like SPC) are based on M11 rather than F, itis possible that by using F, we will have better results. As we willsee, this intuition turns out to be true under some conditions whichwere suggested in Section 1.2. In particular, for essentially the samecomputational budget, our procedure outperforms previous procedures if some genes have small marginal correlations with the phenotype but are, nonetheless, important for predicting the phenotypeconditional on the presence of other genes. Furthermore, even if theassumption in (1) is true, our procedure is not much worse thanexisting approaches.In Section 2, we discuss exactly how to implement our methodology. We examine the behavior of our procedure in Section 3. InSection 3.1, we state an explicit model for the data-generatingmechanism in order to be clear about the conditions under whichour procedure works well. Section 3.2 uses a number of carefullyconstructed simulations to show when our technique works well,and when it doesn’t. In Section 4, we examine our procedure onfour genetics datasets, including the one discussed above. We findthat our methods slightly outperform existing techniques on threeof them, suggesting that the motivation is sound. Finally, inSection 5, we give conclusions and discuss some avenues for futurework.Table 1. This table shows properties of the coefﬁcients of the linear model corresponding to 10 different estimates of the inverse covariancematrix, from complete sparsity on the left (a diagonal matrix) to still more than 99% sparsity on the rightSparsity of RÀ1xx1.00000.99990.99980.99950.99910.99840.99750.99630.99460.9922% Non-zero b’sFalse Negative Rate0.01620.00000.02160.25000.02870.43400.04180.61170.06180.73740.08430.80770.11930.86410.18030.91000.26450.93870.36990.9562Note: The second row is the number of non-zero population regression coefﬁcients corresponding to each inverse covariance matrix. The bottom row showsthe percentage of non-zero regression coefﬁcients which are incorrectly ignored under the assumption on the relationship between marginal correlations and regression coefﬁcients.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018AIMER for prediction with RNA-Seqi3532 Methods and computationsWe now give the details of our methodology. For clarity, we assumethat the design matrix X and the response Y are already centered.Let T be a p-dimensional vector denoting standardized regressioncoefficient estimates i.e. for any j 2 f1; 2; . . . ; pg, tj is the coefficientestimate of standardized univariate regression between response Yand covariate Xj. We use standardized regression so that the coefficient estimates are comparable across disparate covariates. Notethat tj is also the marginal correlation between the response Y andcovariate Xj.For some threshold tÃ , we separate X into two matrices XA andXAc , where A ¼ fj : jtj j > tÃ g. We assume jAj ¼ ‘. The hope is thatXA contains many of the genes that are most predictive of thephenotype under study. Ideally, high marginal correlations will suggest relevant predictors to be emphasized in the decomposition, butunlike other methods, we will also use those genes in the set Ac. Wenow focus on Xnew ¼ ½XA ; XAc   and note that it has the same rangeas X. Therefore, we will use the approximation technique discussedin Section 1.3 to try to estimate the eigendecomposition of Rxx usingsample quantities. Because X> Xnew is symmetric and positive defnewinite, write!X> XAA>F ¼ Xnew XA ¼;X>c XAAand decompose F ¼ UðFÞKðFÞVðFÞ. For some integer d 2 f1; . . . ; ‘g,we defineandbb b À1U ½d  ¼ Xnew V ½d  K ½d  :b bNow, we have estimates for the principal components U ½d  K ½d  .Therefore, just as with principal components regression, we can regress Y on the estimated principal components to produce estimatedcoefficients in principal component space:b À1 b Tb bbC AIMER ¼ argmin jjY À U ½d  K ½d  Cjj2 ¼ K ½d  U ½d  Y:2CThen the coefficient estimates for linear regression in the spacespanned by Xnew are given bybb bb b À1 b Tb AIMER ¼ V ½d  C AIMER ¼ V ½d  K ½d  U ½d  Y:(2)Because our methodology uses marginal regression to select a smallnumber of hopefully relevant predictors before ‘amplifying’ theireigenstructure information with the F matrix, we refer to our technique as ‘Amplified, Initially Marginal, Eigenvector Regression’(AIMER).Unlike previous approaches, the solution given by (2) is notsparse: with probability 1, ðb AIMER Þj 6¼ 0; 8j. However, most of thebcoefficients will be small. We therefore threshold the estimates toproduce our final estimator:bbbb AIMER ðbÞ :¼ b AIMER 1ðb;1Þ ðjb AIMER jÞ;Algorithm 1: Ampliﬁed,Regression (AIMER)InitiallyMarginal,EigenvectorInput: centered design matrix X, centered response Y, thresholds tÃ ; bÃ ! 0, integer d1 Compute marginal correlation tj between Xj and Y for all j;2 Set A ¼ fj : jtj j > tÃ g;3 Set Xnew ¼ ½XA ; XAc  ;;4 Deﬁne F ¼ X> XA ;new5 Decompose F ¼ UðFÞKðFÞVðFÞ> ;b6 Set V ½d  ¼ U½d  ðFÞ;b7 Set K ½d  ¼ K½d  ðFÞ1=2 ;bb b À18 Set U ½d  ¼ Xnew V ½d  K ½d  ;b b À1 b T9 Calculate b ¼ V ½d  K ½d  U ½d  Y;b10 Set b Ã Þ :¼ b ðbÃ ;1Þ ðjbbðbb1bjÞ;bOutput: coefﬁcient estimates bðbÃ ÞTo make predictions given a new observation xÃ , we simply center it using the mean of the original data, reorder its entries to conform to Xnew , multiply by the coefficient vector in (3), and add themean of the original response vector.3 Experimental analysisTo examine the performance of our method, we set up a number ofcarefully constructed simulations under various conditions. We firstdiscuss the generic data model we assume, a latent factor model,which is amenable to analysis via SPC or AIMER.bV ½d  ¼ U½d  ðFÞ;bK½d  ¼ K½d  ðFÞ1=2 ;OðjAj3 Þ. Thus, to leading order, both methods require the sameamount of computation.3.1 Data modelConsider the multivariate Gaussian linear regression modely ¼ x> b þ r1  with y the response, x 2 Rp a column vector of gene expressionmeasurements, b ¼ ðb1 ; . . . ; bp Þ> the coefficients,   a randomGaussian distributed error with zero mean and variance 1, andr1 > 0. We further assume that x $ Np ð0; Rxx Þ has a Gaussian distribution with mean vector 0 and covariance matrix Rxx . We will assume that b is sparse, in that most of its elements are exactly 0indicating no linear relationship between the associated gene andthe response. Finally, the design matrix X and the response vector Yinclude n independent observations of x and y, respectively.Model for X.As Rxx is symmetric and positive (semi-) definite, we can decompose it asRxx ¼ VðRxx ÞLðRxx ÞVT ðRxx Þ010 > 1V1l10CB . CÀÁB..BCB . C;¼ V1 Á Á Á Vp @.A@ . A>0lpVp(3)where b ! 0, and 1A ðwÞ is the indicator function, which returns thevalue one for every element of w 2 A and zero otherwise. We summarize this procedure in Algorithm 1. As with SPC, the computational burden of our method is dominated by the SVD. We use anSVD of F while SPC uses the SVD of XA . However, since the SVD iscubic in the smaller dimension, in both cases the computation is(4)where V1 ; . . . ; Vp are orthonormal eigenvectors on Rp and l1 ! Á Á Á! lp ! 0 are eigenvalues. We assume that there is some 1Gpsuch that the eigenvalues can be separated into two groups, one ofwhich includes relatively large eigenvalues and the other relativelysmall eigenvalues, that is, lk ¼ kk þ r2 for 1kG and lk ¼ r200for k > G where k1 ! Á Á Á ! kG > 0, and r2 > 0:0Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018i354L.Ding and D.J.McDonaldThen, because X is multivariate Gaussian, we can write X as¼ ð U1X ¼ UG KG VT þ r0 EG0 pﬃﬃﬃﬃﬃ10 > 1V1k10BCB . C..CB . C þ r0 EÁ Á Á UG ÞB.@A@ . Apﬃﬃﬃﬃﬃﬃ>0kGVGwhere latent factors U1 ; . . . ; UG are independent and identically distributed (i.i.d.) Nn ð0; IÞ vectors, and the noise matrix E is n Â p withi.i.d. N(0, 1) entries independent of U1 ; . . . ; UG .Model for Y.We assume that Y is a linear function of the first KG latentfactors in UG plus additive Gaussian noise: Y ¼ UK H þ r1 Z; whereH is the coefficient vector, r1 > 0 is a constant, and Z is distributedNn ð0; IÞ, independent of X. Note that the expectation of Y is zeroand that this is a specific form of (4).Implication of the model.Under this model for X and Y, the population marginal covariance between each gene Xj and the response Y can be written as01CovðX1 ; Y ÞBCBC..Rxy ¼ BC ¼ VK KK H:.@AÀÁCov Xp ; YTherefore, the population ordinary least squares coefficients of regressing Y on X (b in (4)) can be written asb ¼ RÀ1 Rxy ¼ VK LÀ1 KK HxxK(6)We will define the set B :¼ fj : ðRxy Þj 6¼ 0g and the setA :¼ fj : bj 6¼ 0g. We note that for K ¼ 1, it is always the case thatA ¼ B. By manipulating the parameters in H, L, and K, we can create a number of scenarios for testing AIMER against alternativemethods.3.2 ExperimentsWe present results under five different experiments. For each of thesimulations which follow, we generate datasets with n ¼ 200 andp ¼ 1000. We use half (n ¼ 100) to estimate the model and test ourpredictions on the other half. We repeat this process 100 times forpﬃﬃﬃﬃeach combination of parameters. Throughout, we use r0 ¼ :1 % :3and r1 ¼ :1. The matrix U is generated with i.i.d. standard Gaussianentries, while the matrix V is constructed by hand to have the correct number of orthogonal components.The first experiment is designed to be favorable to AIMER. Thesecond is designed to be favorable to SPC. The third examines theextent to which the assumption that A ¼ B is beneficial to SPC overAIMER. The fourth examines the impact of using incorrect numbersof components, while the fifth uses cross validation on all the tuningparameters.Simulation 1: Favorable conditions for AIMER.In this simulation, we create data which is amenable to AIMERat the expense of the conditions for SPC, that is we use B & A. Weset parameters in the data model as K ¼ G ¼ 3 and choose k1 ¼ 10;k2 ¼ 5; and k3 ¼ 1. In order to achieve B & A, we set h1 ¼ h2 ¼ 1and solve (5) for h3 so that some corresponding elements of Rxy willbe zero. We make the first 15 elements of b non-zero, five corresponding to each of the three principal components. Thus, the first10 genes have non-zero population marginal correlation and the remaining 990 have zero marginal correlation. In this scenario, SPCshould find the first 10 important genes, but AIMER will find the remaining five important genes as well.In order to focus on the relationship between performance andthe condition B & A, we examine the methods for a fixed computational budget and choose tÃ to select the same 50 most predictivegenes. We examine SPC, SPC with lasso, AIMER(b ¼ 0), andAIMER. We use the first three principal components for regressionin all the methods. For SPC with lasso and AIMER, we choose theremaining tuning parameters via 10-fold cross-validation. We alsogive results for OLS on the first 15 genes. This is the oracle estimator, the best one could hope to do with foreknowledge of the predictive genes.Figure 2 shows the classification performance using a receiveroperating characteristic (ROC) curve for SPC with lasso andAIMER in the left panel (the remaining panels are for the next twosimulations). Examining the figure, it is easy to see that SPC þ lassoidentifies the first 10 genes easily, but AIMER is able to capture all15 predictive genes at a low cost of false positive identifications.A more detailed analysis is given in the first row of Figure 3. Panel1a shows the ability of each method to estimate the b coefficients ofthree different factors. Coefficient estimates for the five genes inFig. 2. Receiver operating characteristic (ROC) Curve for Simulations 1–3. The x-axis is the false positive rate while the y-axis is the true positive rate. The curvespresent averages across 100 replications. SPC is limited to only 50 selected genes, and so its false positive rate is bounded. The dashed line indicates its bestcase theoretical performance were it allowed to continue to select further genesDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018AIMER for prediction with RNA-Seqi355Fig. 3. Estimation and prediction performance of SPC and AIMER in the ﬁrst three simulations. The left panel shows the estimates of the regression coefﬁcients,the middle panel shows the mean squared error (MSE) of estimation for all 1000 genes, and the right panel shows prediction MSE on the held-out data. Theboxes indicate variability across 100 replications. The dashed black horizontal lines indicate the true values of bfactor 1 by AIMER are slightly more accurate, and no more variable, than SPC þ lasso. Furthermore, AIMER is better at estimatingthose b’s associated with factor 2, and much better at those associated with factor 3 (these are assumed zero in SPC). Panel 1b examines the mean square error (MSE) of estimation as the averagesquared difference between the true coefficients and their estimatesfor all 1000 genes. The overall estimation accuracy of AIMER(b ¼ 0) is worse because of the inclusion of so many useless genes (itestimates all 1000), however, by thresholding with AIMER, accuracy is improved and exceeds that of SPC with and without lasso. Inpanel 1c, we show the MSE for prediction, the average squared difference between predicted values and the actual observations, for aDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018i356L.Ding and D.J.McDonaldtest set. This MSE is smaller for AIMER than for SPC much of thetime, but the variance across simulations is large.Simulation 2: Favorable conditions for SPC.This simulation compares the performance of SPC and AIMERunder conditions which are more favorable to SPC. In particular, wechoose parameters such that A ¼ B. While AIMER is likely to perform worse because it will tend to include irrelevant genes, it is nottoo much worse. Most of the parameters are the same as inSimulation 1, except that K ¼ G ¼ 2; k1 ¼ 10, k2 ¼ 1; h1 ¼ h2 ¼ 1,and we use the first two principal components to do regression.Therefore, 10 out of 1000 genes are truly predictive of the response,and all 10 have non-zero marginal correlation with the response (therest have Rxy ¼ 0). Looking again at Figure 2, both SPC þ lasso andAIMER can identify all 10 predictive genes at a small price of falsepositives. Examining Figure 3, we see that the estimation accuracyof SPC/SPC þ lasso is better than that of AIMER as expected, andthe MSE of prediction for AIMER is about twice that of SPC/SPC þ lasso. The estimation MSE (panel 2b) of AIMER is comparable to that of SPC.Simulation 3: Slight perturbations.In this simulation, we adjust only h2 ¼ 3, rather than 1 as insimulation 2, thereby maintaining the condition that A ¼ B.However, in this case AIMER works much better than SPC/SPC þ lasso. Figures 2 and 3 show that AIMER can easily identifyall the predictive genes, has more precise coefficient estimates, andhas much smaller MSE for prediction. The reason is that, eventhough A ¼ B, the marginal correlations for some predictive genesare very small. Therefore, those genes are more difficult for SPC toidentify, but AIMER can compensate.Table 2. Average ﬁnal number of predictive genes in Simulations 1,2 and 3SimulationTrue #SPCSPCþlassoAIMER (b ¼ 0)AIMER1231550 (0)31 (9.011)1000 (0)39 (9.225)1050 (0)39 (3.636)1000 (0)21 (12.750)1050 (0)46 (2.665)1000 (0)16 (7.558)Note: The standard deviation is shown in parentheses.For one further comparison, Table 2 shows the average (standard deviation in parentheses) number of predictive genes selected ineach of the first three simulations. AIMER selects the smallest number of coefficients in most cases.Simulation 4: Choosing the number of components.In the previous simulations, we used the correct number of principal components, though such a choice is unlikely to be possiblegiven real data. In this simulation, we examine the impact choosingthe number of components has on estimation accuracy. We use similar parameter settings as Simulation 1 except with K ¼ G ¼ 2 ratherthan 3 (we maintain the condition that B & A). We then use all themethods with 1, 2 and 3 components. We also adjust the values ofk1 in a range from 5 to 50. As we can see in Figure 4, using two components reduces MSE for AIMER(b ¼ 0) and AIMER across all values of k1 relative to using only one component, while using morethan two components has little impact. With only one component,SPC performs better than AIMER, likely due to smaller variance fora similar bias, but using two or three components leads to largegains for AIMER. In practice, it is worthwhile to try several numbers of components and use cross-validation to decide which worksbest.Simulation 5: The screening threshold.In previous simulations, we choose tÃ so that variable screeningby the marginal correlation would always select exactly 50 genes.Thus, we could compare methods based on their ability to use thesame amount of information. In reality, it may be better to choosethe threshold tÃ using cross validation. In this simulation, we use thesame conditions as in the previous simulation with k1 ¼ 10. It is stillnot appropriate to have more genes than patients, so we allow thenumber of selected genes to be anything less than the number of patients (100). We further use 10-fold cross-validation to choose thebest threshold.As shown in Figure 5, allowing tÃ to be chosen rather than fixedleads to improved results for AIMER relative to SPC/SPC þ lasso.The prediction MSE decreases and fewer genes are selected.4 Performance on real dataWe now illustrate our methods on four empirical datasets in genomics that record the censored survival time and gene expressionmeasurements from DNA microarrays of patients with four differentFig. 4. Prediction MSE averaged across 100 replications for each method for different numbers of components (Simulation 4). We also allow k1 to vary between 5and 50Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018AIMER for prediction with RNA-Seqi357Fig. 5. Performance of each method when we allow tÃ to be chosen by cross validation rather than ﬁxed to choose 50 genes (Simulation 5)Table 3. The MSE on the test set, the number of selected genes and the number of principal components used (d if relevant), each averagedacross the 10 random training-testing splitsDLBCLMethodsMSElassoridgeSPCSPCþlassoAIMER(b ¼ 0)AIMER0.68050.64850.68280.67801.18960.6518Breast cancer# genes2073994131739928dMSE33240.62850.64070.60660.60292.65310.6004Lung cancer# genes947511614475131dMSE22130.81590.77130.83440.84360.94441.0203AML# genes227129199712913dMSE34111.95641.92342.42142.398012.40141.8746# genesd6628324226283362214Note: Bolded values indicate the best predictive performance for each type of method (with and without structure learning) for each dataset.types of cancer. The first dataset comes from Rosenwald et al.(2002) and contains 240 patients with diffuse large B-cell lymphoma(DLBCL) and 7399 genes. The second dataset has 4751 gene expression measurements of 78 breast cancer patients (Van’t Veer et al.,2002). The third consists of 86 lung cancer patients measured on7129 genes (Beer et al., 2002), and finally, we analyze a dataset consisting of 116 patients with acute myeloid leukemia (AML, Bullingeret al., 2004) and 6283 genes.Since the survival times for some patients are censored and rightskewed, we use log ðsurvival time þ 1Þ as the response. A Cox modelwould be more appropriate, but this transformation is enough to illustrate our methodology. In order to assess our method using limited data, we randomly select half of the data as the training set andlet the rest be in the testing set, then estimate each model using thetraining half and predict the held out data. We repeat this procedurefor 10 random splits and report the average error. We use 10-foldcross-validation on the training set to choose all tuning parameters(tÃ ; bÃ , d and k where appropriate), mimicking the procedure of areal data analysis.We apply seven methods on each dataset: (i) PCR; (ii) lasso; (iii)ridge regression; (iv) SPC; (v) SPC þ lasso; (vi) AIMER(b ¼ 0) and(vii) AIMER. We use the R packages pls (Mevik and Wehrens,2007) to perform PCR and glmnet (Friedman et al., 2010) to perform lasso and ridge. For PCR, SPC, SPC þ lasso, AIMER(b ¼ 0)and AIMER, we allow the number of components d to be chosen between 1 and 5.Our results are shown in Table 3. For each dataset, we show theMSE on the testing set, the number of selected genes, and the number of principal components used (if relevant), averaged across the10 random training-testing splits. We do not show results for PCRbecause it is uniformly awful. The results in Table 3 are largely consistent with the conclusions we derive from simulations. AIMERand SPC þ lasso tend to select a similar number of genes, thoughAIMER has better prediction error on three of the four datasets.Interestingly, the genes selected by SPC þ lasso, lasso and AIMERrarely overlap, suggesting that to identify genes for further study,one should try all three methods. The online SupplementaryMaterial lists the genes identified by AIMER for each dataset. In thecase of DLBCL, we also list any previous research relating the selected genes to lymphoma.The Lung Cancer data is rather odd in that AIMER(b ¼ 0) hasbetter performance than AIMER. This anomaly is likely because, incontrast with the other datasets, the lung cancer expression measurements have not been scaled relative to a control group. We triedtwo transformations using only the treatment group to approximatesuch a scaling, but, while the performance of our method becomescomparable to SPC following transformations, it remains slightlyworse. Without a control group, it is difficult to explain this outcome with any certainty. A comparison of these alternative transformations with our results in Table 3 is contained in the onlineSupplementary Material.As seen in the table, ridge regression is sometimes the best of allthe methods. Previous experience suggests that ridge regression isdominant if the genes are highly correlated or when there is not aparticularly predictive set of genes. However, the fact that ridgedoes not screen out unimportant genes is a barrier to its applicationsin genomics. On the other hand, AIMER approaches or exceeds thesmall prediction error of ridge regression while also selecting a smallnumber of predictive genes, making it a better candidate for solvingthese types of problems.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018i3585 DiscussionHigh-dimensional regression methods help in predicting future survival time and identifying possibly predictive genes for diseases.However, the large number of genes, the limited access to patients,and the complex covariance structure between genes make the problem both computationally and statistically difficult. In both simulations and analysis of actual gene expression datasets, AIMER hascomparable or slightly improved prediction accuracy relative toexisting methods and finds small numbers of actually predictivegenes, all while having a similar computational burden. On theother hand, there are some issues which warrant furtherexploration.A major benefit of SPC is that it comes with theoretical guarantees under certain assumptions. While our methodology is intendedto work when these assumptions don’t hold, we do not yet havecomparable guarantees. However, the simulated experiments in thispaper have suggested how we might derive such results in a moregeneral setting.For the real data examples in this paper, we applied a simplemonotonic transformation to the response variable, however, extending our methods to Cox models, which are more appropriate,and other generalized linear models for predicting discrete traits ishighly desirable. It may also be useful to examine other eigenstructure techniques such as Locally Linear Embeddings or LaplacianEigenmaps to produce non-linear predictors. Finally, using othermatrix approximation techniques may yield improved performanceor be more amenable to theoretical analysis.FundingThis work is supported by the National Science Foundation [grant numberDMS–14-07439 to D.J.M.].Conflict of Interest: none declared.ReferencesAlter,O. et al. (2000) Singular value decomposition for genome-wide expression data processing and modeling. Proc. Natl. Acad. Sci. USA, 97,10101–10106.Bair,E. et al. (2006) Prediction by supervised principal components. J. Am.Stat. Assoc., 101, 119–137.Bair,E., and Tibshirani,R. (2004) Semi-supervised methods to predict patientsurvival from gene expression data. PLoS Biol., 2, e108.Barrett,J.C. et al. (2008) Genome-wide association deﬁnes more than 30 distinct susceptibility loci for crohn’s disease. Nat. Genet., 40, 955–962.Beer,D.G. et al. (2002) Gene-expression proﬁles predict survival of patientswith lung adenocarcinoma. Nat. Med., 8, 816–824.Bullinger,L. et al. (2004) Gene expression proﬁling identiﬁes new subclassesand improves outcome prediction in adult myeloid leukemia. New Engl. J.Med., 350, 1605–1616.Burton,P.R. et al. (2007) Genome-wide association study of 14,000 cases ofseven common diseases and 3,000 shared controls. Nature, 447,661–678.Candes,E.J., and Tao,T. (2007) The Dantzig selector: statistical estimationwhen p is much larger than n. Ann. Stat., 35, 2313–2351.L.Ding and D.J.McDonaldElks,C.E. et al. (2010) Thirty new loci for age at menarche identiﬁed by ameta-analysis of genome-wide association studies. Nat. Genet., 42,1077–1085.Friedman,J. et al. (2008) Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9, 432–441.Friedman,J. et al. (2010) Regularization paths for generalized linear modelsvia coordinate descent. J. Stat. Softw., 33, 1.Hastie,T. et al. (2001) Supervised harvesting of expression trees. GenomeBiol., 2, research0003–research0001.Hastie,T. et al. (2000) Identifying distinct sets of genes with similar expressionpatterns via “gene shaving”. Genome Biol., 1, 1–21.Hoerl,A.E., and Kennard,R.W. (1970) Ridge regression: biased estimation fornonorthogonal problems. Technometrics, 12, 55–67.Homrighausen,D., and McDonald,D.J. (2016) On the Nystrom and column¨sampling methods for the approximate principal components analysis oflarge data sets. J. Comput. Graph. Stat., 25, 344–362.Hotelling,H. (1957) The relations of the newer multivariate statistical methods to factor analysis. Br. J. Stat. Psychol., 10, 69–79.Hromatka,B.S. et al. (2015) Genetic variants associated with motion sicknesspoint to roles for inner ear development, neurological processes and glucosehomeostasis. Hum. Mol. Genet., 24, 2700–2708.Johnstone,I.M., and Lu,A.Y. (2009) On consistency and sparsity for principalcomponents analysis in high dimensions. J. Am. Stat. Assoc., 104, 682–693.Jolliffe,I.T. (2002) Principal Component Analysis. Springer, New York.Kendall,M.G. (1965) A Course in Multivariate Analysis. Charles Grifﬁn &Co., London.Kennedy,R.B. et al. (2012) Genome-wide analysis of polymorphisms associated with cytokine responses in smallpox vaccine recipients. Hum. Genet.,131, 1403–1421.Lesage,S., and Brice,A. (2009) Parkinson’s disease: from monogenic forms togenetic susceptibility factors. Hum. Mol. Genet., 18, R48–R59.Lu,A.Y. (2002). Sparse principal component analysis for functional data. PhDThesis, Stanford University, Stanford, CA.Meinshausen,N., and Buhlmann,P. (2006) High-dimensional graphs and vari¨able selection with the lasso. Ann. Stat., 34, 1436–1462.Mevik,B.H., and Wehrens,R. (2007) The pls package: principal componentand partial least squares regression in r. J. Stat. Softw., 18, 1–23.Paul,D. et al. (2008) ‘Preconditioning’ for feature selection and regression inhigh-dimensional problems. Ann. Stat., 36, 1595–1618.Pearson,K. (1901) Principal components analysis. Lond. Edinb. DublinPhilos. Mag. J., 6, 566.Perry,J.R.B. et al. (2014) Parent-of-origin-speciﬁc allelic associations among106 genomic loci for age at menarche. Nature, 514, 92–97.Rosenwald,A. et al. (2002) The use of molecular proﬁling to predict survivalafter chemotherapy for diffuse large-B-cell lymphoma. New Engl. J. Med.,346, 1937–1947.Saito,T. et al. (2016) Pharmacogenomic study of clozapine-induced agranulocytosis/granulocytopenia in a Japanese population. Biol. Psychiatry, 80,636–642.Sladek,R. et al. (2007) A genome-wide association study identiﬁes novel riskloci for type 2 diabetes. Nature, 445, 881–885.Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. Roy.Stat. Soc. B, 58, 267–288.Van’t Veer,L.J. et al. (2002) Gene expression proﬁling predicts clinical outcome of breast cancer. Nature, 415, 530–536.Wall,M.E. et al. (2003) Singular value decomposition and principal component analysis. In: A Practical Approach to Microarray Data Analysis.Springer, New York, p.91–109.Yuan,M., and Lin,Y. (2006) Model selection and estimation in regression withgrouped variables. J. Roy. Stat. Soc. B, 68, 49–67.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i350/3953978by gueston 07 January 2018
5028881996002	PMID28881996	5028881996	https://watermark.silverchair.com/btx262.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881996.main.pdf	Bioinformatics, 33, 2017, i142–i151doi: 10.1093/bioinformatics/btx262ISMB/ECCB 2017Improved data-driven likelihood factorizationsfor transcript abundance estimationMohsen Zakeri, Avi Srivastava, Fatemeh Almodaresi and Rob Patro*Department of Computer Science, Stony Brook University, Stony Brook, NY 11790, USA*To whom correspondence should be addressed.AbstractMotivation: Many methods for transcript-level abundance estimation reduce the computationalburden associated with the iterative algorithms they use by adopting an approximate factorizationof the likelihood function they optimize. This leads to considerably faster convergence of the optimization procedure, since each round of e.g. the EM algorithm, can execute much more quickly.However, these approximate factorizations of the likelihood function simplify calculations at the expense of discarding certain information that can be useful for accurate transcript abundanceestimation.Results: We demonstrate that model simpliﬁcations (i.e. factorizations of the likelihood function)adopted by certain abundance estimation methods can lead to a diminished ability to accuratelyestimate the abundances of highly related transcripts. In particular, considering factorizationsbased on transcript-fragment compatibility alone can result in a loss of accuracy compared to theper-fragment, unsimpliﬁed model. However, we show that such shortcomings are not an inherentlimitation of approximately factorizing the underlying likelihood function. By considering the appropriate conditional fragment probabilities, and adopting improved, data-driven factorizations ofthis likelihood, we demonstrate that such approaches can achieve accuracy nearly indistinguishable from methods that consider the complete (i.e. per-fragment) likelihood, while retaining thecomputational efﬁciently of the compatibility-based factorizations.Availability and implementation: Our data-driven factorizations are incorporated into a branchof the Salmon transcript quantiﬁcation tool: https://github.com/COMBINE-lab/salmon/tree/factorizations.Contact: rob.patro@cs.stonybrook.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionShortly after the RNA-seq assay became popular as a tool for transcriptome profiling and quantification, the computational community began developing principled inference methodologies to allowaccurate transcript-level quantification in the presence of multimapping reads. Tools such as Cufflinks (Trapnell et al., 2010),RSEM (Li et al., 2010), mmseq (Turro et al., 2011) and IsoEM(Nicolae et al., 2011) provided statistical models by whichtranscript-level abundance estimates could be inferred. These methodologies principally rely on maximum likelihood estimation toinfer the transcript abundances that would be most likely given theobserved data (i.e. the alignments of the sequenced fragments to theunderlying genome or transcriptome). Bayesian methodologies suchas BitSeq (Glaus et al., 2012) and Tigar (Nariai et al., 2013) werealso developed and adopt different inferential approaches varyingfrom fully Bayesian approaches like collapsed Gibbs sampling(Glaus et al., 2012) to approximate inference approaches like variational Bayesian optimization (Hensman et al., 2015; Nariai et al.,2013, 2014).These methods vary widely in their details, though adopt a similargenerative model of the underlying RNA-seq experiment; one whichis well-represented by the generative model of RSEM (Li et al., 2010;Li and Dewey, 2011). In this paper, we shall refer to this as the fullmodel. It is a generative model of an RNA-seq experiment that considers the likelihood of observing a collection of alignments as dependent upon the parameters of interest (i.e. the transcriptabundances), as well as the details of each alignment of a sequencedfragment to the reference transcriptome. In this way, the full modelprovides very high fidelity, and is capable of incorporating a tremendous amount of information into the inference procedure (e.g. theCV The Author 2017. Published by Oxford University Press.i142This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018Improved data-driven factorizationsimplied fragment length under each alignment, details about the alignment and the fragment’s quality values, the probability of differentstart positions for the sampled fragment, etc.).Unfortunately, however, this means that straightforward inference procedures that adopt this full model scale in the number ofconsidered alignments per-iteration. For example, a 30 million fragment RNA-seq experiment may produce 100 million fragment alignments, all of which are considered by the inference procedure ineach of its (typically) hundreds to thousands of iterations. This approach, then, poses two problems. First, inference is typically slowsince each iteration must consider a large number of independentprobabilities. Second, so as to prevent the inference algorithm frombecoming even slower, these per-alignment probabilities are typically retained in memory, which can lead memory requirements toscale linearly with the number of alignments. One approach to mitigate the cost associated with optimizing the full model is to alter theactual inference algorithm that is used. For example, eXpress(Roberts and Pachter, 2013) uses an online-EM algorithm, ratherthan a batch-EM algorithm (by default), to infer transcript abundances. This eliminates the need to cache alignments in memory forefficiency, resulting in constant memory usage. However, a singlepass over the data is not always sufficient to achieve the same accuracy as methods that run batch algorithms to convergence.One of the more popular approaches for reducing the computational burden and speeding up the inference procedure is to form anapproximate factorization of the likelihood function (see Section2.1). For example, mmseq introduced a notion of fragment equivalence classes, which treats as equivalent any fragments that align toexactly the same set of transcripts. This leads to a likelihood function in which the counts of fragments compatible with subsets oftranscripts serve as sufficient statistics. The likelihood defined overthese counts is typically orders of magnitude faster to evaluate, butit can discard certain fragment-level information encoded in thealignments. Distinct but related notions of equivalence classes werealso introduced by Salzman et al. (2011) and Nicolae et al. (2011).Because of the computational economy of this approximate factorization, it (or similar variants) were later adopted by new lightweight approaches for transcript quantification like Sailfish (Patroet al., 2014; Srivastava et al., 2016) and kallisto (Bray et al., 2016).By coupling a very fast inference approach with techniques thatremoved the requirement of computing traditional alignments foreach sequenced fragment, such approaches reduced the timerequired to obtain transcript-level quantification estimates by ordersof magnitude over existing approaches. These lightweight methodshave proven an important and popular development. Recently,Patro et al. (2017) introduced a new lightweight approach, Salmon,that uses a ‘dual-phase’ inference algorithm, which combines an online stochastic inference method with an efficient offline inferencealgorithm. While adopting a similar approximate factorization asmmseq, Sailfish and kallisto, Salmon also maintains aggregate (i.e.average) weights per equivalence class that allow retaining some information about fragment-level probabilities during the offline inference algorithm. However, this information is restricted to a singlescalar value per transcript-equivalence class pair, and so is necessarily limited in its ability to represent the full model with completefidelity.In this paper, we argue that the dual-phase algorithm introducedby Salmon allows one to derive a data-driven approximate factorization of the full model likelihood function. The online phase of the algorithm assesses each individual fragment probability, and uses thisinformation to build a highly reduced but accurate proxy for the fulli143model likelihood that can be efficiently optimized during the offlinephase. While only slightly increasing the per-iteration cost of theunderlying inference algorithm, this data-driven factorization canrepresent the fragment-level likelihood function with much higher fidelity. In fact, we demonstrate that a data-driven likelihood factorization can produce transcript-level abundance estimates thatdisplay essentially no loss in accuracy compared to what is obtainedunder the full model. Thus, such a factorization is preferable to themore common compatibility-based approximate factorization, sinceit can provide a substantial improvement in accuracy while introducing only a small increase in the computational burden. We notethat we focus in this paper on how to factorize the likelihood function, and not, specifically, the algorithm by which this function isbest optimized. Thus, we expect the approaches we introduce hereto easily translate to other likelihoods or optimization approaches;e.g. to variational Bayesian optimizations (Nariai et al., 2013), ornatural gradient-based optimization algorithms (Hensman et al.,2015).2 Approach2.1 The likelihood function and its factorizationsWe begin by considering the basic generative model laid out by Liet al. (2010). We consider a transcriptome T to consist of a set of Mtranscripts, t1 ; t2 ; . . . ; tM . In a given sample, there are ci copies of theith transcript. Further, we can assign to each transcript its length,such that the length of ti is given by ‘i . The generative model of anRNA-seq experiment states that the expected number of fragmentssequenced from each transcript type ti is proportional to the totalnumber of sequencable nucleotides that it constitutes in the underlycing mixture—that is we expect that ai / gi ¼ Pi Á‘i —where ai is thec Á‘j j jnumber of fragments drawn from transcripts of type ti. Assumingthat each fragment is drawn independently, the likelihood of a collection F of fragments can be written as:Lðh; F Þ ¼MYXPrðti jhÞPrðfj jti Þ;(1)fj 2F i¼1where h denotes the parameters of the model, which are the underlying transcript abundances. We note that, throughout this manuscript, we use the term ‘fragment’ as a generic term which isrepresented by a single read (in single-end protocols) and a read pair(in paired-end protocols). The methods we propose in Section 3work only in terms of the conditional fragment probabilities, and soare equally applicable in both single and paired-end protocols,though the definition of these conditional probabilities will beprotocol dependent.The primary quantity of interest, with respect to the factorizations being proposed in this paper, are the Prðfj jti Þ terms—that is,the conditional probability of drawing a particular fragment fj, giventranscript ti. This term encodes, given parameters of the model andexperiment, how likely it is to observe a specific fragment fj arisefrom transcript ti. Many terms can be included in such a conditionalprobability, some common terms include:À ÁÀÁPrD djPr dj jfj ; ti ¼ ‘;(2)iPPrD ðkÞk¼1the probability of observing a mapping of implied length dj for fjgiven that it derives from ti , where PrD ðkÞ is the probability ofDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018i144M.Zakeri et al.observing a fragment of length k under the empirical fragmentlength distribution D;ÀÁPr pj jdj ; fj ; ti ¼1;‘ i À dj þ 1(3)the probability of a observing a mapping starting at position pj forfragment fj given that it has implied length dj and is derived from ti ;8(>>> 0:5if unstranded>>ÀÁ <Pr oj jfj ;ti ¼ (;>> 1:0 if compatibleorientation>>>if strandÀ specific:eif incompatibleorientation(4)the probability of observing a mapping with a specific orientation oj(i.e. forward or antisense) with respect to the underlying transcriptfor fj , given ti , e (a user-defined constant), and knowledge of theunderlying protocol, andPrðaj jfj ; oj ; dj ; pj ; ti Þ;(5)the probability of observing the particular alignment (e.g. CIGARstring) aj for fj given it is sampled from transcript ti , has orientationoj, implied length dj and starts at position pj—such a probability iscalculated from a model of alignments, like those presented in (Liet al., 2010; Patro et al., 2017; Roberts and Pachter, 2013).In fact, one can conceive of many such general models of ‘fragment-transcript agreement’ (Patro et al., 2017). The framework wepropose in Section 3 can naturally account for such conditionalprobabilities that one might consider as part of Prðfj jti Þ. However, inthis manuscript, we consider that Prðfj jti Þ is simply the product ofthe conditional probabilities defined in Equations (2) to (5), appropriately normalized.2.2 Equivalence classes and approximate likelihoodfactorizationsHere, we describe the most common definition of fragment equivalence classes, and explain how they are used to derive an approximate factorization of the likelihood function, we adopt a notationsimilar to Patro et al. (2017).Let AðT ; fj Þ be the set of all alignments of fragment fj to thetranscriptome T and let Xðfj Þ ¼ fhi; ti ijti 2 AðT ; fj Þg be the tupleof transcripts to which fj maps—considering the ti are ordered bytheir index i. Fragment equivalence classes are defined in terms ofthe equivalence relation $, such that fm $ fn if and only ifXðfm Þ ¼ Xðfn Þ. Thus, fragment equivalence classes consider asequivalent (for the purposes of inference), sequenced fragments thatalign to the same set of transcripts. We will refer to Xðfj Þ as the labelof fj for all fj 2 F q , where F q is the equivalence class to which fjbelongs. We will also refer to XðF q Þ ¼ Xðfj Þ; 8fj 2 F q as the labelof fj ’s equivalence class. Finally, it will be convenient to define thetotal size of each such equivalence class as N q ¼ jF q j, which is thetotal number of equivalent fragments in the class F q .Now, we can write the equivalence class-based approximationto the likelihood function as:Lðh; F Þ %YFq 2 C0@X1NqPrðti jhÞ Á Prðf jF q ; ti ÞA;(6)hi;ti i2XðF q Þwhere C is the set of all equivalence classes, and Prðf jF q ; ti Þ is theprobability of generating a fragment f given that it comes fromequivalence class F q and transcript ti . The key to the efficiency oflikelihood evaluation (or optimization) under this factorization, isthat the probability Prðf jF q ; ti Þ is assumed to be identical for eachof the N q fragments in each equivalence class F q —hence, we do notsubscript f in Equation (6). This allows one to replace the productover all fragments fj in Equation (1) with a product over all equivalence classes in Equation (6). The approximation, of course, stemsfrom the fact that, under the full model, a fragment fj may have aprobability Prðfj jti Þ that is arbitrarily different from Prðf jF q ; ti Þ.Moreover, the most common approximations, like those adopted inmmseq, Sailfish and kallisto consider this probability to be fixed andessentially independent of any fragment-level information (e.g. it isset to one divided by the effective length of ti ).2.3 What approximate factorizations elideFigure 1 provides an illustrative example why considering conditional fragment probabilities can be important. Consider a multiisoform gene, and a single fragment fj, which aligns equally well(i.e. the sequence of both ends of the fragment match the sequenceof the underlying transcripts equally well) to isoforms A and B ofthis gene. If we consider only transcript-fragment compatibility,then both of the alignments illustrated in Figure 1 are delineatedonly in that isoform A has fewer potential start locations.However, considering the implied length of this fragment, giventhe expected insert size distribution of the experiment (either provided as input to the model, or inferred from the collection of previously aligned fragments), can provide strong evidence that one orthe other of these isoforms was more likely to have generated fj.For example, were the mean of the fragment length distribution250, then we would expect isoform A to be much more likely tohave generated fj. Conversely, were the mean of the fragmentlength distribution 400, then we would expect that, in fact, isoformB might have been more likely to generate this fragment. Standard(i.e. compatibility-based) approximate factorizations of the fulllikelihood function into equivalence classes discard (or collapse)this fragment-level information. For example, compatibility-onlyfactorizations of the likelihood into equivalence classes simplytreat Prðdj jfj ; ti Þ as equal for all transcripts in the equivalence classto which fragment fj belongs. The factorization adopted by Salmonattempts to maintain slightly more information by computing theseconditional probabilities and averaging them; maintaining a singleextra scalar per transcript-equivalence class pair, that representsthe conditional probability that any fragment coming from a particular equivalence class would derive from a particular transcript.Though this maintains some extra information, it is not alwaysFig. 1. A fragment multimapping between two different isoforms (A,B) of agene. Depending on the parameters of the fragment length distribution of theunderlying sample, either multi mapping locus could be more probable a priori. Under the approximate likelihood factorization that considers only compatibility-based equivalence classes, such information is necessarily hiddenfrom the resulting inference algorithm. We note that, of course, such multimapping can also happen between different genes (e.g. paralogs)Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018Improved data-driven factorizationsenough to faithfully approximate the full-model likelihoodfunction.Below, we describe a data-driven approach that allows for amuch more faithful representation of the full model likelihood function, while still greatly reducing the amount of information thatmust be maintained for inference. A broad overview of how thesefactorizations relate to each other is given in Figure 2, and the specific factorizations are described in more detail below.3 Materials and MethodsAs illustrated in Figure 2 and described above, the approximationsthat rely on compatibility-based factorizations can discard information that may be useful for correct transcript abundance estimation.Specifically, such notions of equivalence classes sacrifice perfragment information encoded in the conditional probabilitiesPrðfj jti Þ. We propose here alternative notions of equivalence classesthat take into account both the transcripts with which a fragment iscompatible, as well as the vector of conditional probabilities that encodes how likely the fragment is to have been sequenced from eachsuch transcript. That is, these factorizations account both for the setof transcripts t1 ; . . . ; tk to which a fragment fj maps, as well as theconditional probabilities Prðfj jt1 Þ; . . . ; Prðfj jtk Þ that fj was sampledfrom each of these transcripts. Our approach is agnostic to howPrðfj jti Þ is computed, but, as stated in Section 2.1, we consider hereeach conditional probability to be the product of Equations (2) to(5), appropriately normalized.We accomplish this by defining newequivalence relations over fragments that consider and summarizethese conditional probabilities in a data-driven manner.Fig. 2. There is a conceptual tradeoff between the computational efﬁciency ofan inference technique, and the ﬁdelity with which it models the full, fragment-level likelihood function. kallisto, Sailﬁsh (using quasi-mapping(Srivastava et al., 2016)) and mmseq simply consider the compatibility offragments with transcripts, and thereby discard the conditional fragmentlevel probabilities completely. Salmon collapses the fragment-level conditional probabilities to a single scalar (their average value) per-equivalenceclass; this recovers some of the ﬁdelity lost in the other approaches, but canstill discard useful fragment-level information. Approaches that considereach fragment independently in each round of the optimization algorithm(e.g. RSEM and Salmon-FM and eXpress (ofﬂine)) sacriﬁce no ﬁdelity, buteach iteration scales with the total number of aligned/mapped fragments. Ourproposed data-driven clustering approach (Salmon-RF) captures most of theimportant fragment-level probabilities of the full model, while retaining anupdate time very similar to Salmon ’s standard model in its ofﬂine rounds.The online rounds of Salmon and eXpress are not directly comparable to thebatch rounds considered in this ﬁgure (they update the parameters more frequently), but they do consider the conditional probability of each fragmentindividuallyi145As one divides each equivalence class into smaller sub-classes offragments, the factorized likelihood approaches the likelihood (andhence fidelity) of the full model. Conversely, as the number ofequivalence classes increases so does the complexity of evaluatingand optimizing the likelihood.Here, we introduce two different factorization methods that refine the compatibility-based notion of equivalence classes. Theseapproaches are a refinement in the strict sense that each sub-clusterof fragments that fall within the newly defined equivalence classesalign to the same set of transcripts as all other fragments in the original, compatibility-based definition of the equivalence class.However, in these factorizations, the conditional fragment probabilities (with respect to the set of transcripts) tend to exhibit smallerdistance to mean; i.e. the approximate weight used to summarize theconditional probability of all fragments within these refined equivalence classes is much closer to the individual conditional probabilities of all the fragments placed in the class. Subsequently, this leadsto a more accurate approximation of the likelihood function.Moreover, we find that only a small number of such refined equivalence classes is required to approximate the full likelihood veryclosely, meaning that the computational complexity of evaluatingand optimizing the likelihood function is very close to what isrequired when considering the original compatibility-based equivalence class factorization (see Results).3.1 Rank-based factorizationWe call the first factorization method that we consider to refine thenotion of equivalence classes the ‘rank-based factorization’. We consider all transcripts to which a fragment aligns, and sort the transcripts based on the conditional probability values of the fragmentgiven each transcript. Then, the equivalence class for a fragment isdetermined by the set of transcripts to which it maps, and the rankorder of the conditional probabilities for this fragment given thosetranscripts. For instance, consider 1000 fragments which all align tothe transcripts t1 and t2, where 250 of these fragments align to t1with a higher conditional probability than that with which theyalign to t2 (and vice-versa for the rest). In this case, the rank-basedequivalence relation will induce two equivalence classes (whereasthe compatibility-based relation would have induced 1), the first250 fragments will become members of one equivalence class withlabel fh1; t1 i; h2; t2 ig and the rest will be assigned to anotherequivalence class with the label fh1; t2 i; h2; t1 ig. As with the original notion of rich equivalence classes in Salmon (Patro et al.,2017), a single scalar value per transcript is saved in each equivalence class, which is the mean of all conditional probabilities of thefragments given each transcript. Of course, in this factorization, thetotal number of equivalence classes is typically larger than the number of compatibility-based equivalence classes. Formally, we definethe rank-based equivalence relation $ < as follows: let rðf ; fhi1 ; ti1 i;hi2 ; ti2 i; . . . ; hij ; tij igÞ be a function that returns a permutation r ofðti1 ; ti2 ; . . . ; tij Þ such that Prðf jtr1 ÞPrðf jtr2 Þ...Prðf jtrj Þ,with ties broken arbitrarily in favor of the transcript having thesmaller index. We define two fragments fm and fn to be equivalent(fm $ < fn ) if and only if Xðfm Þ ¼ Xðfn Þ and rðfm ; Xðfm ÞÞ ¼rðfn ; Xðfn ÞÞ.3.2 Range-based factorizationWe consider a second factorization approach that we call ‘range factorization’ (Salmon-RF). In this approach, we seek equivalenceclasses that have fragments which both align to the same set of transcripts and which have similar conditional probabilities with respectDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018i146to these transcripts. To motivate this approach, consider, first, thecase of two fragments that have exactly the same conditional probabilities for the same set of transcripts, then one can safely groupthem together without any loss of accuracy with respect to the original likelihood function. In fact, this is the equivalence relation proposed by Nicolae et al. (2011). However, this particularfactorization can have a negative impact on performance since mostof the time probabilities of fragments are not exactly proportional.Hence, this can lead to a model similar to the full model that considers all fragment-transcript likelihood values. However, we cancompromise the ‘exact’ proportionality of probabilities for thesake of performance. Instead of clustering fragments that haveexactly proportional probabilities, we place fragments with thesame conditional probability ‘range’ into the same equivalenceclass. We first divide the valid range of probabilities ½0; 1  into kbins, and then consider two conditional probabilities equal iftheir values are in the same bin. Two fragments are consideredequivalent under this definition, denoted $r , if they fall into thesame set of bins with respect to all transcripts to which theyalign. Formally, let bk ðf ; fhi1 ; ti1 i; hi2 ; ti2 i; . . . ; hij ; tij igÞ be a function that returns a vector of bin values (one for each transcript,and each between 0 and k – 1). We define two fragments fm and fnto be equivalent (fm $r fn ) if and only if Xðfm Þ ¼ Xðfn Þ andbk ðfm ; Xðfm ÞÞ ¼ bk ðfn ; Xðfn ÞÞ.We can tune the parameter k to tradeoff of the number of suchequivalence classes versus the accuracy they provide. As kapproaches infinity (or, rather, machine precision), the fidelity provided by this factorization approaches that of the full model, because all fragments will end up in either single-member equivalenceclasses, or in equivalence classes of fragments having conditionalprobabilities exactly proportional to theirs. On the other hand, as kgets smaller, the number of clusters gets closer to a small constanttimes the number of compatibility-based equivalence classes, buteach cluster consists of fragments with the wider range of conditional probabilities. In this approach, we do not simply replace eachconditional probability with the center of the bin into which it falls.Rather, for each bin, we record the sum and a total number of conditional probabilities stored in this bin. After processing all fragments, the centroid of each bin is computed and used as therepresentative conditional probability for this bin. This model is anatural extension of the rich equivalence class model used inSalmon, and the models coincide when k ¼ 1. Throughout thispaper, range-based equivalence classes have a number of bins equalpﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃto 4 þ d jXðF q Þje.Figure 3 provides a good example of this factorization and itsimpact on the average of conditional probabilities for each transcript. There are 225 fragments that all are aligned to the two transcripts in this equivalence class. Each dot represents a fragmentwith its x value equal to Prðf jt1 Þ and y value equal to Prðf jt2 Þ. callshows the average value of conditional probabilities of all fragments for transcript t1 and t2. As can be observed, the deviation ofcall from many of the conditional probabilities is large since theconditional probabilities are widely distributed over the rangefrom zero to one. However, when we divide the range into threebins and then separate fragments based on the bin into which theirconditional probabilities fall, we obtain three clusters containingfragments whose within-cluster conditional probabilities fall intomuch smaller ranges. So, in this case, all fragments that have thesame bin for their conditional probability given t1 and their conditional probability given t2 end up in the same cluster. Lines showthe borders of each bin and colored circles show the centroids usedto represent the conditional probabilities in each bin. In this case,M.Zakeri et al.Fig. 3. Factorizing an equivalence class consisting of 225 fragments and 2transcripts into k ¼ 3 bins. Each dot represents one fragment. The verticallines indicate borders of bins for transcript t1 and the horizontal lines showborders of bins for transcript t2. The purple circle with label call shows the center for original equivalence class. The rest of the circles are indicators of thecenters for each cluster after the factorizationwe expect to obtain results closer to the full model; yet, the numberof clusters over which one must iterate to apply the EM algorithmis still much smaller than the total number of fragments (seeResults).Though we have implemented and experimented with both ofthese alternative factorizations, in this paper we will focus on therange-based factorization, as we observe that it almost always provides a better approximation of the likelihood than the rank-basedfactorization.4 ResultsWe test the ability of our proposed factorization to improve the approximation of the full model likelihood on both synthetic and experimental data. We demonstrate that, as expected, the range-basedfactorization almost always provides a very good approximation ofthe full model likelihood. Interestingly, we also observe that it sometimes leads to a slightly more accurate solution than when no factorization is applied (i.e. when the likelihood is evaluated for eachfragment independently). Though we have not investigated this indepth, it is likely that, in some cases, a small degree of smoothing ofthe conditional probabilities can lead to a more stable and accuratesolution.We consider both small-scale and transcriptome-wide simulated data. In Section 4.1 we consider simulations over the transcripts from families of paralogous genes. Such situations representthe most challenging abundance estimation problems for transcriptquantification tools since high levels of multi-mapping are prevalent. We conduct the simulations over many random settings of theabundances of these transcripts, and look at how well differentmethods are able to recover the true abundances at different average coverage levels. We directly observe how, in the most adversarial situations, the proposed factorization allows us to recoverimportant information that leads to improved abundanceestimates.In Section 4.2 we explore the effect that different factorizationshave on abundance estimates transcriptome-wide. Here, we observethat, while the data-driven factorizations lead to improved abundance estimates, the differences between methods becomes muchsmaller, since the statistics are aggregated over the entire transcriptome and since many transcripts fall into the ‘easy’ case of abundance estimation. The differences between methods, while stillDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018Improved data-driven factorizationsmoderate, are larger when we restrict our assessment to a more difficult subset of transcripts.Finally, in Section 4.3, we examine the effect of different factorization methods over experimentally sequenced data. We explorehow closely different factorizations approach the abundance estimates derived by RSEM—though we note (as observed in some ofthe simulated data) that RSEM is not necessarily more accurate thanthe alternative methods or factorizations.In Sections 4.1–4.3 we consider the transcript abundance estimates generated by RSEM, eXpress (both in default mode and with50 batch EM rounds) and variants of Salmon (using different factorizations). We focus on the performance of these tools when quantifying abundances using alignments, instead of mappings(Srivastava et al., 2016). We keep the input data as close as possible,since the purpose of this paper is not an investigation of the effect ofalignment versus mapping on expression estimation, but rather theeffect of the factorization of the likelihood and how that factorization affects inference. We noticed that, regardless of the factorization used, there was a small but persistent gap between nonalignment-based tools (kallisto and mapping-based variants ofSalmon) compared to RSEM and alignment-based variants ofSalmon on the RSEM-sim data. It is not clear that this is due to anyfundamental superiority of alignment compared to mapping, but rather, may be a result of the fact that the specific error model, learnedby RSEM and used to simulate reads in RSEM-sim, acts as a ‘sidechannel’ of information for alignment-based approaches. However,this question, though outside the scope of this paper, deserves further consideration and analysis.In the Supplementary Material, we explore the effect of thesefactorizations on mapping-based solutions by comparing differentvariants of mapping-based Salmon with kallisto (which only allowsusing pseudoalignment for quantification).Alternative factorization variants:Salmon (i.e. without any modification) uses a compatibilitybased notion of equivalence classes called ‘rich’ equivalence classes.Under this notion, the equivalence classes themselves are compatibility-based, but each transcript-equivalence class pair is associatedwith a scalar weight which is computed as the mean conditionalprobability of all fragments in this equivalence class to derive fromthis transcript. We also consider a variant of Salmon (denoted asSalmon-U herein) that adopts a purely compatibility-based notionof equivalence classes. That is, it stores no extra information aboutthe conditional probability of deriving the fragments in each equivalence class from the different transcripts, and during inference considers only that Prðf jtÞ ¼ Prðpjf ; tÞ ¼ 1=~t , where ~t is the effective‘‘length of transcript t and is defined as ~t ¼ ‘t À l‘t . l‘t is the mean‘ddof the truncated empirical fragment length distribution as describedin Patro et al., 2017.We also consider a variant of Salmon, Salmon-FM, that performsno additional factorization. Instead, like RSEM, it considers eachfragment and its relevant conditional probabilities independently. Inthis case, the only difference between Salmon-FM and RSEM is thatthe former computes the conditional fragment probabilities using anonline stochastic inference algorithm, while RSEM recomputes theconditional fragment probabilities after updating auxiliary modelparameters during the first 10 iterations of an offline (i.e. batch) EMprocedure.Finally, we consider a variant of Salmon, Salmon-RF, thatuses the range-factorization described in Section 3.2 to generateequivalence classes based on $r and compute the associatedweights.i147We use both the mean absolute relative difference (MARD) andSpearman correlation to assess performance. We define the absoluterelative difference (ARD) as:8if xi þ yi ¼ 0>0<;(7)ARDi ¼ jxi À yi j>otherwise:ðxi þ yi ÞWhere yi is the estimated number of reads originating from tiand xi is the true (or assumed) number of reads originating from ti .P1The MARD is simply defined as MARD ¼ M M ARDi , where Mi¼1is the total number of transcripts.Experimental setup and software parameters:In the tests below, Salmon v0.8.0 was run in alignment modewith the –useErrorModel flag. Salmon-RF consists of Salmon runwith –useRangeClusterEqClasses 4. Salmon-U consists ofSalmon run with –noRichEqClasses. RSEM v1.3.0 was run withdefault parameters. eXpress v1.5.1 was run with –no-bias-correct and other parameters were left as default (the extra parameter–additional-batch 50 was used to produce the eXpress (þ50)results). All alignments were generated using Bowtie 2 version 2.2.9with the default parameters chosen by RSEM. We note that thesedefault parameters disallow indels in the resulting alignments(though Salmon and eXpress allow indels in the alignments theyprocess, RSEM does not). Further, we note that since we examinesimulated data without bias and since we compare against RSEM(which does not model sequence-specific or fragment-GC bias) inthe experimental data, we run all other methods without bias correction. On experimental RNA-seq data, one might expect bias correction alone to substantially improve the accuracy of a given method.Though those accuracy improvements should be orthogonal to thoseobtained by improving the fidelity of the likelihood function. All thetests are performed on a 64-bit Linux server with 256GB of RAMand 4 x 6-core Intel Xeon E5-4607 v2 CPUs running at 2.60GHz.4.1 Small-scale simulations on RAD51 and its paralogsWe first consider a few small-scale simulations to motivate how theconditional probabilities considered by the full model (and approximated closely by the range-based equivalence classes) might improveabundance estimates. We note that these simulations are specificallyconstructed to represent adversarial and difficult-to-quantify mixtures of highly related isoforms. We consider the transcripts fromlarge families of paralogous genes, under many random distributionsof abundances. Often, the fragments will align to many differenttranscripts with few-or-no nucleotide differences, and sometimeseven with similar implied insert sizes. Thus, we expect that closelyapproximating the conditional fragment probabilities might have alarge effect in this case. We note, however, that such adversarialabundance configurations are likely rare in experimental data.We consider two different, small-scale tests focusing around thegene RAD51 and members of its paralogous family in HomoSapiens. The RAD51 family includes eight paralogous genes including RAD51 itself. RAD51 codes for a 339-amino acid protein that hasa significant role in repairing double strand breaks of DNA (Yateset al., 2015).In the first experiment we apply RSEM and all varieties ofSalmon on all isoforms of the RAD51 gene. We extracted all (10) reference transcripts of RAD51 from the Ensembl (release 80) referencetranscriptome. True reads counts for all transcripts were generatedby sampling a read count for each transcript uniformly over ½1; 200 ;these counts represent base-depth coverage (left) in Figure 4a. Thesecounts were multiplied by 10 to derive the input read counts at 10XDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018i148M.Zakeri et al.(a)(b)Fig. 4. Applying different methods of transcript abundance estimation in alignment mode on two sets of data in 3 depth of fragment sequencing. Top (a) are allisoform transcripts of gene RAD51. The bottom (b) is from transcripts of four different paralogs of RAD51, RAD51B,RAD51C, RAD51D. In each row the left mostplot refers to experiment with counts of 1X coverage, the middle one to 10X and the most right plot refers to the experiment with fragment counts of 100Xcoveragecoverage (Fig. 4a, center) and by 100 to derive the counts at 100Xcoverage (Fig. 4a, right).Given these read counts, the Polyester simulator (Frazee et al.,2015) was then used to simulate five different read sets (replicates)from the same input distribution. This entire procedure was repeated 30 times, setting R’s random seed from 1 to 30 in sequence.Since the reads are simulated, we can assess the deviation of theestimated abundances from the exact abundances for each transcript. We use the absolute relative difference (ARD) of estimatedversus true read counts (Equation (7)) as the metric to evaluate theaccuracy of different methods for each transcript over replicates,and Figure 4a shows a box plot of the distribution of ARD valuesover the 30 simulations.As we expect, Salmon-U generally yields the largest ARDs, failing to utilize the information contained in the conditional fragmentprobabilities. Salmon generally performs better, suggesting that,even in this complex scenario, the aggregate weight maintained inthe rich equivalence classes helps to recover some (but not all) of thefidelity of the full model. However, Salmon-RF, while only slightlyincreasing the number of equivalence classes considered, producesARDs very close to those of RSEM, eXpress (þ50) and Salmon-FM.This suggests that, even in this adversarial scenario, the range-basedequivalence classes allow us to recover the inferential accuracy ofthe full model.To further explore difficult abundance estimation scenarios, weconsider the case of the presence of high abundance isoforms frommore than one gene in the reference. Therefore, in the second set ofexperiments we consider four paralogs of RAD51 (RAD51, RAD51B,RAD51C and RAD51D). We extract all transcripts corresponding tothese genes and we run the same simulation as above with respect toall of these transcripts. Evaluation of ARDs for every transcript inall genes is displayed in Figure 4b. The results in this case are similarto what was observed in the single gene experiment. In some cases,like transcript ENST00000553595 from RAD51B (which is displayed as t10 in Fig. 4b), both Salmon-U and Salmon fail to estimatean accurate abundance. In other cases Salmon performs better thanSalmon-U, e.g. transcript ENST00000585947 from RAD51D (displayed as t50 in Fig. 4b). For almost every transcript, Salmon-RF,Salmon-FM, eXpress (þ50) (eXpress under default settings performsa bit worse) and RSEM all perform similarly and better than themethods that adopt a purely compatibility-based factorization ofthe likelihood. As this simulation contains a large number of transcripts, we plot, in Figure 4b, the box plots for only every tenthtranscript to make the plot more interpretable. The complete plotcontaining the ARD values for all transcripts of this paralogousfamily is provided in Supplementary Figure S1. SupplementaryFigure S2 shows gene specific performance of methods for all transcripts of RAD51C and RAD51D in this experiment. For transcriptsof RAD51C, all factorizations (even the basic rich equivalenceclasses of Salmon) perform relatively well compared to Salmon-U.For accurately estimating the abundances of transcripts fromRAD51D, however, improved factorizations (Salmon-RF) seem tobe essential.We ran quantification tools in non-alignment mode on bothRAD51’s transcripts and also all transcripts from the same RAD51 ’sparalog set we consider above (Supplementary Fig. S3). We also performed similar simulations at three different depths from the geneSEZ6 and its paralogs (SEZ6L and SEZ6L2) and followed the sameset of steps to compare the performance of different tools in nonalignment mode, the result for this gene are presented inSupplementary Figure S4.4.2 Transcriptome-wide analysis on synthetic dataTo assess the performance of the proposed model on a large datasetof RNA seq reads, we generate synthetic data using RSEM-sim,and adopting the procedure used by Bray et al. (2016). RSEMmodel parameters were generated by running RSEM on sampleNA12716_7 from the GEUVADIS (Lappalainen et al., 2013)study. Using these model parameters, RSEM-sim was then used togenerate a sample consisting of 30M 75 bp paired-end RNA-seqreads.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018Improved data-driven factorizationsAgain, we explore the performance of RSEM, eXpress (both indefault mode and with 50 rounds of batch EM) and four differentvariants of Salmon (Salmon-U, Salmon, Salmon-RF and SalmonFM). We compute the Spearman correlation and MARD metrics ofeach of these methods compared with the true (i.e. simulated) abundances. As we observe in Table 1, discarding all weight informationin equivalence classes (Salmon-U) causes a drop in performancecompared to the case with a single scalar per equivalence classtranscript pair (Salmon). Using the range-factorization proposed inthis paper improves both the correlation and MARD measures evenfurther, and brings its accuracy on par with that of RSEM andSalmon-FM, which adopt no factorization and run an EM algorithmthat scales in the number of alignments in each iteration. In the default mode (i.e. using a single online pass), eXpress produces a largerMARD and lower correlation than any of the tools that run thebatch EM until convergence. With 50 extra batch EM rounds(eXpress (þ50)), eXpress performs more similarly to the other tools.We note that, in this data, the number of equivalence classes produced by the range-based factorization is $586 000, only $150 000greater than the $438 000 compatibility-based equivalence classes.Both of these numbers are orders-of-magnitude smaller than the $100 000 000 distinct alignments for this dataset. The number ofequivalence classes for all methods (sequenced fragments forSalmon-FM) is shown in Table 2. This table also reports the numberof ‘hits’. The number of hits is the sum, over each equivalence class,of the number of transcripts in this equivalence class—i.e.PqF q 2C jXðF Þj. This is the total number of items processed duringeach round of the EM algorithm. This small number of equivalenceclasses and hits allows the Salmon-RF model to run as fast asSalmon, which runs considerably faster than Salmon-FM, which, inturn, runs considerably faster than RSEM. With the exception ofeXpress, which implements a constant-memory algorithm by design,the memory usage profiles for these different tools track the timingresults (as expected). For more details, refer to SupplementaryFigures S5 and S6.Though we observe an improvement for Salmon-RF andSalmon-FM over Salmon and especially Salmon-U in this case, wenote that it is relatively small in scale. This is because, while the aggressive compatibility-based factorizations do give up information,common expression patterns may not be complex or difficultenough to be greatly affected by the lossy factorization of the likelihood. Also, however, these aggregate metrics are computed over theentire transcriptome, and so, difficulties of these factorizations indeconvolving particularly complex scenarios may become lost in thenoise of the vast number of good predictions.To focus on the more difficult cases, we computed our accuracymetrics on a subset of the simulated data. Specifically, retaining theoriginal abundance estimates over the entire transcriptome, we restricted our analysis to those transcripts for which RSEM obtainedan ARD between 0.25 and 0.75. The motivation for choosing thesevalues is to discard the particularly ‘easy’ to quantify transcripts(where the full model is likely neither necessary nor particularlyhelpful) as well as the ‘hopeless’ transcripts (those where the inference exhibits significant error even under the reference implementation of the full model). The results of this analysis are shown inTable 3. While the trend is similar to that observed on the full data,the difference between methods (and the impressive performance ofSalmon-RF) becomes more clear. Specifically, we observe thatSalmon outperforms Salmon-U, but this time the gap betweenSalmon and Salmon-RF, Salmon-FM and RSEM is larger. This ismost likely because this particular subset of transcripts presents amore difficult inference challenge, where the conditionali149Table 1. Spearman correlation and MARD of quantiﬁcation resultscompared to true abundances for synthetic data on all transcriptsMARD0.240.220.210.210.290.230.21Salmon-USalmonSalmon-RFSalmon-FMeXpresseXpress (þ50)RSEMSpearman0.800.810.830.830.780.830.82Table 2. The number of equivalence classes and hits, in the simulated data, under different likelihood factorizationsSalmon-U# eq. classes# hitsSalmon438 3935 986 371438 3935 986 371Salmon-RFSalmon-FM625 6388 212 66929 447 710103 663 423Table 3. The performance of different methods when restricted tothe subset of transcripts where RSEM’s ARD is in ½0:25; 0:75 MARDSalmon-USalmonSalmon-RFSalmon-FMeXpresseXpress (þ50)RSEMSpearman0.460.430.410.410.530.480.410.560.580.640.650.540.590.65probabilities provide useful evidence. In the case of these transcripts,running the EM algorithm until convergence seems particularly important, as we observe that eXpress (and even eXpress (þ50)) trailthe other methods, especially in terms of the MARD. This makes itevident that further refinement of the abundance estimates (i.e.more rounds of the EM) over a representation of the data encodingconditional fragment probabilities (as done in RSEM, Salmon-FMand Salmon-RF) is necessary to obtain improved accuracy on thesetranscripts.We further investigate the performance of tools in nonalignment mode as well. Spearman correlation and MARD of transcript quantification with different tools on RSEM simulated data ispresented in Supplementary Tables 1 and 2.4.3 Transcriptome-wide analysis on experimental dataFinally, we explore the effect of our data-driven factorizationmethod with the different versions of Salmon using experimentaldata from the SEQC(MSEQ-III) consortium (Consortium et al.,2014) (NCBI GEO accession SRR1215996 - SRR1217002).Specifically, the library is prepared on Universal Human ReferenceRNA (UHRR) from Stratagene and ERCC Spike-In controls andconsists of $11M 100 bp, paired-end reads sequenced on anIllumina HiSeq 2000 platform. The experiment consists of sevenreplicates with the same flowcell and barcodes but on differentlanes.As described previously in section 4 we compare the performance of Salmon, Salmon-FM, Salmon-RF, Salmon-U, eXpress,eXpress (þ50) with RSEM. However, unlike in previous sections,Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018i150M.Zakeri et al.(a)(b)Fig. 5. Comparison of the transcript abundances in different versions of salmon on the experimental data with seven technical replicates and using rsem abundance estimates as the ground truth (a) The Spearman correlation of transcripts abundance estimations with RSEM results reveals that Salmon-FM is highly correlated with RSEM. Very similar correlation with RSEM is observed by the proposed data-driven factorization, Salmon-RF. Salmon displays a lower correlationthan Salmon-RF, but a higher correlation than Salmon-U. The variants of eXpress show a lower correlation than Salmon-U, with the ofﬂine EM iterations increasing eXpress ’ correlation considerably. (b) Comparing the MARD of estimated transcript fragment counts with respect to RSEM results shows similar trend to thatobserved with the Spearman correlations; i.e. Salmon-FM has the least error rate using RSEM abundances as the truth while Salmon-RF perform equally well.Salmon exhibits a lower MARD than Salmon-U, which is followed by both variants of eXpressTable 4. The number of equivalence classes and hits, in the experimental data, under different likelihood factorizationsSalmon-U# eq. classes# hitsSalmon427 6115 737 414427 6115 737 414Salmon-RFSalmon-FM624 3408 318 6389 077 70850 325 595here, we lack a ground truth. Thus, we measure the accuracy of eachmethod on the estimated number of reads, treating RSEM’s estimations of the number of reads for each transcript (which is observedto be among the most accurate on synthetic data in previous sections) as the truth. We perform a comparison across all seven replicates and consider the Spearman correlation and MARD metrics.Since these are technical replicates, we expect the performance overeach replicate to be very similar, though we plot the results as a distribution in Figure 5a and b. The results on experimental data followthe same trend as we observed on synthetic data. That is, SalmonFM correlates well with RSEM (as expected) because of the availability of full fragment level transcript probabilities. Likewise, weagain observe that our proposed data-driven factorization method,Salmon-RF, performs essentially the same as the full model. Both ofthese methods agree more closely with RSEM than does Salmon,and again, Salmon-U, ignoring all fragment-level conditional probabilities, is further from RSEM’s results. The number of equivalenceclasses for each factorization are shown in Table 4. We also observethat eXpress, in its default mode, performs most differently fromRSEM of the methods we considered. As expected, running additional rounds of the batch EM (eXpress (þ50)) increases the similarity of eXpress ’ estimations with those of RSEM; though it is stillless similar than the other methods.5 ConclusionWhile compatibility-based equivalence class factorizations (Brayet al., 2016; Nicolae et al., 2011; Patro et al., 2014; Srivastava et al.,2016; Turro et al., 2011) have paved the way in terms of substantially improving the efficiency of the iterative optimization procedures used for transcript-level quantification from RNA-seq data,they nonetheless make sacrifices in modeling fidelity to achieve this.While these methods generally perform adequately in terms oftranscriptome-wide assessments, there are still important situationsin which their compatibility-centric factorization of the underlyinglikelihood function discards information that can be important foraccurate abundance estimates. Salmon (Patro et al., 2017) uses adual-phase inference algorithm that allows it to recover some of theinformation discarded by other approaches. It improves upon theapproximate factorization of the full likelihood function by incorporating a notion of rich equivalence classes of fragments. In some, butnot all cases, this improved factorization is sufficient to recover thelost accuracy of the full model.In this paper, we have introduced a data-driven factorizationof the likelihood function that makes use of Salmon ’s dual-phaseinference algorithm (Salmon-RF). We have shown that this improved factorization is able to match the accuracy of the fullmodel while still maintaining a reduced representation that isorders of magnitude smaller than the total number of fragmentalignments.We believe that this data-driven factorization represents theright tradeoff between efficiency and accuracy. Specifically, it demonstrates an almost indistinguishable sacrifice in efficiency beyondthe factorization already employed by Salmon (which, itself, issimilar in size to those employed by mmseq, Sailfish and kallisto),while producing no perceptible loss in accuracy compared to thefull per-fragment likelihood function used by RSEM and similarmethods.In this paper, we have focused on the effect that the adoptedfactorization of the likelihood function can have on the ability of amethod to accurately estimate transcript abundance. However, wenote that there still remain small but interesting differencesbetween methods that employ alignment and those that rely onmapping (i.e. quasi-mapping or pseudoalignment). Fully exploringthe nature of these differences, and how they interact with the factorizations proposed herein, is an interesting direction for futurework.Finally, while we have investigated the effect different factorizations have on maximum likelihood estimates, fully exploring theeffect they have in estimating the variance of these estimates (e.g. viabootstrapping) or even in estimating the full posterior distributionof abundances (e.g. via Gibbs sampling) is another interesting direction for future work.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018Improved data-driven factorizationsFundingWe gratefully acknowledge support from National Science Foundation grantBBSRC-NSF/BIO-1564917.Conflict of Interest: none declared.ReferencesBray,N.L. et al. (2016) Near-optimal probabilistic RNA-seq quantiﬁcation.Nat. Biotechnol., 34, 525–527.Consortium,S.-I. et al. (2014) A comprehensive assessment of rna-seq accuracy, reproducibility and information content by the sequencing quality control consortium. Nat. Biotechnol., 32, 903–914.Frazee,A.C. et al. (2015) Polyester: simulating RNA-seq datasets with differential transcript expression. Bioinformatics, 31, 2778–2784.Glaus,P. et al. (2012) Identifying differentially expressed transcripts fromRNA-seq data with biological variation. Bioinformatics, 28, 1721–1728.Hensman,J. et al. (2015) Fast and accurate approximate inference of transcriptexpression from RNA-seq data. Bioinformatics, 31, 3881–3889.Lappalainen,T. et al. (2013) Transcriptome and genome sequencing uncoversfunctional variation in humans. Nature, 501, 506–511.Li,B. and Dewey,C.N. (2011) RSEM: accurate transcript quantiﬁcation fromRNA-Seq data with or without a reference genome. BMC Bioinformatics, 12, 1.Li,B. et al. (2010) RNA-Seq gene expression estimation with read mapping uncertainty. Bioinformatics, 26, 493–500.i151Nariai,N. et al. (2013) TIGAR: transcript isoform abundance estimationmethod with gapped alignment of RNA-Seq data by variational Bayesian inference. Bioinformatics, btt381.Nariai,N. et al. (2014) TIGAR2: sensitive and accurate estimation of transcript isoform expression with longer RNA-Seq readsonline. BMCGenomics, 15, S5.Nicolae,M. et al. (2011) Estimation of alternative splicing isoform frequenciesfrom RNA-Seq data. Algorithms Mol. Biol., 6, 9.Patro,R. et al. (2014) Sailﬁsh enables alignment-free isoform quantiﬁcationfrom RNA-seq reads using lightweight algorithms. Nat. Biotechnol., 32,462–464.Patro,R. et al. (2017) Salmon provides fast and bias-aware quantiﬁcation oftranscript expression. Nat. Methods, 14, 417–419.Roberts,A. and Pachter,L. (2013) Streaming fragment assignment for realtime analysis of sequencing experiments. Nat. Methods, 10, 71–73.Salzman,J. et al. (2011) Statistical modeling of RNA-Seq data. Stat. Sci. Rev. J.Inst. Math. Stat., 26, 62–83.Srivastava,A. et al. (2016) RapMap: a rapid, sensitive and accurate tool formapping RNA-seq reads to transcriptomes. Bioinformatics, 32,i192–i200.Trapnell,C. et al. (2010) Transcript assembly and quantiﬁcation by RNA-Seqreveals unannotated transcripts and isoform switching during cell differentiation. Nat. Biotechnol., 28, 511–515.Turro,E. et al. (2011) Haplotype and isoform speciﬁc expression estimationusing multi-mapping RNA-seq reads. Genome Biol., 12, 1.Yates,A. et al. (2015) Ensembl 2016. Nucleic Acids Res., gkv1157.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i142/3953977by gueston 07 January 2018
5028881995002	PMID28881995	5028881995	https://watermark.silverchair.com/btx261.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881995.main.pdf	Bioinformatics, 33, 2017, i133–i141doi: 10.1093/bioinformatics/btx261ISMB/ECCB 2017deBGR: an efficient and near-exact representationof the weighted de Bruijn graphPrashant Pandey1, Michael A. Bender1, Rob Johnson1,2 and Rob Patro1,*1Department of Computer Science, Stony Brook University, Stony Brook, NY 11790, USA, 2VMWare, Inc., Palo Alto,CA 94304*To whom correspondence should be addressed.AbstractMotivation: Almost all de novo short-read genome and transcriptome assemblers start by buildinga representation of the de Bruijn Graph of the reads they are given as input. Even when otherapproaches are used for subsequent assembly (e.g. when one is using ‘long read’ technologies likethose offered by PacBio or Oxford Nanopore), efﬁcient k-mer processing is still crucial for accurateassembly, and state-of-the-art long-read error-correction methods use de Bruijn Graphs. Becauseof the centrality of de Bruijn Graphs, researchers have proposed numerous methods for representing de Bruijn Graphs compactly. Some of these proposals sacriﬁce accuracy to save space.Further, none of these methods store abundance information, i.e. the number of times that each kmer occurs, which is key in transcriptome assemblers.Results: We present a method for compactly representing the weighted de Bruijn Graph (i.e. withabundance information) with essentially no errors. Our representation yields zero errors whileincreasing the space requirements by less than 18–28% compared to the approximate de Bruijngraph representation in Squeakr. Our technique is based on a simple invariant that all weighted deBruijn Graphs must satisfy, and hence is likely to be of general interest and applicable in mostweighted de Bruijn Graph-based systems.Availability and implementation: https://github.com/splatlab/debgr.Contact: rob.patro@cs.stonybrook.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 Introduction and related workThe de Bruijn Graph has become a fundamental tool in genomics(Compeau et al., 2011) and the de Bruijn Graph underlies almost allshort-read genome and transcriptome assemblers—(Chang et al.,2015; Grabherr et al., 2011; Kannan et al., 2016; Liu et al., 2016;Pevzner et al., 2001; Simpson et al., 2009; Schulz et al., 2012;Zerbino and Birney, 2008)—among others. De Bruijn graphs, andk-mer-based processing in general, have also proven useful, even forlong read sequence analysis (Carvalho et al., 2016; Koren et al.,2017; Salmela et al., 2016).Despite the computational benefits that the de Bruijn Graph provides above the overlap-layout-consensus paradigm, the graph stilltends to require a substantial amount of memory for large datasets.This has motivated researchers to derive memory-efficient de BruijnGraph representations. Many of these representations build uponapproximate membership query (AMQ) data structures (such asBloom filters) to achieve an economy of space.Approximate membership query data structures are set (or multiset) representations that achieve small space requirements by allowingqueries, occasionally, to return false positive results. The Bloom filter(Bloom, 1970) is the archetypal example of an AMQ. Bloom filtersbegan to gain notoriety in bioinformatics when Melsted and Pritchard(2011) showed how they can be coupled with traditional hash tablesto vastly reduce the memory required for k-mer counting. By insertingk-mers into a Bloom filter the first time they are observed, and addingthem to the higher-overhead exact hash table only upon subsequentobservations. Later, Zhang et al. (2014) demonstrated that the countmin sketch (Cormode and Muthukrishnan, 2005) (a frequency estimation data structure) can be used to approximately answer k-mer presence and abundance queries when one requires only approximatecounts of k-mers in the input. Such approaches can yield order-ofmagnitude improvements in memory usage over competing methods.These ideas were soon applied to the construction and representation of the de Bruijn Graph. For example, Pell et al. (2012) introducea completely probabilistic representation of the de Bruijn Graph usinga Bloom filter to represent the underlying set of k-mers. Though thisrepresentation admits false positives in the edge set, they observe thatthis has little effect on the large-scale structure of the graph until thefalse positive rate becomes very high (i.e. ! 0:15).CV The Author 2017. Published by Oxford University Press.i133This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018i134Building upon this probabilistic representation, Chikhi and Rizk(2013) introduce an exact de Bruijn Graph representation that couples a Bloom-filter-based approximate de Bruijn Graph with anexact table storing critical false positive edges. Chikhi and Rizk’s deBruijn Graph representation exploits the fact that, in the de BruijnGraph, there are very few edges connecting true-positive k-mers tofalse-positive k-mers of the Bloom filter representation of the k-merset. Such edges are called critical false positives. Further, they observe that eliminating these critical false positives is sufficient to provide an exact (navigational) representation of the de Bruijn Graph.This compact representation allows large de Bruijn Graphs to beheld in RAM, which enables relatively efficient assembly of evenlarge and complex genomes.Subsequently, the representation of Chikhi and Rizk was refinedby Salikhov et al. (2013), who improved the memory requirementseven further by replacing the exact table with a cascading Bloom filter. The cascading Bloom filter stores an approximate set usinga combination of an approximate (i.e. Bloom filter-based) representation of the set and a smaller table to record the relevantfalse-positives. This construction can be applied recursively to substantially reduce the amount of memory required to represent theoriginal set. Salikhov et al. (2013) provide a representation that requires as little as 8 À 9 bits per k-mer, yet remains exact from a navigational perspective. Even more memory-efficient exactrepresentations of the unweighted de Bruijn Graph are possible. Forexample, Bowe et al. (2012) introduced the succinct de BruijnGraph (often referred to as the BOSS representation), which provides an exact navigational representation of the de Bruijn Graphthat uses < 5 bits per k-mer, which compares favorably to the lowerbound of %3:24 bits per k-mer on navigational representations(Chikhi et al., 2014).While the above approaches used auxiliary data structures tocorrect errors in an approximate representation of the de BruijnGraph, Pellow et al. (2016) showed how to exploit redundancy inthe de Bruijn Graph itself to correct errors. Essentially, theyobserved that true k-mers are not independent—each true k-mer willhave a k–1-base overlap with another true k-mer. If a Bloom filterrepresentation of the de Bruijn Graph indicates that a particular kmer x exists, but that no k-mer overlapping x exists, then x is likelyto be a false positive. Thus, by checking for the existence of all overlapping k-mers, they can dramatically reduce the false-positive rateof a Bloom-filter-based de Bruijn Graph representation. Our representation of the weighted de Bruijn Graph can be viewed as an extension and generalization of this basic idea. See Sections 2.3 and 3for details.However, the Bloom filter omits critical information—the frequency of each k-mer—that is necessary when performing assemblyof a transcriptome. Thus, ‘topology-only’ representations are inadequate in the case where knowing the abundance of each transcript,and by extension, each k-mer in the de Bruijn Graph that is part ofthis transcript, is essential. In the transcriptomic context, then, oneis interested primarily in the weighted de Bruijn Graph (seeDefinition 2). The weighted de Bruijn Graph associates with each kmer its abundance in the underlying dataset upon which the deBruijn Graph was constructed. Unlike the case of genomic assembly,we expect the counts in the weighted de Bruijn Graph for transcriptomic data to have a very large dynamic range, and maintainingexact or near-exact counts for each k-mer can be important for accurately identifying transcripts.In this paper, we introduce a memory-efficient and essentiallyexact representation of the weighted de Bruijn Graph. Our representation is based upon a recently-introduced counting filter data structureP.Pandey et al.Pandey et al. (2017a) which, itself, provides an approximate representation of the weighted de Bruijn Graph. Observing certainabundance-related invariants that hold in an exact weighted de BruijnGraph, we devise an algorithm that uses this approximate data representation to iteratively self-correct approximation errors in the structure. The result is a data structure that takes 18–28% more spacethan the approximate representation and has zero errors. This makesour new representation, which we call deBGR, essentially an exactrepresentation of the weighted de Bruijn Graph. In datasets with billions of distinct k-mers, deBGR typically exhibits zero topologicalerrors. Further, our algorithm corrects not only the topology of theapproximate representation, but also misestimates of abundance thatresult from collisions in the underlying counting filter.Additionally, while existing space-efficient representations of thede Bruijn Graph, are static, i.e. k-mers cannot easily be deleted fromthe graph; our representation supports removal of edges from the deBruijn Graph. This capability is enabled by the counting quotient filter’s ability to delete items (which cannot be done reliably in Bloomfilters). Since aggressive simplification of the de Bruijn Graph (e.g.to remove spurious topology like bubbles and tips) is typically doneprior to assembly, this deletion capability is important. Previousapproaches avoided the standard simplification step by insteadadopting more complicated traversal algorithms (Chikhi and Rizk,2013). By removing this limitation of the Bloom filter, our representation benefits both from simpler traversal algorithms which allowthe in-memory creation of a more manageable simplified weightedde Bruijn Graph. Recently, Belazzougui et al. (2016) have introduced a dynamic representation of the unweighted de Bruijn Graphbased on perfect hashing, and it will be interesting to explore theability of this approach to represent the weighted de Bruijn Graph.However, to the best of our knowledge, this representation has notyet been implemented.We believe that our representation of the weighted de BruijnGraph can be successfully applied to considerably reduce the computational requirements for de Bruijn Graph-based transcriptomeassembly (Chang et al., 2015; Grabherr et al., 2011; Kannan et al.,2016; Liu et al., 2016). One of the major benefits of our approach isthat weighted de Bruijn Graph construction should require considerably less memory than the approaches taken by these other tools.This will allow for the assembly of larger and more complicatedtranscriptomes on smaller and less expensive computers. Further,since our compact representation of the de Bruijn Graph can be keptcompletely in memory, even for relatively large transcriptomes, wecan avoid the ad hoc and potentially complicated step of partitioning the de Bruijn Graph for further processing (Kannan et al., 2016;Pell et al., 2012).2 BackgrounddeBGR is built on our prototype k-mer counter Squeakr (Pandeyet al., 2017b), which is in turn built on our counting quotient filterdata structure (Pandey et al., 2017a). We explain the key features ofthese systems that are needed to understand deBGR. We then reviewprior work on exploiting redundancy in de Bruijn Graphs to correcterrors in approximate de Bruijn Graph representations. We alsonote that, throughout the paper, we assume a DNA (i.e. 4 character)alphabet.2.1 The counting quotient filterThe counting quotient filter (CQF) supports functionality similar to acounting Bloom filter, but offers much better performance and usesDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018deBGRi135far less space (Pandey et al., 2017a) than a counting Bloom filter. TheCQF is essentially an approximate multiset: items can be inserted anddeleted, and queries return the number of instances of an item that arecurrently in the multiset. Queries may return an incorrect count withprobability e. Like the counting Bloom filter, errors are one-sided—the count returned by the CQF is never smaller than the true count.The CQF stores an approximation of a multiset S   U by storinga compact, lossless representation of the multiset h(S), whereh : U ! f0; . . . ; 2p À 1g is a hash function. To handle a multiset ofup to n distinct items while maintaining a false positive rate of atmost e, the CQF sets p ¼ log2 n (see the original quotient filter paperefor the analysis (Bender et al., 2012)).The counting quotient filter divides h(x) into its first q bits, quotient h0 ðxÞ, and its remaining r bits, remainder h1 ðxÞ. The countingquotient filter maintains an array Q of 2q r-bit slots, each of whichcan hold a single remainder. When an element x is inserted, thecounting quotient filter attempts to store the remainder h1 ðxÞ atindex h0 ðxÞ in Q (which we call x’s home slot). If that slot is alreadyin use, then the counting quotient filter uses a variant of linear probing, to find an unused slot where it can store h1 ðxÞ. The CQF alsomaintains a small amount of additional metadata in order to determine (1) which slots are in use and (2) the home slot of each each remainder stored in Q. The CQF metadata adds 2.125 bits ofoverhead per slot. See the CQF paper for details (Pandey et al.,2017a). In order to maintain good performance, the array of slotscannot be filled beyond 95%.Table 1 summarizes the per-element space required in a Bloomfilter, Cuckoo filter (Fan et al., 2014) and CQF, assuming no duplicates (we can’t compare a Bloom filters or Cuckoo filter to a CQFon multisets, since Bloom and Cuckoo filters do not support tracking the number of instances of each item). The CQF is always morespace efficient than the Cuckoo filter and more space-efficient thanthe Bloom filter for any false positive rate less than 1/64.The CQF is an exact representation of h(S)—all false positivesare due to collisions in h. Thus, by choosing h to be an invertiblehash function, we can use a CQF to store S losslessly. We use bothlossy and lossless CQFs in deBGR.Instead of storing multiple copies of the same item to count, likea quotient filter, the counting quotient filter employs an encodingscheme to count the multiplicity of items. The encoding scheme enables the counting quotient filter to maintain variable-sized counters. This is achieved by using slots originally reserved to store theremainders to, instead, store count information. The metadata bitsmaintained by the counting quotient filter allow this dynamic reuseof remainder slots for large counters while still ensuring the correctness of all counting quotient filter operations. See the CQF paper fordetails (Pandey et al., 2017a).Table 1. Space usage of several AMQs, as a function of e, the falsepositive rate, and a, the load factorFilterBloomCuckooCQFBits per elementlog2 1=eln23þlog2 1=ea2:125þlog2 1=eaMax aN/A0.950.95Note: The CQF is more space efﬁcient than the cuckoo ﬁlter for all falsepositive rates and more space efﬁcient than the Bloom ﬁlter for false positiverates less than 1/64. The Cuckoo ﬁlter and CQF offer good performance until95% load factor. A Cuckoo ﬁlter or CQF offers good performance up to aload factor of 0.95.The variable-sized counters in the counting quotient filter enable thedata structure to handle highly skewed datasets efficiently. By reusingthe allocated space, the counting quotient filter avoids wasting extraspace on counters and naturally and dynamically adapts to the frequency distribution of the input data. The counting quotient filter nevertakes more space than a quotient filter for storing the same multiset.For highly skewed distributions, like those observed in HTS-based datasets, it occupies only a small fraction of the space that would berequired by a comparable (in terms of false-positive rate) quotient filter.In summary, the features of the CQF that we take advantage ofin deBGR are:•••••CQFs support insertions of items and queries for the number ofinstances of an item,queries to a CQF always return a count that is at least as large asthe true count,CQFs can be either lossy or lossless,when used to represent a set losslessly, they support enumeratingthe elements of that set, andCQFs are space efﬁcient, even for skewed input distributions.2.2 SqueakrSqueakr is a k-mer-counter built on CQFs. Essentially, Squeakrreads and parses input files containing reads, and inserts the k-mersinto a CQF. It can then write the CQF to disk for later querying.Squeakr supports two modes: approximate and exact. In exact mode,Squeakr inserts k-mers using an invertible 2k-bit hash function, andhence has no false positives. In approximate mode, Squeakr uses a p-bithash function, where p is chosen as described above to maintain thedesired error rate while handling the expected number of input k-mers.Squeakr is competitive or outperforms state-of-the-art k-mer counters. In exact mode, Squeakr use about half the memory of KMC2 androughly the same amount of memory as Jellyfish2 (both of which areexact k-mer counters). For approximate counts, Squeakr uses considerably less memory (1.5X–4.3X) than Jellyfish2 and KMC2. Squeakroffers counting performance similar to that of KMC2 and faster thanJellyfish2. However, Squeakr offers an order-of-magnitude improvement in query performance. Squeakr offers very fast query performancefor both random queries and de Bruijn Graph traversal workloads.Fast queries turn out to be helpful in downstream data analyses,such as de Bruijn Graph traversals (Chikhi and Rizk, 2013), innerproduct computations (Murray et al., 2016; Vinga and Almeida,2003), and searches (Solomon and Kingsford, 2016).2.3 Prior approximate de Bruijn graph representationsdeBGR extends and generalizes an idea first suggested by Pellowet al. (2016), for correcting errors in approximate de Bruijn Graphrepresentations.The Bloom filter false-positive rate is calculated assuming all theitems inserted in the Bloom filter are independent. However, whenwe use a Bloom filter to represent a de Bruijn Graph, the items (or kmers in this case) are not independent. Each k-mer has a k–1-baseoverlap with adjacent k-mers in the sequence.Pellow et al. (2016) use this redundancy to detect false positivesin a Bloom filter representation of the de Bruijn Graph. Wheneverthey want to determine whether a k-mer x is present in the de BruijnGraph, they first query the Bloom filter for x. If the Bloom filter indicates that x is not present, then they know that x is not in the deBruijn Graph. If, however, the Bloom filter indicates that x might bein the de Bruijn Graph, they then query the Bloom filter for everypossible k-mer that overlaps x in k–1 bases. If the Bloom filter indicates that none of these k-mers is part of the de Bruijn Graph, then xDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018i136is very likely to be a false positive. If the Bloom filter returns true forat least one of the k-mers overlapping with x, then they concludethat x is very likely to be in the de Bruijn Graph.Pellow et al. (2016) present two versions of the k-mer Bloom filter,a one-sided k-mer Bloom filter and a two-sided k-mer Bloom filter. Theone-sided k-mer Bloom filter only looks for the presence of a singleoverlapping neighbor out of the eight possible neighbors (four on eachside) of a k-mer x. The one-sided k-mer Bloom filter achieves a smallerfalse-positive rate than a standard Bloom filter using the same space.The two-sided k-mer Bloom filter achieves an even lower falsepositive rate by requiring that there is an overlapping k-mer presenton either side of x. However, this approach can result in falsenegative results for k-mers that are at the edges of reads, since the kmers at the edges might not have neighbors on both sides.The two-sided k-mer Bloom filter deals with the k-mers at theedges of reads (i.e. start and end k-mers) specially. It maintains aseparate list that contains all the k-mers that occur at the beginningor end of a read. While constructing the k-mer Bloom filter, the firstand last k-mer of each read are stored in separate lists. During aquery for x, if it finds a neighboring k-mer on only one side of x,then it checks whether x is in the list of edge k-mers. If yes, then itreturns positive; else it returns negative.2.4 Lower bounds on weighted de Bruijn graphrepresentationIn the experiments, we perform in Section 4, we find that deBGR ispractically exact from a navigational perspective (i.e. it yields zeroerrors in terms of topology or abundance). It is useful, therefore, tokeep in mind some lower bounds for what is achievable in representing the weighted de Bruijn Graph exactly from a navigational perspective. We know that a navigational structure for the unweightedde Bruijn Graph requires at least 3.24 bits per kmer (Chikhi et al.,2014), and that exactly representing the counts requires at leastPF ¼ k2K d log2 ðfk Þe bits where K is the set of k-mers in a datasetand fk is the frequency of k-mer k, so that a reasonable lower boundFwould be 3:24 þ jKj bits per k-mer. To make such a representationefficient would likely require more space (e.g. a fast way to indexthe encoded, variable-size frequency data).We consider what this bound implies for the datasetGSM984609 in Section 4. Here, we have 1 146 347 598 distinct kmers and F ¼ 1 119 742 769, yielding a lower bound of %4:217 bitsper k-mer for an exact navigational representation of this weightedde Bruijn Graph. Approaching such a bound closely, is, of course, achallenge. For example, the cosmo1 https://github.com/cosmo-team/cosmo implementation of the BOSS data structure requires %5:995bits per k-mer on this dataset, but does not encode the weight ofeach edge. If we couple this with an array of fixed-size counters largeenough to represent the frequency distribution losslessly (for thisdataset, 23 bits per k-mer), it yields a representation requiring %28:995 bits per k-mer. deBGR, on the other hand, requires 26.52bits per k-mer. Thus, this example shows that there is still a considerable gap between what existing approaches achieve and the absolute theoretical lower bound for an exact navigationalrepresentation of a weighted de Bruijn Graph. However, deBGR isdynamic, supports membership queries, and provides efficient access(expected Oð1Þ) to k-mer abundances.P.Pandey et al.how we use this invariant to extend Squeakr (Pandey et al., 2017b) tocreate a near-exact representation of the weighted de Bruijn Graph.3.1 A weighted de Bruijn graph invariantThis section explains the structure of weighted de Bruijn Graphsthat we exploit to correct errors in approximate weighted de BruijnGraph representations, such as that provided by Squeakr.Definition 1. For a k-mer x, we will denote its reverse complementbas xÀ1 . The canonical form of a k-mer x, denoted x , is the lexicographically smaller of x and xÀ1 . For two k-mers x and y, we writeb bx ’ y if x ¼ y .A read is a string of bases over the DNA alphabet A, C, T, and G.Definition 2. The weighted de Bruijn Graph G of k-mers for a setbof reads R has a node n for each ðk À 1Þ-mer n that occurs in R.For each k-mer b1 xb2 in R, where b1 and b2 are bases and x is addðk À 2Þ-mer, there is an edge b1 xb2 connecting the nodes b1 x anddxb2 . The abundance of an edge b, denoted aðbÞ, is the number ofeetimes that b (i.e. e or eÀ1 ) occurs in R.eIn this formalization, a read of length ‘ corresponds to a walk oflength ‘ À k edges in the de Bruijn graph. Figure 1 shows two readsin the de Bruijn graph before canonicalization, and Figure 2 showsthe edges induced by those reads after canonicalization.bDefinition 3. For a node n and edge b, we say that b is a duplex edgeeeof b if there exist bases b and b0 (and possibly b ¼ b0 ) such thatnb ’ bb and b ’ n b0 . We say that b is a left edge of b if b is not a duene ben ebplex edge of n and there exists a base b such that b ’ bb. Similarly,enbb is a right edge of b if b is not a duplex edge of n and there exists aen ebase b such that b ’ n b.e bThere are several subtleties to this definition. Left, right, and dubcplex are defined relative to a node n . An edge b connecting nodes n1eand c may be a left edge of c and a right edge of c , or a left edgen2n1n2of both c and c , or any other combination. Note also that left,n1n2bright, and duplex are mutually exclusive—every edge of n is exactlybone of left, right, or duplex, with respect to n .Our compact representation of the de Bruijn graph is based onthe following observation:b bbObservation 1. Let e1 ; e2 ; . . . ; e‘ be the sequence of edges in a walkcbcorresponding to a read, and n0 ; . . . ; n‘ the corresponding sequencebdof nodes in the walk. Then for i ¼ 1; . . . ; ‘ À 1; ei and eiþ1 cannotbbboth be left edges of ni , nor can they both be right edges of ni .bIn other words, whenever a read arrives at a node n via a leftbbedge of n , it must depart via a right or duplex edge of n , and whenbever it arrives via a right edge of n , it must depart via a left or duplexbedge of n . (When a walk arrives via a duplex edge, it may leave viaany kind of edge.) This is because two successive edges of the walkcorrespond to a substring b1 nb2 of the read, where b1 and b2 aredbbases and n is a ðk À 1Þ-mer. If n ¼ n, then b1 n is a left (or duplex)dbbbedge of n and nb2 is a right (or duplex) edge of n . If n ¼ nÀ1 , thend bdbb1 n ’ n bÀ1 is a right (or duplex) edge of n , and nb2 ’ bÀ1 n is a left12 bb(or duplex) edge of n .The following lemma implies that duplex edges are rare, sinceonly nodes of a special form can have duplex edges.3 Materials and methodsbLemma 1. If a node n has a duplex edge, then either (1) n ¼ nÀ1 or (2)bn is equal to either AkÀ1 or CkÀ1, where A and C are the DNA bases.We begin by first presenting an invariant of de Bruijn Graphs that we exploit in our compact de Bruijn Graph representation. We then describebProof. Suppose node n has a duplex edge b. Without loss of geneberality, we can assume n ¼ n (by replacing n with nÀ1 if necessary).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018deBGRi137Then there exist (possibly equal) bases b and b0 such thatbn ’ b ’ nb0 , i.e., bn ’ nb0 . Let n ¼ n1 Á Á Á nkÀ1 , i.e., ni are the baseseconstituting n. Then there are two cases:••bn ¼ nb0 . In this case, b ¼ n1 ¼ n2 ¼ Á Á Á ¼ nkÀ1 ¼ b0 , i.e. n is astring of equal bases. Thus, n is equivalent to AkÀ1 or CkÀ1 .bn ¼ b0À1 nÀ1 . Thus, n ¼ nÀ1 .We call nodes that can have duplex edges duplex nodes, for examplesee Figure 2.We say that a walk, path, or cycle is left-right-alternating if, forbevery successive pair of edges b and e0 in the path, walk, or cycle,ebbbone is a left edge of n and one is a right edge of n , where n is thebbbnode common to b and e0 . We say that nodes n and n0 have lefteright-alternating distance d if the shortest left-right-alternating pathbbfrom n to n0 has length d.We now explain the main invariant used in our compactweighted de Bruijn Graph representation, as illustrated in Figure 1.This observation leads to the following invariant.Theorem 1 (The weighted de Bruijn Graph invariant). Let R be a setof reads that does not include any duplex edges. Let aðbÞ be theenumber of occurrences of the edge b in a set of reads. Let ‘ðbÞ be theenbnumber of reads that begin or end with a left edge of n , and rðbÞ thenbnumber of reads that begin or end with a right edge of n . Let sðbÞ beeb1 if b is a self-loop, and 0 otherwise. Let n be a node and assume,ebWLOG, that n ¼ n. Then0101B XB@bsðbnÞ2b 2 fA;C;G;TgCB XcaðbnÞC À ‘ðbÞ ¼ Bn @A2bsðnbÞb 2 fA;C;G;Tgbbbvisits n , it must arrive via a right edge of n and depart via a left edge of nb(or vice versa), unless W starts or ends at n . We call an arrival at or debbparture from n a threshold. Each occurrence of n in W corresponds tobtwo thresholds (except with the possible exception of occurrences of n atbthe beginning or end of W). We call an arrival at or departure from n viabba left edge of n a left threshold of n , and define right thresholds similarly.bThus, ignoring occurrences of n at the beginning or end of W, thebnumber of left thresholds of n must equal the number of right threshbnnolds of n . Let LW ðbÞ and RW ðbÞ be the number of left and rightbthresholds, respectively, of n in W. Let ‘W ðbÞ be the number of leftnbthresholds of occurrences of n at the beginning or end of W, and definebrW ðbÞ analogously for right thresholds of n . Thus we have the equalitynLW ðbÞ À ‘W ðbÞ ¼ RW ðbÞ À rW ðbÞ:nnnnEach occurrence of a left edge b of n in W corresponds to a singlee bbbthreshold of n , unless b is a loop connecting n to itself, in which caseeeach occurrence of b corresponds to two thresholds. Note that, sinceeby assumption b is not a duplex edge, if it is a loop, it corresponds toebtwo left thresholds or two right thresholds of n (i.e. it does not corbrespond to one left and one right threshold of n ). Let aW ðbÞ be theenumber of occurrences of b in W. ThenenLW ðbÞ ¼bc2sðbnÞ aW ðbnÞb 2 fA; C;G; TgandCcaðnbÞC À rðbÞ:nAProof. We argue the invariant for a single read. The overall invariant is established by summing over all the reads.Let W be a read. Since W contains no duplex edges, it corresponds toa left-right alternating walk in the de Bruijn Graph. Thus, every time WXRW ðbÞ ¼nXbc2sðnbÞ aW ðnbÞ:b 2 fA; C;G; TgThusXXbc2sðbnÞ aW ðbnÞ À ‘W ðbÞ ¼nb 2 fA; C;G; Tgbc2sðnbÞ aW ðnbÞ À rW ðbÞ:nb 2 fA; C;G; TgThe final result is obtained by summing over all reads W 2 R. hREAD 1: ....CAAAAT....READ 2: ....CAAAAC....AAATCAAACAAAAAAAAAT........CAAA....CAAAAC....ATAAAAAAACAAACde Bruijn Graph InvrariantNum of reads to the left = Num of reads to the rightFig. 1. Weighted de Bruijn Graph invariant. The nodes are 4-mers and edgesare 5-mers. The nodes and edges are drawn from Read1 and Read2 mentioned in the ﬁgure. The solid curve shows the read path. The nodes/edgesare not canonicalizedAAAAACAAAACAAAAAAAT3.2 deBGR: a compact de Bruijn graph representationWe now describe our compact weighted de Bruijn Graph representation. Given a set R of reads, we build counting quotient filters representing the functions a, ‘, and r. For ‘ and r, we use exact CQFs, sothese tables will have no errors. Since ‘ and r have roughly one entryfor each read, these tables will be relatively small (see Section 3.6).For a, we build a space-efficient approximate CQF, which we callaCQF . Since we build exact representations of ‘ and r, we will use ‘and r to refer to both the actual functions and our tables representing these functions. The CQF guarantees that, for every edgeb; aCQF ½hðbÞ  ! aðbÞ. We then compute a table c of corrections toeeeaCQF (we explain how to compute c below). After computing c, aquery for the abundance of an edge b returns gðbÞ, where g is definedeeto be gðbÞ ¼ aCQF ½hðbÞ  À c½b . Thus, since c is initially 0, we initiallyeeehave that gðbÞ ! aðbÞ for all b.eeeAAATAAAAAAAACAAACFig. 2. Types of edges in a de Bruijn Graph. The nodes are 4-mers and edgesare 5-mers. For node AAAA, CAAAA is a left edge and AAAAT, AAAAC areright edges. We introduced another edge AAAAA in order to show a duplexedge. All the nodes/edges are canonicalized and the graph is bi-directionalDefinition 4. We say that g satisfies the weighted de Bruijn Graph invariant for b ifn0101BB@Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018Xb 2 fA; C;G; TgBb cC2sðbnÞ gðbnÞC À ‘½b  ¼ BnA@Xb 2 fA; C;G; Tgb cC2sðnbÞ gðnbÞC À r½b :nAi1383.3 Local error-correction rulesWe first describe our local algorithm for correcting errors in g. Thisalgorithm can be used to answer arbitrary k-mer membershipqueries by correcting errors on the fly. Thus this algorithm can beused to perform queries on a dynamic weighted de Bruijn Graph.The process for computing c maintains the invariant that gðbÞe! aðbÞ for every edge b in the weighted de Bruijn Graph, while usingeethe following three rules to correct errors in g.b1. If we know that g is correct for all but one edge of some node n ,then we can use the weighted de Bruijn Graph invariant to solvefor the true abundance of the remaining edge.2. Since gðbÞ ! aðbÞ for all b, if (1) g satisﬁes the weighted de BruijneeebGraph invariant for some node n and, (2) we know that g is correctbfor all of n ’s left edges, then we can conclude that g is correct for allbof n ’s right edges, as well (and vice versa for ‘left’ and ‘right’).Pb csðbnÞ3. IfgðbnÞ ¼ ‘½b  and r½b  ¼ 0, then the abunnnb2fA;C;G;Tg 2bdance of all of n ’s right edges must be 0 (and vice versa for‘right’ and ‘left’).Given an initial set C of edges for which we know g is correct, wecan use the above rules to correct errors in g and to expand C. Buthow can we get the initial set of edges C that is required to bootstrapthe process? Our algorithm uses two approaches.First, whenever gðbÞ ¼ 0, it must be correct. This is because gecan never be smaller than a. Thus, the above rules always apply toleaves of the approximate weighted de Bruijn Graph and, more generally, to any nodes that have only left or only right edges.Leaves and nodes with only right or only left edges are not sufficient to bootstrap the error correction process, however, becauseweighted de Bruijn Graphs can contain cycles in which each nodehas both left and right edges that are part of the cycle. Starting onlyfrom leaves and one-sided nodes, the above rules are not sufficientto infer that g is correct on any edge in such a cycle, because eachnode in the cycle will always have a left and right edge for which g isnot known to be correct.We can overcome this problem by exploiting the random natureof errors in the CQF to infer that g is correct, with very high probability, on almost all edges of the approximate weighted de BruijnGraph, including many edges that are part of cycles.Theorem 2. Suppose that errors in g are independent and randombwith probability e. Suppose n is not part of a left-right-alternatingcycle of size less than d. Suppose also that g satisfies the weighted deBruijn Graph invariant at every node within a left-right-alternatingbdistance of dd=2e from n . Then the probability that g is incorrect forbany edge attached to n is less than ð4eÞd .Proof. Since g is never smaller than a, if g is incorrect for a leftbedge of some node n and g satisfies the weighted de Bruijn Graph inbbvariant at n , then g must be incorrect for at least one right edge of n .(And symmetrically for right/left). Thus, if g is incorrect for somebedge attached to n , then since g satisfies the weighted de BruijnbGraph invariant for all nodes within a radius d=2 of n , it must bethe case that g is incorrect for every edge along some left-rightbalternating path of length at least d edges. Since n is not part of acycle of length less than d, all the edges in this path must be distinct.Since errors in g are independent and have probability e, the probability of this occurring along any single path is at most ed .Since each node of the weighted de Bruijn Graph has at most 4left and 4 right edges, the total number of left-right-alternating pathsbof length d centered on node n is at most 4d . Hence, by a unionbound, the probability that such a path exists is at most ð4eÞd . hP.Pandey et al.We can use this theorem to infer, with high probability, that g iscorrect for many edges in the graph. We can choose larger or smallerd to control the probability that we incorrectly infer that g is correcton an edge. By choosing d ! log n= log ð1=4eÞ, where n is the number of edges in the approximate weighted de Bruijn Graph, we canmake the expected number of such edges less than 1.On the other hand, we expect many nodes from actual weightedde Bruijn Graphs to meet the criteria of Theorem 2. The vast majority of nodes in a weighted de Bruijn Graph are simple, i.e., they haveexactly 1 left edge and 1 right edge. Therefore, for most nodes, thereare only O(d) nodes within left-right-alternating distance dd=2e.Thus, for most nodes, the probability that they fail to meet the criteria is OðdeÞ. When d ¼ log n= log ð1=4eÞ, this becomesOðe log n= log ð1=4eÞÞ. This means that for most values of n and ethat arise in practice, the vast majority of nodes will meet the criteria2À8 , theof Theorem 2. For example, when n240 and efraction of nodes expected to fail the criteria of Theorem 2 is lessthan 3%.The above analysis suggests that large cycles (i.e. cycled of lengthat least log n= log ð1=4eÞ) in the weighted de Bruijn Graph will haveat least a few nodes that meet the criteria of Theorem 2, so the correction process can bootstrap from those nodes to correct any otherincorrect edges in the cycle. Small cycles (i.e. of size less thanlog n= log ð1=4eÞ), however, still pose a problem, since Theorem 2explicitly forbids nodes in small cycles.We can handle small cycles as follows. Any kmer that is part of acycle of length q < k must be periodic with periodicity q, i.e. it mustbe a substring of a string of the form xdk=qe , where x is a string oflength q. Thus, small cycles are quite rare. We can detect k-mersthat might be involved in a cycle of length less than d during the process of building aCQF and record their abundance in a separate, exactCQF. Since periodic k-mers are rare, this exact CQF will not consume much space. Later, during the correction phase, we can add allthe edges corresponding to these k-mers to the set C.As mentioned before, the weighted de Bruijn Graph invariantonly applies to nodes without duplex edges. Our weighted deBruijn Graph representation handles duplex edges as follows.Suppose a read corresponds to a walk visiting the sequence ofnodes c ; c ; . . . ; c . We treat every time the read visits a duplexn1 n2nqnode as the end of one read and the beginning of a new read. Bybreaking up reads whenever they visit a duplex node, we ensurethat whenever a walk arrives at a node via a left or right edge, it either ends or departs via a left or right edge. Thus we can use theweighted de Bruijn Graph invariant to correct errors in aCQF asdescribed above.3.4 Global, CQF-specific error-correction rulesSo far, our error-correction algorithm uses only local informationabout discprencies in the weighted de Bruijn Graph invariant to correct errors. It also uses the CQF in a black-box fashion—the samealgorithm could work with, for example, a counting Bloom filter approximation of a.We now describe an extension to our error-correction algorithmthat, in our experiments, enables it to correct all errors in the approximate weighted de Bruijn Graph. This extension exploits thefact that the CQF represents a multiset S by storing, exactly, themultiset h(S), where h is a hash function. It also performs a globalanalysis of the graph in order to detect more errors than can be detected by the local algorithm. Thus, this algorithm is appropriate forapplications that need a static, navigational weighted de BruijnGraph representation.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018deBGRNote that applications can mix-and-match the two error correction algorithms. Both the local and global algorithms can be run repeatedly, in any order, and even intermixed with interveningmodifications to the weighted de Bruijn Graph (e.g. after insertingadditional k-mers).For a read set R, let K be the set of distinct k-mers occurring inR. During the k-mer counting phase, every time we see a k-mer e,we increment the counter associated with hðbÞ in aCQF . Thus, afterecounting is completed, for any edge b,eXaCQF ½hðbÞ  ¼eaðbÞxb2K\hÀ1 ðbÞxewhere hÀ1 ðbÞ ¼ fbjhðbÞ ¼ hðbÞg.ex xeThe above equation enables us to use knowledge about the abundance of an edge b to infer information about the abundance ofeother edges that collide with b under the hash function h. For exeample, if we know that we have inferred the true abundance for allbut one edge in some set hÀ1 ðbÞ, then we can use this equation toeinfer the abundance of the one remaining edge.Our algorithm implements this idea as follows. Recall that, withbhigh probability, whenever an edge x 2 C, then gðbÞ ¼ aðbÞ. Thusxxwe can rewrite the above equation as:XXaCQF ½hðbÞ  ÀegðbÞ ¼xaðbÞ;xb2C\hÀ1 ðbÞb2C\hÀ1 ðbÞxex  e where C ¼ KnC. For convenience, write zðbÞ ¼ aCQF ½hðbÞ ÀeePxxyÀ1 ðbÞ gðbÞ. Note that z factors through h, i.e., if hðbÞ ¼ hðbÞ,b2C\h exbthen zðbÞ ¼ zðbÞ, and hence zðbÞ is the same for all x in some setxyxhÀ1 ðbÞ.eThe above equation implies two invariants that our algorithmcan use to infer additional information about edge abundances:••b eFor all e ; aðbÞzðbÞ. This is because, by deﬁnition, aðbÞ ! 0.eeThus, if the algorithm ever ﬁnds an edge b such that gðbÞ > zðbÞ,eeethen it can update c½b  so that gðbÞ ¼ zðbÞ.eeePIf, for some b, b2C\hÀ1 ðbÞ gðbÞ ¼ zðbÞ, then gðbÞ ¼ aðbÞ for allexexxx  e bx 2 C \ hÀ1 ðbÞ. This is because 0eaðbÞxgðbÞ for all x .xThus, in this case, the algorithm can add all the elements of x 2 C \ hÀ1 ðbÞ to C.e3.5 An algorithm for computing abundance correctionsAlgorithm 1 in the Supplementary Material shows our algorithm forcomputing c based on these observations. The algorithm is a standard work queue algorithm—it creates a work queue of edges thatmight have abundance errors and then pulls items off the worklist,looking for opportunities to apply the above rules. To save RAM,the algorithm computes the complement M of C, since for typicalerror rates C would contain almost all the edges in the weighted deBruijn Graph.The worst-case running time of the algorithm is Oðn1þ1= log ð1=4eÞ Þbut, for real weighted de Bruijn Graphs, the algorithm runs in Oðnlog n= log ð1=4eÞÞ time. The running time is dominated by initializingM, which requires traversing the graph and and finding any nodeswithin distance log n= log ð1=4e) of a weighted de Bruijn Graph invariant discrepancy. Since real weighted de Bruijn Graphs havenodes mostly of degree 2, there will usually be OðdÞ ¼ Oðlog n= logð1=4eÞÞ such nodes, giving a total running time ofOðn log n= log ð1=4eÞÞ.When used to perform a local correction as part of an abundancequery, we use the same algorithm, but restrict it to examine the region of the weighted de Bruijn Graph within O(d) hops of the edgei139being queried. In the worst case, this could require examining theentire graph, resulting in the same complexity as above. In the common case, however, the number of nodes within distance d of thequeried edge is O(d), so the running time of a local correction isOðlog n= log ð1=4eÞÞ.The space for deBGR can be analyzed as follows. To represent amultiset S with false positive rate e, the CQF takesOðjSj log2 1=e þ CðSÞÞ, where C(S) is the sum of the logs of thecounts of the items in S. To represent S exactly, assuming that eachelement of S is a b-bit string, takes OðjSj log2 b=jSj þ CðSÞÞ. So let Kbe the multiset of k-mers, and let E   K be the multiset of k-mer instances in K that occur at the beginning or end of a read or visit aduplex node. Then the space required to represent aCQF isOðjKj log 1=e þ CðKÞÞ. The space required for ‘ and r isOðjEj log 4k =jEj þ CðEÞÞ. Note that since E   K, CðEÞCðKÞ.The space required to represent c is OðejKj log 4k =ejKj þ CðKÞÞ.Thus the total space required for deBGR isOðjKj log14k4kþ jEj logþ ejKj logþ CðKÞÞ:ejEjejKj3.6 ImplementationWe extended Squeakr to construct the exact CQFs ‘ and r asdescribed above, in addition to the approximate CQF aCQF that it already built. We then wrote a second tool to compute c from aCQF ; ‘,and r. Our prototype handles duplex nodes and small cycles asdescribed. Our current prototype uses a standard hash table to storeM and standard set to store Q. Also, we use a standard hash table tostore c. An exact CQF would be more space efficient, but c is smallenough in our experiments that it doesn’t matter.3.7 Size of the first and last tablesWe explore, through simulation, how the sizes of the first and lasttables ‘ and r grow with the coverage of the underlying data. Here,for simplicity, we focus on genomic (rather than transcriptomic)data, as coverage is a well-defined notion. We simulated reads generated from the Escheria coli (E. coli) (strain E1728) reference genome at varying levels of coverage, and recorded the number of totaldistinct k-mers, as well as the number of distinct k-mers in ‘ and r(Fig. 3). Reads were simulated using the Art Huang et al. (2012)read simulator, using the error profiles 125 bp, paired-end reads sequences on an Illumina HiSeq 2500. As expected, the number of distinct k-mers in all of the tables grows with the coverage (due tosequencing error), Yet, even at 80x coverage, the ‘ and r tables, together, contain fewer than 25% of total distinct k-mers. On the experimental data examined in Section 4, the ‘ and r tables, together,require between than 18–28% of the total space required by thedeBGR structure.4 EvaluationIn this section, we evaluate deBGR, as described in Section 3.We evaluate deBGR in terms of space and accuracy. The space isthe size of the data structure(s) needed to represent the weighted deBruijn Graph. The accuracy is the measure of how close theweighted de Bruijn Graph representation is to the actual weightedde Bruijn Graph. We also report the time taken by deBGR to construct the weighted de Bruijn Graph representation, perform globalabundance correction, and perform local abundance correction foran edge.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018Number of distinct k-mersi140P.Pandey et al.Table 2. Datasets used in our experiments#Distinct First k-mers#Distinct Last k-mers#Distinct Total k-mers2 · 107DatasetGSM984609GSM981256GSM981244SRR12848951 · 107File size#Files#k-mer instances#Distinct k-mers2622433312124219662773 3301647077482537897872977262351298751146347598111809082414046439832079889717Note: The ﬁle size is in GB. All the datasets are compressed with gzipcompression.0Table 3. Space versus Accuracy trade-off in Squeakr and deBGR10203040 50Coverage607080SystemDatasetSpaceNavigational errors(bits/k-mer) Topological AbundanceFig. 3. Total number of distinct k-mers in First QF, Last QF, and Main QF withincreasing coverage of the same dataset. We generate dataset simulationsusing Huang et al. (2012)As described in Section 3.6, deBGR uses two exact counting quotient filters (‘ and r) in addition to the approximate counting quotient filter that stores the number of occurrences for each k-mer. Theerror-correction algorithm then computes a table c of corrections. Inour evaluation we report the total size of all these data structures,i.e. aCQF ; ‘, r, and c.We measure the accuracy of systems in terms of errors in theweighted de Bruijn Graph representation. There are two kind oferrors in the weighted de Bruijn Graph, abundance errors and topological errors. An abundance error is an error when the weighted deBruijn Graph representation returns an over-count for the query kmer (deBGR never resulted in an undercount in any of our experiments). Topological errors are abundance error for edges whosetrue abundance is 0. Topological errors are also known as falsepositives.In both cases, we report the number of reachable errors. Let Gbe the true weighted de Bruijn Graph and G0 our approximation.Since g is never smaller than a, the set of edges in G0 is a superset ofthe set of edges in G. An error on edge b of G0 is reachable if thereeexists a path in G0 from b to an edge that is also in G. Note thatereachable false positives are not the same as Chikhi et al.’s notion ofcritical false positives (Chikhi and Rizk, 2013). Critical false positives are false positives that are false positive edges that share a nodewith a true positive edge. Reachable false positives, on the otherhand, may be multiple hops away from a true edge of the weightedde Bruijn Graph.We compare deBGR to Squeakr in both its approximate andexact configurations. Recall that the exact version of Squeakr storesk-mers in a CQF using a 2k-bit invertible hash function, so that ithas no false positives. We use the exact version of Squeakr as the reference weighted de Bruijn Graph for computing the number ofreachable errors in Squeakr and deBGR.We do not compare deBGR against other Bloom filter based deBruijn Graph representations (Chikhi and Rizk, 2013; Salikhovet al., 2013) because Bloom filter based de Bruijn Graph representations do not have abundance information.4.1 Experimental setupAll experiments use 28-mers. In all our experiments, the countingquotient filter was configured with a maximum allowable falsepositive rate of 1/256.All the experiments are performed in-memory. We use severaldatasets for our experiments, which are listed in Table 2. All theSqueakrGSM984609Squeakr (exact)deBGRSqueakrGSM981256Squeakr (exact)deBGRSqueakrGSM981244Squeakr (exact)deBGRSqueakrSRR1284895Squeakr (exact)deBGR18.950.826.519.452.127.130.979.837.020.953.9525.3814263577001359125400104629630023272114001665531800158647540012257261002720082100Note: Topological errors are false-positive k-mers. Abundance errors arek-mers with an over count.experiments were performed on an Intel(R) Xeon(R) CPU (E5-2699v4 @ 2.20GHz with 44 cores and 56MB L3 cache) with 512GBRAM and a 4TB TOSHIBA MG03ACA4 ATA HDD.4.2 Space versus accuracy trade-offIn Table 3, we show the space needed and the accuracy (in terms ofnavigational errors) offered in representing the weighted de BruijnGraph by deBGR and the exact and approximate versions ofSqueakr. For deBGR, Table 3 gives the final space usage (i.e.aCQF ; ‘, r, and c). deBGR offers 100% accuracy and takes 48–52%less space than the exact version of Squeakr that also offers 100%accuracy. deBGR takes 18–28% more space than the approximateversion but the appropriate version has millions of navigationalerrors.The space required by deBGR in Table 3 is the total space of alldata structures (aCQF ; ‘, r, and c). In Table 4, we report the maximum number of items stored in auxiliary data structures (seeAlgorithm 1) while performing abundance correction. This gives anupper bound on the amount of space needed by deBGR to performabundance correction.4.3 PerformanceIn Table 5, we report the time taken by deBGR to construct theweighted de Bruijn Graph representation and perform global abundance correction. The time information for construction and globalabundance correction is averaged over two runs.We also report the time taken to perform local abundance correction for an edge. The time for local abundance correction peredge is averaged over 1M local abundance corrections. After performing abundance correction, computing gðbÞ ¼ aCQF ½hðbÞ  À c½b eeetakes 3.45 microseconds on average.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018deBGRi141Table 4. The maximum number of items present in auxiliary datastructures, edges (k-mers) in MBI and nodes (ðk À 1Þ-mers) in workqueue as described in the Algorithm 1, during abundancecorrectionDataset#Edges in M30815799293599132267451550320986GSM984609GSM981256GSM981244SRR1284895#Edges in work queue (Q)761786347260657256309858124558299Table 5. Time to construct the weighted de Bruijn Graph, correctabundances globally in the weighted de Bruijn Graph, and performlocal correction per edge in the weighted de Bruijn Graph (averaged over 1M local corrections)DatasetConstruction(seconds)Global correction(seconds)Local correction(microseconds)6605.655470.8313373.788429.1714857.6815390.5622266.8641218.8512.9318.2516.5016.62GSM984609GSM981256GSM981244SRR12848955 ConclusionWe argue that Squeakr, a space-efficient and approximate representation of the weighted de Bruijn Graph can be extended to build anear-exact representation of weighted de Bruijn Graph with almostno space cost. We demonstrate that abundance information in anapproximate weighted de Bruijn Graph representation can be usedto correct almost all the errors in that representation.Our representation is based on a simple invariant that allweighted de Bruijn Graphs must satisfy, so we believe this techniqueis likely to be of use in other weighted de Bruijn Graph applications.We believe precise abundance information can have a real impact on transcriptome assembly. For example, without error correction, low-abundance transcripts may collide with highabundance transcripts, causing the low-abundance transcripts tobecome lost in the noise. Accurate abundance information can enable applications to detect such faint signals and possibly recoversuch transcripts.FundingWe gratefully acknowledge support from National Science Foundation grantsBBSRC-NSF/BIO-1564917, IIS-1247726, IIS-1251137, CNS-1408695, CCF1439084, and CCF-1617618, and from Sandia National Laboratories.Conflict of Interest: none declared.ReferencesBelazzougui,D. et al. (2016). Fully Dynamic de Bruijn Graphs. SpringerInternational Publishing, Cham, pp. 145–152.Bender,M.A. et al. (2012) Don’t thrash: how to cache your hash on ﬂash.Proc. VLDB Endowment, 5, 1627–1637.Bloom,B.H. (1970) Spacetime trade-offs in hash coding with allowable errors.Commun. ACM, 13, 422–426.Bowe,A. et al. (2012). Succinct de Bruijn graphs. In: Proceedings of the InternationalWorkshop on Algorithms in Bioinformatics, WABI 2012. Springer. pp. 225–235.Carvalho,A.B. et al. (2016) Improved assembly of noisy long reads by k-mervalidation. Genome Res., 26, 1710–1720.Chang,Z. et al. (2015) Bridger: a new framework for de novo transcriptomeassembly using RNA-seq data. Genome Biol., 16, 30.Chikhi,R. and Rizk,G. (2013) Space-efﬁcient and exact de Bruijn graph representation based on a Bloom ﬁlter. Algorith. Mol. Biol., 8, 1.Chikhi,R. et al. (2014). On the representation of de Bruijn graphs. In:Proceedings of the International Conference on Research in ComputationalMolecular Biology, RECOMB 2014. Springer, pp. 35–55.Compeau,P.E. et al. (2011) How to apply de Bruijn graphs to genome assembly. Nat. Biotechnol., 29, 987–991.Cormode,G. and Muthukrishnan,S. (2005) An improved data stream summary: the count-min sketch and its applications. J. Algorith., 55, 58–75.Fan,B. et al. (2014) Cuckoo ﬁlter: Practically better than bloom. In: Proceedingsof the 10th ACM International on Conference on emerging NetworkingExperiments and Technologies, pp. 75–88. ACM, New York, USA.Grabherr,M.G. et al. (2011) Full-length transcriptome assembly from RNAseq data without a reference genome. Nat. Biotechnol., 29, 644–652.Huang,W. et al. (2012) ART: a next-generation sequencing read simulator.Bioinformatics, 28, 593.Kannan,S. et al. (2016) Shannon: an information-optimal de novo RNA-seqassembler. bioRxiv, 039230.Koren,S. et al. (2017) Canu: scalable and accurate long-read assembly viaadaptive k-mer weighting and repeat separation. bioRxiv, 071282.Liu,J. et al. (2016) Binpacker: packing-based de novo transcriptome assemblyfrom RNA-seq data. PLOS Comput. Biol., 12, e1004772.Melsted,P. and Pritchard,J.K. (2011) Efﬁcient counting of k-mers in DNA sequences using a Bloom ﬁlter. BMC Bioinform., 12, 1.Murray,K.D. et al. (2016) kWIP: the k-mer weighted inner product, a de novoestimator of genetic similarity. bioRxiv, 075481.Pandey,P. et al. (2017a) A General-Purpose Counting Filter: Making Every BitCount. In : Proceedings of the 2017 ACM International Conference onManagement of Data, pp. 775–787. ACM, New York, USAPandey,P. et al. (2017b). Squeakr: an exact and approximate k-mer countingsystem. bioRxiv 122077,http://biorxiv.org/content/early/2017/03/29/122077 (1 January 2017, date last accessed).Pell,J. et al. (2012) Scaling metagenome sequence assembly with probabilisticde Bruijn graphs. Proc. Natl. Acad. Sci., 109, 13272–13277.Pellow,D. et al. (2016). Improving Bloom ﬁlter performance on sequence datausing k-mer Bloom ﬁlters. In: International Conference on Research inComputational Molecular Biology, RECOMB 2016. Springer, Switzerland,pp. 137–151.Pevzner,P.A. et al. (2001) An Eulerian path approach to DNA fragment assembly. Proc. Natl. Acad. Sci., 98, 9748–9753.Salikhov,K. et al. (2013). Using cascading Bloom ﬁlters to improve the memory usage for de Brujin graphs. In: Algorithms in Bioinformatics. Springer,pp. 364–376.Salmela,L. et al. (2016) Accurate self-correction of errors in long reads usingde Bruijn graphs. Bioinformatics, btw321.Schulz,M.H. et al. (2012) Oases: robust de novo RNA-seq assembly across thedynamic range of expression levels. Bioinformatics, 28, 1086–1092.Simpson,J.T. et al. (2009) ABySS: a parallel assembler for short read sequencedata. Genome Res., 19, 1117–1123.Solomon,B. and Kingsford,C. (2016) Fast search of thousands of short-readsequencing experiments. Nat. Biotechnol., 34, 300–302.Vinga,S. and Almeida,J. (2003) Alignment-free sequence comparison–a review. Bioinformatics, 19, 513–523.Zerbino,D.R. and Birney,E. (2008) Velvet: algorithms for de novo short readassembly using de Bruijn graphs. Genome Res., 18, 821–829.Zhang,Q. et al. (2014) These are not the k-mers you are looking for: efﬁcient onlinek-mer counting using a probabilistic data structure. PloS One, 9, e101271.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i133/3953976by gueston 07 January 2018
5028881994002	PMID28881994	5028881994	https://watermark.silverchair.com/btx260.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881994.main.pdf	Bioinformatics, 33, 2017, i208–i216doi: 10.1093/bioinformatics/btx260ISMB/ECCB 2017Multiple network-constrained regressionsexpand insights into influenza vaccinationresponsesStefan Avey1,*, Subhasis Mohanty2, Jean Wilson2, Heidi Zapata2,Samit R. Joshi2, Barbara Siconolfi2, Sui Tsang3, Albert C. Shaw2 andSteven H. Kleinstein1,4,*1Interdepartmental Program in Computational Biology and Bioinformatics, Yale University, New Haven, CT 06511,USA, 2Section of Infectious Diseases, Department of Internal Medicine, 3Department of Internal Medicine and4Departments of Pathology and Immunobiology, Yale School of Medicine, New Haven, CT 06520, USA*To whom correspondence should be addressed.AbstractMotivation: Systems immunology leverages recent technological advancements that enable broadproﬁling of the immune system to better understand the response to infection and vaccination, aswell as the dysregulation that occurs in disease. An increasingly common approach to gain insightsfrom these large-scale proﬁling experiments involves the application of statistical learning methods to predict disease states or the immune response to perturbations. However, the goal of manysystems studies is not to maximize accuracy, but rather to gain biological insights. The predictorsidentiﬁed using current approaches can be biologically uninterpretable or present only one ofmany equally predictive models, leading to a narrow understanding of the underlying biology.Results: Here we show that incorporating prior biological knowledge within a logistic modelingframework by using network-level constraints on transcriptional proﬁling data signiﬁcantly improves interpretability. Moreover, incorporating different types of biological knowledge producesmodels that highlight distinct aspects of the underlying biology, while maintaining predictive accuracy. We propose a new framework, Logistic Multiple Network-constrained Regression(LogMiNeR), and apply it to understand the mechanisms underlying differential responses to inﬂuenza vaccination. Although standard logistic regression approaches were predictive, they wereminimally interpretable. Incorporating prior knowledge using LogMiNeR led to models that wereequally predictive yet highly interpretable. In this context, B cell-speciﬁc genes and mTOR signalingwere associated with an effective vaccination response in young adults. Overall, our results demonstrate a new paradigm for analyzing high-dimensional immune proﬁling data in which multiplenetworks encoding prior knowledge are incorporated to improve model interpretability.Availability and implementation: The R source code described in this article is publicly available athttps://bitbucket.org/kleinstein/logminer.Contact: steven.kleinstein@yale.edu or stefan.avey@yale.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionSystems immunology leverages recent technological advancementsin high-dimensional immune profiling to monitor the response toperturbations such as vaccination, as well as the dysregulation thatoccurs in disease. An increasingly common approach to gain insightsfrom these large-scale profiling experiments involves the applicationof statistical learning methods, such as classification, to accuratelypredict immune state or clinical outcome (Larra~aga et al., 2006).nHowever, interpreting these models to gain insights into the underlying process remains a challenge.Although a statistical model must be accurate to be meaningful,many systems biology studies focus on understanding the underlyingCV The Author 2017. Published by Oxford University Press.i208This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018LogMiNeRbiology and not on maximizing predictive accuracy. Even in caseswhere model accuracy is the main goal, such as diagnosis or prognosis, an accurate model that is interpretable in terms of underlyingbiology would likely be preferred over a similarly accurate modelwhich lacked interpretability. Thus, it may be preferable for statistical learning models to be interpretable in terms of what is alreadyknown about a system rather than attempting to maximize accuracyusing a ‘black box’ approach.In order to interpret a model, the most predictive genes can beidentified and tested for enrichment using collections of related genelists known as gene set libraries. However, previous studies haveshown that regression models built from transcriptional profiles canaccurately predict immune state or cancer subtype using multiplecompletely distinct sets of genes (Gruvberger et al., 2001; O’Haraet al., 2013). Strikingly, O’Hara et al. (2013) found that removingall predictive genes from a model and refitting the model can bedone for several iterations before a decrease in accuracy is observed.This result implies that single, parsimonious models may miss genesimportant to the underlying biology of the system of interest.Furthermore, the top predictive genes identified by a model cansometimes fail to be enriched in any gene set libraries, resulting in alack of biological interpretability.In an effort to improve model interpretability, many studies haveproposed network-constrained regularization approaches that utilizeprior knowledge (Chuang et al., 2007; Li and Li, 2008; Rapaportet al., 2007; Sun and Wang, 2012). These methods take advantageof large repositories of biological knowledge (e.g. pathways or protein–protein interactions) by encoding them in gene–gene networksand using these networks to enforce a constraint on the model.Including prior knowledge in the modeling process improves interpretability of the classifiers and in some cases can improve their accuracy over non-network methods. However, these studies wereperformed with a single source of prior knowledge. Thus, it remainsunknown how prior knowledge network choice affects model performance and whether a single model is sufficient to capture theunderlying biology.Here we show that fitting multiple models, each incorporating adifferent source of prior biological knowledge, greatly improvesmodel interpretability. We propose a new framework, LogisticMultiple Network-constrained Regression (LogMiNeR), which utilizes multiple models that each highlight distinct aspects of theunderlying biology, while maintaining predictive accuracy. We firstapply LogMiNeR to transcriptional profiling data to better understand differential influenza vaccination responses and subsequentlyshow that LogMiNeR can be applied to classification of many immune as well as non-immune-mediated diseases. This new paradigmprovides additional insights in systems biology studies which focuson finding predictive signatures that are interpretable in terms ofprior biological knowledge.2 Materials and Methods2.1 Availability of transcriptional profiling dataThe validation data (SDY80) is described in (Tsang et al., 2014).The gene expression data from SDY63 and SDY404 are publishedand described in Thakar et al. (2015). The design of SDY400 is identical to that described in Thakar et al. (2015) except that the sampleswere collected during the 2012–13 vaccine season. Data are available from ImmPort (https://immport.niaid.nih.gov) and GEO(Discovery: GSE59635, GSE59654, GSE59743; Validation:GSE47353).i2092.2 Defining vaccine response endpointVaccination response was calculated from the fold change in antibodytiter post-vaccination compared with pre-vaccination. Titers weremeasured at days 0 and 28 by hemagglutination inhibition assay inthe discovery data and at days 0 and 70 by virus neutralization assayin the validation data. A titer of half the first dilution was assigned tosamples in which the first dilution was negative and the largest dilution was reported if it was positive. High and low responders weredefined as the top and bottom 30%, respectively, of the maximum adjusted fold change as defined by Tsang et al. (2014).2.3 Data preprocessingThe discovery datasets were initially quantile normalized acrossarrays, and the processed validation data was used as provided.Following array normalization, each study went through severalpreprocessing steps independently in order to mitigate batch effects.First, probes were mapped to Entrez Gene IDs using theBioconductor tool AnnotationDbi (Pages et al., 2015), and probeswere collapsed to unique genes by choosing the probe with maximum average expression. Next, genes located on the X and Ychromosomes were removed to avoid selection of sex-linked genesthat may be confounded with vaccine response. The log fold changebetween day 7 and 0 was calculated for each gene, and the 1000genes with the largest fold change magnitudes in the discovery datasets were selected as the initial feature set. The gene fold changeswere standardized by subtracting the mean and dividing by thestandard deviation. Finally, the pre-processed data from SDY63,SDY404 and SDY400 were combined to form the discovery dataand SDY80 was used as the validation data.The data for the additional case studies were downloaded fromGEO using the Bioconductor R package GEOquery (Sean andMeltzer, 2007). The coefficient of variation was calculated for eachgene and the 500 genes with largest variation were selected as theinitial feature set.The classifier was built to distinguish the 292 samplesfrom subjects with SLE from the 20 control samples at baseline(time 0).GSE45291:The classifier was built to distinguish the 195 samplesfrom subjects with active tuberculosis from the 167 samples with latent tuberculosis.GSE37250:The classifier was built to distinguish the 82 samplesfrom subjects with idiopathic dilated cardiomyopathy from the 95samples with ischemic heart disease.GSE57338:2.4 Network-constrained logistic regressionNetwork-constrained logistic regression was performed as describedby Sun and Wang (2012). To account for pairs of connected genesthat were expected to have similar magnitude but potentially opposite effects on response, the adaptive regularization proceduredescribed in Section 3.4 of Sun and Wang (2012) was performed.Briefly, the signed Laplacian matrix was first calculated for eachnetwork by setting the uvth entry in the following way (Equation 1),81if u ¼ v and du 6¼ 0;>><pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃluv ¼ Àsu sv = du dv if u and v are connected;(1)>>:0otherwise:where du and dv are the degrees of genes u and v and su and sv arethe signs of the coefficients for genes u and v estimated byDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018i210S.Avey et al.correlation with the response variable during each round of crossvalidation. The model was then fit to minimize the objective function (Equation 2)À1 Xn½y log ðpðxi ÞÞ þ ð1 À yi Þ log ð1 À pðxi ÞÞ  þ PðhÞi¼1 in(2)where response yi is 0 for low responders or 1 for high respondersand xi is a vector of gene expression values for subject i. The penaltyfunction took the formPðhÞ ¼ ka!2Xp X1su husv hvpﬃﬃﬃﬃﬃ À pﬃﬃﬃﬃﬃjh j þ kð1 À aÞ u¼1(3)j¼1 ju$v2dvduXpwhere u $ v indicates the set of node pairs which are connected to uin the network. The first term is the L1 penalty that results in modelsparsity and the second term is the network constraint in Laplacianquadratic form that results in smoothness of coefficients over thenetwork.2.5 Fitting modelsLasso and Elastic Net Logistic Regression were performed usingthe glmnet R package v2.0.2 (Friedman et al., 2010). Networkconstrained logistic regression was performed using the pclogit Rpackage v0.2 (Sun and Wang, 2012). 200 values of the tuning parameter lambda were chosen and 20 values of alpha were used inthe interval [0,1]. Five-fold cross validation was performed tochoose the optimal values of lambda and alpha. The crossvalidation procedure was repeated 50 times for robustness to different splits of the discovery data. Cross-validation folds were balanced by study, response endpoint and gender for the fluvaccination case study and by response endpoint in the additionalcase studies. The parameters (lambda, alpha) were chosen by selecting the best model (Lasso: most sparse; Elastic Net andLogMiNeR: least sparse) with cross validated error within onestandard error of the minimum.2.6 Calculating Biological InterpretabilityThe mean-rank gene set enrichment test (Michaud et al., 2008) wasperformed using the absolute value of the model coefficients to rankthe genes. The test was performed using the geneSetTest function inthe limma R package v3.24.15 (Ritchie et al., 2015). Gene sets forKEGG, Reactome, and GO were downloaded from the MolecularSignatures Database v5.1 (Subramanian et al., 2005). Gene sets forBTM (Li et al., 2013) and CELLS (Abbas et al., 2005) were obtainedfrom the original publication. Only sets which had at least 15 genesoverlapping with the gene expression feature set were used. Falsediscovery rates were obtained using the method of Benjamini andHochberg (1995).2.7 Prior knowledge networksNetworks were defined for Reactome, GO, BTM and CELLS byconnecting all pairs of genes within each gene set. The KEGG network incorporated pathway topology and was built using theKEGGgraph R package v1.26.0 (Zhang and Wiemann, 2009).ImmuneGlobal and ExpOnly_ImmuneGlobal networks were obtained from ImmuNet and edges were restricted to those with confidence of at least 0.1 (Gorenshteyn et al., 2015). The STRINGnetwork incorporated all experimental evidence from the STRINGdatabase v10.0. The processed LINCS L1000 gene signatures generated as part of the Broad Institute Connectivity Map (Lamb et al.,2006) were obtained from the Enrichr downloads page (Chen et al.,2013), and a network was created by connecting all pairs of geneswithin each gene set.2.8 Identifying potential drug interactionsThe nearly 8000 sets included in the LINCS L1000 signatures werefiltered to the drug response gene sets that were significantly enriched and reversed the signature of high vaccine response. Thesedrug response gene sets contained genes which were significantlyaltered in the same direction upon treatment with the drug in theoriginal dataset and when comparing low–high responders. To limitthe number of false positives, we only considered gene sets that weresignificantly enriched (FDR < 0.001) in at least 50% of the crossvalidation runs.2.9 Visualizing significantly enriched gene setsThe distance between each pair of gene sets which were significantlyenriched in at least 10% of the 50 runs was calculated using theJaccard distance. The minimum spanning tree was calculated forthis network. This tree was then visualized in Cytoscape (Shannonet al., 2003, 2013) and the MultiColoredNodes plugin (Warsowet al., 2010) was used to color each node according to the priorknowledge network that led to its enrichment.3 Results3.1 Lasso and elastic net models are minimallyinterpretableSystems vaccinology seeks to use high-throughput profiling of immune responses to vaccination in order to gain insights into theunderlying biological mechanisms that lead to protection (Pulendranet al., 2010). Such approaches may be especially useful for improving the response to influenza vaccination, which is an importantpublic health tool, but fails to induce an antibody response in a significant fraction of individuals (Sasaki et al., 2011). To better understand why some individuals successfully generate antibodyresponses, while others fail to do so, we recruited healthy youngadults (21–30 years old) over three vaccination seasons and measured vaccine-specific antibody titers immediately prior to and 28days post-vaccination with the seasonal trivalent inactivated influenza vaccine. Genome-wide transcriptional profiling of blood samples prior to and 7 days post-vaccination was carried out for asubset of these individuals with strong and weak antibody responses.To quantify the strength of the vaccine response, we used the adjusted Maximum Fold Change endpoint proposed by Tsang et al.(2014). This endpoint adjusts the maximum fold change in antibodytiter for correlations with baseline titers and defines high and lowvaccine responders as the top and bottom 30th percentile, respectively (see Section 2.2).To predict the antibody response to influenza vaccination, wefirst applied standard Lasso logistic regression to the transcriptional profiling data (Tibshirani, 1996). The gene expression profiles were filtered to retain the 1000 genes which changed most 7days post-vaccination (see Section 2.3). These profiles served asthe discovery data to find a predictive signature to classify highand low vaccine responders. The models built from 50 runs of 5fold cross validation on these discovery data were subsequentlyvalidated on an independent cohort from the NIH Center forHuman Immunology (Tsang et al., 2014). In total, we trained ourmodel on 18 high and 17 low responders and validated our modelon 11 high and 10 low responders (Supplementary Table S1). TheLasso models fit the discovery data with a median accuracy ofDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018LogMiNeRi211ADBBECAFFig. 2. The number of signiﬁcantly enriched gene sets (FDR < 0.05) in multiplegene set libraries (rows) for L1 and EN models (A) or network-constrained(LogMiNeR) models (B). The diagonal in the dashed box indicates models forwhich the same gene set library was used for prior knowledge and enrichment testing. Bars represent mean and whiskers represent one standarderrorFig. 1. The classiﬁcation accuracy of models from 50 runs of 5-fold cross validation. Discovery (A) and Validation (B) accuracies for the Lasso (L1) andElastic Net (EN) models. (C) The optimal value of the tuning parameter alpha[0,1] controlling the trade-off between the L1 and L2 constraints. Discovery(D) and Validation (E) accuracies for the network-constrained (LogMiNeR)models. (F) The optimal value of the tuning parameter alpha [0,1] controllingthe trade-off between the L1 and network constraints86% (Fig. 1A), but predicted response with poor accuracy in thevalidation data (Fig. 1B).In order to understand what biological factors were able to discriminate high and low responders in the discovery data, we testedwhether any gene sets were enriched among the predictive genesusing a mean-rank gene set test (Michaud et al., 2008) on the modelcoefficients (see Section 2.6). The gene set libraries used to test enrichment included KEGG (Kanehisa and Goto, 2000) and Reactome(Croft et al., 2014; Milacic et al., 2012) pathways, blood transcriptional coexpression modules (BTM) (Li et al., 2013), gene ontology(GO) terms (Ashburner et al., 2000), and blood cell subset signatures (CELLS) (Abbas et al., 2005). On average, the models weresignificantly enriched (FDR < 0.05) for less than one gene set fromfour out of five gene set libraries tested (Fig. 2A). The Lasso modelswere enriched for an average of four GO terms including catabolicprocess and cellular catabolic process. These results show that Lassologistic regression is predictive on the discovery data and interpretable in terms of GO terms, but does not validate well in an independent cohort.The explicit goal of Lasso regression models is to reduce thenumber of features (genes) in the model, which may not be preferable when trying to interpret the underlying biology. In fact, theLasso models chose only five genes on average, which may explainwhy these models do not validate and are minimally interpretable.In order to improve model accuracy and interpretability, we nextfit Elastic Net logistic regression models (Zou and Hastie, 2005).The Elastic Net promotes a grouping effect that tends to include alarger number of relevant genes and was previously used to predictinfluenza vaccine response accurately from transcriptional profiles(Furman et al., 2013). The Elastic Net models fit the discoverydata with a median accuracy of 94% (Fig. 1A) and successfullypredicted response in the validation data with a median accuracyof 69% (Fig. 1B). The hyperparameter controlling the trade-off between Lasso (alpha ¼ 1) and ridge (alpha ¼ 0) constraints favoredthe ridge constraint (Fig. 1C). Accordingly, the average number ofgenes used to predict vaccine response with Elastic Net (88) wasmuch higher than for Lasso (5). However, when we tested whetherany gene sets were enriched among these predictive genes, on average, the models were significantly enriched (FDR < 0.05) for lessthan one gene set from four out of five gene set libraries tested(Fig. 2A). Two GO terms, on average, were enriched using ElasticNet including positive regulation of metabolic process and peptidase activity.In order to test whether these models of vaccination response weremore accurate than expected by chance, we permuted the class labelsin the discovery set and retrained the Lasso and Elastic Net models.The validation accuracy after permuting class labels was 50%, whichwas significantly lower (P ¼ 0.03, two-sided t-test) than Elastic Netbut not significantly different from Lasso (P ¼ 0.66, two-sided t-test),suggesting that the Elastic Net models were indeed fitting real differences in vaccination response and not noise (Fig. 1B). The differencein model accuracy was due to higher sensitivity using Elastic Net compared with Lasso (Supplementary Figs S3A, B, S4A and B). Overall,we found that Elastic Net models can predict vaccination responsefrom transcriptional changes 7 days post-vaccination, but the genesunderlying the predictions cannot be easily interpreted in terms ofexisting pathways or coexpression modules.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018i2123.2 Network-constrained models improveinterpretabilityWe next investigated whether biological interpretability could beimproved by models that select different sets of genes. Prior worksuggested that high-dimensional data often contain additional predictive genes beyond the ones chosen by standard machine learningmethods. For example, iterative feature removal can lead to multiplemutually exclusive subsets of genes that predict equally well and implicate different pathways (O’Hara et al., 2013).We hypothesized that including prior knowledge directly intoour modeling framework would improve interpretability by limitingthe search space of the models according to known biological relationships. To test this, we coded prior knowledge in the form ofgene–gene networks where edges represented associations betweentwo genes. The type of association depended on the prior knowledgesource (e.g. membership in the same pathway for KEGG). Multiplegene networks, representing a range of association types, served asprior knowledge to fit multiple models (Supplementary Table S2;see Section 2.7). All the gene set libraries used to test enrichment(KEGG, Reactome, BTM, GO, CELLS) were converted to networksand used as prior knowledge. Since existing databases are likely incomplete, we included additional prior knowledge networks derivedfrom Bayesian integration of functional interactions from multipleevidence sources (ImmuneGlobal) or gene expression only(ExpOnly_ImmuneGlobal) (Gorenshteyn et al., 2015) as well asprotein–protein interactions with experimental evidence in theSTRING database (Jensen et al., 2009). The nine networks varied inedge density from 0.1% (KEGG) to 11.5% (GO) but all networkswere filtered to contain the same set of genes used as input to theLasso and Elastic Net models (Supplementary Table S2). A networkconstraint was added in addition to the Lasso constraint so thatmodel coefficients would be smoothed over the network (Li and Li,2008; Sun and Wang, 2012). The constraint is motivated by the assumption that genes closely connected in the prior knowledge network should contribute similarly to prediction. Connected genes arecoerced to have model coefficients with similar magnitudes bypenalizing the squared difference between coefficients. At the sametime, we allow for the sign of the coefficients to vary. Thus we allowfor anticorrelated genes, such as two gene products in a pathwaywhere one negatively regulates the other, to contribute similarly toprediction but in opposite directions (see Section 2.4).We applied this approach to the influenza vaccination data andfound that the network-constrained models maintained similar predictive discovery accuracies as the Elastic Net models. Furthermore,the validation accuracies were predictive and not significantly different from Elastic Net (P ! 0.05, two-sided t-test) (Fig. 1D and E).Each network-constrained model achieved a median discovery accuracy of at least 90% while maintaining a median validation accuracy of at least 62%. Interestingly, even when the prior knowledgenetworks were rendered biologically meaningless by gene label permutation, the network-constrained models retained similar validation accuracies, AUROC, sensitivities and specificities to theElastic Net models (Supplementary Figs S1D, E, S2C, D, S3C, D,S4C and D). The subjects correctly classified as high or low responders in the validation dataset were consistent and did not depend on the source of prior knowledge. On average, thehyperparameter controlling the trade-off between model sparsity(alpha ¼ 1) and network smoothness (alpha ¼ 0) favored the network constraint (Fig. 1F). This indicates that the network constraintwas enforced and consequently the magnitude of model coefficientswas smooth over the network (see Section 2.5). Taken together,S.Avey et al.these results indicate that any of the network-constrained modelsare equally valid predictors of vaccination response, and are comparable with models built using Elastic Net.In order to test whether network-constrained models improve interpretability compared with Lasso and Elastic Net, we again quantified interpretability as the average number of significantly enrichedgene sets for each model. On average network-constrained modelsselected a comparable number of genes (45–150 depending on thenetwork) to Elastic Net (88). Yet, in contrast to both the Lasso andElastic Net models, the network-constrained models were highly enriched for known pathways and gene sets (Fig. 2B). For every geneset library, interpretability was better using a network-constrainedmodel compared with using either standard Lasso or Elastic Netmodels. As expected, each prior knowledge network produced alarge number of enriched gene sets based on that same network (e.g.using GO as prior knowledge leads to nine enriched GO sets).However, enrichment on the Reactome gene set library was greatestusing the KEGG network as the prior knowledge source. Eventhough the accuracy of network-constrained models was similarwhen gene labels were permuted, the models built from permutednetworks were minimally enriched for all gene set libraries tested(Supplementary Fig. S5B). In fact, the levels of enrichment using biologically meaningless networks closely mirrored the Elastic Netmethod. Thus, the network-constrained models are not only accurate, but also interpretable in terms of existing pathways, coexpression modules, GO terms, and blood cell subsets.Although network-constrained models tend to choose more interpretable genes, individual genes that are not associated with anygene set libraries were still included in the predictive models. 240 ofthe 1000 genes used as the initial feature set were not present in anyof the selected gene set libraries. All of these genes were used in atleast one network-constrained model, and seven were consistentlyselected in at least 50% of cross-validation runs. One such gene,JCHAIN, is a positive predictor of high response in 72% of runs.JCHAIN encodes the joining chain for multermeric IgA and IgMantibodies and is highly expressed in antibody secreting cells (ASCs)(Nakaya et al., 2011) suggesting an increase in circulating ASCs orantibody production 7 days post-vaccination in high responders.Thus, this method is able to identify novel predictive genes that arenot annotated in existing gene set libraries.3.3 Multiple models provide context-specific insightsWe next asked whether the gene sets enriched by Lasso, Elastic Net,and network-constrained methods were context-specific or sharedacross multiple methods or prior knowledge networks. We defined‘consistently enriched’ gene sets for each model as those that weresignificantly enriched (FDR < 0.05) in at least 10% of cross validation runs. Only a single gene set, GO peptidase activity, was consistently enriched regardless of the method used. (SupplementaryFig. S8). Of the 65 gene sets consistently enriched by any method 16(25%) were enriched by two or more different methods. Althoughthere were some shared insights, 49 (75%) gene sets were contextspecific (i.e. consistently enriched by only one method).Our results show that using different networks as prior knowledge leads to many predictive models that highlight context-specificas well as shared aspects of the underlying biology of vaccination response. Thus, we propose this approach, which we term‘LogMiNeR’ (Logistic Multiple Network-constrained Regression),as a new framework for systems immunology studies. In contrastwith previous methods, LogMiNeR fits multiple models, each usingDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018LogMiNeRi213Fig. 3. Overview of the gene sets (nodes) arranged by Jaccard distance on a minimum spanning tree. Only gene sets signiﬁcantly enriched (FDR < 0.05) by atleast one model were included. We further limited the visualization to the sets consistently enriched in at least 10% of the runs. Groups of context-speciﬁc (solidellipses) or shared (dashed ellipses) gene sets with similar annotations were manually identiﬁeddifferent sources of prior knowledge, to gain a broader understanding of the underlying biology.LogMiNeR allows for the interpretation and visualization ofmultiple models simultaneously (Fig. 3). To visualize the results ofLogMiNeR, gene sets were arranged by set similarity resulting intightly connected groups of related gene sets. The gene sets identifiedby multiple models (25%) were annotated to processes such as interferon signaling, transcription factor activity, and metabolic regulation. In addition, lymphoid and specifically B cell signature geneswere enriched by multiple models and were positive predictors of asuccessful vaccination response. The B cell enrichment was drivenprimarily by TNFRSF17, also known as B cell maturation antigen(BCMA). Other shared insights were only found by a subset of themethods. For example, the Reactome interferon alpha/beta signalingpathway positively predicted vaccine response and was only consistently enriched when the Reactome, KEGG or STRING networkswere used as prior knowledge. (Supplementary Fig. S7). The manycontext-specific gene sets identified by only a single model (75%)were mainly found using the KEGG pathway network and includedsets involved in inflammatory/stress response, signal transduction,lipid metabolism, and hemostasis. Overall, LogMiNeR allowed usto find both shared and context-specific insights into the influenzavaccination response which were dependent on the input priorknowledge network.To further demonstrate the utility of LogMiNeR and the influence of the prior knowledge network, we applied this framework toidentify drug interactions with the potential to negatively affect vaccine response (see Section 2.8). We hypothesized that if high vaccineresponses were characterized by specific changes in gene expression,and if drug treatment caused the opposite changes in gene expression in model cell lines, then that drug has the potential to negativelyimpact a proper vaccine response. This is similar to the computational approach for drug repositioning suggested by Sirota et al.(2011) and provides a straightforward method to identify drugs thatmight interfere with a successful influenza vaccination response. Toaccomplish this analysis, we applied LogMiNeR using a drug response network built from the Library of Integrated Network-basedCellular Signatures (LINCS) L1000 dataset (Lamb et al., 2006). Thevalidation accuracy was predictive and was not significantly different than Elastic Net (P ! 0.05, two-sided t-test). Out of the nearly8000 drug response signatures tested, we identified seven consistently and significantly enriched (FDR < 0.001) drug response genesets representing six distinct drugs (Supplementary Table S3). Oneof the six drugs, GDC-0980, is an mTOR inhibitor, suggesting thatindividuals taking mTOR inhibitors may have less effective responses to the seasonal influenza vaccination. The other drugs arekinase inhibitors specific to EGFR (gefitinib), MET/VEGFR2 (foretinib), MEK (AZD-8330, selumetinib) and Akt (A443654). Notably,these compounds target growth factor receptors (EGFR, VEGFR2,MET) or members of the Ras-ERK or PI3K-mTOR signaling pathways which lead to cell survival, proliferation, and motility(Mendoza et al., 2011). Together, these results suggest potentialdrug interactions that may adversely affect vaccine response as wellas highlight the importance of cell growth, proliferation and motilityto generating a successful antibody response upon influenzavaccination.3.4 LogMiNeR improves interpretability on additionaldatasetsIn order to assess whether the results we obtained on the influenzavaccination data were generalizable to other datasets with largersample sizes, we tested LogMiNeR on classification of publiclyavailable gene expression data from an autoimmune disorder (SLEversus healthy), a bacterial infection (latent versus active TB), and anon-immune mediated disease (ischemic heart disease versus dilatedcardiomyopathy) (see Section 2.1). In all three additional datasetstested, the accuracy of the network-constrained models was similarDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018i214to the non-network methods (Supplementary Figs S11A, B, S12A, B,S13A and B).The classification of SLE versus healthy subjects from peripheralblood was performed with nearly perfect accuracy (98–99%) regardless of which method was used. Although this highly accurateprediction suggests a strong signal in the data, the standard Lassoand Elastic Net methods were only enriched for an average of twoGO terms—negative regulation of biological process and cell–cellsignaling (Supplementary Fig. S11C). Using LogMiNeR with theSTRING network, we found enrichment of the Reactome pathwayscytokine signaling in immune system and, more specifically, interferon alpha/beta signaling (Supplementary Fig. S11D).We next tested for enrichment of gene set libraries on models classifying latent versus active tuberculosis. The number of enriched signatures was similar for all methods, with the exception of GO setswhich were more highly enriched with LogMiNeR (SupplementaryFig. S12C and D). Regardless of the method used, the Reactome pathways immune system and cytokine signaling in immune system wereconsistently and significantly enriched. In addition, LogMiNeR led toenrichment of GO terms beyond those shared by all methods including response to external stimulus and defense response which werepositive predictors of active tuberculosis.Finally, we applied LogMiNeR to classify patients with ischemicheart disease or dilated cardiomyopathy from transcriptional profilingdata of heart tissue (GSE57338). This dataset is different from theothers in that the samples are not from peripheral blood tissue and donot characterize an immune-mediated disease. Both Lasso andReactome network-constrained models are enriched for multiple genesets above background (Supplementary Fig. S13C and D). The enrichments of the Lasso models are more consistent across runs and includeinnate immune sets such as the KEGG toll-like receptor signalingpathway, Reactome innate immune system and BTM regulation ofantigen processing and presentation and immune response (M5.0).Thus, we suggest that Lasso and Elastic Net be used alongside ofLogMiNeR as they can provide additional insights.Our results demonstrate that LogMiNeR can be successfullyapplied to additional datasets and leads to models that are similarlyaccurate, but have the potential to expand the interpretability ofstandard logistic regression approaches.4 DiscussionWe propose LogMiNeR as a new framework for analyzingclassification-based studies that leads to increased biological interpretability. A series of case studies using multiple transcriptionalprofiling datasets demonstrate that LogMiNeR leads to many accurate models that are biologically interpretable. In contrast, the application of standard classification methods to these same dataproduces models that are no more accurate, but are often severelylimited in interpretability.Our primary case study focused on understanding influenza vaccination responses. Specifically, we sought to use transcriptionalprofiling data from post-vaccination blood samples to predictwhether individuals would have high or low vaccine responses asdetermined by antibody titers. Lasso models were not predictiveabove random on an independent validation dataset and few genesets were consistently enriched by either standard classificationmethod (Lasso or Elastic Net logistic regression). Elastic Net modelswere validated in an independent study and associated an increase inB cell signature genes, specifically TNFRSF17 (BCMA), 7 days postvaccination with increased vaccination response. The expression ofS.Avey et al.BCMA 7 days post-vaccination is a known positive predictor ofantibody response to both yellow fever vaccine 17D (Querec et al.,2009) and inactivated influenza vaccine (Li et al., 2014; Nakayaet al., 2011; Obermoser et al., 2013). LogMiNeR models were similarly predictive to Elastic Net and expanded interpretability by identifying over 60 consistently and significantly enriched gene sets notfound by standard classification methods. LogMiNeR allowed us toidentify known components of the vaccination response that werenot identified using Lasso or Elastic Net logistic regression. For example, we identified gene sets involved in interferon signaling topositively predict a successful influenza vaccination response.Interferon signaling has previously been reported 1–3 days postimmunization (Bucasas et al., 2011; Li et al., 2014; Tsang et al.,2014), and it is possible that the network constraint helped enrichfor this signal at later time points as well. We also identified 6 drugsfrom a library of nearly 8000 drug response profiles whose expression patterns suggested a potential negative effect on vaccination response in our cohort of young adults. We speculated that one suchdrug, the mTOR inhibitor GDC-0980, may negatively impact vaccination response. Although mTOR inhibitors were reported to improve immune responses to influenza vaccination in the elderly(Mannick et al., 2014), these inhibitors reportedly decreased the response to pandemic influenza vaccination in a cohort of youngadults receiving solid organ transplants (Cordero et al., 2011), suggesting that the effect may be age-dependent. Furthermore, the drugswe identified were all inhibitors of growth factor receptors or members of the Ras-ERK or PI3K-mTOR signaling pathways (Mendozaet al., 2011). This finding points to greater cell growth and proliferation in high vaccine responders. Whether these signatures are a primary cause leading to response or a by-product of B cellproliferation and migration, such as an increase in circulating plasmablasts (Tsang et al., 2014), will require further study. Thus,LogMiNeR helped us identify known components of the vaccine response as well as generate new hypotheses about how drugs mightaffect response to seasonal influenza vaccination.We also applied LogMiNeR to three larger datasets, each containing hundreds of samples, as case studies of disease classification.We found that all LogMiNeR models were similarly accurate, buthad the potential to expand interpretability of standard logistic regression approaches. For example, when we classified SLE versushealthy samples from peripheral blood, LogMiNeR, but not standard approaches, resulted in models that were enriched for the interferon alpha/beta signaling pathway. This finding is consistent withmany previous observations that individuals with SLE have a stronginterferon signature in their peripheral blood (Ronnblom and¨Eloranta, 2013). In two other datasets the significantly enrichedgene sets were similar whether or not network knowledge was incorporated. The LogMiNeR models classifying individuals with latentversus active TB were enriched beyond general immune system pathways to include more specific sets that positively predicted active TBincluding response to external stimulus and response to stress. Onthe other hand, Lasso models classifying ischemic heart diseaseversus dilated cardiomyopathy were more consistently enriched thanLogMiNeR models. Notably, this dataset profiled heart tissue in anon-immune-mediated disease whereas many of our prior knowledge sources were, by design, blood tissue or immune-specific.Because Lasso and Elastic Net can provide additional insights, wesuggest that non-network-constrained methods be used alongsideLogMiNeR.Interpretability was increased by each individual networkconstrained model while maintaining model accuracy. Others havealso reported this increased interpretability of network-constrainedDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018LogMiNeRregression approaches (Chuang et al., 2007; Li and Li, 2008;Rapaport et al., 2007). The novelty of our approach is that we fit multiple models using different sources of prior knowledge. When calculating interpretability, we noticed that the largest number of enrichedsets obtained with LogMiNeR tended to come from the network thatcorresponds to the same gene set library. On the flu vaccination data,the most enriched KEGG pathways are identified when a KEGG network is used as prior knowledge. In cases where this does not hold,the network is similar to the gene set library. For example, we foundthat the KEGG network led to the most enriched Reactome sets, likelybecause both are pathway databases and share many edges in the network. These results suggest that the prior knowledge networks givenas input to LogMiNeR should be chosen based on the context used tointerpret the models. On all four datasets tested, the accuracy ofLogMiNeR was similar to Elastic Net models. These findings are consistent with a survey evaluating eight network-based methods appliedto prognostic biomarker discovery which concluded that incorporating prior knowledge does not significantly improve classification accuracy. Surprisingly, LogMiNeR was similarly accurate to Elastic Netmodels even when gene labels in the network were permuted. Thismay be explained in part by the selection of a similar number of genesregardless of gene label permutation or by biases from isolated nodesin our prior knowledge networks.LogMiNeR can be viewed as a hybrid approach between singlegene methods and gene set methods, which calculate set activity andare able to reduce the signal-to-noise ratio (Levine et al., 2006).Each gene in LogMiNeR has its own weight, but these weights arecoerced to be similar for genes connected in the prior knowledgenetwork. We demonstrated that LogMiNeR not only improved interpretability on gene sets but also led to identification of predictivegenes which were not annotated in existing gene set libraries. Thus,LogMiNeR models have the freedom to discover novel predictivegenes while at the same time using prior knowledge to improve interpretability in terms of what is already known. Although we evaluated unweighted networks in this study, LogMiNeR could also beadapted to weighted networks. Furthermore, this method is not limited to gene expression data and can be applied whenever logistic regression is appropriate and prior knowledge networks exist.In summary, we present a new framework, LogMiNeR, to perform classification while increasing interpretability. We usedLogMiNeR to identify B cell-specific genes and mTOR signaling aspredictive signatures of human influenza vaccination responses andshow that using multiple prior knowledge networks can expand interpretability across many datasets. This framework could provideinsights in systems biology studies which focus on finding predictivesignatures that are interpretable in terms of prior biologicalknowledge.AcknowledgementWe wish to thank Daniel Chawla for careful reading of this article.FundingComputational resources and support were provided by the Yale Center forResearch Computing [National Institutes of Health grants RR19895 andRR029676-01]. This work was supported by the National Institutes ofHealth [grant number U19AI089992]. A.C.S. was supported by K24AG042489. S.A. was supported by the National Science FoundationGraduate Research Fellowship Program [grant number DGE-1122492]. Anyopinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of theNational Science Foundation.i215Conﬂict of Interest: none declared.ReferencesAbbas,A.R. et al. (2005) Immune response in silico (IRIS): immune-speciﬁcgenes identiﬁed from a compendium of microarray expression data. GenesImmun., 6, 319–331.Ashburner,M. et al. (2000) Gene ontology: tool for the uniﬁcation of biology.Nat. Genet., 25, 25–29.Benjamini,Y. and Hochberg,Y. (1995) Controlling the false discovery rate: apractical and powerful approach to multiple testing when researchers tendto select pursuing multiple the (statistically) and support of conclusions. Anunguarded use in a greatly results of single-inference inc. J. R Stat. Soc., 57,289–300.Bucasas,K.L. et al. (2011) Early patterns of gene expression correlate with thehumoral immune response to inﬂuenza vaccination in humans. J. Infect.Dis., 203, 921–929.Chen,E.Y. et al. (2013) Enrichr: interactive and collaborative HTML5 genelist enrichment analysis tool. BMC Bioinformatics, 14, 128.Chuang,H.-Y. et al. (2007) Network-based classiﬁcation of breast cancer metastasis. Mol. Syst. Biol., 3, 140.Cordero,E. et al. (2011) Therapy with m-TOR inhibitors decreases the response to the pandemic inﬂuenza A H1N1 vaccine in solid organ transplantrecipients. Am. J. Transplant., 11, 2205–2213.Croft,D. et al. (2014) The Reactome pathway knowledgebase. Nucleic AcidsRes., 42, 472–477.Friedman,J. et al. (2010) Regularization paths for generalized linear modelsvia coordinate descent. J. Stat. Softw., 33, 1–22.Furman,D. et al. (2013) Apoptosis and other immune biomarkers predict inﬂuenza vaccine responsiveness. Mol. Syst. Biol., 9, 659.Gorenshteyn,D. et al. (2015) Interactive big data resource to elucidate humanimmune pathways and diseases. Immunity, 43, 605–614.Gruvberger,S. et al. (2001) Estrogen receptor status in breast cancer is associated with remarkably distinct gene expression patterns advances in brief estrogen receptor status in breast cancer is associated with remarkablydistinct. J. Cancer Res., 61, 5979–5984.Jensen,L.J. et al. (2009) STRING 8 - a global view on proteins and their functionalinteractions in 630 organisms. Nucleic Acids Res., 37(Suppl. 1), 412–416.Kanehisa,M. and Goto,S. (2000) KEGG: Kyoto encyclopedia of genes andgenomes. Nucleic Acids Res., 28, 27–30.Lamb,J. et al. (2006) The connectivity map: using gene-expression signaturesto connect small molecules, genes, and disease. Science, 313, 1929–1935.Larra~aga,P. et al. (2006) Machine learning in bioinformatics. Brief.nBioinformatics, 7, 86–112.Levine,D.M. et al. (2006) Pathway and gene-set activation measurement frommRNA expression data: the tissue distribution of human pathways.Genome Biol., 7, R93.Li,C. and Li,H. (2008) Network-constrained regularization and variable selection for analysis of genomic data. Bioinformatics, 24, 1175–1182.Li,S. et al. (2013) Systems biological approaches to measure and understandvaccine immunity in humans. Semin. Immunol., 25, 209–218.Li,S. et al. (2014) Molecular signatures of antibody responses derived from asystems biology study of ﬁve human vaccines. Nat. Immunol., 15,195–204.Mannick,J.B. et al. (2014) mTOR inhibition improves immune function in theelderly. Sci. Transl. Med., 6, 268ra179–268ra179.Mendoza,M.C. et al. (2011) The Ras-ERK and PI3K-mTOR pathways: crosstalk and compensation. Trends Biochem. Sci., 36, 320–328.Michaud,J. et al. (2008) Integrative analysis of RUNX1 downstream pathwaysand target genes. BMC Genomics, 9, 363.Milacic,M. et al. (2012) Annotating cancer variants and anti-cancer therapeutics in Reactome. Cancers, 4, 1180–1211.Nakaya,H.I. et al. (2011) Systems biology of vaccination for seasonal inﬂuenza in humans. Nat. Immunol., 12, 786–795.Obermoser,G. et al. (2013) Systems scale interactive exploration reveals quantitative and qualitative differences in response to inﬂuenza and pneumococcal vaccines. Immunity, 38, 831–844.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018i216O’Hara,S. et al. (2013) Iterative feature removal yields highly discriminativepathways. BMC Genomics, 14, 832.Pages,H. et al. (2015). AnnotationDbi: Annotation Database Interface. Rpackage version 1.30.1.Pulendran,B. et al. (2010) Systems vaccinology. Immunity, 33, 516–529.Querec,T.D. et al. (2009) Systems biology approach predicts immunogenicityof the yellow fever vaccine in humans. Nat. Immunol., 10, 116–125.Rapaport,F. et al. (2007) Classiﬁcation of microarray data using gene networks. BMC Bioinformatics, 8, 35.Ritchie,M.E. et al. (2015) limma powers differential expression analyses forRNA-sequencing and microarray studies. Nucleic Acids Res., 43,Ronnblom,L. and Eloranta,M.-L. (2013) The interferon signature in autoim¨mune diseases. Curr. Opin. Rheumatol., 25, 248–253.Sasaki,S. et al. (2011) Limited efﬁcacy of inactivated inﬂuenza vaccine in elderly individuals is associated with decreased production of vaccine-speciﬁcantibodies. J. Clin. Invest., 121, 3109–3119.Sean,D. and Meltzer,P.S. (2007) GEOquery: A bridge between the Gene ExpressionOmnibus (GEO) and BioConductor. Bioinformatics, 23, 1846–1847.Shannon,P. et al. (2003) Cytoscape: a software environment for integrated models of biomolecular interaction networks. Genome Res., 13, 2498–2504.Shannon,P.T. et al. (2013) RCytoscape: tools for exploratory network analysis. BMC Bioinformatics, 14, 217.S.Avey et al.Sirota,M. et al. (2011) Discovery and preclinical validation of drug indicationsusing compendia of public gene expression data. Sci. Transl. Med., 3, 96ra77.Subramanian,A. et al. (2005) Gene set enrichment analysis: a knowledgebased approach for interpreting genome-wide expression proﬁles. Proc.Natl. Acad. Sci. USA, 102, 15545–15550.Sun,H. and Wang,S. (2012) Penalized logistic regression for high-dimensionalDNA methylation data with case-control studies. Bioinformatics, 28,1368–1375.Thakar,J. et al. (2015) Aging-dependent alterations in gene expression and amitochondrial signature of responsiveness to human inﬂuenza vaccination.Aging, 7, 38–52.Tibshirani,R. (1996) Regression selection and shrinkage via the Lasso. J. RStat Soc B, 58, 267–288.Tsang,J.S. et al. (2014) Global analyses of human immune variation revealbaseline predictors of postvaccination responses. Cell, 157, 499–513.Warsow,G. et al. (2010) ExprEssence–revealing the essence of differential experimental data in the context of an interaction/regulation net-work. BMCSyst. Biol., 4, 164.Zhang,J.D. and Wiemann,S. (2009) KEGGgraph: A graph approach to KEGGPATHWAY in R and bioconductor. Bioinformatics, 25, 1470–1471.Zou,H. and Hastie,T. (2005) Regularization and Variable Selection via theElastic Net. J. R Stat. Soc. B (Stat. Methodol.), 67, 301–320.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i208/3953975by gueston 07 January 2018
5028881993002	PMID28881993	5028881993	https://watermark.silverchair.com/btx259.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881993.main.pdf	Bioinformatics, 33, 2017, i369–i378doi: 10.1093/bioinformatics/btx259ISMB/ECCB 2017Genomes as documents of evolutionary history:a probabilistic macrosynteny model for thereconstruction of ancestral genomesYoichiro Nakatani* and Aoife McLysaght*Department of Genetics, Smurﬁt Institute of Genetics, Trinity College Dublin, University of Dublin, Dublin 2, Ireland*To whom correspondence should be addressed.AbstractMotivation: It has been argued that whole-genome duplication (WGD) exerted a profound inﬂuence on the course of evolution. For the purpose of fully understanding the impact of WGD, severalformal algorithms have been developed for reconstructing pre-WGD gene order in yeast and plant.However, to the best of our knowledge, those algorithms have never been successfully applied toWGD events in teleost and vertebrate, impeded by extensive gene shufﬂing and gene losses.Results: Here, we present a probabilistic model of macrosynteny (i.e. conserved linkage orchromosome-scale distribution of orthologs), develop a variational Bayes algorithm for inferringthe structure of pre-WGD genomes, and study estimation accuracy by simulation. Then, by applying the method to the teleost WGD, we demonstrate effectiveness of the algorithm in a situationwhere gene-order reconstruction algorithms perform relatively poorly due to a high rate of rearrangement and extensive gene losses. Our high-resolution reconstruction reveals previouslyoverlooked small-scale rearrangements, necessitating a revision to previous views on genomestructure evolution in teleost and vertebrate.Conclusions: We have reconstructed the structure of a pre-WGD genome by employing a variational Bayes approach that was originally developed for inferring topics from millions of textdocuments. Interestingly, comparison of the macrosynteny and topic model algorithms suggeststhat macrosynteny can be regarded as documents on ancestral genome structure. From this perspective, the present study would seem to provide a textbook example of the prevalent metaphorthat genomes are documents of evolutionary history.Availability and implementation: The analysis data are available for download at http://www.gen.tcd.ie/molevol/supp_data/MacrosyntenyTGD.zip, and the software written in Java is available uponrequest.Contact: yoichiro.nakatani@tcd.ie or aoife.mclysaght@tcd.ieSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionIt has been proposed that whole-genome duplication (WGD) had alarge impact on the course of evolution over a long evolutionarytimescale in the following ways: WGD provided raw genetic material for creating new gene functions (Ohno, 1970), and was associated with developmental innovations in vertebrate (Holland, 1998);it also facilitated speciation through hybrid incompatibility causedby reciprocal gene losses (Scannell et al., 2006), and increased thechance of surviving mass extinction events (Van de Peer et al.,2009). In addition, it has been suggested that ancient WGD events,occurring more than 500 million years ago, still exert profound influence on the present-day human genome through retained ohnologs (i.e. paralogs created simultaneously by WGD (Wolfe, 2000)):specifically, ohnologs shape the landscape of copy-number variations among human populations (Makino et al., 2013), and duplications/deletions of ohnologs are associated with severe humangenetic diseases (Makino and McLysaght, 2010; McLysaght et al.,2014; Rice and McLysaght, 2017).In order to fully understand the impact of WGD, we needto construct a comprehensive catalog of ohnologs, which requiresCV The Author 2017. Published by Oxford University Press.i369This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018i370in-depth synteny analyses (e.g. gene order conservation, genomerearrangement, etc.) and inference of genome structures before andafter WGD events. Several formal algorithms have been developedfor reconstructing gene order, gene adjacency and contigous ancestral regions in pre-WGD ancestral genomes, and applied to WGDevents in yeasts and plants (El-Mabrouk et al., 1998; El-Mabroukand Sankoff, 2003; Gagnon et al., 2012; Gavranovi  et al., 2011;cGordon et al., 2009; Jahn et al., 2012; Sankoff et al., 2007; Zhenget al., 2008; Zheng and Sankoff, 2013; see El-Mabrouk andSankoff, 2012 for review). However, to the best of our knowledge,those algorithms that target WGDs in yeasts and plants have neverbeen successfully applied to teleost and vertebrate genomes (seeMuffato and Roest Crollius, 2008 for review; see alsoSupplementary Material Section S7.1). This limitation is presumably attributable to difficulties arising from extremely ancient occurrence of the teleost and vertebrate WGDs compared with theyeast and plant WGDs (Box 1 in Van de Peer et al., 2009). In particular, high rates of gene loss and genome rearrangement after ancient WGDs impede reconstruction by those algorithms relying onstrong conservation of gene order and microsynteny (i.e. conservation of gene clusters or gene content in local chromosomalregions).Instead of employing those algorithms, several previous studieshave reconstructed ancestral teleost and vertebrate genomes by combining synteny blocks into ancestral linkage groups without determining ancestral gene order (Jaillon et al., 2004; Kasahara et al.,2007; Muffato, 2010; Nakatani et al., 2007; Putnam et al., 2007;Putnam et al., 2008). The fundamental idea underlying these studiesis that traces of the pre-WGD genome architecture still remain inpresent-day genomes as macrosynteny, or conserved chromosomescale distribution of orthologs, even after ancestral gene order is lostor shuffled extensively (Jaillon et al., 2004). We formulated this ideainto a rigorous probability model. We then found that the macrosynteny model is similar in terms of abstract probability modelstructure (and equivalent in a special case) to a model used in document analysis (Blei et al., 2003; Blei, 2012), and thus optimal solutions can be computed by using the same approach with necessarymodifications.By developing a variational Bayes algorithm for the macrosynteny model, we have resolved several drawbacks of the previousmacrosynteny-based studies. First, previous methods were practicalbut described critically as ad hoc and lacking a formal framework(Ouangraoua et al., 2009; Ouangraoua et al., 2011). Second, previous studies lacked explicit quantification of reconstruction confidence (Muffato and Roest Crollius, 2008). Third, a large fraction ofmodern genomes was excluded from previous reconstructions inorder to avoid regions with ambiguous synteny (Jaillon et al., 2004;Kasahara et al., 2007).Consequently, our high-resolution reconstruction revealed previously overlooked small-scale rearrangements, necessitating a revision to previous views on genome evolution in teleost andvertebrates. Specifically, it has been argued that teleost lineages hadremarkably low rates of structural change for a long evolutionarytime after the teleost WGD (TGD) (Jaillon et al., 2004; Kasaharaet al., 2007), while several early vertebrate lineages underwent massive structural changes in a short evolutionary time (Nakatani et al.,2007). Our reconstruction refines this view by showing that (1)some chromosomes accumulated small-scale inter-chromosomal rearrangements even in slowly evolving teleost genomes and (2) bycontrast, some chromosomes might have experienced exceptionallystrong structural constraints, preserving ancestral vertebrate linkageeven in rapidly changing genomes.Y.Nakatani and A.McLysaght2 Model2.1 Probabilistic macrosynteny model involving theteleost WGDFirst, we present a hypothetical scenario of teleost genome evolutionwith four pre-TGD chromosomes (Fig. 1A and B), showing how genome rearrangements might have disrupted the original 1-to-2 correspondence between pre- and post-TGD chromosomes (Fig. 1A). (Seefigure legend for more detail.) Following genome rearrangements, nonTGD chromosomes are syntenic to more than two post-TGD chromosomes, as shown in the plots of orthologs (Fig. 1C) between non-TGDchromosomes (x-axis) and post-TGD chromosomes (y-axis).Nevertheless, non-TGD segments deriving from individual pre-TGDchromosomes (i.e. blocks painted in the same color) still retain distinctortholog distributions over the modern post-TGD chromosomes,which enabled reconstruction of the pre-TGD genome structure.Intuitively, our aim is to paint the present-day genomes as in Figure1B given non-TGD segments (Fig. 1E) and ortholog relationships (Fig.1C). Although the model may seem simplistic, this macroscopic view ofsynteny conservation recapitulates the essential idea underlying the previous studies (Jaillon et al., 2004; Kasahara et al., 2007; Nakatani et al.,2007; Putnam et al., 2007; Putnam et al., 2008), and enables Bayesianinference of the pre-TGD genome structure.Before presenting the inference algorithm, we give the formaldefinition of the macrosynteny model below. Symbols used in themodel are illustrated in Figure 1D–H. The analysis constants shownbelow are dependent on observed data and are fixed throughout theanalysis.KTCtDðtÞSGs: number of pre-WGD chromosomes: number of post-WGD species: number of chromosomes in species t (t ¼ 1; . . . ; T): maximum number of co-orthologs in species t: number of segments in non-WGD species: number of genes in non-WGD segment s (s ¼ 1; . . . ; S)2.2 ParametersWe assume that pre-WGD chromosome k ðk ¼ 1; . . . ; KÞ is characterized by categorical distributions CatðCt ; Vk;t Þ with a Ct-dimensional parameter vector Vk;t ¼ ðVk;t;c : c ¼ 1; . . . ; Ct Þ for each postWGD species t (Fig. 1D), and non-WGD segment s ðs ¼ 1; . . . ; SÞhas a categorical distribution CatðK; Us Þ with a K-dimensional parameter vector Us ¼ ðUs;k : k ¼ 1; . . . ; KÞ (Fig. 1F). In the Bayesiansetting, the model parameters are themselves independent randomvariables and are assigned conjugate priors; in this case, the conjugate priors are Dirichlet distributions with parameters a ¼ ða1 ; . . . ;ðtÞðtÞaK Þ and bðtÞ ¼ ðb1 ; . . . ; bCt Þ, where ak > 0 ðk ¼ 1; . . . ; KÞ andðtÞbc > 0 ðt ¼ 1; . . . ; T; c ¼ 1; . . . ; Ct Þ. Specifically, the probabilitydensity functions (pdfs) of Us and Vk;t , denoted by pUs and pVk;t respectively, are given as follows:pUs ðus Þ ¼ CKX!akk¼1CtXÀ ÁpVk;t vk;t ¼ CbðtÞcc¼1KY!À1Cðak Þk¼1!CtY    C bðtÞcc¼1KYuak À1s;k(1)k¼1!À1CtYðtÞvbc À1k;t;c(2)c¼1Ð 1 zÀ1 ÀtPPe dt forfork us;k ¼ 1 andc vk;t;c ¼ 1, where CðzÞ ¼ 0 tz > 0 is the (complete) gamma function.Here and subsequently, we denote by U and V (and also X, Y,etc.) vectors of random variables (i.e., Us;k and Vk;t;c ) with all possible subscript and superscript values: e.g., U ¼ ð Us;k : s ¼ 1; . . . ;S; k ¼ 1; . . . ; K Þ. Lower case letters (e.g., us and vk;t ) denote valuesDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018Probabilistic macrosynteny modelAi371CDBEFGHFig. 1. Probabilistic macrosynteny model involving the teleost WGD. (A) The teleost WGD (TGD) doubled the pre-TGD ancestral teleost genome with four chromosomes (represented by bars painted in red, yellow, green and blue), creating two copies of the pre-TGD chromosomes. (B) The ancestral genome underwent extensive rearrangements, and thus present-day genomes are patchworks of fragments from multiple pre-TGD chromosomes. (Fragments were colored accordingto their originating pre-TGD chromosomes.) (C) Nevertheless, the pre-TGD genome structure can still be traced through the distribution of orthologs among present-day genomes: Speciﬁcally, non-TGD segments (e.g. human chromosome segments) derived from the same pre-TGD chromosomes have similar orthologdistributions over post-TGD chromosomes. (A dot represents a pair of orthologous genes: the x-axis shows the position of the non-TGD ortholog, and the y-axisshows the chromosome in which the post-TGD ortholog is located. The background regions were painted according to the non-TGD chromosomes arrangedalong the x-axis). (D) In the macrosynteny model, each pre-TGD chromosome is characterized by a distinct distribution of orthologs over the post-TGD chromosomes. Bar charts indicate the proportion of genes moved from a pre-TGD chromosome (k ¼ 1; . . . ; 4) to the post-TGD chromosomes shown on the y-axis. (E)Synteny breakpoints (shown as boundaries of colored fragments) in the non-TGD genome can be identiﬁed by sequence segmentation algorithms. Although themajority of the boundaries are clear and can be identiﬁed accurately (s ¼ 3 and 14), some segmentation errors are inevitable especially in regions where intensivelocal rearrangements have occurred (s ¼ S). We thus assume that a segment might be a mixture of genes from multiple pre-TGD chromosomes. (F) This assumption can be best represented by a mixture distribution assigned to each segment: In the generative model of macrosynteny, each segment (s) is associated with acategorical distribution (CatðK ; Us Þ) over the K (¼ 4 in this ﬁgure) pre-TGD chromosomes. (G) The mixture distribution for each segment (s) generates orthologdistribution as follows. For each gene (g), one of the pre-TGD chromosome (Xs;g ) is drawn from the categorical distribution (black arrows), and then, its orthologst ;dare distributed (colored downward-arrows) to one of the post-TGD chromosomes ðYs;g Þ following the ortholog distribution (CatðCt ; Vt;k Þ) shown in D. The color oft;da downward arrow indicates which distribution in D has been chosen to draw orthologs Ys;g . (H) As a result, the ﬁrst gene (g ¼ 1) in segment s ¼ 14 has two1;11;D ðtÞorthologs in post-TGD species t ¼ 1: one ortholog on chr6 ðY14;1 ¼ 6Þ and the other on chr7 ðY14;1 ¼ 7Þ. In post-TGD species t ¼ T , the gene has one ortholog onT ;1T ;D ðtÞt;dchr7 ðY14;1 ¼ 7Þ, while the other ortholog has been deleted and thus Y14;1 is not deﬁned. Finally, by plotting all Ys;g , we obtain the ortholog distributions for s ¼ 1and T shown in CDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018i372Y.Nakatani and A.McLysaghtof corresponding random variables written in upper case letters(e.g., Us and Vk;t respectively). Then the joint pdf of H ¼ ðU; VÞ,denoted by pH , is given aspH ðhÞ ¼ pU ðuÞpV ðvÞ¼SY!pUs ðus Þs¼1K TYYÀ ÁpVk;t vk;t :(3)WGD chromosomes, segments and location of orthologs (i.e.post-WGD chromosomes), respectively. In document analyses,the topic structure has been inferred through the variationalBayes approach (Blei et al., 2003; Murphy, 2012). Thus, wedeveloped a variational Bayes algorithm for the probabilisticmacrosynteny model, whose output is an inferred pre-WGD genome structure.k¼1 t¼13 Methods2.3 Latent variablesGene g in non-WGD segment s is associated with a latent variableXs;g , which assigns the gene to one of the K pre-WGD chromosomesaccording to the segment’s categorical distribution CatðK; Us Þ: formally, random variables f Xs;g : g ¼ 1; . . . ; Gs g are conditionally independent given Us, and the probability mass function of Xs;gconditioned by Us is defined bypXs;g jUs ðkjus Þ ¼ us;k ;(4)and the probability mass function of X conditioned by U is given byS GsYYpXjU ðxjuÞ ¼pXs;g jUs ðxs;g jus Þ:(5)s¼1 g¼12.4 Observed ortholog dataSuppose that gene g in non-WGD segment s has ortholog d in postt;dWGD species t, and let Ys;g denote the post-WGD chromosome inwhich the ortholog is located. Since the number of orthologs can bedifferent for each gene g in non-WGD segment s (due to gene lossðtÞafter WGDs, incomplete genome assembly, etc.), we define Ds;g asthe number of orthologs in species t, which is assumed to be given asan input parameter. Then for each (s, g), we assume that randomðtÞt;dvariables f Ys;g : t ¼ 1; . . . ; T; d ¼ 1; . . . ; Ds;g g are conditionally int;ddependent given Xs;g and f VXs;g ;t : t ¼ 1; . . . ; T g, and that Ys;g is distributed over Ct chromosomes following categorical distributionCatðCt ; VXs;g ;t Þ. Specifically, the conditional probability mass funct;dtion of Ys;g is defined bypY t;d jXs;g ;VXs;gs;g ;tðcjk; vk;t Þ ¼ vk;t;cs;gTaken together, the joint probability density function of H, Xand Y is given bypH;X;Y ðh; x; yÞ ¼ pH ðhÞpXjU ðxjuÞpYjX;V ðyjx; vÞ:Now we wish to calculate pH;XjY ðh; xjyÞ, the posterior probabilitydensity function (pdf) of the model parameters H ¼ ðU; VÞ and latent variables X conditioned by the observed ortholog data Y. Directcomputation of the posterior pdf is infeasible since it involves ahigh-dimensional integration over all parameters and latent variables; therefore, in variational Bayes approaches, it is approximatedby another pdf, q, that is chosen from a family of tractable, positivepdfs. A commonly employed criterion for approximating the posterior is to minimize the Kullback–Leibler divergence from q to pH;XjY ,which is defined byhib bb bKLðqjjpH;XjY Þ ¼ E log ðqðH; XÞ=pH;XjY ðH; XjyÞÞ ;(8)bbwhere E denotes expectation and H and X are random variablesthat have q as their joint pdf (see Supplementary Material SectionS7.2.1 for calculation of expectations). It is easily seen that the KLdivergence can be transformed as KLðqjjpH;XjY Þ ¼ log ðpY ðyÞÞÀFðqÞ, where – F is known as the free energy in statistical physics andis defined byhib bb bFðqÞ ¼ E log ðpH;X;Y ðH; X; yÞ=qðH; XÞÞ :(9)Since log ðpY ðyÞÞ is constant with respect to q, the KL divergencecan be minimized by choosing q that maximizes FðqÞ. To this end,we employ the following algorithm, which iteratively updates thedistributions over the model parameters and latent variables one byone until FðqÞ converges to a local maximum.(6)and the joint pdf of Y, denoted by pYjX;V , is given by the product ofpY t;d jXs;g ;VX ;t with respect to all s, g, t and d.s;g3.1 Inference of the structure of pre-WGD genomes as amodel-based optimization problem3.2 Variational Bayes inference algorithmIn the variational Bayes expectation-maximization (VBEM) algorithm, the posterior pH;XjY is approximated by qb b that isH ;Xfactorized asqb b ðh; xÞ ¼ q b ðuÞq b ðvÞq b ðxÞ;H ;XUVX(7)Finally, we define shorthand notation for ortholog counts. Wes;gdenote by nt;c the number of genes that are orthologous to gene g insegment s and that are located in chromosome c of post-WGDPDðtÞs;gs;gspecies t: that is, nt;c ¼ d¼1 dc;yt;d , where di;j is the Kroneckers;gdelta (i.e., di;j ¼ 1 if i ¼ j and di;j ¼ 0 if i 6¼ j).b bbwhere q b ; q b and q b are marginal pdfs of U; V and X respectively.UVXThen, through the standard VBEM procedure (Blei et al., 2003;Murphy, 2012), we obtain the following analytical update formulas(See Supplementary Material Section S7.3 for details), which iteratively optimize qb b .H ;XFirst, the marginal pdfs q b and q b that give a local maximumUs2.5 Relationship to the probabilistic topic modelThe model described here is similar to a model referred to as latent Dirichlet allocation or probabilistic topic model (Blei et al.,2003; Blei, 2012; Murphy, 2012), which has been developed foranalyzing millions of text documents and organizing them into asmall number of topics. Indeed, the two models are equivalent inðtÞprobabilistic structure when T ¼ 1 and Ds;g ¼ 1 for all s, g and t:in this case, topics, documents and words correspond to pre-(10)V k;tof Fðqb b Þ are shown to be Dirichlet with the followingH ;Xparameters:Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018b ðsÞ ¼ ak þakb ðk;tÞ ¼ bðtÞ þbccGsXg¼1q b ðkÞ;S GsXXs¼1 g¼1X s;gq b ðkÞns;g :t;cX s;g(11)(12)Probabilistic macrosynteny modeli373ABCt;dSecond, writing as k ¼ xs;g and c ¼ ys;g , we definet;dBs;g ðkÞ ¼ w0b ðk;tÞbc À w0CtX!b ðk;tÞ ;bi(13)i¼1nþ1dwhere wn ðxÞ ¼ dxnþ1 log ðCðxÞÞ is the polygamma function. Then thefollowing update maximizes Fðqb b Þ:H ;XðtÞðsÞlog ðq b ðkÞÞ ¼ w0 ðb k Þ þaX s;gT Ds;gXXt;dBs;g ðkÞ þ C;(14)t¼1 d¼1where C is a constant that cancels when we normalize q b ðkÞ soX s;gPthat K q b ðkÞ ¼ 1.k¼1X s;gFinally, using the update formulas above, we obtain the VBEMalgorithm for the macrosynteny model as Algorithm 1. TheNewton–Raphson method for optimizing hyper-parameters, parameter initialization and convergence criteria are described inSupplementary Material Sections S7.4, S7.5 and S7.6, respectively.4 Results4.1 Simulation of the macrosynteny modelIn order to evaluate the estimation accuracy, we generated syntheticortholog data from the generative model described in Sections 2.2 to2.4 with K ¼ 13; T ¼ 4; C1 ¼ 24; C2 ¼ 21; C3 ¼ 21 and C4 ¼ 25.ðtÞFirst, we set ak ¼ a (k ¼ 1; . . . ; K) and bc ¼ b (t ¼ 1; . . . ; T andc ¼ 1; . . . ; Ct ), where either a or b was fixed to 0.1 and the other parameter was varied from 0.01 to 1. Second, we generated S ¼ 1000segments and chose the number of genes for each segment from theexponential distribution with parameter k ¼ 0:0078, which approximates the actual distribution of human segment length. Third, wechose values of Us (s ¼ 1; . . . ; S) and Vk;t (k ¼ 1; . . . ; K andt ¼ 1; . . . ; T) randomly from their prior distributions: namely, thesymmetric Dirichlet distributions with parameters a and b, respectively. Forth, we generated orthologs from the categorical distributionsFig. 2. High estimation accuracy of the macrosynteny model algorithm. (A)Simulations showed strong correlation between true and estimated valuesb(i.e. Us;ks and E½U s;ks   with ks ¼ argmaxi Us;i ). A parameter value (either a orb) was varied from 0.01 to 1 (x-axis) and Pearson’s correlation coefﬁcient isshown on the y-axis with 25% and 75% quantiles indicated by boxes.Correlation was strong in general, although the correlation coefﬁcient wasrelatively low for a ! 0:5. (B) The estimation error was larger for more rearranged genomes. The mean absolute estimation error (y-axis) was small especially for small a or b values (x-axis), which correspond to low rates ofrearrangement in non- and post-WGD genomes, respectively. (C) The association with pre-WGD chromosomes was accurately inferred for most nonWGD segments. The y-axis shows the proportion of non-WGD genes that arein segments with correct argmaxi Us;i inferenceUs and Vk;t as shown in Figure 1H, where two post-WGD orthologswere generated for each non-WGD gene with probability 0.1. Afterbapplying the macrosynteny model algorithm, estimates E½U s;k   for allnon-TGD segments were compared with the true values Us;k , where kis chosen for each s as k ¼ argmaxi Us;i . Then, estimation accuracywas evaluated in terms of three quantities: (A) Pearson’s correlationcoefficient between simulated and estimated values, (B) mean absolutePSbestimation error defined byjUs;k À E½U s;k  j=S with kss¼1ss¼ argmaxi Us;i and (C) proportion of non-WGD genes that are in segments with correct argmax inference (i.e. segments s satisfyingbargmaxi E½U s;i   ¼ argmaxi Us;i ). This simulation procedure was repeated 20 times for each set of parameter values.Overall, the simulation result (Fig. 2) shows that the algorithmgave accurate estimates. In addition, we confirmed factors that affect the inference accuracy. First, shorter non-WGD segments tendto have larger estimation errors. Specifically, segment length (i.e.number of genes in the segment) negatively correlated with estimabtion error jUs;ks À E½U s;ks  j with ks ¼ argmaxi Us;i (Spearman’s rankcorrelation coefficient was –0.51 when a ¼ 0:1 and b ¼ 0:1).Second, accuracy is dependent on genome rearrangements in bothnon- and post-WGD lineages. Indeed, by increasing a and b parameter values, which represent degrees of genome shuffling in non- andpost-WGD genomes respectively, we observed a decrease in the correlation coefficient and an increase in the mean absolute estimationerror (Fig. 2A and B).Importantly, the influence of these factors seems to be minimalin the actual teleost genome analysis for three reasons: short segments were not used in the initialization step (see SupplementaryDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018i374Material Section S7.5); most synteny breakpoints were detected accurately by using a Bayesian segmentation algorithm (see Section4.2.1); and post-TGD genomes are known to have had low rates ofinter-chromosomal rearrangement (Jaillon et al., 2004; Kasaharaet al., 2007). In fact, in our teleost genome analysis, the average akðtÞ  ðtÞand bc values were estimated to be   k ¼ 0:066 and b c ¼ 0:075a(averaged with respect to all values of k, t and c) using theNewton–Raphson method described in Supplementary MaterialSection S7.4. Figure 2 shows high estimation accuracy for theseparameter values.4.2 Analysis of the teleost WGD4.2.1 Reconstruction of the pre-TGD genome structureWe obtained vertebrate orthologs and gene trees from Ensembl (release 75) and removed small-scale duplications to avoid ambiguityin subsequent synteny analyses. In other words, duplications wereretained only if they were annotated as Clupeocephala (indicatingthat the duplication events occurred to the last common ancestor ofteleost species in Ensembl), and the other duplications wereremoved. Then we compared four teleosts (medaka, stickleback,Tetraodon and zebrafish), spotted gar, four non-mammalian amniotes (chicken, zebra finch, turkey and anole lizard), and four mammals (opossum, dog, mouse and human) to define conserved syntenyblocks in the human, mouse, dog, opossum, anole lizard, chickenand spotted gar genomes. Specifically, we employed a Bayesian segmentation model (Liu and Lawrence, 1999) with a dynamic programming algorithm for computing the maximum a posteriorisegmentation (Auger and Lawrence, 1989). In short, the segmentation algorithm identifies synteny breakpoints and divides chromosomes, represented as sequences of genes, into chromosomalsegments each of which is characterized by a homogeneous orthologdistribution in the genomes under comparison. The segmentation algorithm was applied twice to the human genome: first, for identifying boundaries of teleost synteny blocks (i.e. blocks of doublyconserved synteny, Jaillon et al., 2004) by using the four teleost genomes; second, for identifying boundaries of amniote synteny blocksby using the four non-mammalian amniote genomes. Then, by merging the two sets of boundaries, the human genome was partitionedinto 152 segments. In the same way, the mouse, dog, opossum, anolelizard, chicken and spotted gar genomes were partitioned into 252,209, 159, 77, 71 and 78 segments, respectively. This constructionallows us to assume that the resulting segments have been conservedand mostly free from large-scale inter-chromosomal rearrangementsamong the non-TGD species and pre-TGD ancestor.These segments and Ensembl orthologs were used as the inputdata for the macrosynteny model algorithm. We present the result inFigure 3, which was obtained by the macrosynteny model algorithmwith parameters ak ¼ 0:1 (k ¼ 1; . . . ; K), bðtÞ ¼ 0:1 (t ¼ 1; . . . ; Tcand c ¼ 1; . . . ; Ct ), K ¼ 13, T ¼ 4, DðtÞ ¼ 2 for all t, and L ¼ 10,where L is explained in Supplementary Material Section S7.5.Visualization scheme is described in the figure legend.4.2.2 Number of pre-TGD chromosomesThe number of pre-TGD chromosomes (denoted by K in the macrosynteny model) has been estimated as K ¼ 12 (Jaillon et al., 2004)or K ¼ 13 (Kasahara et al., 2007), but the exact number remains unknown. We reconstructed the pre-TGD chromosomes for K ¼ 10;. . . ; 16 and compared with the K ¼ 13 case shown in Figure 3 to seehow reconstructions are affected by the choice of K.In order to decide the true number of chromosomes, it is important to discern the difference between ortholog distributionsY.Nakatani and A.McLysaghtproduced by chromosome fusion and fission. For example, in Figure1 the post-TGD species t ¼ 1 has a fusion chromosome (chr4, fusionof yellow and green chromosomes). In this case, we can see that yellow non-TGD segments share orthologs with chr3 and chr4 but notwith chr5, while green non-TGD segments share orthologs withchr4 and chr5 but not with chr3 (except for a small number of translocated genes). On the other hand, chr7 and chr8 (blue chromosomes) were created by fission, and consequently some bluenon-TGD segments are syntenic to the three blue post-TGDchromosomes (chr6, chr7 and chr8). Therefore, chromosome fusionand fission can be distinguished by the existence of many non-TGDsegments that are syntenic to three post-TGD chromosomes. Thisdistinction can be obscured by inter-chromosomal rearrangements,and thus we focused on ortholog distribution in medaka with a lowrate of inter-chromosomal rearrangement (Kasahara et al., 2007).When we increased K from K11 to K ! 12, a pre-TGDchromosome was divided into two chromosomes (namely, chr10and chr11 in Figure 3 with 57 and 38 non-TGD segments, respectively). Investigation of medaka ortholog distribution showed thatthese segments are syntenic to either Ola3-Ola6 pair or Ola23-Ola6pair of medaka chromosomes (Fig. 4A), which is expected under thefusion scenario (i.e. the three medaka chromosomes derive from twopre-TGD chromosomes and Ola6 is a fusion chromosome). This observation suggests that the true number of chromosomes in the preTGD genome is more than 11 and setting K11 caused the falsemerger of two pre-TGD chromosomes (chr10 and chr11 in Fig. 3)into one in the reconstruction. Similarly, when we increased thenumber from K13 to K ! 14, a pre-TGD chromosome (namely,chr13 in Fig. 3 with 159 non-TGD segments) was divided into twochromosomes. The medaka ortholog distribution (Fig. 4B) showedthat many segments are syntenic to both Ola3 and Ola23 (in addition to Ola17), which is expected under the fission scenario (i.e. thethree medaka chromosomes derive from a single pre-TGD chromosome, where Ola4 and Ola20 were separated by fission). Thus, it islikely that the reconstructions with K ! 14 have falsely inferred twopre-TGD chromosomes where there really should be one. Taken together, Figure 4 supports the reconstructions with K ¼ 12 and 13.The difference between K ¼ 12 and K ¼ 13 reconstructions involves several post-TGD chromosomes (i.e. Ola1, Ola18, Ola10 andOla14), suggesting a complicated rearrangement history after TGD.Further comparison with previously proposed rearrangement scenarios(Jaillon et al., 2004; Kasahara et al., 2007) showed that the model proposed by Jaillon et al. is not consistent with our K ¼ 12 reconstruction(e.g. assignment of Tetraodon chromosomes Tni6 and Tni17 to preTGD chromosomes differs), while the model proposed by Kasaharaet al. largely agrees with our K ¼ 13 reconstruction. In addition, Ola1and Ola14 have orthologs in non-overlapping regions in the non-TGDgenomes (e.g. see Supplementary Fig. S8), supporting the inference thatthe two chromosomes derive from two distinct pre-TGD chromosomesas in the K ¼ 13 reconstruction. Considering these observations, wepresented the reconstruction with K ¼ 13 in the main text and an alternative reconstruction with K ¼ 12 in the supplementary document.These reconstructions may be validated by comparison with genomes ofbasally diverging post-TGD species such as African butterfly fish andarowana. For example, Ola4 and Ola20 are syntenic to the samechromosome (chr12) in golden arowana (Bian et al., 2016), suggestingthat Ola4 and Ola20 were indeed created by chromosome fission (asdiscussed in Fig. 4B) after divergence from the arowana lineage.4.2.3 Evolution of genome structure in teleostFigure 3 shows that the inferred structure of the pre-TGD genome islargely consistent with the previous reconstructions presented inDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018Fig. 3. Reconstruction of the pre-TGD genome structure comprising of 13 chromosomes. We assigned distinct colors to the pre-TGD chromosomes and painted the post- and non-TGD chromosomes accordingly. Speciﬁcally,bthe non-TGD segments were painted with the color of the pre-TGD chromosome that had the maximum reconstruction conﬁdence score (i.e., argmaxk E½U s;k  ). To visualize local reconstruction conﬁdence, lines were drawn byweighted window averaging of qb ðkÞ for genes in the surrounding W Mb regions (W ¼ 1:5; 1:36; 1:2; 0:5 and 0.48 for human, mouse, dog, chicken and spotted gar, respectively, reﬂecting their genome size difference). LinesX s;gare visible only when there exists at least one gene in the window, and local conﬁdence scores are shown in the range from 0.3 to 1. With regard to the post-TGD genomes, lines were drawn by weighted window averaging ofbgene-wise reconstruction conﬁdence values, where gene-wise reconstruction conﬁdence of a post-TGD gene was calculated by averaging E½U s;k   of all segments that contain non-TGD orthologs since reconstruction probability was not assigned to post-WGD genes within the model. The window sizes were 0.44, 0.23, 0.18 and 0.7 Mb for medaka, stickleback, Tetraodon and zebraﬁsh, respectively. Then, post-TGD chromosomes were painted withthe color associated with the pre-TGD chromosome that had the maximum window average value. Chromosome length is proportionate within each species, but do not scale between species. In the main text, chromosomenumbers were preﬁxed by three letter codes (i.e. Ola, Tni and Hsa for medaka, Tetraodon and human, respectively)Probabilistic macrosynteny modelDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018i375i376AY.Nakatani and A.McLysaghtBFig. 4. Two contrasting patterns were observed for ortholog distributionsover three medaka chromosomes produced by fusion (A) or ﬁssion (B). Foreach non-TGD segment, we counted the numbers of orthologs in the threemedaka chromosomes shown in the x, y and z axes (Ola stands for Oryziaslatipes chromosome). The three-dimensional count vector was projectedonto the sphere to show the proportion of orthologs among the threechromosomes. (A) Ortholog distribution consistent with the fusion scenario.Non-TGD segments assigned to pre-TGD chromosomes chr10 and chr11were plotted in light blue and pink, respectively. Ola6 is likely to have beenproduced by fusion of two chromosomes, one duplicated from chr10 and theother from chr11. Consequently, any non-TGD segments syntenic to Ola6 arealso syntenic to either Ola3 or Ola23, but no segment is syntenic to both ofOla3 and Ola23. (B) Ortholog distribution consistent with the ﬁssion scenario.Non-TGD segments assigned to pre-TGD chromosome chr13 were plotted.Ola4 and Ola20 are likely to have been created by ﬁssion of a chromosomethat was a duplicate of Ola17. Since the three medaka chromosomes arederived from a single pre-TGD chromosome, many non-TGD segments aresyntenic to both Ola4 and Ola20 in addition to Ola17(Jaillon et al., 2004; Kasahara et al., 2007), confirming their findingson teleost genome evolution: (1) after the TGD, the rate of interchromosomal rearrangement has been relatively low in the teleostgenomes compared with the mammalian genomes; (2) in medaka,stickleback and Tetraodon, only a small number of chromosome fusion or fission events have occurred since the TGD, and the otherchromosomes have retained one-to-one correspondence; and (3) bycontrast, many inter-chromosomal rearrangements have occurred inthe zebrafish lineage, presumably because of increased activity oftransposable elements as argued in (Jaillon et al., 2004).The major difference of our reconstruction from previous onesis that we have increased the coverage of the genomes and assignedconfidence scores along chromosomal regions for representing uncertainty in reconstruction, whereas previous studies had excludeda large part of the human genome from their reconstructions toavoid regions with ambiguous synteny (for example, Jaillon et al.excluded 20 blocks in the human genome. See Fig. 6 in Jaillonet al., 2004). In many cases, previously excluded regions were divided into several smaller blocks with varying degree of confidencein our reconstruction (see Fig. 6 in Jaillon et al., 2004, blocks W1,Z1, Z3, etc).As a result, our reconstruction shows previously unidentifiedsmall-scale rearrangements occurring after the TGD. For instance,Ola18 and Ola22 were considered in (Kasahara et al., 2007) tohave been free of large-scale inter-chromosomal rearrangementssince the TGD, except for a small translocation from Ola18 toOla10; by contrast, in Figure 3, Ola18 (purple) and Ola22 (lightgreen) are the most rearranged chromosomes in medaka, havingexperienced several small-scale rearrangements as indicated byshort segments painted in several different colors. Thus, eventhough the medaka genome in general has been characterized by alow rate of inter-chromosomal rearrangement after the TGD(Kasahara et al., 2007), our reconstruction indicates that somechromosomes experienced substantially higher rates of rearrangement, which suggests relatively weak structural constraints or localexpansion of repetitive sequences.4.2.4 Evolution of genome structure in vertebrateComparison of the pre- and non-TGD genomes in Figure 3 revealedmacrosynteny evolution before the TGD: (1) large chromosomes inchicken and spotted gar tend to consist of multiple segments paintedin different combinations of colors, suggesting that these chromosomes were formed by inter-chromosomal rearrangements (possiblyby fusion of smaller chromosomes) in the individual lineages; (2) bycontrast, many of chicken microchromosomes (i.e. smaller chromosomes such as Gga9 to Gga32) have retained one-to-one correspondence to the spotted gar chromosomes (Braasch et al., 2016; see alsoSupplementary Fig. S8), but none of them were retained as singlechromosomes in the pre-TGD genome, indicating a large number ofchromosomes in the common ancestor and fusion events in the preTGD lineage; and (3) all pre-TGD chromosomes except for chr11(pink) are divided into smaller parts and distributed to multiplechromosomes in chicken and spotted gar. These observations corroborate a previously proposed model that the bony-vertebrate ancestor had a large number of chromosomes and intensivechromosome fusion events shaped the pre-TGD genome structure(Nakatani et al., 2007). Intriguingly, one of $40 ancestral chromosomes created by the vertebrate WGDs (Nakatani et al., 2007) hadbeen retained in the pre-TGD genome as a single chromosome(namely, chr11 painted in pink), surviving the period when the otherancestral chromosomes underwent fusion to form pre-TGD chromosomes. This linkage conservation might suggest that pre-TGD chr11had been under strong structural constraint.Taken together, our reconstruction with higher resolution helpsrefine previous models of genome evolution in teleost and vertebrate. In particular, it has been highlighted that some lineages hadremarkably slow rates of large-scale inter-chromosomal rearrangement for a long evolutionary time, while other lineages had intensivechromosome fusions in a short evolutionary time (Jaillon et al.,2004; Kasahara et al., 2007; Nakatani et al., 2007). Our reconstruction refines this view by showing that (1) some chromosomes mighthave accumulated small-scale inter-chromosomal rearrangementseven in a slowly evolving genome and (2) by contrast, some mighthave experienced exceptionally strong structural constraints and retained ancestral linkage even in a rapidly changing genome.5 DiscussionWe considered that reconstructions computed independently by previous algorithms might be useful in validating and also improvingour reconstruction. In particular, we presumed that previous geneorder reconstruction algorithms can infer reliable fragments ofpre-TGD chromosomes; then (1) those fragments can be used forvalidating our macrosynteny-based reconstruction, or alternatively(2) those fragments can be treated as input to the macrosyntenymodel algorithm for obtaining an improved reconstruction of thepre-TGD genome structure.For this purpose, we have chosen GapAdj (Gagnon et al., 2012),a sophisticated algorithm designed for reconstructing ancestral genomes in yeasts and plants. The major advantage of GapAdj (compared with other gene-order reconstruction algorithms) is that ittakes account of gene duplications by WGDs and subsequent genedeletions, which are essential for the analysis of the TGD. TheGapAdj algorithm reconstructs contiguous ancestral regions (CARs)based on gene adjacencies in modern genomes, where a gene is represented as two extremities (i.e., start and end) and a pair of genes isregarded as adjacent if there are less than or equal to a given number(denoted by d) of gene extremities between the pair. With a smallDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018Probabilistic macrosynteny modelABFig. 5. Gene-order reconstruction of the pre-TGD genome by GapAdj. (A) Thenumber of pre-TGD CARs remained larger than the expected chromosomenumber even with less stringent large d values. The x-axis shows the value ofd, and the y-axis shows the number of CARs reconstructed by GapAdj. Theplot shows three data series: ﬁrst, we analyzed gene order in the human, medaka and Tetraodon genomes (  ); second, we added mouse, dog, chicken,spotted gar, zebraﬁsh and stickleback (•); third, we analyzed only the spottedgar genome and the four teleost genomes (þ). In all cases, the number ofCARs was too large, indicating that reconstruction of entire pre-TGD chromosomes was not feasible presumably due to a weak gene-order conservation.(B) Ortholog distribution over medaka chromosomes suggested that largeCARs were likely to be unreliable. CARs inferred by GapAdj with d ¼ 1; 20;and 100 (data series • in panel A) were plotted, where the x-axis shows thenumber of genes in the CARs and the y-axis shows the proportion of theirorthologs located on the two most syntenic medaka chromosomes. Largey-axis values were expected if CARs were inferred accurately, because majority of their medaka orthologs should be found in two duplicated chromosomes. The top panel shows that GapAdj reconstructed only short fragmentswith d ¼ 1, indicating that the value was too stringent for teleost genome analysis. On the other hand, larger CARs were inferred by using less stringent d¼ 20 and 100 (middle panels). However, compared with large y-axis valuesfor the non-TGD segments (bottom), relatively low y-axis values for largeCARs indicate that presently available gene-order information was not sufﬁcient for accurate reconstruction of large CARsnumber of d, GapAdj produces numerous short CARs, representingfragments of ancestral chromosomes; on the other hand, with a largenumber of d, GapAdj reconstructs entire ancestral chromosomes atthe cost of reconstruction accuracy (Gagnon et al., 2012).First, we evaluated whether GapAdj is capable of reconstructingentire pre-TGD chromosomes and validating our reconstruction. Tothis end, we obtained gene order information from the human,mouse, dog, chicken, spotted gar, medaka, stickleback, Tetraodonand zebrafish genomes, and applied GapAdj with d ¼ 1; . . . ; 100.Figure 5A shows that GapAdj generated considerably larger numbers of CARs (116 CARs with the least stringent d ¼ 100) than theexpected pre-TGD chromosome number ($13), indicating thatgene-order conservation was not sufficiently strong for recoveringthe gene order along entire pre-TGD chromosomes.Next, to see if our reconstruction improves by integratingGapAdj, we applied the macrosynteny model algorithm to the 116CARs with parameters chosen to be the same as in our teleost genome analysis (Section 4.2). Assuming that the CARs were inferredaccurately, we expected that most genes in a single CAR should belong to a single pre-TGD chromosome, resulting in small ak andblarge maxk E½U s;k   estimates. However, we found the opposite tendency: first, the average of the estimated ak values was 0.154 for theCARs, which was larger than the average of 0.066 in our teleostbgenome analysis; and second, values of maxk E½U s;k   were considerably smaller for the CARs (0.51 on average) than the non-TGD segments in our teleost genome analysis (0.78 on average). Although nostatistical test was performed because of dependence of the estimates, the result indicated that the CARs were more ‘rearranged’i377than the non-TGD segments, suggesting that the CARs might nothave been reconstructed with sufficient accuracy.In accord with this argument, we observed that the CARs tend tohave orthologs in more than two medaka chromosomes. Ideally, if aCAR was reconstructed accurately, all genes in the CAR must havebeen on the same pre-TGD chromosome and consequently most oftheir medaka orthologs should be found on two duplicated chromosomes (although some genes might have moved away by rearrangement). Therefore, we quantified accuracy of a CAR in terms of theproportion of its medaka orthologs located on the two most syntenicchromosomes (i.e. the two chromosomes that have the largest andsecond largest numbers of orthologs in the CAR) as shown on they-axes in Figure 5B. The proportion was significantly lower for the116 CARs inferred by GapAdj with d ¼ 100 than for the 152 nonTGD segments in human (59% and 86% on average for the CARsand human segments, respectively; p ¼ 8:4 Â 10À16 , Mann–WhitneyU test). This indicates that gene-order conservation was not strongenough for reliable reconstruction of large CARs.In fact, our observation was consistent with the previous argument that inference accuracy of GapAdj was reduced by high ratesof rearrangement and gene loss in simulations (Gagnon et al., 2012).Taken together, the present analysis indicates that gene-order conservation is not sufficiently strong among the non- and post-TGDgenomes, making a reliable inference of pre-TGD gene order infeasible even with advanced algorithms like GapAdj (see SupplementaryMaterial Section S7.7 for confirmation of this observation usingother gene-order reconstruction methods).Although we could not improve our reconstruction at present byintegrating a gene-order reconstruction method, such a hybrid approach is potentially useful in future teleost and vertebrate genomeanalyses. Future work should also include (1) improving the methodfor detecting synteny blocks by using gene-order reconstructionmethods and comparing with many low-coverage genome sequences, (2) developing more realistic probability models of rearrangement and macrosynteny evolution and (3) applying otherdocument models and algorithms to the problem of ancestral genome reconstruction.6 ConclusionWe have developed a probabilistic macrosynteny model and inferredthe structure of the pre-TGD genome. Unlike previous studies (e.g.Jaillon et al., 2004; Kasahara et al., 2007), which had a drawbackthat their reconstructions lack a quantified level of confidence(Muffato and Roest Crollius, 2008), we have calculated reconstruction confidence scores along chromosomes of present-day genomes,representing reconstruction uncertainty due to intensive local rearrangements, incomplete genome sequencing, errors in identifyingsynteny breakpoints, errors in gene trees, etc.The major difference between our method and previous geneorder-based algorithms is that the macrosynteny model is abstractand does not directly model actual evolutionary processes: specifically, it focuses on ortholog distribution rather than conservation ofgene order or gene adjacency. This has been an essential idea underlying previous studies (Jaillon et al., 2004; Kasahara et al., 2007;Muffato, 2010; Nakatani et al., 2007; Putnam et al., 2007; Putnamet al., 2008). We have formulated the idea into a rigorous probabilistic framework (Section 2), and as a result, our method works effectively (Section 4.2) even in a situation where gene-order reconstructionalgorithms do not work reliably (Section 5) due to ancient occurrenceof WGDs and high rates of rearrangement and gene loss.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018i378Finally, the similarity between the macrosynteny and topic models (Blei et al., 2003; Blei, 2012; Murphy, 2012) suggests an interesting interpretation of conserved synteny: that is, a block of conservedsynteny can be regarded as a piece of writing about local structureof ancestral genomes. Consequently, by compiling and summarizingthose ‘documents’ using an algorithm originally developed for textanalyses, we were able to recover the ancestral genome structure asif we inferred the ‘topic structure’ underlying the macrosynteny‘documents.’ Viewed in this way, the present study provides a perfect example of the prevalent metaphor that genomes are documentsof evolutionary history (Boussau and Daubin, 2010; Crow, 1994;Kihara, 1946; Zuckerkandl and Pauling, 1965).AcknowledgementWe thank all members of the McLysaght research group for valuablediscussions.FundingThis work is supported by funding from the European Research Councilunder the European Union’s Seventh Framework Programme (FP7/2007–2013)/European Research Council grant agreement 309834.Conﬂict of Interest: none declared.ReferencesAuger,I. and Lawrence,C. (1989) Algorithms for the optimal identiﬁcation ofsegment neighborhoods. Bull. Math. Biol., 51, 39–54.Bian,C. et al. (2016) The Asian arowana (Scleropages formosus) genome provides new insights into the evolution of an early lineage of teleosts. Sci. Rep.,6, 24501.Blei,D.M. et al. (2003) Latent Dirichlet allocation. J. Mach. Learn. Res., 3,993–1022.Blei,D.M. (2012) Probabilistic topic models. Commun. ACM, 55, 77–84.Boussau,B. and Daubin,V. (2010) Genomes as documents of evolutionary history. Trends Ecol. Evol., 25, 224–232.Braasch,I. et al. (2016) The spotted gar genome illuminates vertebrate evolution and facilitates human-teleost comparisons. Nat. Genet., 48, 427–437.Crow,J.F. (1994) Hitoshi Kihara, Japan’s pioneer geneticist. Genetics, 137,891–894.El-Mabrouk,N. et al. (1998) Genome halving. In Combinatorial PatternMatching, Lect. Notes Comput. Sci., 1448, 235–250.El-Mabrouk,N. and Sankoff,D. (2003) The reconstruction of doubled genomes. SIAM J. Comput., 32, 754–792.El-Mabrouk,N. and Sankoff,D. (2012) Analysis of gene order evolution beyond single-copy genes. In: Evolutionary Genomics, Methods Mol. Biol.Humana Press, pp. 397–429.Gagnon,Y. et al. (2012) A ﬂexible ancestral genome reconstruction methodbased on gapped adjacencies. BMC Bioinform., 13, S4.Gavranovi ,H. et al. (2011) Mapping ancestral genomes with massive genecloss: a matrix sandwich problem. Bioinformatics, 27, i257–i265.Y.Nakatani and A.McLysaghtGordon,J.L. et al. (2009) Additions, losses, and rearrangements on the evolutionary route from a reconstructed ancestor to the modern Saccharomycescerevisiae Genome. PLoS Genet., 5, e1000485.Holland,P.W.H. (1998) Major transitions in animal evolution: a developmental genetic perspective. Am. Zool., 38, 829–842.Jahn,K. et al. (2012) A consolidation algorithm for genomes fractionated afterhigher order polyploidization. BMC Bioinform., 13, S8.Jaillon,O. et al. (2004) Genome duplication in the teleost ﬁsh Tetraodonnigroviridis reveals the early vertebrate proto-karyotype. Nature, 431,946–957.Kasahara,M. et al. (2007) The medaka draft genome and insights into vertebrate genome evolution. Nature, 447, 714–719.Kihara,H. (1946) Story on Wheats. Sogen-sha; Tokyo.Liu,J.S. and Lawrence,C.E. (1999) Bayesian inference on biopolymer models.Bioinformatics, 15, 38–52.Makino,T. and McLysaght,A. (2010) Ohnologs in the human genome are dosage balanced and frequently associated with disease. Proc. Natl. Acad. Sci.USA, 107, 9270–9274.Makino,T. et al. (2013) Genome-wide deserts for copy number variation invertebrates. Nat. Commun., 4, 2283.McLysaght,A. et al. (2014) Ohnologs are overrepresented in pathogenic copynumber mutations. Proc. Natl. Acad. Sci. USA, 111, 361–366.Muffato,M. (2010) Reconstruction de genomes ancestraux chez les vertebres.´´ ´PhD Thesis, Universite d’Evry-Val d’Essonne, Evry, France.´Muffato,M. and Roest Crollius,H. (2008) Paleogenomics in vertebrates, or therecovery of lost genomes from the mist of time. BioEssays, 30, 122–134.Murphy,K.P. (2012) Machine Learning: A Probabilistic Perspective. MITPress, Cambridge.Nakatani,Y. et al. (2007) Reconstruction of the vertebrate ancestral genomereveals dynamic genome reorganization in early vertebrates. Genome Res.,17, 1254–1265.Ohno,S. (1970) Evolution by Gene Duplication. Springer-Verlag, New York.Ouangraoua,A. et al. (2009) Prediction of contiguous regions in the amnioteancestral genome. Lect. Notes Comput. Sci., 5542, 173–185.Ouangraoua,A. et al. (2011) Reconstructing the architecture of the ancestralamniote genome. Bioinformatics, 27, 2664–2671.Putnam,N.H. et al. (2007) Sea anemone genome reveals ancestral eumetazoangene repertoire and genomic organization. Science, 317, 86–94.Putnam,N.H. et al. (2008) The amphioxus genome and the evolution of thechordate karyotype. Nature, 453, 1064–1071.Rice,A.M. and McLysaght,A. (2017) Dosage sensitivity is a major determinantof human copy number variant pathogenicity. Nat. Commun., 8, 14366.Sankoff,D. et al. (2007) Polyploids, genome halving and phylogeny.Bioinformatics, 23, i433–i439.Scannell,D.R. et al. (2006) Multiple rounds of speciation associated with reciprocal gene loss in polyploid yeasts. Nature, 440, 341–345.Van de Peer,Y. et al. (2009) The evolutionary signiﬁcance of ancient genomeduplications. Nat. Rev. Genet., 10, 725–732.Wolfe,K. (2000) Robustness? it’s not where you think it is. Nat. Genet., 25,3–4.Zheng,C. et al. (2008) Guided genome halving: hardness, heuristics and thehistory of the Hemiascomycetes. Bioinformatics, 24, i96–i104.Zheng,C. and Sankoff,D. (2013) Practical aliquoting of ﬂowering plant genomes. BMC Bioinform., 14, 1–8.Zuckerkandl,E. and Pauling,L. (1965) Molecules as documents of evolutionary history. J. Theor. Biol., 8, 357–366.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i369/3953974by gueston 07 January 2018
5028881992002	PMID28881992	5028881992	https://watermark.silverchair.com/btx258.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881992.main.pdf	Bioinformatics, 33, 2017, i217–i224doi: 10.1093/bioinformatics/btx258ISMB/ECCB 2017Image-based spatiotemporal causality inferencefor protein signaling networksXiongtao Ruan1, Christoph Wulfing2 and Robert F. Murphy1,3,4,*¨1Computational Biology Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA15213, USA, 2School of Cellular and Molecular Medicine, University of Bristol, Bristol BS8 1TD, UK, 3Departmentsof Biological Sciences, Biomedical Engineering, and Machine Learning, Carnegie Mellon University, Pittsburgh, PA15213, USA and 4Freiburg Institute for Advanced Studies and Faculty of Biology, Albert Ludwig University ofFreiburg, Freiburg im Breisgau, Baden-Wurttemberg 79104, Germany¨*To whom correspondence should be addressed.AbstractMotivation: Efforts to model how signaling and regulatory networks work in cells have largely either not considered spatial organization or have used compartmental models with minimal spatialresolution. Fluorescence microscopy provides the ability to monitor the spatiotemporal distributionof many molecules during signaling events, but as of yet no methods have been described for largescale image analysis to learn a complex protein regulatory network. Here we present and evaluatemethods for identifying how changes in concentration in one cell region inﬂuence concentration ofother proteins in other regions.Results: Using 3D confocal microscope movies of GFP-tagged T cells undergoing costimulation,we learned models containing putative causal relationships among 12 proteins involved in T cellsignaling. The models included both relationships consistent with current knowledge and novelpredictions deserving further exploration. Further, when these models were applied to the initialframes of movies of T cells that had been only partially stimulated, they predicted the localizationof proteins at later times with statistically signiﬁcant accuracy. The methods, consisting of spatiotemporal alignment, automated region identiﬁcation, and causal inference, are anticipated to beapplicable to a number of biological systems.Availability and implementation: The source code and data are available as a ReproducibleResearch Archive at http://murphylab.cbd.cmu.edu/software/2017_TcellCausalModels/Contact: murphy@cmu.edu1 IntroductionIdentifying the signaling pathways that control cellular processes isa critical goal of biomedical research. Current approaches emphasize high-throughput experimentation and computational analysisto identify proteome-scale interaction networks and regulatory relationships. Among various computational analysis frameworks, causality inference has been recently applied to analysis of biologicalsystems, (Chang et al., 2015; Chindelevitch et al., 2012; Welf andDanuser, 2014). There are two predominant characteristics ofcausal relationships: (i) Cause always precedes effect; (ii) Performingan action (at least one example is known to exist) on the causewould change the effect (Eichler, 2012). For temporal causal modeling, the Granger causality test (Granger, 1969) has been an important tool. The idea of Granger causality is that if we can learn abetter linear model for one variable by including information aboutanother variable than by not including that variable, then wesuppose there to be a causal relationship between them. Variousvariants and generalizations of Granger causality and applicationsfor biological problems have been described (Fujita et al., 2010;Shojaie and Michailidis, 2010). However, Granger causality is apairwise test, which limits its application to build large scale networks where the interactions are much more complicated than pairwise relationships. Another popular framework for temporal causalinference is the dynamic Bayesian model, a framework for statisticalmachine learning. Dynamic Bayesian models can be used to treat theprotein network as a probabilistic graphical model, and can learnthe structure and parameters from the data; however, it is controversial to treat the relationships in probabilistic graphs as causal relationships (Pearl, 2009). When compared with the Granger causalitytest method, dynamic Bayesian models perform better for short timeseries (Zou and Feng, 2009). However, when the structure of thenetwork is unknown, it is not trivial to learn it; this is especially trueCV The Author 2017. Published by Oxford University Press.i217This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i217/3953973by gueston 07 January 2018i218when there are a lot of variables (e.g. proteins), as is usually the casefor biological problems.In biological systems, many researchers resort to the secondcharacteristic of causality, that is, to intentionally change (or perturb) the amount of a gene or protein suspected to play a regulatoryrole (e.g. through mutation, gene knockout or knockdown) and observe the effect on the suspected target or behavior. This approachhas been used for analysis of single targets (e.g. genetic screens, highthroughput or high content drug screens) and for inferring static ordynamic regulatory networks from measurements of the expressionof many genes under perturbed and unperturbed conditions (Segalet al., 2003). Although the perturbation approach has yielded manyimportant results, the complexity of many cellular pathways has ledto a growing sense that causal relationships may be better addressedthrough analysis of kinetics and heterogeneity without explicit perturbation (Welf and Danuser, 2014). In this case, the various learning frameworks mentioned earlier could be applied.Fluorescence microscopy is particularly suited for seeking causal relationships without relying on perturbation (of course, labeling withfluorescent probes may itself cause a different type of perturbation, butthis is constant over the course of imaging experiments). Fluorescenttags can be used to identify particular molecules (which may representcauses or effects) and follow them in time and space in many livingcells. Fluctuation analysis can then be used to look for correlations inthe dynamics of two events (Vilela et al., 2013). For example, Alonand colleagues (Farkash-Amar et al., 2014) identified specific proteinswhose expression level and/or localization is correlated with differencesin cell motility by taking advantage of natural variation. Danuser andcolleagues (Machacek, et al., 2009) have used fluctuation analysis tolearn the relationship between myosin II localization and various aspects of cell shape and motility. These approaches have largely focusedon identifying individual factors that influence one particular process,rather than constructing a model that identifies key steps that influencedifferent proteins at various times and places. They have also requiredthe ability to image both the putative cause (e.g. tagged protein) andthe putative effect (e.g. cell shape) in the same cell; this permits theidentification of correlated fluctuations.Here we present machine learning pipelines for inferring putativecausal relationships in regulatory or signaling networks. We note anumber of differences from previous related work. First, our methoddoes not require that pairs of proteins be observed in the same cells;relaxing this requirement is very useful when increasingly large numbers of proteins are addressed in the same system. Second, themethod does not require manual specification of regions of the cellin which effects may occur; it allows for unbiased discovery. Last, ityields a graph of potentially interconnected influences rather than alist of individual factors.Building on recent work (Roybal et al., 2016), we applied thesecausal inference methods to study the dynamics of proteins involvedin T cell signaling. This is an interesting and important system forstudying a complex spatiotemporal process. T cells become activatedthrough interaction with antigen presenting cells. This is primarilyaccomplished by the T cell receptor recognizing antigen-derived peptide presented on the antigen-presenting cell (APC) surface.However, engagement of costimulatory receptors that interact withligand on the APC is required for efficient T cell activation. The mostpotent costimulatory receptor is CD28, which binds to the B7 familyligands (CD80 and CD86). T cell activation produces rapid and transient accumulation of actin at the T cell/APC interface (also called animmunological synapse). Since T cells express many signaling proteins that can interact in complex manners, how the numerous individually well-characterized proteins integrate into a coherent systemX.Ruan et al.of signal transduction is not well understood. It therefore represents agood test system for seeking causal relationships. We constructed models learned from movies of cells exposed to full stimulation and founda number of interesting relationships consistent with prior knowledge.Furthermore, applying the model to data for cells in which stimulationwas partially blocked revealed that the model could make predictionsabout protein dynamics with statistically significant accuracy.During initial analysis of the T cell movies, we found that severalproteins displayed quite similar spatiotemporal patterns; this isprobably due to the binding of these with each other and formationof persistent complexes. The spatiotemporal patterns of these proteins are therefore highly correlated, and thus the pattern of one caneasily be predicted from the pattern of another. This is a commoncomplication in causal analysis: the inability to distinguish correlation from causation. To deal with this situation, we eliminated correlated proteins using clustering when constructing our models.It is important to note that the causal relationships mentionedbelow are the result of statistical inference and are intended to be readas putative causal relationships even when this is not explicitly stated.2 Materials and methods2.1 DataWe used the data and methods previously described (Roybal et al.,2016) for creating maps of the spatial distribution of ten differentsensors under two conditions. The maps consist of the probabilitythat a molecule of a given protein would be found in each of 6628voxels in a standardized half-ellipsoid template; the flat surface ofthe template corresponds to the synapse. The values thus reflect relative, not absolute, concentrations. Maps were originally created fortwelve time points relative to synapse formation, but we used onlythe first 10: À40, À20, 0, 20, 40, 60, 80, 100, 120 and 180 s forcausal inference. We created maps for two new proteins: Ezrin andPIP2, using the same methods, and also slightly improved the alignment algorithm. Both the original and refined maps are availablefrom http://murphylab.web.cmu.edu/data/TcellModels.2.2 Region definitionEach voxel in the template was represented by a vector of length 120containing the amounts of each of 12 sensors at ten time points. Thesevectors were grouped into voxel types by k-means clustering. Eachconnected component (using 26-connectedness) with the same clustertype and containing more than four voxels was defined as a region.2.3 Causal graphical process model2.3.1 Graph representationA graph was created with a node for each region for each sensor; thenode values are the concentration of all voxels in the region for that sensor. We consider that observing the graph yields a set of node valuesx ¼ ðx1 ; . . . ; xn Þwhere n is the number of nodes.For observation of a time series graph, we represent the nodes attime point k as  ðkÞx½k  ¼ x1 ; . . . ; xðkÞn2.3.2 Causal graph process modelGiven these values, we wish to construct an adjacency matrix A thatcaptures the relationships between nodes, as well as to find aDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i217/3953973by gueston 07 January 2018Image-based spatiotemporal causality inferencefunction that describes how the node values change over time. Asone approach, we chose to use Causal Graph Process (CGP) modeling (Mei and Moura, 2017) to do this. In the CGP, the function isdefined as a polynomial in A, and the order depends on the lag M:x½t  ¼ w½t  þPMi¼1 Pi ðAÞx½t À i  ¼ w½t  þ ðc10 I þ c11 AÞx½t À 1 þðc20 I þ c21 A þ c22 A2 Þx½t À 2  þ ðcM0 I þ cM1 A þ ÁÁÁ þ cMM AM Þx½t À M where w½t  is the residual for x½t  (the amount of x½t ) not accountedfor by the model), and cij is the coefficient term for the jth order forthe polynomial Pi ðAÞ. We can write cij more compactly asc ¼ ðc10 ; c11 ; c20 ; c21 ; c22 ; . . . ; cMM ÞGiven this formulation, the goal is to find A and cij ði ¼ 1; . . . ;M; j ¼ 0; . . . ; MÞ. This is done by solving the optimizationproblemminA;c2K MX1 X x½k  ÀPi ðAÞx½k À i  þ2 k¼Mþ1i¼12k1 kvecðAÞk1 þ k2 kck1Where k1 and k2 are regularization parameters. vecðAÞ meansreshaping matrix A as a vector, and K is the number of time points.The problem is non-convex. As described previously in Mei andMoura (2017), we first solve Pi ðAÞ, then estimate A from Pi ðAÞ,and then estimate c from Pi ðAÞ and A. Projected gradient descent(Figueiredo et al., 2007) is applied for each step. In order to preventself-prediction, we add a constraint that the block of connections between nodes of the same protein is set to zero. This is easy to implement by setting these values to zeros in each iteration.i219lag j, and k1 and k2 are regularization parameters. k Á k2 means theFFrobenius norm of a matrix.We used the alternating direction method of multipliers algorithm (Boyd et al., 2011) to determine the adjacency matrices. Onedifference from CGP is that constrained elastic net regression(CENR) learns separate adjacency matrices for each time lag ratherthan a single adjacency matrix. Here we prevented self-prediction bysetting the diagonal values to zeros after each iteration.2.5 Grouping proteinsTo avoid effects of correlations in spatiotemporal patterns thatwould interfere with the causal inference process, proteins weregrouped based on hierarchical clustering. To do this, each proteinwas represented by the amount of that protein in each cell region atall-time points. Three similarity measures were used to constructdendrograms and decide the number of clusters: city block distance,Euclidean distance, and correlation coefficient. The mean kinetics ofall proteins in a cluster were used to represent that cluster for causalmodel construction (for display, the cluster was given the name ofthe alphabetically first protein in it).2.6 Parameter estimationThe best free parameter combinations for each method (k1 and k2for CGP, a, b, k1 and k2 for CENR) were chosen using 5-fold crossvalidation over the kinetics for single cells. For construction of eachtraining and test set, the mean kinetics of each region for each protein were calculated. The parameter combination giving the lowestaverage root mean normalized squared error (RMSE) over all proteins was chosen.2.7 Timing of important events2.4 Constrained elastic net regression2.4.1 Granger causality testPairwise Granger causality tests were performed for each pair ofnodes. After calculation of the P-values, Bonferroni-Holm correction for multiple hypothesis tests was performed. A threshold a onthe corrected P-values was used to identify significant pairs for constraining below. In this step, we also considered at which lag timethe prediction is strongest by comparing the cross correlation withdifferent lag times. The sign of the relationship (slope) from the linear models was also recorded to form the constraints in the nextstep.2.4.2 Constrained elastic net regressionFor identified pairs, if they have a positive linear relationship, thenwe forced the element in the adjacency matrix to be greater than b(b > 0), otherwise it is forced to be < Àb. The basic optimizationproblem is2K MX1 X x½k  ÀAi x½k À i A1 ;:::;AM 2i¼1k¼Mþ1min2M M  2XX þk1 vecðAi Þ þ k2 Ai  i¼11i¼1FSubject toAi ðk; lÞ ! b; Aj ðm; nÞÀbwhere nodes k, l are identified positive causal relationships at a particular lag i, and nodes m, n are identified negative causal relationships atTo aid in interpretation of the model, we sought to identify specifictime points at which effects captured by causal models were maximally observed. To do this (for lag M), we calculated for each timet the cross correlation between the concentrations of cause X fortime points tÀM, . . ., t and the concentrations of effect Y for timepoints t, . . ., t þ M. If the value of A was positive for this effect, wefound the t that maximized this cross correlation, and when thevalue of A was negative we found the t that minimized it (largestnegative value).After defining the time point t of important event, the trend forcause and effect was defined as the slope of the linear model of causeX for time points tÀM, . . ., t and of effect Y for time points t, . . ., tþ M, respectively.2.8 Permutation testsTo examine the generalizability of the models, we applied them topredict kinetics for the B7 blockade condition not used for trainingthe models. In the test, in each iteration, we permutate on the orderof kinetics for each protein and each region (on Full stimulus condition) in order to disrupt the association between different proteinsand regions. Then We used the node values from the B7 blockademaps for time points t À 1, . . ., tÀM to predict values for t (t ¼ M þ1, . . ., K) using the models trained with the full stimulus maps. As ameasure of error of these predictions, the normalized RMSEs werecomputed between the predicted time series and the actual one, forall three regions for each sensor. To estimate the significance of obtaining a given RMSE for a given protein, permutation tests wereperformed in which predictions for a given protein in each of thethree regions were randomly drawn from the values at all-timepoints in all regions for that protein. This process was repeatedDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i217/3953973by gueston 07 January 2018i220X.Ruan et al.Fig. 1. Region deﬁnitions. Regions within the standardized template found by voxel clustering and connected component analysis; different colors representdifferent regions, and magenta outside the cell indicates the background50 000 times to obtain a background distribution of the predictionerrors. The P-values for the predictions were corrected using theBonferroni-Holm method and the corrected P-values below 0.05were considered significant.3 Results3.1 Creating graphs of protein spatiotemporalinteractions3.1.1 Defining regionsWe begin with maps of the relative concentration of twelve differentproteins in each voxel of a standardized T cell template at varioustimes before and after synapse formation (Roybal et al., 2016).These maps were generated from many movies of many sets of cells,each set expressing (at near the endogenous level) a different fluorescently tagged protein. Each movie showed an individual T cell forming a synapse with an antigen-presenting cell. The position and timeof synapse formation was identified, the T cell boundary was foundin each frame, and the fluorescence distribution was morphed to astandardized template. The amount of protein in each voxel of thestandardized template was then averaged across many cells to provide a ‘map’ of the concentration.If we consider the amount of each sensor in each position of thecell to form a fully connected graph, we can consider how the sensors change over time to be dictated by some process operating onthat graph. This is the framework of the two methods: CGP andCENR method. However, if we treat each voxel separately, that is,as a node in the graph, then there will be 79 536 nodes (6628 voxelsÂ 12 proteins); learning the model would require estimating an adjacency matrix of over 6 billion values for CGP and 12 billion forCENR. Considering that nearby voxels have similar spatiotemporalpatterns in the same process, we therefore chose to consider asmaller number of multi-voxel regions and defined these as set ofconnected voxels that behaved similarly: that is, those voxels thathad similar kinetics of all sensors at all-time points. Note that thisdoes not mean that all sensors were equal or that the amounts donot change, but rather only that if a sensor changes, it changes similarly among all voxels in a region. Possible regions were found usingclustering of voxels into various numbers of clusters (see Section 2).Three clusters were observed to give the highest score by theCalinski Harabasz criterion, and therefore three voxel types (whichhappened to yield three regions) were used; the regions are shown inFigure 1.3.1.3 Building modelsWe next considered what temporal resolution we should attempt tocapture within the graph process. Since changes in actin and its regulators happen rapidly (on the time frame of seconds to tens of seconds), and since we used only ten time points, we chose to learnmodels that for each time point depended on only the two prior timepoints (M ¼ 2).3.1.2 Hierarchical clusteringAs discussed earlier, we used clustering to eliminate spurious causalrelationships due to high correlations among protein patterns.However, different similarity measures yielded different apparentnumbers of clusters. For example, using city block distance, the clustering results shown in Figure 2 were obtained. Based on the figure,we created one group consisting of ARP3 and HS1 and a secondgroup consisting of CPa1, Coronin1A, WASP and WAVE2. As discussed in Section 2, these groups were represented by the averageacross all proteins in the group.CGP modelGiven the kinetics for each protein or group, we constructedCGP models and optimized parameters as discussed in Section 2.The resulting model consists of an adjacency matrix A, whichprovides the overall estimate of the effect of each node on eachother, and the coefficients of the polynomials P that operate onit. The adjacency matrix is shown in Figure 3. The data (throughthe parameter optimization process) do not yield a very sparsemodel, suggesting there may be a complicated network amongthe proteins.Fig. 2. Hierarchical clustering using city block metric. Left: The dendrogramshows the clustering of the proteins using the city block metric as describedin the text. Right: the image shows the pairwise distance between each pairof proteins. The cophenetic coefﬁcient of the dendrogram is 0.8866Fig. 3. Adjacency matrix from the CGP. The X axis shows the cause and the Yaxis shows the effect. The color of each element represents the estimatedstrength of the cause and effect relationship between a pair. ARP3 and CPa1each represent groups with multiple proteinsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i217/3953973by gueston 07 January 2018Image-based spatiotemporal causality inferenceCENR modelBased on the threshold on corrected P-values, we identified 8 pairsof strong potential causal relationships. Based on the constraints, wetrained the model with the regularization parameters. Again, wealso suppressed self-prediction by setting the diagonal elements tozero. For this model, we learned one adjacency matrices for each lagtime. These are shown in Figure 4. Despite our attempt to focus themodel on strong effects (and similar to the case for the CGP model),the adjacency matrices are not very sparse.3.2 Major findings3.2.1 Major causal relationshipsFirst, by hierarchical clustering, we identified proteins that havesimilar kinetics across all regions, suggesting that they are largelypresent in persistent complexes. There is prior evidence to suggestthat this may be the case. Uruno et al. (2003) has shown that HS1 isthe upstream regulator for the activation of APR3, and activatedARP3 will bind to Actin and stabilize the nuclearization of Actin.Fig. 4. Adjacency matrices from CENR. (A) lag time À1. (B) lag time À2. Forboth panels, the X axis shows the cause and the Y axis shows the effect. Thecolor of each element in the matrix represents the strength of the cause andeffect relationship between a pair at that lag time. ARP3 and CPa1 each represent groups with multiple proteinsi221WASP and WAVE2 are also upstream regulators of ARP3(Burkhardt et al., 2008). Coronin1A is known as an Actin-bindingprotein that regulates actin through the coordination with ARP2/3complex and cofilin (Gandhi et al., 2009). Therefore, our clusteringresults are consistent with prior studies. A comparison of the kinetics of cause and effect are shown for some of the largest relationships in Figure 5.3.2.2 Region 1 as the most important regionIn the adjacency matrices, we can see that the strongest relationshipsare mostly between proteins in region 1, the synapse region. This isthe major region into which actin and its regulators are recruitedand activated. This makes sense: this region is relatively small compare to other regions, and most proteins are accumulated in certaintimes, i.e. 0–40 s, i.e. the variation of the proteins is largest amongthe three regions. So clearly, these interesting patterns are consistentwith what is known about actin dynamics in T cell synapse formation. However, we can also find some other pairs that are not in region 1, MRLC in the cortex (region 3) can predict the ARP3 groupin both the synapse and the cortex. The dominant pair is that enrichment of myosin at the cell cortex drives accumulation of the coreactin turnover machinery at the cellular interface. This suggestsmechanical coupling between the cell body and the interface, anintriguing hypothesis that should and can be tested.3.2.3 Critical time pointsWe can further explore these effects by identifying at which time theeffect is maximally seen. This was done by finding, for positive andnegative effects, respectively, the time point that had the largest orsmallest (most negative) value in the cross-correlation function ofthe cause and effect proteins/regions. Using these, we can begin tolink events into a network by examining whether the protein and regions that are the effect of an event at one time are the cause of anevent at a later time. This is shown in Figure 6 for a larger numberof relationships (a lower threshold on the adjacency matrix).Inspecting this figure yields a number of interesting confirmatoryand speculative conclusions. There are two major stages. First, mostof the relations to actin in the synapse are observed to be maximalat the earliest two time points, i.e. an increase in cofilin and theARP3 group in the synapse predicts the increase of actin and CPa1in the synapse. This is consistent with the rapid spreading of interface actin at these time points when the actin regulators would beexpected to be most dynamic. Second, in the later stage, there is a reverse causal relationship for actin and its regulators such that a decrease of actin in synapse predicts a decrease of cofilin and theARP3 group there. This is again consistent with the disassembly ofactin after synapse formation. Moreover, proteins involved in immediate actin turnover, i.e. actin nucleation, capping and severing, arewell interconnected yet mostly separated from three furtherFig. 5. Example kinetics for putative cause and effect relationships identiﬁed by CENR. The four pairs with the largest absolute values in the adjacency matrix areshow. In each panel, the x-axis shows time relative to synapse formation (seconds) and y-axis shows relative concentration. The cause protein and region areshown above each panel, followed by ‘Pos’ or ‘Neg’ to indicate the direction of the regulation and then the effect protein and region. The kinetics for the causeproteins are shown in solid black lines and the kinetics for the effect are shown in dotted red lines. Note the lag between changes in the cause and changes in theeffectDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i217/3953973by gueston 07 January 2018i222X.Ruan et al.Fig. 6. Critical time points identiﬁed by the CGP. The time on the left when a putative causal relationship is maximally observed between a pair of proteins.Arrows indicate events and are colored by the sign of the change in the cause and effect: black correspond to both cause and effect increasing, red to bothdecreasing, green to cause increasing and effect decreasing, and blue to cause decreasing and effect increasingupstream signaling regulators included in this analysis, LAT, Ezrinand myosin. This is to be expected. Surprisingly, the analysis identifies myosin as the key causal connector between upstream signalingand actin turnover. Although this observation needs to be confirmedby inclusion of a larger number of upstream signaling intermediates,a central role of myosin in coupling signaling to actin turnover isintriguing and testable.3.2.4 Common pairsTo see how robust the relationships are across different methods, wecompared the networks from the two methods (Fig. 7A). It is interesting that most relationships are found by both methods, especiallythe ones that are closely related to actin, such as those with theARP3 group and cofilin. This suggests that these relationships aredominant in the actin dynamics in synapse formation, which is consistent with what we already know about actin dynamics. And onthe other hand, it suggests both methods are reliable at finding themajor relationships.3.3 Performance of methodsAs further evaluation of the ability of the methods to find potentialcausal relationships, we evaluated how well they found known relationships using Receiver-Operating Curve analysis. Based on database and literature searches, we identified potential relationshipsbetween these 12 proteins; however, since these sources often do notspecify the directionality of relationships, we expressed them as anundirected graph (Fig. 7A). Since the strongest relationships occur inregion 1, we considered whether those relationships could be foundin region 1. The AUC values for recovery of the known relationshipsare shown in Table 1. Interestingly, both methods perform quitewell; the AUC values of 0.709 and 0.644 (Case 1) are both muchbetter than random.3.4 Prediction for B7 dataThe studies (Roybal et al., 2016) upon which this work is based alsoexamined the effect of partial inhibition of T cell signaling upon sensor distributions. This inhibition was accomplished by blocking asecondary stimulation pathway, costimulation by CD28, and isreferred to as B7 blockade. Maps of the same sensors were also generated under this condition. To explore how well the models learnedFig. 7. Network comparison. (A) (partial) Ground truth, (B) Combined network.In (B), the edges are colored by which method identiﬁed a relationship: blueindicates the edge occurs in both two methods, green indicates the edge onlywas found by CENR. The numbers on the edges are the maximum absoluteweight between the two methodsfrom the full stimulus conditions would be predictive of other conditions, we used the node values from the B7 blockade maps for timepoints t À 2, and t À 1 to predict values for t. Comparison of the actual and predicted kinetics are shown Figure 8. We then computedDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i217/3953973by gueston 07 January 2018Image-based spatiotemporal causality inferencei223Table 1. Comparison of different methods for different groups ofproteinsUngroupedCase 1Case 2 (7)Case 3 (25)CGP CENR CGP CENR CGP CENR CGP CENRCV error 0.092 0.062 0.086 0.069 0.096 0.074 0.107 0.070B7 error 0.119 0.080 0.120 0.085 0.117 0.081 0.123 0.082AUC0.629 0.565 0.709 0.644 0.589 0.580 0.614 0.589Results are shown for the two methods for four different groupings of theseven similar proteins: ungrouped; (CPa1,WASP,Coronin1A,WAVE2), actin,(ARP3,HS1); all grouped; and (CPa1,WASP,Coronin1A,WAVE2, actin),(ARP3,HS1). CV error shows the mean testing error in 5-fold cross validationfor the best parameter combination. The B7 prediction error was calculatedafter averaging any grouped proteins. The AUC values are for recovery of previously documented relationships in region 1.the normalized mean squared errors between the predicted time series and the actual one, for each sensor, as a measure of error of thepredictions. To test if the errors of these predictions were significantly less than expected at random, we performed permutationtests as described in Section 2. All predictions passed this test at theP ¼ 0.05 level, indicating that at least the relationships between sensors and regions are conserved between the full stimulus and the B7blockade conditions. The results suggest that the method can predictthe turnover of actin and its regulators in the critical region evenafter perturbation.3.5 Comparison of performance of different methodsFinally, we repeated the ROC curve analysis and the B7 predictionanalysis for models constructed by both CGP and CENR eitherusing all 12 proteins or using with various numbers of clusters foundby the different clustering methods. The results are shown inTable 1. We conclude that the grouping method using city blockmetrics has the best performance among all conditions for bothCGP and CENR in terms of AUC. Although CGP generally has better performance for identifying previously known relationships, itdoes not perform as well as CENR in terms of accuracy of the modelfor full stimulation or prediction for B7 data. This may be due to thefact that CGP has few parameters and thus the generalization abilityis not as good as CENR.4 DiscussionThere has been extensive modeling of signaling processes in systemssuch as the T cell (for overview see Chylek et al., 2014), includingmodeling that considers spatial organization (Angermann et al.,2012). These have been based on known biochemical reactions andthus the need for causal inference is minimal. We have describedhere a novel approach to understanding the potential causative relationships between the spatiotemporal distributions of proteinsinvolved in signaling or other regulatory networks. We have demonstrated that the approach yields a number of conclusions that areconsistent with current knowledge of actin regulators, and also relationships that deserve further exploration.An important issue is how these methods would scale for largersets of proteins or more extensive kinetics. For optimizing the CGP,the worst-case time complexity is O(M2N3þKMN) for M blocks ineach iteration, where M is the lag time, K is the total number of timepoints, and N is the number of nodes (Mei and Moura, 2017).In our case, N ¼ PR, where P and R are the numbers of proteins andregions. Therefore the time complexity is O(M2P3R3þKMPR).Fig. 8. B7 prediction using CGP. For each ﬁgure, the x-axis show time aftersynapse formation, and the y-axis shows relative concentration of a protein.The actual kinetics are shown in black and the predictions in redFor CENR, the worst-case time complexity is O(M2N3) if we canpre-compute some quantities in order to get rid of K for each iteration. Therefore for our problem, the complexity is O(M2P3R3) ineach iteration. Though the time complexities are high, we were ableto solve them in reasonable time since the numbers of proteins andregions were small. On a single 2.4 GHz Intel node, inference took<10 s for CGP and <1 s for CENR. The full process including crossvalidation for parameter estimation and permutation tests for significance took 200 and 25 CPU hours, respectively.Although the analysis can provide some promising perspectivefor image-based causal inference for protein networks, we mustpoint out that this analysis is still preliminary and has some limitations. First, the number of proteins is relatively small (12 proteins),due to limited available data. Second, the proteins were not randomly chosen and they therefore may have more relationships thanexpected. Third, we attempted to remove confounding effectsthrough clustering but this is a complex subject that will require further exploration.Despite these limitations, we believe that the consistency of theresults with prior knowledge, and the fact that the model learnedunder one condition could make statistically significant predictionsof concentrations under another condition not used in the modellearning, to be highly encouraging. In the future, we plan to expandthe number of proteins analyzed, which will give a clearer picture ofthe whole network and may identify both additional putative relationships and resolve potentially confounding relationships.AcknowledgementsWe thank Dr Laura McMillan for help in constructing the partial groundtruth.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i217/3953973by gueston 07 January 2018i224FundingThis work was supported in part by US National Institutes of Health [grantP41 GM103712], by US National Science Foundation [grant MCB1616492]and by UK BBSRC [grant BB/P11578/1-15 NFSBIO].Conﬂict of Interest: none declared.ReferencesAngermann,B.R. et al. (2012) Computational modeling of cellular signalingprocesses embedded into dynamic spatial contexts. Nat. Methods, 9, 283–289.Boyd,S. et al. (2011) Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn., 3, 1–122.Burkhardt,J.K. et al. (2008) The actin cytoskeleton in T cell activation. Annu.Rev. Immunol., 26, 233–259.Chang,R. et al. (2015) Causal inference in biology networks with integratedbelief propagation. In Paciﬁc Symposium on Biocomputing, Kohala Coast,Hawaii, p. 359.Chindelevitch,L. et al. (2012) Causal reasoning on biological networks: interpreting transcriptional changes. Bioinformatics, 28, 1114–1121.Chylek,L.A. et al. (2014) Modeling biomolecular site dynamics in immunoreceptor signaling systems. In: A Systems Biology Approach to Blood.Springer, New York, pp. 245–262.Eichler,M. (2012) Causal inference in time series analysis. In: Berzuini,C.,Dawid, A.P., and Bernardinelli, L. (eds.) Causality: Statistical Perspectivesand Applications. John Wiley & Sons, Chichester, pp. 327–354.Farkash-Amar,S. et al. (2014) Noise genetics: inferring protein function bycorrelating phenotype with protein levels and localization in individualhuman cells. PLoS Genet., 10, e1004176.Figueiredo,M.A. et al. (2007) Gradient projection for sparse reconstruction:application to compressed sensing and other inverse problems. IEEE J.Select. Top. Signal Process., 1, 586–597.X.Ruan et al.Fujita,A. et al. (2010) Granger causality in systems biology: modeling genenetworks in time series microarray data using vector autoregressivemodels. In: Brazilian Symposium on Bioinformatics. Springer, Berlin, pp.13–24.Gandhi,M. et al. (2009) Coronin switches roles in actin disassembly dependingon the nucleotide state of actin. Mol. Cell, 34, 364–374.Granger,C.W. (1969) Investigating causal relations by econometric modelsand cross-spectral methods. Econometrica, 37, 424–438.Machacek,M. et al. (2009) Coordination of Rho GTPase activities during cellprotrusion. Nature, 461, 99–103.Mei,J. and Moura,J.M. (2017) Signal processing on graphs: Causal modelingof unstructured data. IEEE Trans. Signal Process, 65, 2077–2092.Pearl,J. (2009) Causality. Cambridge University Press, Cambridge.Roybal,K.T. et al. (2016) Computational spatiotemporal analysis identiﬁesWAVE2 and coﬁlin as joint regulators of costimulation-mediated T cellactin dynamics. Sci. Signal, 9, rs3.Segal,E. et al. (2003) Module networks: identifying regulatory modules andtheir condition-speciﬁc regulators from gene expression data. Nat. Genet.,34, 166–176.Shojaie,A. and Michailidis,G. (2010) Discovering graphical Granger causalityusing the truncating lasso penalty. Bioinformatics, 26, i517–i523.Uruno,T. et al. (2003) Haematopoietic lineage cell-speciﬁc protein 1 (HS1)promotes actin-related protein (Arp) 2/3 complex-mediated actin polymerization. Biochem. J., 371, 485–493.Vilela,M. et al. (2013) Fluctuation analysis of activity biosensor images forthe study of information ﬂow in signaling pathways. Methods Enzymol.,519, 253.Welf,E.S. and Danuser,G. (2014) Using ﬂuctuation analysis to establish causalrelations between cellular events without experimental perturbation.Biophys. J., 107, 2492–2498.Zou,C. and Feng,J. (2009) Granger causality vs. dynamic Bayesian networkinference: a comparative study. BMC Bioinformatics, 10, 122.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i217/3953973by gueston 07 January 2018
5028881991002	PMID28881991	5028881991	https://watermark.silverchair.com/btx257.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881991.main.pdf	Bioinformatics, 33, 2017, i252–i260doi: 10.1093/bioinformatics/btx257ISMB/ECCB 2017Exploiting sequence-based features forpredicting enhancer–promoter interactionsYang Yang1, Ruochi Zhang2, Shashank Singh3 and Jian Ma1,*1Computational Biology Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA15213, USA, 2Department of Automation, Tsinghua University, Beijing 100084, China and 3Machine LearningDepartment, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA*To whom correspondence should be addressed.AbstractMotivation: A large number of distal enhancers and proximal promoters form enhancer–promoterinteractions to regulate target genes in the human genome. Although recent high-throughputgenome-wide mapping approaches have allowed us to more comprehensively recognize potentialenhancer–promoter interactions, it is still largely unknown whether sequence-based features aloneare sufﬁcient to predict such interactions.Results: Here, we develop a new computational method (named PEP) to predict enhancer–promoter interactions based on sequence-based features only, when the locations of putative enhancers and promoters in a particular cell type are given. The two modules in PEP (PEP-Motif andPEP-Word) use different but complementary feature extraction strategies to exploit sequencebased information. The results across six different cell types demonstrate that our method is effective in predicting enhancer–promoter interactions as compared to the state-of-the-art methods thatuse functional genomic signals. Our work demonstrates that sequence-based features alone canreliably predict enhancer–promoter interactions genome-wide, which could potentially facilitatethe discovery of important sequence determinants for long-range gene regulation.Availability and Implementation: The source code of PEP is available at: https://github.com/macompbio/PEP.Contact: jianma@cs.cmu.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionHigh-throughput whole-genome mapping technologies such as Hi-C(Lieberman-Aiden et al., 2009) and ChIA-PET (Tang et al., 2015)have provided us with new opportunities to study genome-widechromatin interactions. In particular, these methods have allowed usto identify enhancer–promoter interactions (EPIs) at much morerefined resolution. In mammalian and vertebrate genomes, gene promoters and distal regulatory enhancers may be millions of base-pairsaway from each other; and an enhancer oftentimes does not regulateits closest gene (Zhang et al., 2013; Dixon et al., 2015). Studiesusing global chromatin interaction data based on Hi-C and ChIAPET have shown that a large fraction of enhancers form long-rangeEPIs to regulate genes far away in the genome (Sanyal et al., 2012;Li et al., 2012). However, the general principles at the sequence levelunderlying such long-range EPIs remain largely elusive.In this paper, our goal is to reveal whether there are sequencebased features within enhancer elements and promoter elements thatare strongly predictive for EPIs. It is generally unclear whether, andto what extent, the information encoded in the genome sequencecontains important potential instructions for forming EPIs. Thereare recent computational methods in predicting EPIs based on functional genomic features, in particular, RIPPLE (Roy et al., 2015)and TargetFinder (Whalen et al., 2016). In both methods, manyfunctional genomic datasets were used, including DNase-seq, histone marks, transcription factor (TF) ChIP-seq and gene expression.In addition, TargetFinder also used CAGE and DNA methylationdata. The general approach of using functional genomic signals asfeatures for the machine learning classifier in both RIPPLE andTargetFinder is similar. From these studies, we now know that signals from aforementioned functional genomic data are informativeto computationally distinguish EPIs from non-interacting enhancer–promoter pairs. However, it is still unknown whether the information in genome sequences within enhancers and promoters issufficient to distinguish EPIs. In this paper, we aim to answer the following question: if we are only given the locations of putative enhancers and promoters in a particular cell type, can we train aCV The Author 2017. Published by Oxford University Press.i252This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018Exploiting sequence-based features for predicting EPIspredictive model for that cell type to identify EPIs directly from thegenomic sequences without using any functional genomic signals?Note that our main goal of this work is to build a predictive modelfor individual cell type to assess if sequence features are sufficientfor EPI predictions.An overview of our method is shown in Figure 1. We developedan algorithm based on a boosted tree ensemble model to predictlong-range EPIs by incorporating two strategies for extracting features directly from the DNA sequences of enhancer and promoterelements. We call our algorithm PEP (Predicting Enhancer–Promoter interactions) with two modules, PEP-Motif andPEP-Word, which use different feature extraction approaches. InPEP-Motif, we search for patterns of known transcription factorbinding site (TFBS) motifs in the sequences involved in EPI. The normalized occurrence frequencies of these TFBS motifs are then usedas features representing an enhancer or a promoter. In PEP-Word,we use the word embedding model (Mikolov et al., 2013a, 2013b)to directly embed the sequences of enhancer and promoter regionsinto a new feature space. Each sequence is then represented by acontinuous feature vector. In both PEP-Motif and PEP-Word modules, we concatenate the individual feature vectors to form featurerepresentations of any given enhancer–promoter pair. If the pairedregions have identified interactions based on Hi-C data, the pair islabeled as a positive sample; otherwise, it is labeled as a negativesample. We then developed a predictive model based on an ensemblelearning method—Gradient Tree Boosting (GTB) (Friedman, 2001).We evaluated the performance of our method and compared withTargetFinder (Whalen et al., 2016) as well as RIPPLE (Roy et al.,2015). In six different cell lines, we show that PEP (both modules)achieved competitive results as compared to the state-of-the-artmethods that use non-sequence features from functional genomicsignals. Overall, our results suggest that, without relying on information from functional genomic signals, sequence-based featuresalone are effective in predicting EPIs in a specific cell type, if we aregiven the locations of putative enhancers and promoters in that celltype. We believe that our new method has the potential to become ageneric model to allow us to elucidate sequence-based instructionsthat determine long-range gene regulation.2 Results2.1 Predicting EPIs using features based on TFBS motifsWe first evaluated the performance of PEP-Motif on the E/P(Enhancer/Promoter) datasets (see Methods section) in six cell lines(GM12878, K562, IMR90, HeLa-S3, HUVEC and NHEK), as compared to TargetFinder (Whalen et al., 2016) based on its E/P/W performance (i.e. functional genomic signals from enhancer, promoterand the window between them) on the same datasets. Results areshown in Figure 2 and Supplementary Table S3, where we also include results from PEP-Word and PEP-Integrate (discussed in thefollowing sections). Different metrics including AUROC (AreaUnder the Receiver Operating Characteristic curve), AUPR (AreaUnder the Precision-Recall curve), Precision, Recall, F1 score andMCC were used to evaluate the predictions (see SupplementaryMethods A.8) based on 10-fold cross validation. Due to class imbalance in our data (>95% of samples are negative), AUPR, F1 scoreand MCC are more appropriate performance measures thanAUROC (Davis and Goadrich, 2006), though we include the latterfor completeness.Our results show that PEP-Motif reaches comparable performance as compared to TargetFinder (E/P/W) on average, and achievesi253Fig. 1. Method overview of PEPimprovement in certain cell types. The average AUPR achieved byPEP-Motif across six cell lines is 0.84, similar to that ofTargetFinder (E/P/W) (0.86). We found that PEP-Motif outperformsTargetFinder (E/P/W) with higher AUPR, F1 score, MCC, Precisionand Recall in GM12878, which has the largest sample size. PEPMotif also performs competitively as compared to TargetFinder(E/P/W) in IMR90, reaching higher F1 score, MCC, Recall and similar Precision. In K562, NHEK, and HUVEC, TargetFinder performsbetter than PEP-Motif. Overall, the performance of each methodvaries across different cell lines. However, on average the performance of PEP-Motif is comparable to TargetFinder (E/P/W) (Fig. 2).This suggests that using sequence features based on TF motifs alonecan achieve competitive results as compared to TargetFinder whichuses a large number of features of functional genomic signals. Wethen asked if we can use a subset of the features to achieve similarperformance. Motif features were selected in PEP-Motif based onimportance ranking to reduce the feature dimension while maintaining prediction performance (Supplementary Fig. S1). The feature importance was estimated by the GTB model and features were rankedaccordingly (see Methods). We observed that AUPR initially increases quickly as top-ranking features are being added. The performance improvement then slows dramatically, approachingsaturation when the number of features exceeds around 100, whichis <8% of all the features (1280 in total). This suggests that most ofthe prediction strength of PEP-Motif can be captured by a small subset of important features.2.2 Predicting EPIs using features based on wordembedding modelWe evaluated PEP-Word using the same datasets from the six celllines as compared to TargetFinder (E/P/W) (Fig. 2 andSupplementary Table S3). The average AUPR achieved by PEPWord across six cell lines is 0.85, similar to that of TargetFinder(E/P/W) (0.86). The most significant overall improvement wasobserved in IMR90. PEP-Word achieves AUPR, Precision, Recall, F1score and MCC of 0.84, 0.90, 0.75, 0.82 and 0.81 in IMR90, respectively, each of which is increased from that of TargetFinder(E/P/W) (0.82, 0.85, 0.73, 0.78 and 0.78, respectively). We thencompared PEP-Word with PEP-Motif (Supplementary Table S3).We found that on average PEP-Word can achieve better performance as compared to PEP-Motif. PEP-Word outperforms PEP-MotifDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018i254Y.Yang et al.0.91.0F1MCC0.8AUROCAUPRPrecision RecallF1MCCMCCPrecision RecallF1MCCF1MCC1.00.60.70.80.91.00.80.70.6F10.5Precision RecallAUPRNHEK0.91.00.90.80.7AUPRAUROCHUVEC0.6AUROC0.5Precision RecallAUROCAUPRPrecision RecallF1MCC0.5AUPR0.50.60.60.70.70.80.90.80.70.60.5AUROCHeLa−S30.5IMR901.0K562TargetFinder(E/P/W)PEP−MotifPEP−WordPEP−Integrate0.91.0GM12878AUROCAUPRPrecision RecallFig. 2. Evaluation of PEP-Motif, PEP-Word and PEP-Integrate (K ¼ 6 for K-mers) on E/P data from six cell lines in comparison with TargetFinder (E/P/W) in terms ofAUROC, AUPR, Precision, Recall, F1 and MCCin three out of the six cell lines (HUVEC, IMR90 and NHEK) withabout 2–3% improvement on AUPR, Precision, F1 score and MCC.In particular, in NHEK, where PEP-Motif is not competitive toTargetFinder (E/P/W), PEP-Word achieves 0.90 AUPR and 0.92Precision, reaching comparable level with TargetFinder (E/P/W),along with around 2.5% increase for Recall, F1 score and MCC ascompared to PEP-Motif. PEP-Motif performs slightly better inHeLa-S3. The two modules achieve similar performance inGM12878 and K562. These results suggest that PEP-Word canachieve competitive performance as compared to TargetFinder (E/P/W) using only sequence features based on word embedding, withouteven relying on known TF binding motif models. In addition, PEPWord generally shows some improvement over PEP-Motif, althoughthese two PEP models seem to have different advantages in differentcell lines.2.3 Integrating features from PEP-Motif and PEP-Wordto predict EPIsTo test if we can further improve the prediction by integrating PEPMotif and PEP-Word, we formed a combination of important features from both modules. We call the result ‘PEP-Integrate’ features(Supplementary Methods A.5). We used the top 300 important TFmotif features from PEP-Motif to concatenate with the PEP-Wordfeature vector, followed by retraining of the GTB model and evaluation (results in Fig. 2 and Supplementary Table S3). Here, the number of motif features to select for integration is evaluatedby sequentially increasing the feature selection set (SupplementaryFig. S2).In general, PEP-Integrate achieves higher performance as compared to PEP-Word and PEP-Motif individually. We also found thatoverall PEP-Integrate is more competitive now as compared toTargetFinder (E/P/W). The largest improvement was observed inIMR90. PEP-Integrate achieves AUPR, F1 score and MCC of 0.85,0.84 and 0.83, respectively, in IMR90, which are 3–5% higher thanthose from TargetFinder (E/P/W) (AUPR: 0.82, F1 score: 0.78,MCC: 0.78). PEP-Integrate also reaches higher Precision thanTargetFinder (E/P/W) in five of the six cell lines (GM12878, K562,HeLa-S3, IMR90 and NHEK), with improvement ranging from 2%to 8%. These results demonstrate that the integration of the featuresfrom PEP-Motif and PEP-Word can lead to overall more accurateEPI predictions.2.4 Additional comparison with TargetFinder andRIPPLE using EE/P datasetsWe further tested PEP-Integrate, PEP-Word and PEP-Motif on theEE/P (Extended Enhancer/Promoter) datasets (results shown inSupplementary Fig. S3 and Table S4; see Methods section for the description of EE/P data). The EE/P data were used in Whalen et al.(2016) for training and evaluating the model utilizing extended enhancer and promoter regions [TargetFinder (EE/P)]. Note that theextended enhancers are similar in length ($6 kb) to the enhancers($5 kb) used by RIPPLE (Roy et al., 2015). Therefore, RIPPLE wasalso applied to EE/P data for performance comparisons(Supplementary Methods A.9).Our results show that PEP-Integrate outperforms TargetFinder(EE/P) with higher AUPR on five of the six cell lines (SupplementaryTable S4). PEP-Integrate also achieves higher F1 score and MCC inthree (GM12878, HUVEC and NHEK) and four (GM12878,IMR90, HUVEC and NHEK) cell lines, respectively. PEP-Motif andPEP-Word both achieve the same average of AUPR as TargetFinder(EE/P) across the six cell lines, which is 0.85, but show different advantages. PEP-Motif outperforms TargetFinder (EE/P) inGM12878, HeLa-S3 and HUVEC on AUPR, while PEP-Wordachieves higher AUPR than TargetFinder (EE/P) in HeLa-S3,HUVEC and NHEK. Additionally, PEP-Motif, PEP-Word and PEPIntegrate outperform RIPPLE in all six cell lines on almost allmetrics.2.5 Important TFBS motif-based features discoveredby PEP-MotifTo assess the contribution of features in PEP-Motif, we ranked themotif features according to their importance estimated by GTB.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018Exploiting sequence-based features for predicting EPIsWe categorized the motif features into different importance levelsbased on quantiles spaced at 5% of the feature importance distribution in each cell line. For instance, motif features ranked top 5% areconsidered as the most important. Each motif corresponds to twomotif features (enhancer-associated or promoter-associated). Thetop 5% important features are thus taken from the union of enhancer and promoter features. We observed that top-ranked motif features are mostly cell-type specific. Specifically, 135 motif features inenhancers and 38 motif features in promoters are ranked top 5% inimportance in one cell line. However, some of these cell-type specificimportant motifs share highly similar Position Weight Matrices(PWMs) and can be grouped. Utilizing motif similarities we developed a motif clustering approach (Supplementary Methods A.2),which resulted in 503 motif representatives, including 427 singlemotifs and 76 small size clusters (each with 2–4 motifs). A motif representative denotes a single motif or a motif cluster.We next assessed motif feature importance at the level of motifrepresentatives to have more robust evaluation of cell-type specificimportant features. For every motif cluster in each cell line, the highest feature importance of the member motifs is selected to representimportance of the associated motif cluster. We found 139 motif representatives in enhancers and 48 in promoters, respectively, thati255have top 5% feature importance in at least one cell line. Amongthese most predictive features, we discovered 98 motif representatives in enhancers and 26 in promoters that have top 5% importancein only one cell line. For more stringent discrimination of the celltype specificity of these motif representatives, we further examinedfeature importance of motifs close to them in the constructed similarity graph. For each member of a motif representative, wesearched for all motifs within distance of 2. If none of such neighboring motifs has top 5% feature importance in other cell lines, theexamined motif representative is regarded as cell-type specific.Using this approach, we found 79 cell-type specific top 5% important motif representatives in enhancers and 17 in promoters(Supplementary Tables S5 and S6).In addition to cell-type specific ones, some motif representativesare recognized by multiple cell lines as highly predictive. We foundthat 62 top 5% predictive motif representatives in enhancers and 29in promoters are shared by at least two cell lines. Moreover, 31motif representatives in enhancers and eight in promoters rank top10% in at least four of the six cell lines. We also noticed that somecell-type specific top 5% important motif features have predictiveeffect in other cell lines, though at lower importance levels, whichcan be observed from Figure 3.Fig. 3. Estimated feature importance of motifs in PEP-Motif that have top 5% importance in at least one cell line. The feature importance is scaled between 0 (lowimportance) and 1 (high importance). Of the 503 motif representatives (427 single motifs and 76 motif clusters) found by PEP-Motif, 139 in enhancers and 48 inpromoters have top 5% feature importance in at least one cell line. Here we display the top 100 of 139 predictive motif representatives in enhancers and all 48 predictive motif representatives in promoters. Each motif is represented by the name of its corresponding TF. If a TF has multiple associated motifs, alternativemotifs are marked according to their identities in the database [e.g. EHF(S) denotes a single site motif of EHF (Kulakovskiy et al., 2016)]. If a motif represents amotif cluster, names of all the member motifs are shown in combination. We performed hierarchical clustering on both motifs (rows of the feature importancematrix) and cell types (columns) to have the motif features grouped. A cell is highlighted with white border if the corresponding motif has top 5% feature importance in the respective cell typeDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018i256TFs associated with predictive motifs or motif representativesmay belong to a TF family. We found that some TF families are enriched with highly predictive TFs. FOX, STAT, TEAD, IRF, SOX,SPI, E2F, ERR, ETS and SP/KLF are among those families. For example, motifs of FOXA1, FOXO3, FOXG1, FOXH1, FOXO1 andFOXO4 each have top 5% feature importance but in different celllines. All are from the FOX family, which is featured by a conservedDNA-binding domain and important in regulating cell growth.Overall, PEP-Motif systematically estimated predictive power of allTFBS motifs for EPIs from one of the most complete and up-to-datemotif databases. Results in six cell lines reveal either cell-type specific or more ubiquitous feature importance of different TF motifs.2.6 Estimated important TFBS motifs are consistentwith existing studiesBased on feature importance estimation for motif based features fromPEP-Motif, we found that the highly predictive motif features are generally quite consistent with existing studies. For example, we foundthat the CTCF motif in enhancer regions (extended with flanking regions) is among the top 5% most important features across all of thesix cell lines, reflecting the fact that CTCF is a key player in mediatingchromatin loops (Ong and Corces, 2014; Bonev and Cavalli, 2016).In addition, ZNF143 has been identified by recent studies as achromatin-looping factor with sequence specificity dependency atpromoters (Bailey et al., 2015), where ZNF143 binds directly to promoters and contributes to chromatin interactions connecting promoters to distal regulatory elements. Other studies have stronglyimplicated that ZNF143 partners with CTCF in establishing the conserved chromatin structure by cooperating with cohesin (Ye et al.,2016). The results of PEP-Motif demonstrated that ZNF143 motif inpromoters ranks top 30% in estimated feature importance across fourcell lines (GM12878, K562, HeLa-S3 and NHEK), with much higherfeature importance than ZNF143’s importance in enhancers, supporting that it functions mainly through promoters.The important TF motifs estimated by PEP-Motif (which uses sequence features only) are also quite consistent with the results fromTargetFinder (where functional genomic signals including many TFChIP-seq data were used). TargetFinder has 209 types of functionalgenomic features in total across all cell lines for a single region (enhancer/window/promoter), of which 162 are TF ChIP-seq signals.However, not all the TFs used by TargetFinder (E/P/W) with ChIPseq data have binding site models in HOCOMOCO Human v10motif database (Kulakovskiy et al., 2016) used by PEP-Motif. Wetherefore focused on the TFs shared by both methods to have a faircomparison. Similar to the feature ranking approach used in PEPMotif, we categorized all the features in TargetFinder (E/P/W) intodifferent importance levels based on quantiles spaced at 5% of thefeature importance distribution estimated in (Whalen et al., 2016).We found that there are 60, 59, 29, 7, 6 and 1 comparable TFs (i.e.shared between TargetFinder and PEP-Motif) in GM12878, K562,HeLa-S3, IMR90, HUVEC and NHEK, respectively, mainly becauseGM12878, K562 and HeLa-S3 cell lines have more available functional genomic features than the other three cell lines. Recall thatTFs in enhancer/window regions and those in promoter regions areconsidered distinct features. For enhancer/window, on average morethan 95% of the comparable TFs with medium or high feature importance (ranked top 50%) in TargetFinder (E/P/W) are also at top50% importance level estimated by PEP-Motif (SupplementaryTable S7). At the high importance level, there are 21, 27 and 12 TFsestimated as top 25% in importance by both methods in GM12878,K562 and HeLa-S3, respectively (Supplementary Table S8). AmongY.Yang et al.the TFs identified by both methods as important features, ZNF384,TBP, RUNX3, SPI1, SP1, CEBPB, SRF, JUND and MAX may havereceived less attention on their functions in mediating EPIs, aspointed out in TargetFinder (Whalen et al., 2016). In PEP-Motif,motifs for these TFs in enhancers were all estimated as top 30%most important in multiple cell lines. We also identified commonpromoter-associated important TFs for the two methods(Supplementary Table S9).We next explored the interacting enhancer-associated motif features and promoter-associated motif features (E-P feature interactions), utilizing the feature combination structures learned by thedecision trees of the GTB model. As an ensemble of decision trees,the GTB model is able to learn high order of feature interactions(Friedman, 2001, 2002). We utilized the method XGBFIR(Kostenko, 2016) to analyze the GTB model in PEP-Motif and extracted predictive feature interactions in each cell type. Results areshown in Supplementary Table S10 and Figures S8–S10. We foundthat existing studies support some of our predicted feature interactions. For examples, the interaction between CTCF in enhancerregion and CTCF in promoter region has been identified as important feature interaction across multiple cell lines (SupplementaryTable S10), which reflects the role of CTCF in mediating chromatinloops (Ong and Corces, 2014; Bonev and Cavalli, 2016).Additionally, we found many other TF motif features involved inpredictive feature interactions, although the overall significance ofthese pairs have yet to be determined (Supplementary Table S10 andFig. S8–S10). Nevertheless, the feature interactions detected fromour model can be used to further study more complicated combinations of TFs in mediating EPI.2.7 Potentially important TFs identified only byPEP-MotifPEP-Motif also identified potentially important TF motif featuresthat may play key role in mediating EPIs but were not studied byTargetFinder due to data unavailability. To mitigate variance due tosmall training sample size in a single cell line, we considered onlyfeatures found to be important in at least two cell lines at specifiedfeature importance levels. With all the TF features used by bothTargetFinder and PEP-Motif in any of the six cell lines excluded, wediscovered 24 enhancer-associated TFs and six promoter associatedTFs that have top 5% feature importance in at least two cell lines(Table 1). There are six enhancer-associated TFs that have top 10%feature importance in at least four cell lines, including ANDR (clustered with GCR and PRGR in the motif similarity graph), EGR3,EHF, ETV5, HAND1 and ZSC16. In particular, EHF is ranked top25% in HeLa-S3 and top 5% in the other five cell lines. Among thehighly predictive promoter-associated TFs, MNT (clustered withSPIC) is ranked top 5% in three cell lines (GM12878, K562 andHeLa-S3). HOXD8 (clustered with POU5F1B, POU2F2 andPOU3F3) and KLF4 (clustered with KLF1 and KLF3) are bothranked top 15% in five cell lines. There is experimental evidencefrom existing research regarding the functions of some of these TFs.For example, recent studies showed that KLF4 plays an importantrole in organizing long-range chromatin interactions with POU5F1(OCT4) by recruiting cohesin to POU5F1 enhancer (Wei et al.,2013). In addition, it was reported that POU2F2 can interact withANDR, PRGR and GCR (also named AR, PGR and NR3C1)(Prefontaine et al., 1999), consistent with the estimated importance´of these TFs by PEP-Motif in forming EPIs. Although the exact functions of these TFs in forming EPIs have yet to be further investigated, our results suggest that PEP-Motif can identify potentiallyDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018Exploiting sequence-based features for predicting EPIsTable 1. Important TF motifs discovered by PEP-Motif (but not byTargetFinder) to be of top 5% feature importance in at least twocell lines (the upper part) and those of top 10% feature importancein at least three cell lines (the lower part)Cell lines2 (E)3 (E)2 (P)3 (P)Potential novel predictive TF of top 5% importance(in PEP-Motif but not in TargetFinder)(PRGR, ANDR, GCR), BHE23, (TBX4,TBR1,TBX21),CEBPE, EGR3, ENOA,(EVX1, FOXG1,FOXL1),FOXO1,HAND1, HBP1, HOXA1, MCR, (NFAT5,NFAC3),STAT6,(NR1I2, NR1I3), RHXF1, SMAD3, SOX10, SOX4,BRAC, TEAD3, ZNF713EHF, ZSC16AP2D, (KLF4, KLF1,KLF3), NKX32, PLAG1(HOXD8, POU5F1B, POU2F2, POU3F3), (MNT,SPIC)Cell linesPotential novel predictive TF of top 10% importance(in PEP-Motif but not in TargetFinder)3 (E)BHE23, ERR2, ENOA, EOMES, FEV, GCM1,(HSF1, HSF2, HOXB2, HSF4), HOXA1, (KLF3, KLF1,KLF4),MCR, MITF, MYOD1, (RORG, RORA, NR1D1),(NR1I2, NR1I3), SMAD3, TEAD3, TF7L2, ZBT18,ZNF713(ANDR, PRGR, GCR), EGR3, ETV5, HAND1, ZSC16EHF(HOXD8, POU5F1B, POU2F2, POU3F3), PLAG1(KLF4, KLF1, KLF3), (MNT, SPIC)4 (E)5 (E)3 (P)4 (P)Note: Each TF is represented by one or multiple motifs. If the corresponding motif is associated with a motif cluster, all members of the cluster are displayed and the motif reaching the speciﬁed importance level is in italic. ‘E’represents enhancer regions and ‘P’ represents promoter regions. The rowname represents the exact number of cell lines where the motif reaches thespeciﬁed importance level, e.g. ‘2 (E)’ denotes that the feature in the enhancerregion has top 5% importance in exactly two cell lines.important TF motifs based on known PWMs without using ChIPseq data, further highlighting its ability to discover key sequence features that may be involved in mediating EPIs.2.8 PEP-Word features are informative inidentifying EPIsDifferent from PEP-Motif, PEP-Word uses word embedding modelto directly embed sequences into a numerical feature space, withoututilizing information of TF binding motif models. Therefore, the extracted features are abstract and not limited to explicit associationwith certain TFBS. We then assessed whether these features are informative for discriminating between EPIs and non-EPIs.We used t-Distributed Stochastic Neighbor Embedding (t-SNE)(Van Der Maaten, 2014) to visualize the feature vectors of randomlyselected positive samples (EPI) and negative samples (non-EPI)(Supplementary Fig. S6). The n dimensional (n ¼ 600) features werereduced to two dimensions. We found that the positive samples andnegative samples exhibit visibly different distributions in thetwo-dimensional space, even though dimension reduction for visualization causes some information loss of discriminative features. Wefurther examined the distribution of positive and negative sampleson the original dimensions of the embedded feature space(Supplementary Fig. S7) and found that the features are quite different between the two classes. The results suggest that EPIs and nonEPIs are distributed differently in the new feature space constructedfrom the word embedding model of PEP-Word. Therefore, PEP-i257Word is shown to be effective in feature representation of EPIs usingonly DNA sequences.3 Methods3.1 DatasetsIn this work we used the same datasets in TargetFinder (Whalenet al., 2016), which include enhancer–promoter interaction data in sixcell lines (GM12878, K562, IMR90, HeLa-S3, HUVEC and NHEK).The dataset of each cell line has cell-line specific annotations of putative enhancers and promoters as well as interacting and noninteracting enhancer–promoter pairs. Active enhancers and promotersin each cell line were identified using annotations from the ENCODEProject (ENCODE Project Consortium et al., 2012) and the RoadmapEpigenomics Project (Kundaje et al., 2015). The Hi-C data (Raoet al., 2014) were used to annotate EPIs in (Whalen et al., 2016). Foreach positive sample, 20 negative samples were sampled, followingthe same constraint of distance between the positive pairs (Whalenet al., 2016). Thus, the negative sample size is 20 times of the positivesample size on each cell line. The distance between the paired enhancer and promoter in the datasets ranges from 10 kb to 2 Mb.In our performance evaluation, we used both E/P (Enhancer/Promoter) data and EE/P (Extended Enhancer/Promoter) data definedin Whalen et al. (2016) for the six cell lines. In EE/P data, an extendedenhancer is defined as an enhancer with 3 kb flanking regions on bothsides (Whalen et al., 2016) and interactions are identified on basis ofextended enhancers and promoters. The numbers of interacting andnon-interacting enhancer–promoter pairs in each of the six cell linedatasets are given in Supplementary Tables S1 and S2.3.2 Classification using gradient tree boosting modelIn PEP, we use Gradient Tree Boosting (GTB) as the predictivelearning algorithm to predict EPI based on feature representationsgenerated by PEP-Motif or PEP-Word. GTB is a specializedGradient Boosting Machine (GBM) (Friedman, 2001, 2002) withdecision trees used as base learners. Given the training samplesfxi ; yi gN , the GTB model aims to learn a mapping function FÃ ðxÞi¼1from x to y. Here xi is the feature vector of the i-th sample and yi isits label (see Supplementary Methods A.1 for details). In our studythe prediction of long-range EPI is formulated as a two-class classification problem. Numerical features are extracted from sample DNAsequences of the paired regions based on either TFBS motifs (forPEP-Motif) or word embedding model (for PEP-Word). We used theXGBClassifier implemented by the XGBoost learning library (Chenand Guestrin, 2016a, 2016b) to train the model. We performed10-fold cross validation to train and tune a classifier on each cellline. The classifier estimates the probabilities of a sample belongingto the two classes. We merged predictions on the test data of eachfold, obtaining predictions on the whole dataset. As the positive andnegative samples are heavily imbalanced, when training the classifier, positive samples were given a larger weight (proportional to theratio of negative sample size to positive sample size). Accordingly,several different metrics were used to measure performance duringevaluation. Note that the evaluation and comparisons were performed with consistent 10-fold cross-validation approach for allmethods. More details on the GTB model and the definitions ofevaluation metrics are in Supplementary Methods A.1 and A.9.3.3 Feature extraction for TFBS motifs in PEP-MotifThe sequences for enhancer/promoter annotations used in this studyare based on human genome assembly hg19. In Whalen et al. (2016)Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018i258Y.Yang et al.the authors found that epigenetic markers within the window between an enhancer–promoter pair are highly useful in predicting theinteractions, and much of the prediction strength can be recoveredby only exploiting 3 kb flanking regions of the enhancers on bothsides. We used the 4 kb extended flanking regions of enhancers toextract sequence-based features by balancing of computational efficiency and performance robustness, although we also evaluated different lengths of flanking regions (Supplementary Methods A.6 andFig. S5). We used FIMO (Grant et al., 2011) for motif scanningalong the sequences with the PWMs from HOCOMOCO Humanv10 database (Kulakovskiy et al., 2016), one of the most comprehensive human TF binding motif databases, with 641 nonredundant TF binding PWMs for 601 TFs. We used a P-valuethreshold of 1e-04 to identify motif matches from FIMO. For a certain motif, we computed the normalized motif occurrences for eachof the enhancer or promoter sequences to form the feature vector.ðiÞ ðiÞðiÞGiven M motifs, let l1 ; l2 ; . . . ; lM be the number of occurrences ofthe respective motifs in the i-th sequence with length Li. The featureðiÞðiÞvector of the i-th sequence is f ðiÞ ¼ ðf1 ; . . . ; fM Þ, whereðiÞðiÞfm ¼ lm =Li ; m ¼ 1; . . . ; M. Finally, to formulate the feature representation of an enhancer–promoter pair, we concatenated the feature vectors of the enhancer region and the promoter region.We then estimated feature importance for each of the motif features based on our GTB model. The importance is first estimated fora single decision tree based on how the feature at each split point improves the performance, which is then normalized by the number ofsamples classified by the corresponding split node. The feature importance estimates from individual trees are then averaged across allthe decision trees in the ensemble. Additionally, we explored whetherthe prediction strength of PEP-Motif can be captured by a selectedsubset of the motif features. Features were ranked by their estimatedimportance in descending order. We sequentially added features tothe selection set based on their importance rankings. A subset of features was used for model training and evaluation each time.3.4 Feature extraction in PEP-Word based on wordembedding modelPEP-Word uses a continuous distributed representation of sequencesto extract informative features encoded in the enhancer–promotersequence pairs. PEP-Word is based on word embedding (Mikolovet al., 2013a, 2013b), which was initially developed in the field ofNatural Language Processing (NLP) to obtain continuous distributed representation of words. In our model, a DNA subsequence oflength K (denoted as K-mer) is used as a word. The model is trainedto embed each K-mer into a new n-dimensional feature space.Weighted pooling is then performed to generate feature vectors forsequences of varied lengths. The word embedding model can be presented as a projection matrix of size jVj Â n, where jVj is the vocabulary size and n is the dimensionality of the embedded featurespace. The vocabulary V is the set of all distinct words of interest. Inour case jVj % 4K since our words are K-mers of nucleotides. Eachrow of the matrix represents the embedded n-dimensional featurevector of the corresponding word.We use the Continuous Bag-of-Words (CBOW) method(Mikolov et al., 2013b) to implement word embedding. In CBOW,our goal is to maximize the following objective function:Y Yarg maxpðwt jc; hÞ;(1)hwt 2V c2Cðwt Þwhere h represents the model parameters, wt is the target word, c isa context word of wt, V is the vocabulary, and Cðwt Þ is the set of allthe context words of wt, which depends on the context window size.For example, if 20 words prior to the target word are used as context, the context window size is 20. pðwt jcÞ is the probability ofobserving the target word wt given the context c. Originally, pðwt jcÞis formulated by a softmax function:exp ðvT vwt Þc;Tw2V exp ðvc vw Þpðwt jc; hÞ ¼ P(2)where vwt and vc represent the embedded feature vectors of a targetword wt and a context word c obtained from the word embeddingmodel, respectively. vT vwt is a score measuring the compatibility bectween vc and vwt . However, it is computationally expensive to normalize these probabilities over all word pairs. Instead, negativesampling is used to generate negative samples of word pairs, bywhich a set of negative samples are selected instead of all word pairs.A positive sample ðwt ; cÞ is a pair of a target word and its context.~~A negative sample ðw; cÞ is a pair of a context c and a noise word wnot associated with the context. The problem is then formulated asbinary logistic regression. Our goal is to optimize the word embedding model for better discrimination between positive and negativesamples. Negative sampling is a variation of Noise ContrastiveEstimation (NCE) (Mnih and Kavukcuoglu, 2013). The objectivefunction is to maximize:X(log Qh ðD ¼ 1jwt ; cÞ þkX)~E½log Qh ðD ¼ 0jwi ; cÞ  ; (3)i¼1ðwt ;cÞ2Dwhere wt and c denote the target word and the context, respectively.~~wi represents a noise word and wi $ Pnoise ðwÞ, where Pnoise ðwÞ is theprobability distribution of noise words. k is the number of negativesamples drawn from the noise distribution per positive sample. Drepresents the training data of pairs of target word and context.Qh ðD ¼ 1jwt ; cÞ is the probability that the word pair ðwt ; cÞ isobserved in D, given the parameters h of the word embedding~~model. Qh ðD ¼ 0jwi ; cÞ is the probability that ðwi ; cÞ is not observedin D, given h. The expectation E in the second term of Equation (3)represents the expected log probability of producing a negative sample under the noise distribution. For negative sampling, the objectivefunction takes the following alternative form (Goldberg and Levy,2014), which is approximately equivalent to Equation (3):arg maxhYQh ðD ¼ 1jwt ; cÞYQh ðD ¼ 0jwt ; cÞ;(4)ðwt ;cÞ2D0ðwt ;cÞ2DwhereQh ðD ¼ 1jwt ; cÞ ¼1;1 þ exp ðÀvT t vc Þw(5)1:1 þ exp ðvT t vc Þw(6)Qh ðD ¼ 0jwt ; cÞ ¼D0 represents the set of randomly sampled negative word pairs.In PEP-Word, cell-type specific word embedding models aretrained for both enhancer regions and promoter regions, respectively. Features resulted from the respective models of the paired regions are then concatenated to form a feature representation of thepair. For a given cell line, we extracted the DNA sequences of all theannotated enhancers (or promoters). An overlapping window of sizeK and sliding stride 1 was used to obtain all the K-mers in thesequence sequentially. Thus, a sequence of length L generatesðL À K þ 1Þ K-mers in order, forming a ‘sentence’. Sentences fromall the enhancers (or promoters) were pooled to form a corpus usedDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018Exploiting sequence-based features for predicting EPIsfor unsupervised training of a word embedding model. In our implementation, we set K ¼ 6, the feature vector size n to be 300, and thecontext window size of a target word (K-mer) to be 20. Namely, 20context words of a target word were used in training. We choseK ¼ 6 to keep the balance between computational efficiency and information complexity contained by each K-mer (Supplementary Fig.S5). We also tested by varying the embedding dimension n from 100to 600 and found that setting n ¼ 300 best balanced computationalefficiency and performance (Supplementary Fig. S5).We then used the trained word embedding model to formulatefeature representations of enhancers (or promoters). Similar to PEPMotif, 4 kb flanking regions of the enhancers were included insequence extraction and feature representation. Evaluation with respect to different choices of flanking region size is shown inSupplementary Fig. S5. The feature vector representing the i-th sePjVjquence is: fi ¼ k¼1 aðwk ; si Þvwk , where wk is the k-th word (K-mer)in the vocabulary, aðwk ; si Þ is the weight of wk for sequence si andvwk is the feature vector of wk generated by the word embeddingmodel. In our method aðwk ; si Þ ¼ tfidfðwk ; si ; DÞ based on weightedpooling (see Supplementary Methods A.3).4 Conclusion and discussionAlthough new high-throughput mapping approaches such as Hi-Cand ChIA-PET have become increasingly useful to identify potentiallong-range interactions between enhancers and promoters genomewide, it is still unclear whether the sequence-based features are sufficient to define and predict EPIs. In this work, we have developedPEP to answer this question. We use two different but complementary approaches in PEP, including PEP-Motif, which only uses TFbinding motifs as features, and PEP-Word, which uses word embedding model to extract more generic sequence features. Based on ourresults, we have demonstrated that sequenced-based features alonecan indeed effectively predict EPIs in a cell type if we are given thegenomic locations of putative enhancers and promoters in that particular cell type. We found that features based on TF binding motifsonly (from PEP-Motif) and features based on word embedding(from PEP-Word) can both achieve performance competitive withthe state-of-the-art methods that use non-sequence-based, functionalgenomic signals. In addition, the combined model (from PEPIntegrate) showed further improvement in predicting EPIs. Overall,we demonstrated that PEP is a promising predictive model with thepotential to reveal important sequence-level instructions that guidelong-range gene regulation in the genome. Such a model may be further developed into a powerful tool to predict non-coding variantsthat may disrupt long-range interactions.There are a number of areas that our model can be improved.For example, we have limited ability to interpret the features extracted from word embedding model due to the difficulty in intuitively explaining the embedded space. This is actually also achallenge in the field of NLP even though word embedding strategies have been applied in NLP widely. Indeed, visualization methods such as t-SNE can be used to provide an idea of the embeddedspace and its ability in distinguishing the samples. Therefore, eventhough we have shown that PEP-Word seems to achieve better performance than PEP-Motif in most cell lines, we are not able toclearly identify the sequence features captured by PEP-Word but notby PEP-Motif. Nevertheless, the comparison between PEP-Motifand PEP-Word together with the findings from PEP-Integrate provides useful insights that the important sequence-based features thatdetermine EPIs are likely to be a combination of TF binding motifsand other non-motif sequence features; and such combination mayi259vary across different cell types. Additionally, if we use the PEP classifier trained on one cell line to predict EPIs in another cell line, theperformance is generally quite low. Using functional genomic signals, the recently developed method RIPPLE (Roy et al., 2015)explored cross-cell-type EPI prediction and found that multipletypes of functional genomic signals from multiple cell types can beutilized to make predictions of regulatory interaction in a new celltype. Therefore, it would be interesting to explore new approachesof optimally selecting sequence-based features and functional genomic features in order to achieve the strongest possible predictionsin a different cell type, especially for a new cell type where the functional genomic data have yet to be generated. Such an effort wouldbe highly informative to better understand EPIs and their variationacross different cell types.AcknowledgementWe would like to thank Dechao Tian and Yuchuan Wang for the discussionsand the anonymous reviewers for their suggestions.FundingThis work is supported in part by National Institutes of Health grantHG007352 (to J.M.), and National Science Foundation grants 1054309 and1262575 (to J.M.). R.Z. is supported by Tsinghua University’s Top Open program for his visit to Carnegie Mellon University during summer 2016. S.S. issupported by a National Science Foundation Graduate Research FellowshipDGE-1252522.ReferencesBailey,S.D. et al. (2015) ZNF143 provides sequence speciﬁcity to secure chromatin interactions at gene promoters. Nat. Commun., 2, 6186.Bonev,B., and Cavalli,G. (2016) Organization and function of the 3d genome.Nat. Rev. Genet., 17, 661–678.Chen,T., and Guestrin,C. (2016a) XGBoost. https://github.com/dmlc/xgboost.Chen,T., and Guestrin,C. (2016b) XGBoost: A scalable tree boosting system.In: Proceedings of the 22nd ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, p.785–794. ACM, New York, NY,USA.Davis,J., and Goadrich,M. (2006) The relationship between precision-recalland ROC curves. In: Proceedings of the 23rd International Conference onMachine Learning, p.233–240. ACM, New York, NY, USA.Dixon,J.R. et al. (2015) Chromatin architecture reorganization during stemcell differentiation. Nature, 518, 331–336.ENCODE Project Consortium. et al. (2012) An integrated encyclopedia ofDNA elements in the human genome. Nature, 489, 57–74.Friedman,J.H. (2001) Greedy function approximation: a gradient boostingmachine. Ann. Stat., 29, 1189–1232.Friedman,J.H. (2002) Stochastic gradient boosting. Comput. Stat. Data Anal.,38, 367–378.Goldberg,Y., and Levy,O. (2014) word2vec explained: deriving Mikolovet al.’s negative-sampling word-embedding method. arXiv:1402.3722[cs.CL], https://arxiv.org/abs/1402.3722.Grant,C.E. et al. (2011) FIMO: scanning for occurrences of a given motif.Bioinformatics, 27, 1017–1018.Kostenko,B. (2016) XGBoost feature interactions reshaped. https://github.com/limexp/xgbﬁr.Kulakovskiy,I.V. et al. (2016) HOCOMOCO: expansion and enhancement ofthe collection of transcription factor binding sites models. Nucleic AcidsRes., 44, D116–D125.Kundaje,A. et al. (2015) Integrative analysis of 111 reference human epigenomes. Nature, 518, 317–330.Li,G. et al. (2012) Extensive promoter-centered chromatin interactions provide a topological basis for transcription regulation. Cell, 148, 84–98.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018i260Lieberman-Aiden,E. et al. (2009) Comprehensive mapping of long-range interactions reveals folding principles of the human genome. Science, 326,289–293.Mikolov,T. et al. (2013a) Distributed representations of words and phrasesand their compositionality. In: Advances in Neural Information ProcessingSystems (NIPS), p.3111–3119. Curran Associates, Inc., USA.Mikolov,T. et al. (2013b) Efﬁcient estimation of word representations in vector space. arXiv:1301.3781 [cs.CL], https://arxiv.org/abs/1301.3781.Mnih,A., and Kavukcuoglu,K. (2013) Learning word embeddings efﬁcientlywith noise-contrastive estimation. In: Advances in Neural InformationProcessing Systems (NIPS), p.2265–2273. Curran Associates, Inc., USA.Ong,C.-T., and Corces,V.G. (2014) CTCF: an architectural protein bridginggenome topology and function. Nat. Rev. Genet., 15, 234.Prefontaine,G.G. et al. (1999) Selective binding of steroid hormone receptors´to octamer transcription factors determines transcriptional synergism at themouse mammary tumor virus promoter. J. Biol. Chem., 274, 26713–26719.Rao,S.S. et al. (2014) A 3d map of the human genome at kilobase resolutionreveals principles of chromatin looping. Cell, 159, 1665–1680.Y.Yang et al.Roy,S. et al. (2015) A predictive modeling approach for cell line-speciﬁc longrange regulatory interactions. Nucleic Acids Res., 43, 8694–8712.Sanyal,A. et al. (2012) The long-range interaction landscape of gene promoters. Nature, 489, 109–113.Tang,Z. et al. (2015) CTCF-mediated human 3d genome architecture revealschromatin topology for transcription. Cell, 163, 1611–1627.Van Der Maaten,L. (2014) Accelerating t-SNE using tree-based algorithms.J. Mach. Learn. Res., 15, 3221–3245.Wei,Z. et al. (2013) Klf4 organizes long-range chromosomal interactions withthe oct4 locus in reprogramming and pluripotency. Cell Stem Cell, 13,36–47.Whalen,S. et al. (2016) Enhancer–promoter interactions are encoded by complex genomic signatures on looping chromatin. Nat. Genet., 48, 488–496.Ye,B.-Y. et al. (2016) ZNF143 is involved in CTCF-mediated chromatin interactions by cooperation with cohesin and other partners. Mol. Biol., 50,431–437.Zhang,Y. et al. (2013) Chromatin connectivity maps reveal dynamicpromoter-enhancer long-range associations. Nature, 504, 306–310.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i252/3953972by gueston 07 January 2018
5028881990002	PMID28881990	5028881990	https://watermark.silverchair.com/btx256.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881990.main.pdf	Bioinformatics, 33, 2017, i199–i207doi: 10.1093/bioinformatics/btx256ISMB/ECCB 2017A new method to study the change ofmiRNA–mRNA interactions due toenvironmental exposuresFrancesca Petralia1, Vasily N. Aushev2, Kalpana Gopalakrishnan2,Maya Kappil2, Nyan W. Khin2, Jia Chen2, Susan L. Teitelbaum2,*and Pei Wang1,*1Department of Genetics and Genomic Sciences and 2Department of Environmental Medicine and Public Health,Icahn School of Medicine at Mount Sinai, New York, NY 10029, USA*To whom correspondence should be addressed.AbstractMotivation: Integrative approaches characterizing the interactions among different types of biological molecules have been demonstrated to be useful for revealing informative biological mechanisms. One such example is the interaction between microRNA (miRNA) and messenger RNA(mRNA), whose deregulation may be sensitive to environmental insult leading to altered phenotypes. The goal of this work is to develop an effective data integration method to characterize deregulation between miRNA and mRNA due to environmental toxicant exposures. We will use datafrom an animal experiment designed to investigate the effect of low-dose environmental chemicalexposure on normal mammary gland development in rats to motivate and evaluate the proposedmethod.Results: We propose a new network approach—integrative Joint Random Forest (iJRF), whichcharacterizes the regulatory system between miRNAs and mRNAs using a network model. iJRF isdesigned to work under the high-dimension low-sample-size regime, and can borrow informationacross different treatment conditions to achieve more accurate network inference. It also effectivelytakes into account prior information of miRNA–mRNA regulatory relationships from existing databases. When iJRF is applied to the data from the environmental chemical exposure study, we detected a few important miRNAs that regulated a large number of mRNAs in the control group butnot in the exposed groups, suggesting the disruption of miRNA activity due to chemical exposure.Effects of chemical exposure on two affected miRNAs were further validated using breast cancerhuman cell lines.Availability and implementation: R package iJRF is available at CRAN.Contacts: pei.wang@mssm.edu or susan.teitelbaum@mssm.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionWith the past decades rapid technological advances in genomic profiling, multiple types of genomic profiles can be collected on the sameset of samples. Recently, we carried out an animal study to assess theeffect of three environmental chemicals on miRNA and mRNA activity in mammary tissue (Gopalakrishnan et al., 2017). In this and similar studies, there is increasing interest to characterize changes in theregulatory patterns among different molecular types across differentexperimental conditions. Compared to commonly used marginal analyses, integrative approaches examining interactions often help toreveal more subtle yet biologically important mechanisms. For instance, it is well known that miRNAs drive the development of manydiseases via the regulation of post-transcriptional gene expression(Jansson and Lund, 2012; Nogales-Cadenas et al., 2016). Thus it willbe more powerful to characterize miRNA activities through monitoring the global regulatory system between miRNA and mRNA (Arnerand Kulyte, 2015).´Multiple challenges arose during the construction of the high dimensional miRNA–mRNA interaction networks. First, such analysisinvolves thousands or tens of thousands of genes but a much smallerCV The Author 2017. Published by Oxford University Press.i199This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018i200sample size (n ¼ 20 in the motivating chemical exposure study). Thisproblem, well known in statistics as the ‘large p, small n’ paradigm(Bernardo et al., 2003), arises in many biological applications(Kosorok et al., 2007). Second, samples under different conditionsin the same study often share common properties, and borrowing information across conditions is crucial to maximize the power of theestimation process (Flutre et al., 2013; Li et al., 2011; Petretto et al.,2010). Third, properly incorporating information from existingdatabases in the analysis could greatly enhance the accuracy of theinference (Bernard and Hartemink, 2004; Petralia et al., 2015;Werhli et al., 2007; Yip et al., 2010; Zhu et al., 2008).However, recent methodologies for integrating different genomicprofiles (for example, Mo et al., 2013; Hwang et al., 2005; Ebrahimet al., 2016; Ritchie et al., 2015) either were not designed for characterizing interaction networks, or do not fully address the above challenges, especially the latter two. Recently, random forest basedmethods have been utilized to estimate multiple networks simultaneously (Petralia et al., 2016). In particular, we demonstrated the advantage of joint learning and showed the high performance ofrandom-forest compared to Gaussian graphical models such asDanaher et al. (2014). In this work, we will extend these ideas tomodel the relationship of dependence among miRNAs and mRNAs.Moreover, in Petralia et al. (2015), we introduced a probability sampling scheme to effectively incorporate prior information in buildingrandom forest models. This framework can be effectively modifiedto incorporate existing knowledge on miRNA–mRNA regulatory relationships documented in miRNA–mRNA databases (Agarwalet al., 2015; Betel et al., 2010; Hsu et al., 2010; Kertesz et al.,2007).Therefore, in this paper, we propose a new method—integrativeJoint Random Forest (iJRF), which borrows information across multiple chemical exposure conditions and takes into account prior information from existing databases when inferring miRNA–mRNAinteractions. iJRF is built upon our two previous random-forestbased algorithms (Petralia et al., 2015, 2016). The advantages ofour integrative framework are multiple. First, its ensemble natureallows the delivery of excellent performance with moderate samplesize requirements. Second, treatment-specific tree ensembles are designed to share common structures, so that miRNAs regulationsplaying a crucial role in multiple conditions will be detected moreaccurately. Third, existing databases are utilized in order to prioritize miRNA–mRNA interactions. Specifically, TargetScan (Agarwalet al., 2015) was chosen over other databases (Betel et al., 2010;Kertesz et al., 2007) for its comprehensive list of predicted miRNA–mRNA interactions and, as mentioned in Lee et al. (2015), its consistency with databases containing experimentally validated targets(Farazi et al., 2014; Helwak et al., 2013).Applying iJRF to the chemical exposure study, we simultaneously estimated miRNA–mRNA interaction networks for the control group and three chemical exposed groups: diethyl phthalate(DEP), methyl-paraben (MPB) and triclosan (TCS). We found thatthe interaction among miRNAs and mRNAs was greatly reduced inthe chemical exposed groups compared to the control group.Among all chemicals, DEP exposure was associated with the highestloss of connectivity. iJRF also detected two important miRNAs:miR-200a and miR-375, that played crucial roles in the inferredregulatory network of the control group, but lost more than 90%connectivity in the network corresponding to the DEP exposure group. mRNAs connecting with miR-200a and miR-375 in control network only, were enriched for the ‘Mammary GlandDevelopment’ and ‘Gland Morphogenesis’ pathways. This suggestsa mediating role for these miRNAs associated with chemicalF.Petralia et al.exposure in mammary gland development. We then confirmed theeffect of DEP on miR-375 and miR-200a using human breast cancercell lines.2 Materials and methods2.1 Random forest for network constructionRandom forest is a non-linear algorithm that models the responsevariable via a series of decision trees where each tree is constructedbased on a random subset of samples (Breiman, 2001). At eachnode, a random subset of predictors is considered and the predictormaximizing a certain utility function (i.e. decrease in node impurity)is chosen to split observations into two subsets. Recently, HuynhThu et al. (2010) introduced GENIE3, a random forest based modelfor inferring gene regulatory networks (GRNs). In GENIE3, first,the expression of each target gene k is modeled as a function of theexpression of all other genes via random forest, then, the regulatoryevents fðj ! kÞgj¼k are ranked based on random forest importancescores. Specifically, the importance score Ij!k is defined as the totaldecrease in node impurities from splitting on the jth predictor, averaged over all trees. Recently, Petralia et al. (2015) proposediRafNet—a new random-forest based algorithm for network construction which can integrate prior information from database andindependent datasets. According to iRafNet, potential regulatorsconsidered important by other datasets are prioritized and sampledmore often within the random-forest framework. Petralia et al.(2016) extended the original random-forest algorithm to estimatemultiple related networks (JRF). As shown by Petralia et al. (2016),borrowing information across multiple networks is crucial to accurately detect common mechanisms. In particular, information acrossdifferent class of data is borrowed by using the same splitting variables for the tree construction. In this paper, JRF and iRafNet arecombined to jointly estimate miRNA–mRNA interactions from different exposure conditions while integrating information from existing databases.2.2 iJRF: integrative joint random forestWe are interested in inferring miRNA - mRNA interactions in tissues samples for control and three common environmental chemicals: diethyl phthalate (DEP), methyl paraben (MPB) and triclosan(TCS). Denote g ¼ 1; 2; . . . ; G as the index over different treatmentconditions. For each treatment condition g, we observe the expression of M miRNAs and p mRNAs for ng samples. Denote yg and xgijikas the expression of the kth mRNA and the jth miRNA for the ithsample exposed to the gth treatment.An overview of the proposed algorithm is shown in Figure 1.Specifically, for each treatment condition g, the expression of thekth mRNA is modeled as a function of the expression of miRNAsgggvia random forest, i.e. yik ¼ fgk ðxi1 ; . . . ; xiM Þ. Therefore, for eachtarget mRNA, G random forest models corresponding to G treatment conditions are constructed. The key idea of iJRF is to buildG tree ensembles simultaneously. Let sg denote the current node inthe gth tree model corresponding to the gth condition. As illustratedin Figure 1, the allocation processes for fsg gG are performed simg¼1ultaneously through the following steps:1. iRafNet Step (Petralia et al., 2015): For different tree ensemblescorresponding to different treatments, the same set of predictors(miRNAs) are proposed for the splitting rule of nodes fsg gG .g¼1This subset of predictors is selected by prioritizing miRNAs thathave similar sequences to that of the target mRNA and, thus, aremore likely to bind to the target mRNA. Speciﬁcally, we sampleDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018iJRF to estimate miRNA-mRNA interactionsa set S s containing N miRNAs from the entire set of miRNAswith probabilities01B sCsBCðp1 ; . . . ; pM Þ ¼ B M 1!k ; . . . ; MM!k C@PAPs‘!ks‘!k‘¼1‘¼1fs‘!k gM‘¼1are derived based on prior information onwhere scoresmiRNA–mRNA regulations from existing databases. In this study,TargetScan (Agarwal et al., 2015) was chosen over other databasesgiven its comprehensive list of predicted miRNA–mRNA interactions. Also, as mentioned in Lee et al. (2015), TargetScan is consistent with databases containing experimentally validated targets(Farazi et al., 2014; Helwak et al., 2013). For each interactionj ! k, TargetScan provides a context score cj!k based on sequencesimilarity (Garcia et al., 2011). Context scores are non-positivewith more negative values corresponding to more favorable sites.We then calculate: sj!k ¼ exp facj!k g with a ¼ log ð2Þ=b and bccbeing the minimum context score. Scores fsj!k g take values in theinterval [1, 2] and the probability to sample the miRNA with themost similar sequence to that of the target mRNA will be twicethe probability of the least similar miRNA (context score equalto zero). This transformation was considered in order to not excessively penalize interactions that are not contained in priordatabases.2.JRF Step (Petralia et al., 2016): Among predictors contained insubset S s , the optimal splitting variable of nodes fsg gG is theg¼1predictor maximizing the summation of the decrease in nodeimpurity across different treatment conditions, i.e.,i201kÃ ¼ arg maxj:j2SssGX Csgjg¼1ngswith Cj g being the decrease in node impurity observed in the gth treeafter splitting sg based on the jth predictor. Specifically, the decreasesssin node impurity is defined as Cj g ¼ ðv½P sg   À v½Lj g   À v½Rj g  Þ wheresgvðAÞ is the variance of observations allocated to set A; P is the setof samples allocated to node sg in the gth tree ensemble; while setsssLj g and Rj g are respectively the sets of samples allocated to the leftchild and right-child of node sg according to a splitting rule basedon the jth predictor.Once G random forest models are constructed for the kthmRNA, interactions between miRNAs and the kth mRNA underdifferent conditions are ranked based on random forest importancescores. In order to derive the final unweighted networks, a propercut-off value for importance scores needs to be chosen. Specifically,we follow the same permutation based procedure described inPetralia et al. (2016), whose details are provided in SupplementarySection 1.The computational complexity of the proposed algorithm is inthe same order as the complexity of JRF (Petralia et al., 2016), i.e.POðpTN G log ðng Þng Þ with T being the number of random forestg¼1trees. The computational burden can be greatly reduced by estimating in parallel random forest models corresponding to different target mRNAs (further information can be found in SupplementarySection 2.1). In practice, the number of trees (T) and the number ofpotential regulators to be sampled at each node (N) are parametersto be specified by the users. In this paper, we used the conventionalpﬃﬃﬃﬃﬃchoice of T ¼ 1000 and N ¼ M with M being the number of predictors (Breiman, 2001).Fig. 1. Joint Random Forest with iRafNet sampling scheme. For each exposure condition, model the expression of mRNAs as function of the expression ofmiRNAs via random forest. At each node, sample miRNAs prioritizing those present in TargetScan (Agarwal et al., 2015). Following JRF model (Petralia et al.,2016), the four random forest tree ensembles (Control, DEP, MPB and TCS) use the same splitting variables (miRNAs) to build trees. In this way we achieve borrowing information across them. This procedure is repeated for each mRNA and, then, interactions are ranked based on random forest importance scoresDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018i2023 Data3.1 Data overviewExposure to environmental chemicals, especially during mammarygland development, has been linked to breast cancer risk using animal models (Manservisi et al., 2015; Moral et al., 2008; Rudelet al., 2011; Russo et al., 2001). Increased understanding of the biological and genomic mechanisms mediating the effects of chemicalexposures can lead to better prevention and treatment of the disease.In a recent pioneering study using a Sprague-Dawley (SD) rat model,animals were exposed chronically from birth through adulthood(postnatal day 146) to three environmental chemicals present indaily personal care products: diethyl phthalate (DEP), methyl paraben (MPB) and triclosan (TCS) at an exposure level comparable tothose observed in human population (Teitelbaum et al., 2016). Thestudy involved 20 rats in each of the following groups, control, DEPand MPB treatment groups, as well as 15 rats in the TCS treatmentgroup. Gopalakrishnan et al. (2017) provides a detailed descriptionof the animal study and the chemical treatment for MPB and TCSexposure. Likewise other chemicals, diethyl phthalate (DEP) (CAS #84-66-2, lot # STBB0862V, 99% purity) was supplied in plastic containers (Sigma Aldrich, Italy). The experimental oral dose of DEPwas 0.1735 mg/Kg/day, which represented 1=1; 000 of no observedadverse effect levels (NOAEL) of DEP (Brown et al., 1978; Moodyand Reddy, 1978; Oishi and Hiraga, 1980).3.2 Data processingThe expression matrices of both mRNAs and miRNAs can be foundin the GEO database (ID: GSE72276) (Barrett et al., 2005). Qualitycontrol of.CEL files and preprocessing based on the robust multiarrayaverage method (RMA) were done using the expression console software (Affymetrix, CA). For mRNA data, batch effects were removedusing the ComBat package (Leek et al., 2012) available in R CRAN.We applied a signal intensity filter to retain only those probesets withhigh and stable expression (signal value > 30th percentile in at least 1of the experimental groups). A variance-based filter was used to retainthe top 50% of the probesets with high interquartile range. FormiRNA data, 283 miRNAs were profiled and only miRNAs withvariance different from zero were considered. The data were normalized using the package NanoStringNorm available in R CRAN(Waggott et al., 2012). The filtered data contained 7546 genes and272 miRNAs which were used for downstream analyses. For bothmiRNA and mRNA data matrices, quantile normalization acrosssamples was performed.4 Results4.1 Network estimationFor ease of explanation, we will refer to the network from controlrats as Control-Net and networks from different chemical exposedrats as DEP-Net, MPB-Net and TCS-Net. For the analyses, we considered 7546 messenger RNAs and 272 miRNAs. As mentioned insection 3, the sample size was 20 each for the control, DEP andMPB treatment groups, and 15 for TCS. In order to implement theproposed algorithm, 1000 trees were considered and a total of Npﬃﬃﬃﬃﬃﬃﬃﬃ¼ 272 miRNAs (predictors) were sampled at each node (Breiman,2001). The four networks were estimated using iJRF and mRNAmiRNA interactions were derived using permutation techniquesconsidering an FDR cut-off of 0.001 (further information can befound in Supplementary Section 1).Table 1 shows the total number of interactions inferred for eachnetwork as well as the number of interactions shared acrossF.Petralia et al.networks. As shown, all three chemicals result in a loss of interaction compared to Control-Net which involved 6829 edges linking47 miRNAs and 2270 mRNAs. In particular, the total number ofinteractions in DEP-Net, MPB-Net and TCS-Net were respectively44%, 84% and 52% of the total number of interactions in ControlNet. Therefore, DEP was the chemical exposure resulting in themost dramatic loss of interaction compared to control.Figure 2(a) shows the top 10 hub-miRNAs in Control-Net,which were responsible for more than 85% of connecting edges inControl-Net. In particular, a substantial portion of those interactions (> 65%) were not present in any of the chemical-networks.Figure 2(b) compares each chemical-Net and Control-Net showing,for each miRNA, the number of edges shared between chemical-Netand Control-Net (green bar), the number of control-specific edges(blue bar) and the number of chemical-specific edges (red bar). Thethree quantities have been normalized dividing them by the totalnumber of connecting edges present in either Control-Net orchemical-networks. As shown, DEP-Net has miRNAs such as miR375-3p, miR-200a-3p and miR-214-3p with remarkable loss in connectivity (> 90%) compared to Control-Net. Given the dramaticloss in connectivity observed in DEP-Net, we decided to focus onthis chemical and miRNAs miR-375-3p, miR-200a-3p and miR214-3p for further investigation. Besides the consistent loss of connection in chemical-networks compared to Control-Net, differencesamong chemical-networks were also observed, as shown inSupplementary Section 2.2. Further investigation is needed to understand the biological implication of these differences across chemicalnetworks.It is important to note that, no significant results were detectedby traditional univariate analysis (i.e. unpaired t-test, Wilcoxon test)when testing was conducted on one miRNA at-a-time, suggestingthe advantage of the proposed network based approach (further information on univariate analysis based on the Wilcoxon test can befound in Supplementary Section 3.1). As a comparison, we also estimated miRNA–mRNA interactions under different treatment conditions via Pearson’s correlation test, a commonly used approach inthe literature (Mertins et al., 2016; Zhang et al., 2014, 2016). Asshown in Supplementary Section 3.2, when considering the sameFDR cut-off (fdr ¼ 0.001) as in the iJRF analysis, the correlation testdetects far fewer edges than iJRF, and fails to reveal any informativehub structure or pathway enriched network module. We thenrelaxed the FDR cut-off to 0.01 for the correlation tests to obtainmore connected networks, and tested for enriched GO terms. Asshown in Supplementary Figure S6, the correlation test resulted infar fewer enriched GO terms compared to iJRF. This suggests thatiJRF is more effective to detect biologically relevant interactions.Moreover, the percentage of shared edges across networks based oncorrelation tests is much smaller than that based on iJRF, whichmakes the detection of treatment-specific interactions particularlyvulnerable to high false positive rates. This result is expected sinceiJRF, through joint learning, is more effective in detecting commonTable 1. Number of interactions inferred in Control-Net, DEP-Net,MPB-Net and TCS-Net and number of interactions shared acrossnetworksControlControlDEPMPBTCSDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018DEPMPBTCS682920793018359384257432374163034913557iJRF to estimate miRNA-mRNA interactionsi203(a)(b)Fig. 2. (a) Plot of top ten hub-miRNAs in Control-Net with interactions detected only in Control-Net (Shannon et al., 2003). These miRNAs were responsible formore than 85% of connecting edges in Control-Net. The total number of interactions detected in Control-Net was 6829. (b) For each miRNA, we show the numberof edges shared by chemical and control (green bar), the number of control-speciﬁc edges (blue bar) and the number of chemical-speciﬁc edges (red bar). Thethree quantities have been normalized dividing them by the total number of connecting edges in either Control-Net or chemical-networksTable 2. List of interactions in Control-Net contained in miRTarBasefor miR-200a and miR-375miRNAsmRNAsmiR-375miR-375miR-375miR-375miR-200amiR-200amiR-200aHER2, TMTC4, SFT2D2, KRT8PLAG1, CCDC88A, CELF2GATA6CMTM4, FOLR1, CTSCZEB2HOXB5DLC1DEPMPBTCSxxxxxxFor each interaction, we indicate if it was contained in other networks suchas DEP-Net, MPB-Net and TCS-Net.associations than algorithms handling different treatment conditionsseparately (Petralia et al., 2016).4.2 miR-375, miR-214 and miR-200aOverview: One of the top hub miRNAs in Control-Net poorly connected in chemicals was miR-375-3p. In particular, the number ofmRNAs interacting with miR-375-3p in DEP-Net, MPB-Net andTCS-Net was, respectively, 2%, 51% and 27% of the number ofinteracting mRNAs in Control-Net. Recent studies have investigatedthe role of miR-375 in breast cancer (Madhavan et al., 2016; Wardet al., 2013; Zehentmayr et al., 2016). In particular, Ward et al.(2013) reported a loss of miR-375 expression in drug-resistantbreast cancer cells; while Madhavan et al. (2016) showed that miR375 was significantly associated with breast cancer survival. Asshown in Figure 2(b), other miRNAs poorly connected in DEP-Netwere miR-214 and miR-200a. Their role in breast cancer has beeninvestigated in several papers (Kalniete et al., 2015; Ming et al.,2015; Penna et al., 2015; Pieraccioli et al., 2013; Wang et al., 2015;Yao et al., 2014; Yu et al., 2015). In particular, miR-214 was shownto be associated with breast cancer survival (Kalniete et al., 2015)and drug sensitivity (Yu et al., 2015). On the other hand, miR-200ahas been associated with survival in metastatic breast cancer(Madhavan et al., 2016) and its role in cell proliferation inhibitionhas been demonstrated (Yao et al., 2014).Overlap with MirTarBase: Some Control-specific interactions detected by iJRF are contained in miRTarBase (Hsu et al., 2010)—thedatabase of experimentally validated miRNA–mRNA interactions.Table 2 shows the list of interactions in Control-Net contained inmiRTarBase for miR-375 and miR-200a (other interactions containedin miRTarBase can be found in Supplementary Table S2). As shown inTable 2, some of these interactions were contained in DEP-Net, MPBNet and TCS-Net as well. Particularly interesting is the interaction ofmiR-375 with the human epidermal growth factor receptor 2 (HER2)(Pillai et al., 2014; Shen et al., 2014) only contained in Control-Netand MPB-Net. HER2 stimulates the growth of breast cancer cells andis one of the main targets for breast cancer survival and therapy.Another interesting interaction is miR-200a - ZEB2 which has beeninvestigated in different ovarian and breast cancer studies (Ahmadet al., 2011; Bracken et al., 2008; Jang et al., 2014; Park et al., 2008;Truong et al., 2014; Wu et al., 2011). In particular, many articles havedescribed ZEB2 as the crucial target of miR-200 family members(Burk et al., 2008; Christoffersen et al., 2007; Korpal et al., 2008).Brabletz and Brabletz (2010) showed that ZEB2 and miR-200a areinvolved in a ‘feedback loop’ which drives the progression of metastasis in breast cancer. As shown in Table 2, the interaction betweenZEB2 and miR-200a is only contained in Control-Net, suggesting theimpact of all chemicals on the regulatory mechanism between miR200a and ZEB2.Enrichment Analysis: Among hub-miRNAs in Control-Net,interesting pathways were obtained for miR-375-3p and miR-200a3p. Figure 3(a) shows some interesting enriched categories formRNAs connected to miR-375-3p and miR-200a-3p in Control-Netbut not in DEP-Net (enriched pathways for other hub-miRNAs inDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018i204F.Petralia et al.(a)(b)Fig. 3. (a) We consider genes connected to miR-375-3p and miR-200a-3p in Control-Net but not in DEP-Net and derived enriched categories using David Tools(Huang et al., 2008). Pathways ‘Gland Development’, ‘Plasma Membrane’ and ‘Mammary Gland Development’ were enriched for both miR-375-3p and miR-200a3p with Benjamini adjusted p-values smaller than 0.10. Pathway ‘Gland Morphogenesis’ was enriched only for miR-375-3p. (b) Density of absolute correlation between miR-375-3p and miR-200a-3p with mRNAs connected only in Control-Net for DEP exposed data (red) and control data (black)Control-Net can be found in Supplementary Table S3). The enrichment analysis was performed using David Tool 6.7 (Huang et al.,2008) and Benjamini adjusted p-values were reported in Figure 3(a).As shown, miR-375-3p and miR-200a-3p share some enriched categories such as ‘Mammary Gland Development’ with Benjamini adjusted p-values less than 0.1. This result is not surprising since thetwo miRNAs share some connections in Control-Net. Various studies have shown that chemical exposure can alter mammary glanddevelopment (Manservisi et al., 2015; Mandrup et al., 2015;Schwarzman et al., 2015). In this context, DEP exposure might alterthe regulatory mechanism of miR-375-3p and miR-200a-3p and affect mammary gland development. As shown in Figure 3(a), enriched pathways include genes such as ERBB2 (HER2), FOXA1 andSFRP1 which play a crucial role in breast cancer. To further demonstrate the loss in connectivity in DEP-Net, Figure 3(b) shows the correlation density between each miRNA and mRNAs connected inControl-Net but not in DEP-Net. As expected, the correlation density for DEP exposure is shifted to the left compared to that of control revealing a loss of correlation. Supplementary Figure S7 showsthat the loss of correlation observed in DEP-Net for both miR-3753p and miR-200a-3p is significant.4.3 Validation via cell line experimentsIn Section 4.1,we showed that DEP-Net resulted in a dramatic lossof interaction compared to Control-Net. In particular, the threehub- miRNAs poorly connected in DEP-Net were miR-375,miR-214 and miR-200a. To validate the effect of DEP, in vitro experiments of a human breast cancer cell line are utilized.Experiments: MCF-7 cells were maintained in phenol-red-freeDMEM (Gibco #11054) containing 5% (v/v) dextran-charcoalstripped fetal calf serum, with 1 Â 10À5 M diethyl phthalate, for4 weeks. These cells were subcultured every 3-4 days with a confluence level less than 70%. The total RNA was isolated from 10 cmPetri plate using the Promega Maxwell simplyRNA kit according tothe manufacturer’s instructions. Reverse transcription was doneusing the Exiqon Universal cDNA Synthesis kit and the detection ofqPCR was performed with the Exiqon ExiLENT SYBR Green master mix on Roche LightCycler 480 machine.Results: We quantified the expression of miR-200a, miR-375and miR-214 in MCF-7 cells. Unfortunately, miR-214 was not expressed in this cell line, thus only results for miR-200a and miR-375are reported. As shown in Figure 4, the expression levels of bothmiR-375 and miR-200a are significantly different between controland DEP-exposed cells. This result suggests that the two miRNAsare affected by DEP exposure in breast cancer human cell lines.Further analyses are necessary to elucidate the role of these twomiRNAs on mammary gland development.5 DiscussionIn this paper, we focused on exposure to chemicals commonly usedin personal care products during mammary gland developmentDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018iJRF to estimate miRNA-mRNA interactionsFig. 4. Plot of cell line experiments. Expression levels of miR-375 and miR200a for normal and DEP exposed cells. Red diamonds indicate average overthe three replicates. Benjamini adjusted p-values from the unpaired t-test arereported(birth to adulthood) and their effect on miRNA–mRNA interactions. Some challenges arose given the large number of miRNAs/genes involved and the limited sample size of our data. In order toovercome this problem, we proposed iJRF, an ensemble algorithmwith small sample size requirement. The advantage of iJRF is dual.First, it is designed to borrow information across different treatmentconditions so that associations shared across treatments can be detected more accurately. Second, iJRF can integrate information fromexisting miRNA–mRNA databases such as TargetScan.Using our newly developed algorithm, we estimated miRNA–mRNA interaction in mammary tissues for control and chemicalexposed animals. All chemical-networks registered a loss inconnectivity compared to control. In particular, DEP was the chemical registering the most dramatic loss of interaction compared to control. In fact, the total number of interactions in DEP-Net, MPB-Netand TCS-Net were respectively 44%, 84% and 52% of the total number of interactions in Control-Net.Among the leading miRNAs in Control-Net, miR-200a, miR214 and miR-375 lost more than 90% of connectivity in DEP-Netcompared to Control-Net. Messenger RNAs connected to miR-200aand miR-375 in Control-Net but not in DEP-Net were enriched in‘Gland Morphogenesis’ and ‘Mammary Gland Development’, indicating their potential involvement in mammary gland developmentmechanisms. Among genes in these pathways, we found targets inbreast cancer such as ERBB2, FOXA1 and SFRP1. Recent studieshave investigated the role of miR-375, miR-214 and miR-200a inbreast cancer. For example, Ward et al. (2013) reported a loss ofmiR-375 expression in tamoxifen-resistant breast cancer cells; whileMadhavan et al. (2016) reported a significant association betweensurvival in metastatic breast cancer and both miR-375 and miR200a. The expression of miR-214 was shown to be associated withbreast cancer survival (Kalniete et al., 2015) and drug sensitivity (Yuet al., 2015); while Yao et al. (2014) demonstrated the role of miR200a in the inhibition of cell proliferation in breast cancer.Given the dramatic loss of connectivity observed in DEP-Net, wevalidated the effect using cell line experiments. Specifically, wefocused on miRNAs with the highest loss in connectivity: miR-375,miR-214 and miR-200a. Using MCF-7 cells, we showed that the expression levels of both miR-375 and miR-200a were significantlydifferent between the control and DEP exposed groups. In thisi205study, we hypothesized that chemical exposures first affect miRNAactivity, which then changes the regulatory pattern among miRNAsand mRNAs. In our validation experiments, we demonstrated the effects of chemical exposure on miRNA activity. Future research iswarranted to further validate the detected changing of regulatory relationships among miRNAs and mRNAs.In this paper we examined miRNA–mRNA networks for singleexposure conditions. On the other hand, humans are exposed to amixture of different chemicals with the interaction and combinationof chemicals playing a crucial role. In order to deal with such complex data, as future work, we will design a model to estimate networks which vary across exposure conditions in a dynamic way.Once chemical-induced mechanisms will be identified, their association to breast cancer phenotypes will be assessed.Finally, the proposed algorithm can be utilized in different biological applications. As an example, eQTL analysis might be performed for different tissues simultaneously while borrowinginformation from existing databases. It is well known that borrowing information across tissues is crucial to detect shared eQTLsmore accurately (Flutre et al., 2013) and, iJRF is a non-parametricmodel that can be easily implemented for such analyses.AcknowledgementThis work was supported in part through the computational resources andstaff expertise provided by the Department of Scientiﬁc Computing at theIcahn School of Medicine at Mount Sinai.FundingF.P. and P.W. were supported by grant U24 CA210993, from the NationalCancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC)and NIH grants R01 GM108711 and R01 CA189532. The entire experimentwas supported by NIH/NIEHS/NCI grant U01 ES019451. J.C., K. G, V. A.and S. T. were also supported by NIH/NCI grant R01 CA172460.Conﬂict of Interest: none declared.ReferencesAgarwal,V. et al. (2015) Predicting effective microRNA target sites in mammalian mRNAs. Elife, 4, e05005.Ahmad,A. et al. (2011) Phosphoglucose isomerase/autocrine motility factormediates epithelial-mesenchymal transition regulated by mir-200 in breastcancer cells. Cancer Res., 71, 3400–3409.Arner,P. and Kulyte,A. (2015) MicroRNA regulatory networks in human adi´pose tissue and obesity. Nat. Rev. Endocrinol., 11, 276–288.Barrett,T. et al. (2005) Ncbi geo: mining millions of expression proﬁlesdatabase and tools. Nucleic Acids Res., 33, D562–D566.Bernard,A. and Hartemink,A.J. (2004). Informativestructure priors: jointlearning of dynamic. Biocomputing, 2005, 459.Bernardo,J. et al. (2003) Bayesian factor regression models in the large p, smalln paradigm. Bayesian Stat., 7, 733–742.Betel,D. et al. (2010) Comprehensive modeling of microRNA targets predictsfunctional non-conserved and non-canonical sites. Genome Biol., 11, R90.Brabletz,S. and Brabletz,T. (2010) The zeb/mir-200 feedback loop a motor ofcellular plasticity in development and cancer?. EMBO Rep., 11, 670–677.Bracken,C.P. et al. (2008) A double-negative feedback loop between zeb1-sip1and the microrna-200 family regulates epithelial-mesenchymal transition.Cancer Res., 68, 7846–7854.Breiman,L. (2001) Random forests. Mach. Learn., 45, 5–32.Brown,D. et al. (1978) Short-term oral toxicity study of diethyl phthalate inthe rat. Food Cosmetics Toxicol., 16, 415–422.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018i206Burk,U. et al. (2008) A reciprocal repression between zeb1 and members ofthe mir-200 family promotes EMT and invasion in cancer cells. EMBORep., 9, 582–589.Christoffersen,N.R. et al. (2007) mir-200b mediates post-transcriptional repression of zfhx1b. RNA, 13, 1172–1178.Danaher,P. et al. (2014) The joint graphical lasso for inverse covariance estimation across multiple classes. J. R. Stat. Soc. Ser. B (Stat. Methodol.), 76,373–397.Ebrahim,A. et al. (2016) Multi-omic data integration enables discovery of hidden biological regularities. Nat. Commun., 7,Farazi,T.A. et al. (2014) Identiﬁcation of distinct miRNA target regulation between breast cancer molecular subtypes using ago2-par-clip and patientdatasets. Genome Biol., 15, R9.Flutre,T. et al. (2013) A statistical framework for joint EQTL analysis in multiple tissues. PLoS Genet., 9, e1003486.Garcia,D.M. et al. (2011) Weak seed-pairing stability and high target-siteabundance decrease the proﬁciency of lsy-6 and other microRNAs. Nat.Struct. Mol. Biol., 18, 1139–1146.Gopalakrishnan,K. et al. (2017) Changes in mammary histology and transcriptome proﬁles by low-dose exposure to environmental phenols at criticalwindows of development. Environ. Res., 152, 233–243.Helwak,A. et al. (2013) Mapping the human mirna interactome by clash reveals frequent noncanonical binding. Cell, 153, 654–665.Hsu,S.-D. et al. (2010) mirtarbase: a database curates experimentally validated microRNA–target interactions. Nucleic Acids Res., gkq1107.Huang,D.W. et al. (2008) Systematic and integrative analysis of large gene listsusing David bioinformatics resources. Nat. Protoc., 4, 44–57.Huynh-Thu,V.A. et al. (2010) Inferring regulatory networks from expressiondata using tree-based methods. PloS One, 5, e12776.Hwang,D. et al. (2005) A data integration methodology for systems biology.Proc. Natl. Acad. Sci. U. S. A., 102, 17296–17301.Jang,K. et al. (2014) Loss of microrna-200a expression correlates with tumorprogression in breast cancer. Transl. Res., 163, 242–251.Jansson,M.D. and Lund,A.H. (2012) MicroRNA and cancer. Mol. Oncol., 6,590–610.Kalniete,D. et al. (2015) High expression of mir-214 is associated with a worsedisease-speciﬁc survival of the triple-negative breast cancer patients. Hered.Cancer Clin. Pract., 13, 1.Kertesz,M. et al. (2007) The role of site accessibility in microRNA target recognition. Nat. Genet., 39, 1278–1284.Korpal,M. et al. (2008) The mir-200 family inhibits epithelial-mesenchymaltransition and cancer cell migration by direct targeting of e-cadherin transcriptional repressors zeb1 and zeb2. J. Biol. Chem., 283, 14910–14914.Kosorok,M.R. et al. (2007) Marginal asymptotics for the large p, small n paradigm: with applications to microarray data. Ann. Stat., 35, 1456–1486.Lee,E. et al. (2015) Inferred miRNA activity identiﬁes miRNA-mediated regulatory networks underlying multiple cancers. Bioinformatics, btv531.Leek,J.T. et al. (2012) The SVA package for removing batch effects and otherunwanted variation in high-throughput experiments. Bioinformatics, 28,882–883.Li,J. et al. (2011) An adaptively weighted statistic for detecting differentialgene expression when combining multiple transcriptomic studies. Ann.Appl. Stat., 5, 994–1019.Madhavan,D. et al. (2016) Circulating mirnas with prognostic value in metastatic breast cancer and for early detection of metastasis. Carcinogenesis,bgw008.Mandrup,K.R. et al. (2015) Mixtures of environmentally relevant endocrinedisrupting chemicals affect mammary gland development in female andmale rats. Reprod. Toxicol., 54, 47–57.Manservisi,F. et al. (2015) Effect of maternal exposure to endocrine disruptingchemicals on reproduction and mammary gland development in femaleSprague–Dawley rats. Reprod. Toxicol., 54, 110–119.Mertins,P. et al. (2016) Proteogenomics connects somatic mutations to signalling in breast cancer. Nature, 534, 55–62.Ming,J. et al. (2015) Identiﬁcation of mir-200a as a novel suppressor of connexin 43 in breast cancer cells. Biosci. Rep., 35, e00251.F.Petralia et al.Mo,Q. et al. (2013) Pattern discovery and cancer gene identiﬁcation in integrated cancer genomic data. Proc. Natl. Acad. Sci. U. S. A., 110,4245–4250.Moody,D.E. and Reddy,J.K. (1978) Hepatic peroxisome (microbody) proliferation in rats fed plasticizers and related compounds. Toxicol. Appl.Pharmacol., 45, 497–504.Moral,R. et al. (2008) Effect of prenatal exposure to the endocrine disruptorbisphenol a on mammary gland morphology and gene expression signature.J. Endocrinol., 196, 101–112.Nogales-Cadenas,R. et al. (2016) MicroRNA expression and gene regulationdrive breast cancer progression and metastasis in PYMT mice. BreastCancer Res., 18, 75.Oishi,S. and Hiraga,K. (1980) Testicular atrophy induced by phthalic acidesters: effect on testosterone and zinc concentrations. Toxicol. Appl.Pharmacol., 53, 35–41.Park,S.-M. et al. (2008) The mir-200 family determines the epithelial phenotype of cancer cells by targeting the e-cadherin repressors zeb1 and zeb2.Genes Dev., 22, 894–907.Penna,E. et al. (2015) mir-214 as a key hub that controls cancer networks:small player, multiple functions. J. Invest. Dermatol., 135, 960–969.Petralia,F. et al. (2015) Integrative random forest for gene regulatory networkinference. Bioinformatics, 31, i197–i205.Petralia,F. et al. (2016) New method for joint network analysis reveals common and different coexpression patterns among genes and proteins in breastcancer. J. Proteome Res., 15, 743–754. PMID: 26733076.Petretto,E. et al. (2010) New insights into the genetic control of gene expression using a Bayesian multi-tissue approach. PLoS Comput. Biol., 6,e1000737.Pieraccioli,M. et al. (2013) Activation of mir200 by c-myb depends on zeb1expression and mir200 promoter methylation. Cell Cycle, 12, 2309–2320.Pillai,M.M. et al. (2014) Hits-clip reveals key regulators of nuclear receptorsignaling in breast cancer. Breast Cancer Res. Treatment, 146, 85–97.Ritchie,M.D. et al. (2015) Methods of integrating data to uncover genotype–phenotype interactions. Nat. Rev. Genet., 16, 85–97.Rudel,R.A. et al. (2011) Environmental exposures and mammary gland development: state of the science, public health implications, and research recommendations. Environ. Health Perspect., 119, 1053.Russo,J. et al. (2001) Mammary gland architecture as a determining factor inthe susceptibility of the human breast to cancer. Breast J., 7, 278–291.Schwarzman,M.R. et al. (2015) Screening for chemical contributions to breastcancer risk: a case study for chemical safety evaluation. Environ. HealthPerspect., 123, 1255–1264.Shannon,P. et al. (2003) Cytoscape: a software environment for integrated models of biomolecular interaction networks. Genome Res., 13, 2498–2504.Shen,Z.-Y. et al. (2014) mir-375 inhibits the proliferation of gastric cancercells by repressing erbb2 expression. Exp. Therapeutic Med., 7, 1757–1761.Teitelbaum,S.L. et al. (2016) Paired serum and urine concentrations of biomarkers of diethyl phthalate, methyl paraben, and triclosan in rats.Environ. Health Perspect., 124, 39.Truong,H.H. et al. (2014) b1 integrin inhibition elicits a prometastatic switchthrough the tgfb–mir-200–zeb network in e-cadherin–positive triplenegative breast cancer. Sci. Signal., 7, ra15–ra15.Waggott,D. et al. (2012) Nanostringnorm: an extensible r package for the preprocessing of nanostring mRNA and miRNA data. Bioinformatics, 28,1546–1548.Wang,F. et al. (2015) microrna-214 enhances the invasion ability of breastcancer cells by targeting p53. Int. J. Mol. Med., 35, 1395–1402.Ward,A. et al. (2013) Re-expression of microrna-375 reverses both tamoxifenresistance and accompanying emt-like properties in breast cancer.Oncogene, 32, 1173–1182.Werhli,A.V. et al. (2007) Reconstructing gene regulatory networks with bayesian networks by combining expression data with multiple sources of priorknowledge. Stat. Appl. Genet. Mol. Biol., 6, 15.Wu,Q. et al. (2011) Microrna-200a inhibits cd133/1þ ovarian cancer stemcells migration and invasion by targeting e-cadherin repressor zeb2.Gynecol. Oncol., 122, 149–154.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018iJRF to estimate miRNA-mRNA interactionsYao,J. et al. (2014) microRNA-200a inhibits cell proliferation by targetingmitochondrial transcription factor a in breast cancer. DNA Cell Biol., 33,291–300.Yip,K.Y. et al. (2010) Improved reconstruction of in silico gene regulatory networks by integrating knockout and perturbation data. PloS One, 5, e8121.Yu,X. et al. (2015) Mir-214 increases the sensitivity of breast cancer cells totamoxifen and fulvestrant through inhibition of autophagy. MolecularCancer, 14, 208–223.i207Zehentmayr,F. et al. (2016) Hsa-mir-375 is a predictor of local control in earlystage breast cancer. Clin. Epigenet., 8, 1.Zhang,B. et al. (2014) Proteogenomic characterization of human colon andrectal cancer. Nature, 513, 382–387.Zhang,H. et al. (2016) Integrated proteogenomic characterization of humanhigh-grade serous ovarian cancer. Cell, 166, 755–765.Zhu,J. et al. (2008) Integrating large-scale functional genomic data to dissectthe complexity of yeast regulatory networks. Nat. Genet., 40, 854–861.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i199/3953971by gueston 07 January 2018
5028881988002	PMID28881988	5028881988	https://watermark.silverchair.com/btx254.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881988.main.pdf	Bioinformatics, 33, 2017, i161–i169doi: 10.1093/bioinformatics/btx254ISMB/ECCB 2017Discovery and genotyping of novel sequenceinsertions in many sequenced individualsPınar Kavak1,†, Yen-Yi Lin2,†, Ibrahim Numanagi 2, Hossein Asghari2,cTunga Gungor1, Can Alkan3,* and Faraz Hach2,4,5,*¨¨1Department of Computer Engineering, Bo azic University, Istanbul 34342, Turkey, 2School of Computing Science,g ¸iSimon Fraser University, Burnaby, V5A 1S6, Canada, 3Department of Computer Engineering, Bilkent University,Ankara 06800, Turkey, 4Vancouver Prostate Centre, Vancouver, V6H 3Z6, Canada and 5Department of UrologicSciences, University of British Columbia, Vancouver, V5Z 1M9, Canada*To whom correspondence should be addressed.†These authors contributed equally to this work.AbstractMotivation: Despite recent advances in algorithms design to characterize structural variation usinghigh-throughput short read sequencing (HTS) data, characterization of novel sequence insertionslonger than the average read length remains a challenging task. This is mainly due to both computational difﬁculties and the complexities imposed by genomic repeats in generating reliable assemblies to accurately detect both the sequence content and the exact location of such insertions.Additionally, de novo genome assembly algorithms typically require a very high depth of coverage, which may be a limiting factor for most genome studies. Therefore, characterization of novelsequence insertions is not a routine part of most sequencing projects.There are only a handful of algorithms that are speciﬁcally developed for novel sequence insertiondiscovery that can bypass the need for the whole genome de novo assembly. Still, most such algorithms rely on high depth of coverage, and to our knowledge there is only one method (PopIns)that can use multi-sample data to “collectively” obtain a very high coverage dataset to accuratelyﬁnd insertions common in a given population.Result: Here, we present Pamir, a new algorithm to efﬁciently and accurately discover and genotype novel sequence insertions using either single or multiple genome sequencing datasets. Pamiris able to detect breakpoint locations of the insertions and calculate their zygosity (i.e. heterozygousversus homozygous) by analyzing multiple sequence signatures, matching one-end-anchored sequences to small-scale de novo assemblies of unmapped reads, and conducting strand-aware localassembly. We test the efﬁcacy of Pamir on both simulated and real data, and demonstrate its potential use in accurate and routine identiﬁcation of novel sequence insertions in genome projects.Availability and implementation: Pamir is available at https://github.com/vpc-ccg/pamir.Contact: fhach@{sfu.ca, prostatecentre.com} or calkan@cs.bilkent.edu.trSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionGenomic structural variations (SVs) are broadly defined as alterations that affect more than 50 base pairs (bp) of DNA (Alkanet al., 2011), and they have major impact on both evolution andhuman disease (Alkan et al., 2011; Sharp et al., 2006). Such alterations may be in various forms including deletions, insertions, inversions, duplications, and retrotranspositions (Alkan et al.,2011). Thanks to the wide availability and cost efficiency of highthroughput sequencing (HTS), we now have the ability tocharacterize SVs in the genomes of many individuals, as exemplified by large-scale projects such as the 1000 Genomes Project(Mills et al., 2011; The 1000 Genomes Project Consortium, 2015).Accurate characterization of SVs required the development ofmany novel algorithms (Alkan et al., 2011; Medvedev et al., 2009)that are benchmarked within the 1000 Genomes and the Genomein a Bottle (Zook et al., 2014) projects.Novel sequence insertions, or alternatively, “deletions from thereference”, are genomic segments that are not represented in theCV The Author 2017. Published by Oxford University Press.i161This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018i162reference genome assembly (Kidd et al., 2010a). Similar to “deletions from the sequenced sample”, they may harbor sequences offunctional importance such as coding exons or regulatory elements(Kidd et al., 2010a), which underline the importance of their accurate characterization. The non-reference sequences identified in various genome studies are thus “added” to the reference genome asadditional sequence. However, due to the complexity of these newsequences and their polymorphism in different populations, there isnow a push towards building graph-based representations (Churchet al., 2015; The Computational Pan-Genomics Consortium, 2017).Although several forms of SVs such as deletions, tandem duplications and mobile element insertions are investigated to a certainextent (Alkan et al., 2011; Chaisson et al., 2015a,b), characterization of novel sequence insertions longer than read lengths is stilllagging. This is mainly because long sequence insertions can be discovered only through sequence assembly, which is computationallychallenging and may lead to incorrect or fragmented sequence reconstructions due to common repeats that may lie within or closeto such insertions (Hajirasouliha et al., 2010; Kidd et al., 2010a).Cortex (Iqbal et al., 2012) aims to improve the accuracy in complex regions by using colored de Bruijn graphs, but a recent studyfound that it has high computational requirements (Kehr et al.,2015).Aside from computationally intensive assembly-based algorithms, only a handful of mapping and local assembly based methods for novel sequence insertion discovery are currently available.The first of such algorithms is NovelSeq (Hajirasouliha et al.,2010) that we have previously developed to find insertions >200 bp using paired-end whole-genome Illumina sequencedata. Briefly, NovelSeq identifies one-end anchored reads (OEA),where one end of a pair maps to the reference and the other remains unmapped, and calculates the best match between local assembly of OEA reads and de novo assembly of orphan (both endsunmapped) reads to identify both the content and the approximatelocation of the insertion. However, NovelSeq was designed to analyze one genome at very high sequence coverage. It could find insertions of length up to a couple of kilobase pairs, but it does notprovide the exact content of the insertion, the exact breakpoint location and the genotyping information. MindTheGap (Rizk et al.,2014) was developed for finding insertion breakpoints and their sequences in a single sequenced genome based on an assembly-firststrategy. BASIL & ANISE (Holtgrewe et al., 2015) are also designed for detecting novel sequence insertions where BASIL detectsthe breakpoints by clustering one-end anchored reads and ANISEassembles the novel insertions with an overlap-layout-consensusgraph based assembler.A more recent algorithm, PopIns (Kehr et al., 2015) follows asimilar approach and also incorporates the split-read sequence signature (Alkan et al., 2011) to discover and then genotype commonsequence insertions within a large cohort of samples. Using “softclipped” reads, another algorithm Swan (Xia et al., 2016) can onlyfind breakpoints of long insertions without providing its content.In this paper, we present Pamir, a new tool to provide exactbreakpoint positions, sequence contents, and genotypes of novel sequence insertions either in single or multiple genomes sequencedwith the Illumina technology. We show that, when a single genomeis used, it outperforms MindTheGap (Rizk et al., 2014), BASIL &ANISE (Holtgrewe et al., 2015), and PopIns (Kehr et al., 2015).Additionally, using simulated low coverage data (5 samples at 10Xcoverage each) we demonstrate that Pamir has better precision andrecall rates than PopIns, which is the only other insertion characterization tool that can use multiple genomes.P.Kavak et al.2 Materials and methodsWe developed Pamir to characterize novel sequence insertions usingpaired-end whole genome sequencing (WGS) data generated by theIllumina platform. Pamir is based on the observation that structuralevents such as “novel sequence insertion” leave a group of one-endanchors, i.e. one-end is mapped while the other is unmapped,around their breakpoint location when aligning the donor sequencesto the reference genome (Hajirasouliha et al., 2010; Kidd et al.,2010a,b). Furthermore, the insertions longer than the paired-endfragment size will leave another group of reads known as orphanreads, i.e. read pairs where none of the ends can be mapped to thereference. Figure 1 depicts the mapping information in the vicinityof the hypothetical novel insertion. Pamir uses both types of reads tocharacterize the novel sequence contents and their insertion breakpoints. First, it starts with generating a de novo assembly of the orphan reads to obtain orphan contigs. Next, Pamir clusters the OEAread pairs based on their mapping locations on the reference genome. It then remaps the OEA reads to orphan contigs to match theorphan contigs with OEA clusters. Finally, it outputs the putativenovel insertion by assembling the updated cluster and re-aligning thegenerated contig to the respective reference region (Fig. 2). In thissection, we provide a detailed description of the Pamir algorithm.Pamir versus NovelSeq. While they both are based on similar observations, Pamir significantly improves accuracy, performance, andusability of NovelSeq. For a candidate insertion breakpoint location,NovelSeq first assembles two OEA clusters on its upstream (OEAþ)and downstream side (OEA-), and then matches these two OEA contigs with orphan contigs. Rather than providing precise breakpointsand insertion content, NovelSeq reports a range of breakpoint locations based on associations between OEA contigs and orphan contigs. On the other hand, Pamir collects nearby OEAs to build acluster, and includes all relavant orphan contigs to this cluster basedon the association obtained from mapping OEA reads to orphancontigs. It then assembles each cluster and obtains the insertion content through aligning the contig to the respective reference region.Combined with the post-analysis steps, Pamir provides the breakpoint locations at single-nucleotide resolution, exact insertion content, and genotype information, which are all missing in NovelSeq.2.1 Pre-processingPamir accepts both raw reads (in FASTQ format) or aligned reads(in SAM/BAM files) as input. If raw reads are provided, Pamir firstmaps them to the reference genome using mrsFAST-Ultra (Hachet al., 2010, 2014) in best mapping mode. Pamir skips the mappingstep if the read alignment is provided, i.e. BAM file. Next, PamirFig. 1. Classiﬁcation of donor sequence regions in terms of read mappings.Concordant read: both ends map in correct orientation and within expectedinsert size. OEA read: one-end anchored, only one end maps to the reference.Split read is an OEA read whose unmapped end crosses the breakpoint andgenerates split mapping. Orphan read: none of the ends map to the referenceDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018Novel sequence insertionsAi163BCDFig. 2. General overview of Pamirextracts OEA and orphan reads using the alignment results. Pamirthen remaps the OEA reads using mrsFAST-Ultra in multi-mappingmode since the breakpoints of a sequence insertion may lie withinrepeats, which causes mapping ambiguity (Bailey et al., 2001;Firtina and Alkan, 2016) (Fig. 2A). Using multi-mapping locationsmay introduce false positives in repeat regions, which we eliminatein a post-processing step. Pamir assembles the orphan reads usingVelvet (Zerbino and Birney, 2008) with the k-mer length set to31 bp, although any other assembler may also be used for this step(Fig. 2B). After the assembly, we subject the contigs to a contaminant filter by querying the nt/nr database, and we remove those contigs that map to vector and/or bacterial sequences and other knowncontaminants. We then map the unmapped end of OEA read pairsto the orphan contigs using mrsFAST-Ultra in the multi-mappingmode to match the OEAs to the corresponding orphan contigs. Inthis way, the OEA-to-orphan remapping stage allows an OEA to bealigned to more than one orphan contig (Fig. 2C). To avoid missingany associations between split reads (Fig. 1) and orphan contigs, wealso split the unmapped OEAs from the previous stage into a half,i.e. balanced splits, and remap them to the orphan contigs.In summary, the pre-processing step generates four types of information required to discover a novel sequence insertion event: (i)the mapping information of the OEA mapped reads; (ii) unmappedOEA sequences; (iii) orphan contigs; and (iv) pairwise associationbetween unmapped OEA reads and orphan contigs.2.2 Cluster formationPamir clusters OEAs based on the mapping locations of theirmapped end to detect potential insertion breakpoints. It then employs an iterative greedy strategy, which anchors the first clusterwith the leftmost mapping locus x of an OEA on the genome. Next,it extends the cluster to include any other OEA mappings overlapping with the interval ½x; x þ 2L  where L is the fragment size (Let Lbe the fragment size of paired-end reads which can be estimatedfrom concordant mappings. For an insertion in breakpoint p, mostof its OEA anchors should be mapped within ½p À L; p þ L , whichspans a 2ÂL interval on the reference genome). Once all such OEAmappings are added to the existing cluster, the iterative strategythen greedily anchors the next cluster with the first OEA mappingthat is not included in the previous cluster. Note that in this strategyeach OEA mapping can only be part of a single cluster. However, asingle read pair may generate multiple OEA mappings (and thus belong to multiple OEA clusters) due to the use of multi-mappingstrategy.After the first clustering pass is completed, Pamir adds the unmapped OEA mates of the reads and their associated orphan contigsinto each cluster (Fig. 2C). To find the associated orphan contigs,the “OEA-to-orphan contig” mapping information generated in thepre-processing step is used. A contig is added to a cluster if (i) thecluster contains OEAs that map to the both ends of the orphan contig; or (ii) at least 30% of the OEAs in the cluster map only to eitherend of the contig. We allow the second condition to avoid missingany partially assembled orphan contigs.In summary, each cluster generated in this step contains the following information: (i) the number of the OEA reads and their associated contigs; (ii) the leftmost OEA mapping location; (iii) therightmost OEA mapping location; (iv) unmapped OEA read information (see below); and (v) contigs associated with unmapped OEAreads. For each unmapped end of an OEA read pair, the followinginformation is kept in the cluster: (i) read name; (ii) strand (based onits corresponding mapped mate); and (iii) read sequence.2.3 Insertion discovery2.3.1 Candidate insertion contig assemblyPamir generates a new assembly for each cluster to compute the putative insertion that consists of both left and right flanking regionsthat overlap with the reference genome and its main body whichconstitutes the insertion (Fig. 2C). The resulting cluster-aware assembly represents a potential novel insertion sequence.We assemble the reads and contigs in each cluster using an efficient in-house overlap-layout-consensus (OLC) assembler. Wefound most of the available off-the-shelf assemblers to be too slowfor this task, especially because the total number of clusters is measured in millions. Additionally, existing tools cannot be modified toconsider strand information that can be inferred from the mappinginformation while our in-house assembler is strand-specific.Furthermore, use of naıve greedy strategy for assembly is not suit¨able for our goal because such method cannot obtain optimal contigs necessary for accurate insertion detection.The objective of the in-house assembler is to construct a contigthat maximizes the total sum of overlaps between the reads. Thisproblem can be optimally solved by modeling it as an instance ofmaximum weighted path problem in a directed graph G(V, E) as follows. Let each vertex v represent a read in the cluster. Two verticesm and v are connected with a directed edge em;v of weight wm;v if themaximum prefix-suffix overlap between the reads represented bythose vertices is of length wm;v . We can optimally calculate the maximum weighted path via a dynamic programming formulation asfollows.Suppose that there exists some ordering < v of the vertices of G,where parentðvÞ < v v always holds for any vertex v and its parent,parentðvÞ. Furthermore, let r be the root of the graph G (as long as< v exists, root can be selected as the smallest vertex with respect to< v ). We can calculate the value of maximum path from the root rto any vertex v, denoted as f(v) by the following equation:f ðvÞ ¼ maxparentðvÞ ff ðparentðvÞÞ þ wparentðvÞ;v g(1)assuming that initially f(r) ¼ 0 for any root r (i.e. vertex with no incoming edges). The Equation (1) can be implemented in iterativefashion by iterating over vertices v in order < v . This dynamic programming formulation has the complexity OðjRj þ jEjÞ, where jRjDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018i164denotes the number of reads in the given cluster, and jEj is the totalnumber of edges in G, since we visit each vertex only once, and foreach such visit, we only consider the incoming edges. Furthermore,it will always produce the optimal solution as long as there existsordering < v with the above-mentioned properties. The most naturalchoice for < v is topological ordering of G, which maintains the necessary invariant parentðvÞ < v v. Topological ordering can be efficiently calculated in OðjRj þ jEjÞ using Kahn’s algorithm (Kahn,1962).However, both topological sorting and Equation (1) requireacyclic G, which might not be always true, especially if the target region contains some repeat. In that case, maximum weighted pathproblem is NP-hard, which can be easily shown by reducing the longest path problem in a graph to the maximum weighted path problem. If cycles are present, we remove any cycle from G in a greedyfashion by iteratively removing cycle edges whose endpoint is thevertex with the smallest indegree in G in order to provide a feasibleassembly. Because the size of each cluster is small, and because therepeats are not often present in G, such cyclic graphs are not common. Thus, in the majority of the cases, our assembler is guaranteedto produce an optimal assembly for a given cluster.2.3.2 Breakpoint and content detectionThe cluster assembly provides the sequence content. The insertionbreakpoint can be inferred using the provided assembled contigs, theleftmost and the rightmost mapping locations kept for each cluster.Thus, to characterize the exact insertion breakpoint, we align theassembled contigs to the reference in the vicinity of each clusterusing a modified variant of Smith-Waterman (Smith and Waterman,1981) algorithm where the assembled contig is fully aligned to asubstring of the genomic sequence, i.e. global to local alignment. Weonly consider those candidate insertions that align to the referenceby at least 6bp at both sides. We finally return the sequence betweenthese two flanking sequences as the novel insertion and the end ofthe left-mapping flank as the exact breakpoint location (Fig. 2D).P.Kavak et al.Fig. 3. Genotyping novel sequence insertions with Pamir. Here we show anexample for calculating r, i and x based on the Figure: r ¼ 2 (the # of mappingspassing through the breakpoint on R); il ¼ 9 (the # of mappings passingthrough the left breakpoint on I); ir ¼ 7 (the # of mappings passing through theright breakpoint on I); i ¼ (ilþir)/2 ¼ 8; x ¼ (i-r)/(i þ r) ¼ 0.62.4.3 GenotypingFinally, we perform a genotype inference from the reported sequences as follows. We first construct the following two temporarysequences I and R as shown in Figure 3: I is the concatenation of (i),(ii) and (iii) as the temporary donor sequence that contains the novelinsertion as described above. R is the concatenation of (i) and (iii) asthe temporary reference that does not contain the insertion. We thenalign all reads to these two temporary reference sequences. Let r bethe number of reads that align across the breakpoint location in Rand il, ir be the number of reads that align across, respectively, theleft and right breakpoint locations in I. We then predict the genotype using the Equation (2) below. We tested various values for cand we found c ¼ 0:3 yielded the best genotyping accuracy in simulated data. We report the final set of calls in standard VCF format(Danecek et al., 2011).8if xÀc> No Insertion><il þ iriÀri¼; Genotype ¼ Homozygous if x ! c;x ¼>iþr2>:Heterozygous otherwise(2)2.4 Post-processing and genotyping2.4.1 False positive removalTo refine our candidate list and eliminate false positives, for a dataset with fragment size L, we construct a temporary reference segment by concatenating three sequences: (i) L bp upstream of thebreakpoint from the reference; (ii) the obtained insertion sequencefrom the previous step; and (iii) L bp downstream of the breakpointfrom the reference. We then map all OEAs and orphan reads to thistemporary reference and we report the insertion if for each breakpoint, there exists a concordant mapping in which only one mateoverlaps the insertion sequence and the other mate is in the flankingregion. With this method, we guarantee that both breakpoints arecovered by supporting reads, which are signatures of an insertion.A false positive case will miss these reads and will be eliminated.2.4.2 Mapping ambiguity resolutionThere might be still some reads which map to multiple novel insertions. We assign each such read to the insertion with the highest support via set-cover algorithm, where the set of reads represents theuniverse, and where clusters represent the sets. By selecting the minimal number of sets which describe all of the available reads, weeliminate low-support insertions and ensure that each read belongsto only one insertion event. Because the set cover is an NP-hardproblem, we use a fast greedy strategy to calculate the minimal setof events that covers all reads (Johnson, 1974).2.5 Discovery with pooled dataPamir supports population-scale insertion discovery by first detecting insertions in pooled samples, and then genotyping all events ineach sample. In other words, Pamir extracts OEAs and orphansfrom all samples to construct one OEA dataset and one orphan dataset. It then analyzes the combined dataset to detect the list of potential insertions for the whole population. After obtaining the initiallist of potential insertions, Pamir genotypes each insertion for eachsample using the reads from that specific dataset, as explained inSection 2.4.3 ResultsWe performed four sets of experiments to evaluate our method: two experiments with simulated data, and two experiments using real data.In simulation experiments, we inserted 350 new sequences intochromosome 21 of the GRCh37 reference in 7 different size ranges(10–100 bp, 100–200 bp, 200–500 bp, 500–1K bp, 1K–2K bp, 2K–5K bp, 5K–10K bp) with each range containing 50 insertions. Weused randomly selected segments from the Methylobacterium reference genome for this purpose, which are guaranteed to be missing inthe human genome reference. Next we generated 6 high coverageWGS datasets using the ART read simulator (Huang et al., 2012) totest Pamir under different conditions:Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018Novel sequence insertionsi165Table 1. Precision and recall of Pamir, PopIns, MindTheGap and BASIL & ANISE on simulated 30x datasets generated for different sequencing platforms with varying read lengthsError freeHiSeq2500-100 bpPPamirPopInsMindTheGapBASIL & ANISEa1.0000.9731.0000.989RNoisyHiSeq2500-150 bpbaP0.9540.8140.9000.757HiSeq2000-100 bpbR1.0000.9581.0000.989a0.9600.7260.9000.763bPR1.0000.9721.0000.9890.9510.8230.9000.763HiSeq2500-100 bpabPR1.0000.9691.0000.9890.9260.8000.9000.757HiSeq2500-150 bpabPR1.0000.9680.9650.9890.9430.7890.8970.754HiSeq2000-100 bpPaRb1.0000.9380.9050.9740.8260.7090.8110.743Best results are marked with bold typeface.aPrecision.bRecall.Table 2. Precision and recall rates of perfect Illumina HiSeq2000100 bp simulation data with respect to different ranges of insertionsizes where each range contains 50 insertionsInsertionLength (bp)10–100100–200200–500500–1K1K–2K2K–5K5K–10KTotalPamirPopInsMindTheGapBASIL &ANISERaPbRaPbRaPbRaPb1.001.001.000.980.960.920.800.951.001.001.001.001.001.001.001.000.100.820.841.001.001.001.000.821.000.980.950.980.940.930.930.970.880.920.920.880.920.840.940.901.001.001.001.001.001.001.001.000.000.601.000.981.000.980.980.761.001.001.001.000.981.001.000.99single haploid sample (CHM1) (Chaisson et al., 2015b) and compared our results with novel insertions found in the same genomewith the SMRT-SV algorithm that uses long read, i.e. PacificBiosciences, sequencing technology. Finally, we evaluated Pamir ’sperformance in multi-sample insertion discovery and genotypingusing 10 low-coverage WGS datasets generated as part of the 1000Genomes Project (The 1000 Genomes Project Consortium, 2015).Best results for total are highlighted in boldface.aR: Recall.bP: Precision.1. error-free reads generated as (a) 2x100bp Illumina HiSeq 2000,(b) 2x100bp Illumina HiSeq 2500, and (c) 2x150bp IlluminaHiSeq 2500;2. noisy reads, i.e. introduced small variants as SNPs and indelsand sequencing errors, generated as (a) 2x100bp Illumina HiSeq2000, (b) 2x100bp Illumina HiSeq 2500, and (c) 2x150bpIllumina HiSeq 2500.All 6 high coverage simulated datasets were created at 30x sequence coverage using default parameters of ART that are set foreach sequencing machine’s error model.We also evaluated the efficacy of Pamir on low-coverage multisample data. For this purpose, we simulated 5 2x100bp WGS datasets at 10x sequence coverage using ART’s default parameters fornoisy Illumina HiSeq 2500 sequencer. Each of the 5 datasets issampled from a different simulated genome: the first genome includes all 350 novel insertions, and the remaining 4 each includes280 randomly selected insertions. In all single-sample simulation experiments we compared Pamir with MindTheGap, BASIL &ANISE, and PopIns using their default parameters. In multi-sampledatasets, we compared Pamir with PopIns (the only tool beforePamir capable of finding insertions in multi-sample data), using default parameters of PopIns.We tested Pamir on real datasets in two experiments. First, weapplied Pamir on a high coverage WGS dataset generated from a3.1 Simulations3.1.1 High coverage single sampleTPWe compared all tools in terms of precision (TPþFP, where TP is number of True Positives and FP is number of False Positives) and recallTP(TPþFN, where TP is number of True Positives and FN is number ofFalse Negatives). We summarize the results of our simulation experiment in Table 1. Briefly, Pamir outperforms BASIL & ANISE,MindTheGap and PopIns in all simulation experiments in terms ofrecall. In terms of precision, Pamir outperforms PopIns and BASIL& ANISE; and has better or equal precision to MindTheGap. Herewe consider a predicted insertion to be correct only if the breakpointmatches that of the simulated insertion. Note that if we also requirethe lengths of the predicted insertions to be the same with the simulation, Pamir has the best precision and recall among the tools wetested (Supplementary Table S10 and Supplementary Fig. S1). Wepresent range specific precision recall rates of all tools for error-freeIllumina HiSeq2000-100bp data in Table 2. A detailed version ofthis table can be found in Supplementary Table S1.3.1.2 Low coverage multiple samplesNext, we tested the prediction performance of Pamir when multiplegenomes with low coverage data are available. In this experimentwe compared Pamir only with PopIns, as it is the only other multisample novel sequence insertion discovery tool. To evaluate theimportance of multiple samples, we tested the same five genomessimulated at 10x sequence coverage both separately and collectively(Table 3). We found that Pamir ’s precision was substantially higherthan that of PopIns when each sample is processed separately, anduse of multiple genomes resulted in higher recall rates for both tools.We also predicted genotypes on all five samples using Pamir(Table 4). Here we first characterized insertions using all five samples simultaneously as described above, and then calculated genotypes for each predicted insertion in all samples separately. Weobserved no incorrect heterozygous versus homozygous genotypingresults for any insertions, except 5 calls in 3 samples are identified asheterozygous although they were homozygously inserted. All 5 insertions map to common repeats, i.e. LINE elements.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018i166P.Kavak et al.Table 3. Precision and recall rates of 5 simulated samples (noisy HiSeq2500 2*100 bp 10x)PooledSamples# of InsertionsIndividualAll350S1350S2280S3280S4280S5280PaPamirPopInsRbPaRbPaRbPaRbPaRbPaRb1.0000.9770.9110.8111.0000.5750.7260.6571.0000.5910.7110.6751.0000.5750.7040.6571.0000.5740.7140.6461.0000.6030.7140.668Best results are marked with bold typeface.aPrecision.bRecall.Precision and recall rates of both individual and pooled calls of ﬁve low coverage samples. The paired-end reads (2*100 bp) are generated using IlluminaHiSeq2500 error model. We have simulated 350 insertions in this dataset: S1 have all insertions, and genomes of the other four individuals contains 280 events.The column All shows performances of Pamir and PopIns based on pooling simulation reads, and each column Si represents single sample detection results for i-thindividual.Table 4. Evaluation of predicted genotypes using 5 simulated genomesSamples# of Insertions# of Insertions not in the sampleS13500S228070S328070S428070S528070PamirCorrect (INS)Correct (REF)Incorrect zygosityNo call (INS)No call (REF)PopInsPamirPopInsPamirPopInsPamirPopInsPamirPopIns317–231–284–066–25366027421054050162526612742145605214253642256225590551125960021102275705313Best results are marked with bold typeface.Evaluation of genotyping results for the same ﬁve samples as in Table 3, based on pooling simulated reads. The paired-end reads (2*100 bp) are generated usingIllumina HiSeq2500 error model. We have simulated 350 insertions in this dataset: S1 have all insertions, and genomes of the other four individuals contains 280events. Correct (INS) lists the number of insertions that are correctly genotyped. Correct (REF) shows the number of detections discarded after genotyping, whichare not actual insertions in an individual but falsely predicted based on pooling reads. Incorrect zygosity provides the number of insertions incorrectly genotypedas heterozygous; only 5 calls were identiﬁed as heterozygous in S1, S3 and S4 although they were homozygously inserted. All insertions map to common repeats.The No call (INS) row shows the number of insertions missed in the pooled run for each sample, i.e. false negatives. No call (REF) provides the number of insertions missed in the pooled run but the insertion was not inserted into this sample.3.2 Real data3.2.1 High coverage sequencing of CHM1Our tests using real data also included two types of datasets: i)high coverage single sample WGS, and ii) low coverage multiplesample WGS. First, we evaluated Pamir using WGS data at 40xcoverage generated from a haploid cell line with the Illumina technology (CHM1, SRA ID: SRX652547) (Chaisson et al., 2015b).We have identified a total of 22,676 insertions that corresponds to593.5 Kb in total, of which, 2,444 were >50bp (348 Kb total)(Table 5). Chaisson et al. (2015) also generated de novo assemblyof the same genome using a long read sequencing technology(Pacific Biosciences) from the same cell line, and predicted insertions with the SMRT-SV algorithm using this dataset (Chaissonet al., 2015b). Here we used an updated call set (! 50bp) mappedto human GRCh38 (Huddleston et al., 2016) for comparisons.Pamir showed low recall rates when compared to the long readbased SMRT-SV results (Chaisson et al., 2015b). We could identifyonly 488 of the 12,998 insertions detected by SMRT-SV when weconsider only nearby matches (less than 10bp distance) in breakpoint predictions. One of the reasons for such discrepancy is thefact that more than half of PacBio-predicted insertions are locatedwithin various repeat regions (Table 6), and short-length IlluminaTable 5. Summary of insertions predicted in CHM1AllNumber of insertionsMinimum lengthMaximum lengthAverage length22,67654,13526.2050bp20,23255012.12> 50bp2,444514,135142.51reads are not sufficient to properly assemble such regions. Thesame effect was also observed in the original publication (Chaissonet al., 2015b), where only a handful of insertions were also identified in another assembly of the same genome that was constructedwith a reference-guided methodology using both Illumina WGSand bacterial artificial chromosome datasets (Steinberg et al.,2014). We observed that approximately 45% of the insertionscharacterized by SMRT-SV are contain either very low ( 20%) orhigh (! 60%) GC%, which are known to be problematic to sequence using the Illumina platform (Benjamini and Speed, 2012;Ross et al., 2013). Additionally, we found that 14,121 out of our22,676 predicted insertions were reported in dbSNP version 147(Within 10 bp breakpoint resolution.).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018Novel sequence insertionsi167Table 6. Comparison of insertions in CHM1 by SMRT-SV using PacBio reads versus Pamir and PopIns using Illumina reads allowing 10bpbreakpoint resolutionPacBioIlluminaSMRT-SVInsertion Length1–50 bp50–100 bp100–200 bp200–500 bp>500 bpAllPredictiona187 (60%, 57%)4,384 (54%, 53%)2,959 (54%, 50%)3,123 (55%, 37%)2,345 (60%, 32%)12,998 (55%, 45%)PamirPopInsPredictionShared with SMRT-SVPredictionShared with SMRT-SV20,232 (56%, 38%)1,273 (70%, 18%)815 (75%, 13%)291 (74%, 7%)65 (63%, 3%)22,676 (58%, 36%)27 (63%, 14%)205 (52%, 14%)125 (58%, 13%)97 (61%, 1%)34 (50%, 3%)488 (56%, 10%)21 (71%, 24%)246 (73%, 4%)793 (66%, 4%)1,074 (65%, 3%)1,286 (59%, 3%)3,420 (58%, 3%)017 (70%, 0%)120 (62%, 1%)141 (58%, 1%)207 (51%, 1%)485 (56%, 1%)For each category, we report (i) the percentile of the calls that fall into repeat regions compared to repeat masker ﬁle, and (ii) the percentile of the calls withbiased GC ratios ( 20% or !60%) in the form (% of repeat regions, % of biased GC ratios) in the parentheses.aAll events reported have a length of 50bp. Note that the comparisons are based only on breakpoint positions without consideration about contents of insertions. If we simultaneously consider insertion lengths and contents, most of PopIns predictions will be ﬁltered out as shown in Supplementary Tables S5 and S6. Itis worth mentioning that Pamir can call most of the predictions as PopIns. However, it ﬁlters most of them because of the stringent rules.Table 7. Hierarchical non-redundant analysis of predicted CHM1 insertions with Pamir and PopIns with respect to other datasetsPamirPopIns50 - 200 bp# of insertionsIn GRCh38In CHM1_1.1 (Steinberg et al., 2014)In CHM1 PacBio (Chaisson et al., 2015b)In SMRT-SV (Huddleston et al., 2016)In long insert clonesa (Kidd et al., 2008)In repeat regionsRemainder200 - 500 bp>500 bpTotal50 - 200 bp200 - 500 bp>500 bpTotal2,08817251213732121,065257291154134721126296512231112162,444193072491312341,2122921,03801551185652211141,0751821326271911141,28611121937052141603,399224194431,897626388Here we provide a hierarchical non-redundant breakdown of comparison of insertions we predicted in the CHM1 genome with Pamir and PopIns. We comparethe predictions in the following order: the GRCh38 assembly, then remaining to the reference-guided CHM1_1.1 assembly, the Paciﬁc Biosciences (PacBio) assembly, SMRT-SV call set, long insert clones and those that are in repeat regions.aLong insert clones include both fosmid clones and bacterial artiﬁcial chromosomes (BAC). Since we apply more stringent rules to ﬁlter false positives in Pamir,many of our discarded calls are still kept by PopIns. This will affect recall rate of Pamir, especially for longer insertions whose orphan contigs are difﬁcult to beassembled.To test whether the insertions we predicted in CHM1 were alsopreviously discovered in other studies, we mapped the longer insertions (>50 bp) with spanning regions around the breakpoint on thereference (GRCh37) to the latest version of the reference (GRCh38)using BLAST (Altschul et al., 1990). Note that our predictions werebased on the GRCh37 version. In this experiment we required onlyhighly identical (!98%) hits that covered at least 98% of the predicted insertion. We repeated the same remapping experiment toboth the long read-based assembly (Chaisson et al., 2015b) and thealternative reference-guided assembly of the same genome(Steinberg et al., 2014). Finally, we also mapped the same sequencesto the nt/nr database to detect whether the sequences were also contained within other WGS studies, in particular, fosmid end-sequencedata (Kidd et al., 2008). In summary, out of 2,444 (> 50 bp) insertions we predicted, 1,446 are not found in any database, of which1,212 mapped to common repeats (Table 7). We performed thesame experiment using PopIns (Table 7). 1,014 out of 3,399 PopInscalls are not found in any database, of which 388 mapped to common repeats. 56% of PopIns calls map to long insert clones, butonly a handful were included in the latest version of the human genome reference, and assemblies of the same DNA resource.Table 8. Summary of novel sequences found in 10 low coverageWGS datasets from the 1000 Genomes ProjectTotalNumber of insertionsMinimum lengthMaximum lengthAverage lengthIn 1000 Genomes ProjectIn dbSNP version 147*> 50bp49,47351,92828.87214,83714,4096,846511,928128.0854252,027*We intersected with dbSNP after removing those insertions that are foundin the 1000 Genomes Project.3.2.2 Low coverage genomes from the 1000 genomes projectFinally, we tested Pamir using low coverage WGS datasets generatedfrom 10 samples as part of the 1000 Genomes Project (The 1000Genomes Project Consortium, 2015) (Table 8). We found 39,554 insertions when we pooled all 10 genomes, 13,255 of them were reported in 1000 Genomes project, and another group of 11,019insertions was seen in dbSNP version 147 (Considering 10 bp breakpoint resolution.). We then genotyped for each sample (Table 9).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018i168P.Kavak et al.To test whether the insertions we predicted in these 10 sampleswere also previously discovered in other studies, we mapped the longer insertions (>50 bp) to the latest version of the reference(GRCh38) using BLAST (Altschul et al., 1990). We also mapped thesame sequences to the nt/nr database (Table 10).BASIL & ANISE and $6.8 times less time than MindTheGap.Note that PopIns is faster than Pamir, but in many cases it does notprovide the full inserted sequences.4 Discussion3.3 Detections of insertions within repeat regionsTo better understand the improvements in detecting insertions fallingwithin repeat regions, we compared the performance of Pamir,PopIns, and MindTheGap using the Illumina HiSeq2500 100bp simulation dataset. 170 out of 350 insertions in our simulation are in repeat regions. As shown in Figure 4, Pamir maintains a zero falsepositive rate in repeat regions. In contrast, PopIns has a false positiverate of 5.4% (8/147), higher than the rate when considering only theinsertions in unique regions (6/152, about 3.9%). In Figure 5, weshow that Pamir also outperforms MindTheGap in finding insertionswithin repeat regions. These results demonstrate that Pamir has anedge in detecting insertions with ambiguously mapped reads, which isa major issue for insertion detection when using NGS datasets.The last few years since the introduction of HTS platforms witnessed the development of many algorithms that aim to characterizegenomic structural variation. The first such algorithms focusedmainly on the discovery of deletions, and other forms of complexSV, especially inversions and translocations were largely neglecteddue to the sequence complexity around their breakpoints and theambiguity in mapping to these regions.Although novel sequence insertions can be considered “simpler”than most other SV classes, their accurate characterization is stilllacking due to the need for constructing either global or local denovo assembly. However, they may fail to generate long and accurate contigs due to the repeats that may occur around or within novelsequence insertions.3.4 Running timesFinally, we evaluated the running time of all the benchmarkedsoftware. We ran Pamir, PopIns, MindTheGap and BASIL &ANISE on a 800Mhz AMD machine with 256Gb memory with1 thread on a high coverage simulation dataset (2*100bp errorfree reads sampled from human chromosome 21 based on IlluminaHiSeq2500 model at 30X coverage) until genotyping phase.Running times are given in Table 11. Pamir takes $3.6 timesless time than BASIL & ANISE and $4.3 times less timethan MindTheGap where PopIns takes $5.7 times less time thanTable 9. Genotyping results for the novel sequences found in the1000 Genomes Project datasetsHomozygousNA06985NA07357NA10851NA11840NA11918NA11933NA12004NA12044NA12234NA12286HeterozygousTotal insertion length (bp)22,97122,58223,27420,97322,61021,04919,02418,75320,84119,02710,24610,1589,46512,7459,99411,09212,65013,00210,80412,622941,868921,225930,766959,017953,968936,615928,371919,212916,251922,799Fig. 4. Performance comparison of PopIns and Pamir in IlluminaHiSeq2500 100bp simulation dataset with 170 calls falling in repeat regionsFig. 5. Performance comparison of PopIns and MindTheGap in IlluminaHiSeq2500 100bp simulation dataset with 170 calls falling in repeat regionsTable 10. (Pamir & PopIns) Analysis of insertions found in low-coverage samples with respect to other datasetsPamirPopIns50–200 bp# of insertionsIn GRCh38In long insert clonesIn repeat regionsRemainder200–500 bp>500 bpTotal50–200 bp200–500 bp>500 bpTotal6,050311,0723,8371,1106672894888812913171266,846341,1924,3961,2245,96303,5151,5429064,06802,5929475292,83841,78461343712,86947,8913,1021,872Here, we provide a hierarchical non-redundant breakdown of comparison of insertions we predicted in the 10 1000 genomes. We compare our predictions inthe following order: the GRCh38 assembly, then remaining to the long insert clones and those that are in repeat regions. Before mapping to GRCh38 reference weextracted 200 bp left and right spanning regions of the insertion breakpoints on GRCh37 reference sequence, inserted the discovered sequence in between andsearched the obtained sequence in GRCh38.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018Novel sequence insertionsi169Table 11. Running times of Pamir, PopIns, MindTheGap, andBASIL & ANISE on a 2*100bp simulation dataset based onHiSeq2500 model with 30X coveragePamir3min 9secPopInsMindTheGapBASIL & ANISE1min 59sec13min 25sec11min 16secIn this paper, we presented Pamir, a new algorithm to discoverand genotype novel sequence insertions in one or multiple humangenomes. Pamir uses several read signatures (one-end-anchored,read pairs, split reads, and assembly) to characterize insertions thatspan a wide size range. We demonstrated its performance on bothsimulated and real datasets and showed that it outperforms theexisting tools designed for the same purpose. We believe that furtherdevelopment and extensive testing of the Pamir algorithm will helpmake the novel insertion discovery a routine analysis for whole genome sequencing studies.AcknowledgementWe thank Alex Gawronski for proof reading and suggestions during the preparation of the manuscript.FundingThe work was supported by an installation grant from the EuropeanMolecular Biology Organization to C.A. (EMBO-IG 2521), NSERCDiscovery grant, and NSERC Discovery Frontiers grant on ‘Cancer GenomeCollaboratory’ to F.H. P.K. acknowledges foreign collaborative researchstudy support by The Scientiﬁc and Technological Research Council of€ __Turkey, TUBITAK- BIDEB under the 2214-A programme. I.N. was supportedby Vanier Canada Graduate Fellowship.Conﬂict of Interest: none declared.ReferencesAlkan,C. et al. (2011) Genome structural variation discovery and genotyping.Nat. Rev. Genet., 12, 363–376.Altschul,S.F. et al. (1990) Basic local alignment search tool. J. Mol. Biol., 215,403–410.Bailey,J.A. et al. (2001) Segmental duplications: organization and impactwithin the current human genome project assembly. Genome Res., 11,1005–1017.Benjamini,Y. and Speed,T.P. (2012) Summarizing and correcting the gc content bias in high-throughput sequencing. Nucl. Acids Res., 40, e72.Chaisson,M.J.P. et al. (2015a) Genetic variation and the de novo assembly ofhuman genomes. Nat. Rev. Genet., 16, 627–640.Chaisson,M.J.P. et al. (2015b) Resolving the complexity of the human genomeusing single-molecule sequencing. Nature, 517, 608–611.Church,D.M. et al. (2015) Extending reference assembly models. GenomeBiol., 16, 13.Danecek,P. et al. (2011) The variant call format and vcftools. Bioinformatics,27, 2156–2158.Firtina,C. and Alkan,C. (2016) On genomic repeats and reproducibility.Bioinformatics, 32, 2243–2247.Hach,F. et al. (2010) mrsFAST: a cache-oblivious algorithm for short-readmapping. Nat. Methods, 7, 576–577.Hach,F. et al. (2014) mrsFAST-Ultra: a compact, SNP-aware mapper for highperformance sequencing applications. Nucl. Acids Res., 42(Web Serverissue), W494–W500.Hajirasouliha,I. et al. (2010) Detection and characterization of novel sequenceinsertions using paired-end next-generation sequencing. Bioinformatics, 26,1277–1283.Holtgrewe,M. et al. (2015) Methods for the detection and assembly of novelsequence in high-throughput sequencing data. Bioinformatics, 31,1904–1912.Huang,W. et al. (2012) ART: a next-generation sequencing read simulator.Bioinformatics, 28, 593–594.Huddleston,J. et al. (2016) Discovery and genotyping of structural variationfrom long-read haploid genome sequence data. Genome Res., doi: 10.1101/gr.214007.116.Iqbal,Z. et al. (2012) De novo assembly and genotyping of variants using colored de Bruijn graphs. Nat. Genet., 44, 226–232.Johnson,D.S. (1974) Approximation algorithms for combinatorial problems.J. Comput. Syst. Sci., 9, 256–278.Kahn,A.B. (1962) Topological sorting of large networks. Commun. ACM, 5,558–562.Kehr,B. et al. (2015) PopIns: population-scale detection of novel sequence insertions. Bioinformatics, 32, 961–967.Kidd,J.M. et al. (2008) Mapping and sequencing of structural variation fromeight human genomes. Nature, 453, 56–64.Kidd,J.M. et al. (2010a) Characterization of missing human genome sequencesand copy-number polymorphic insertions. Nat. Methods, 7, 365–371.Kidd,J.M. et al. (2010b) A human genome structural variation sequencing resource reveals insights into mutational mechanisms. Cell, 143, 837–847.Medvedev,P. et al. (2009) Computational methods for discovering structural variation with next-generation sequencing. Nat. Methods, 6 (11 Suppl), S13–S20.Mills,R.E. et al. (2011) Mapping copy number variation by population-scalegenome sequencing. Nature, 470, 59–65.Rizk,G. et al. (2014) MindTheGap: integrated detection and assembly of shortand long insertions. Bioinformatics, 30, 3451–3457.Ross,M.G. et al. (2013) Characterizing and measuring bias in sequence data.Genome Biol., 14, R51.Sharp,A.J. et al. (2006) Structural variation of the human genome. Annu Rev.Genom. Hum. Genet., 7, 407–442.Smith,T.F. and Waterman,M.S. (1981) Identiﬁcation of common molecularsubsequences. J. Mol. Biol., 147, 195–197.Steinberg,K.M. et al. (2014) Single haplotype assembly of the human genomefrom a hydatidiform mole. Genome Res., 24, 2066–2076.The 1000 Genomes Project Consortium. (2015) A global reference for humangenetic variation. Nature, 526, 68–74.The Computational Pan-Genomics Consortium. (2017) Computational pangenomics: status, promises and challenges. Brief. Bioinform, doi:10.1093/bib/bbw089.Xia,L.C. et al. (2016) A genome-wide approach for detecting novel insertiondeletion variants of mid-range size. Nucl. Acids Res., 44, e126.Zerbino,D.R. and Birney,E. (2008) Velvet: algorithms for de novo short readassembly using de Bruijn graphs. Genome Res., 18, 821–829.Zook,J.M. et al. (2014) Integrating human sequence data sets provides a resource of benchmark SNP and indel genotype calls. Nat. Biotechnol., 32,246–251.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i161/3953969by gueston 07 January 2018
5028881987002	PMID28881987	5028881987	https://watermark.silverchair.com/btx253.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881987.main.pdf	Bioinformatics, 33, 2017, i319–i324doi: 10.1093/bioinformatics/btx253ISMB/ECCB 2017Efficient simulation of intrinsic, extrinsic andexternal noise in biochemical systemsDennis Pischel1, Kai Sundmacher1,2 and Robert J. Flassig2,*1Otto-von-Guericke-University Magdeburg, Process Systems Engineering, 39106 Magdeburg, Germany and 2MaxPlanck Institute for Dynamics of Complex Technical Systems, Process Systems Engineering, 39106 Magdeburg,Germany*To whom correspondence should be addressed.AbstractMotivation: Biological cells operate in a noisy regime inﬂuenced by intrinsic, extrinsic and externalnoise, which leads to large differences of individual cell states. Stochastic effects must be takeninto account to characterize biochemical kinetics accurately. Since the exact solution of the chemical master equation, which governs the underlying stochastic process, cannot be derived for mostbiochemical systems, approximate methods are used to obtain a solution.Results: In this study, a method to efﬁciently simulate the various sources of noise simultaneouslyis proposed and benchmarked on several examples. The method relies on the combination of thesigma point approach to describe extrinsic and external variability and the s-leaping algorithm toaccount for the stochasticity due to probabilistic reactions. The comparison of our method to extensive Monte Carlo calculations demonstrates an immense computational advantage while losing anacceptable amount of accuracy. Additionally, the application to parameter optimization problemsin stochastic biochemical reaction networks is shown, which is rarely applied due to its huge computational burden. To give further insight, a MATLAB script is provided including the proposedmethod applied to a simple toy example of gene expression.Availability and implementation: MATLAB code is available at Bioinformatics online.Contact: ﬂassig@mpi-magdeburg.mpg.deSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionVariability and heterogeneity are fundamental properties of biological systems. Cells differ in all kinds of attributes including cell size,protein abundances and morphology (Spiller et al., 2010), which iscaused by various sources of noise. In this study we refer to intrinsicnoise as an inherent stochastic biochemical process, extrinsic noiseas cell-to-cell variability and external noise as environmental fluctuations. Intrinsic noise is very dominant for small biochemical reaction systems involving low copy numbers of chemical species (e.g.gene networks), whereas extrinsic and external noise increase withsystem size (Patnaik, 2006). Since the cellular abundance of numerous chemical species span all scales from just a few (e.g. genes) toseveral millions (e.g. proteins) all sources of noise contribute to biological heterogeneity. The interplay between intrinsic, extrinsic andexternal noise and their effects on system dynamics is hardly exploited due to experimental and numerical challenges (Spiller et al.,2010; Fernandes et al., 2011; Delvigne et al., 2014). Monte Carlo(MC) techniques are the standard approach to tackle stochastic biochemical reaction networks, but they suffer from an immensecomputational burden especially in optimization problems, where asystem has to be simulated numerous times. Approximations haveto be used in order to make computations feasible.Figure 1A illustrates a cell population that is corrupted by intrinsic, extrinsic and external noise. In this scenario, a heterogeneouspopulation of cells under realistic conditions, which differ in size,shape and number of organelles is situated in an inhomogeneous medium containing concentration gradients (gray background). In addition to that, intrinsic noise caused by switching of a single genebetween different states contributes to the overall variability. To clarify our understanding of intrinsic, extrinsic and external noise theirimpact on a simple decay process P ! 1 describing the degradationof the protein P is shown in Figure 1B–D. Here, intrinsic noise is modeled by the Gillespie algorithm (SSA) (Gillespie, 1977) that treats reactions as stochastic events, whereas extrinsic noise is modeled bydistributed initial conditions, which accounts for cell-to-cell variability. Both effects rely on different mechanisms, but result in a probability density function (PDF) characterizing the abundance of the protein(red). Adding both effects yields a further spread of the resulting PDFCV The Author 2017. Published by Oxford University Press.i319This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i319/3953968by gueston 07 January 2018i320D.Pischel et al.ABCDFig. 1. Noise in biochemical systems. (A) Various sources of noise corrupt biochemical reaction systems. The inﬂuence of intrinsic noise (B), extrinsic noise (C)and intrinsic noise combined with extrinsic noise (D) on a decay process results in differences of the probability density function (PDF), the mean and the standard deviation (std)indicated by an increase of the standard deviation, see Figure 1D.Note that external noise was left out in the investigation of the decayprocess, because in the case of a stationary random process its mathematical treatment is identical to extrinsic noise. In case the externalnoise is a stochastic dynamic process, its mathematical treatment isidentical to intrinsic noise. For simplicity, we refer from now on onlyto the terms extrinsic and intrinsic noise, representing a stationaryrandom variable or a stochastic dynamic process, respectively.In this study, we propose an approximate method to model distributed stochastic processes by means of stochastic differential equations.The method relies on the combination of the sigma point (SP) approach(Julier et al., 2000) accounting for extrinsic noise and the s-leaping algorithm (Cao et al., 2006) capturing probabilistic reactions. The articleis organized as follows. In Section 2 a brief introduction to modeling ofstochastic biochemical systems and a short overview of existing methods is given. In Section 3 the idea and advantages of the proposedmethod are described. In Section 4 a detailed benchmark on an example of gene expression is performed followed by the application ofour method on several parameter optimization examples. Section 5summarizes and discusses our results. We provide SupplementaryMaterial with additional information concerning our methodology andthe benchmark systems. The gene expression example is illustratedwith a MATLAB script using the proposed method.2 Theoretical backgroundThe simplest and most common approach to model biochemical systems is by means of ordinary differential equations (ODEs)dxðtÞ ¼ f ðxðtÞ; uðtÞ; hÞ ¼ NvðxðtÞ; uðtÞ; hÞ:dt(1)They describe the temporal evolution of the continuous state vector x by reaction rate equations. In this context N denotes the stoichiometric matrix, v the reaction rates, h a set of parameters and uexternal deterministic forcing. ODEs in this form neglect stochasticity introduced by intrinsic noise and fail to model the underlyingprocess correctly. A more detailed description can be achieved withthe chemical master equation (CME) (Gillespie et al., 2013)mXdPðxðtÞ; tÞ ¼ak ðxðtÞ À N k ÞPðxðtÞ À N k ; tÞ À . . .dtk¼1to find the system in the discrete state x. ak denotes the propensity ofreaction k and N k the k’s column of the stoichiometric matrix. Forsimplicity u and h are left out in Equation (2), but they can be easilyintroduced by interpretation as additional reaction channels (Sanftet al., 2011). From the solution P for all reachable states a PDF qdescribing the stochastic variable x can be reconstructed for every timepoint. q can be interpreted as a vector, whose entries represent theprobability of a certain abundance interval of a chemical species. Formost biochemical systems it is not possible to find an exact solution ofthe CME, so approximate methods have to be used. The SSA and itsderivatives (Gillespie et al., 2013), such as leaping (Cao et al., 2006;Fu et al., 2013) and time-scale separation approaches (Marchettiet al., 2016) are powerful methods, which rely on a statistical mechanics ansatz treating chemical reactions as discrete molecular collisionevents. Although these algorithms are very popular they fail to capturevariability introduced by extrinsic noise. The finite state projection algorithm (Munsky and Khammash, 2006) obtains a solution by integration of the CME. This algorithm accounts for intrinsic andextrinsic noise, but due to the curse of dimensionality it is not applicable to systems with a large state space. Another approach is themethod of moments (Lakatos et al., 2015), which relies on the integration of coupled ODEs for the statistical moments. This method incorporates intrinsic and extrinsic noise, but does not reconstruct the statevector’s PDF and numerical instabilities make it difficult to handle(Lee et al., 2009; Azunre et al., 2011). A promising approach consistsof solving the CME directly via tensor trains, but the derivation of thetensor trains is nontrivial (Kazeev et al., 2014). To overcome thesedrawbacks, we propose a novel method paving the way to furtherunderstanding of variability and heterogeneity in biochemical systems.(2)ak ðxðtÞÞPðxðtÞ; tÞ;taking into account the inherent stochastic nature of biochemical reactions. The CME governs the temporal evolution of the probability P3 Efficient modeling of intrinsic andextrinsic noiseA straight forward approach to model intrinsic and extrinsic noisesimultaneously is to perform MC sampling for extrinsic noise e.g.distributed initial conditions or parameters and to compute intrinsicnoise i.e. the temporal evolution of a stochastic process with theSSA. In the limit of infinite function evaluations the obtained histogram is equal to the solution of the CME. Since both methods relyon the generation of random numbers they are extremely time consuming and computationally intense. In order to speed up the algorithm, it is possible to lower the resolution of the histograms and usefewer, broader bins or to perform kernel smoothing, which resultsin the summation of kernel distributions while keeping a highDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i319/3953968by gueston 07 January 2018Efficient simulation of intrinsic, extrinsic and external noise in biochemical systemsAi321DBECFig. 2. Approximation of MC methods. Histogram binning (A) and kernel smoothing (B) represent common methods to derive PDFs from MC trajectories (gray)sampled from distributed initial conditions (red, left). The proposed SP approach (C) provides an efﬁcient simulation technique sampling from 2nH þ 1 startingpoints (red, left). The red distribution on the right indicates the pseudo exact solution obtained by MC sampling combined with the SSA. The black histogram illustrates the histogram binning in (A) and the black distributions in (B) and (C) are summed up to obtain an approximate solution (not shown) to the exact solution. For better visualization all distributions are scaled. The accuracy (D) and convergence (E) elucidate the computational advantages of the proposed methodresolution. This is illustrated for the Schlogl model (Schlogl, 1972)¨¨in Figure 2A and B. In this study, only Gaussian kernels are considered with the bandwidth denoting the kernel’s standard deviation. The idea of the here proposed method is to further acceleratethe computation by approximating the PDF of the extrinsic noiseand propagating it through time by a stochastic process, see Figure2C. We suggest using the SP approach to account for the extrinsicnoise. In contrast to MC sampling, the SP approach chooses only2nH þ 1 sigma points / deterministically and estimates mean E andcovariance C of a random variable Y given by a nonlinear transformation h (see Supplementary Material for further information).In this context Y refers to the abundance of chemical species and hgoverns their temporal evolution due to chemical reactions. nH denotes the number of distributed initial conditions H, but it can alsoinclude kinetic parameters of the right hand side of Equation (2).The samples are chosen according to/0 ¼ EðHÞ(3)pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ/i ¼ EðHÞ6 ðnH þ jÞCi ;(4)where Ci denotes the ith row of the covariance matrix in the original domain. We chose the free parameter j according to j ¼ 3 À nH (seeSupplementary Material for more information). The transformed samplesyi ¼ hð/i ; tÞ1:(9)2ðnH þ jÞAssuming normality, log-normality or any other appropriate PDFcharacterized by mean and variance-covariance, the underlying distribution b can be estimated from the mean and covariance (Julierqet al., 2000).To incorporate intrinsic noise due to stochastic reactions on topof the extrinsic variability, the solution of CME at each point intime can be attributed a corresponding transformation hCME of aprobability function. Alternatively, when approximating the CMEby sampling the underlying stochastic process with the SSA, singlerealizations of the SSA yield realizations of a transformation hSSA ofrealizations of x in Rnx . Note that hCME is a mapping between functional spaces with elements Pt ðxÞ7!PtþDt ðxÞ, whereas hSSA mapsfrom Rnx þnH ! Rnx i.e. ðxt ; hÞ7!xtþDt . To overcome the SSA’s hugecomputational load we used an approximate version, which is the sleaping algorithm implementation of StochKit2 (Sanft et al., 2011).Note that by combining intrinsic and extrinsic noise we have to dealwith a distributed CME. As for real valued stochastic variables, thecorresponding ensemble of probability functions or PDFs of theCME may be characterized by statistical moments. To obtain an estimate of the true average PDF q, the scheme is repeated n times andthe resulting distributions are used aswi ¼~q¼(5)nXk¼1are used to compute the mean and covarianceEðYÞ ¼2nHXwi yi(6)wi ðyi À EðYÞÞ Á ðyi À EðYÞÞT(7)i¼0CðYÞ ¼2nHXi¼0with the weightsw0 ¼jnH þ j(8)qxk bk ¼n1Xb ;qn k¼1 k(10)where we assumed that xk ¼ 1=n. A scheme of the algorithm isgiven in Figure 3 and the corresponding pseudo code can be foundin the Supplementary Material.The sigma points have been applied frequently to deterministicODEs e.g. (Flassig and Sundmacher, 2012; Schenkendorf et al.,2009; Toni and Tidor, 2013), whereas (Toni and Tidor, 2013) haveproposed a combination of sigma points and X-expansion to describe extrinsic and intrinsic variability. We are not aware of applications of sigma points to stochastic ODEs, which we apply here forsampling realizations of the CME to ultimately get an approximatesolution to a distributed CME. However, the accuracy of deterministic functions given in (Julier et al., 2000) should also apply forDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i319/3953968by gueston 07 January 2018i322each incremental time step s in the s-leaping realization, since foreach realization we have formally a deterministic mapping, whichwe can expand into a Taylor series as done in (Julier et al., 2000) forthe accuracy analysis. We note, however, that the convergency of~q ! q is in general not guaranteed. This depends on the ensemble ofhSSA and the choice of xk and bk . Even though our results show thatqthe proposed approach works well, a thorough analysis is requiredat this point which is, however, out of scope of this contribution.4 Benchmarking4.1 Comparison to extensive Monte Carlo samplingTo compare all methods illustrated in Figure 2 the Euclideandistanceqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ(11)D ¼ ð~ À qÞ2qis used as similarity measure (Cha, 2007) between the approximatesolution and the pseudo exact solution obtained by MC samplingD.Pischel et al.combined with the SSA. Since the final distribution of all methods isa random variable due to the underlying stochastic process, which isused for temporal evolution of the system, the computation was repeated nstat ¼ 103 times to derive a statistical statement. The meanand standard deviation of the Euclidean distance of the nstat distributions are exploited as measures for accuracy and convergence. InFigure 2D and E the dependency of accuracy and convergence onthe number of function evaluations for the Schlogl model introduced¨in the previous section is illustrated. For the histogram and kernelmethod, the number of function evaluations is given by the numberof MC samples and for the proposed method by nð2nH þ 1Þ. As canbe seen, our proposed method outperforms the others in accuracy aswell as convergence for up to 310 function evaluations, which highlights its computational benefit. For this illustration we used a kernelbandwidth of 10 and a histogram binwidth of 75.To further demonstrate the computational efficiency of the proposed method an example of gene expression resulting in multimodal probability distributions is investigated. The gene expressionsystem consists of the following reactionsk1geneoff ! geneonk2k3geneon ! geneon þ Ak4A þ B ! 1:Fig. 3. Workﬂow of the proposed algorithm. Note that in 4, one may assumeany PDF that is characterized by mean and covariance and also in 5 one mayuse different weights xk for each sample k. However, the choice is a priori notclear, and we therefore suggest to use a Gaussian PDF and an equal weighting(12)(13)(14)A single gene is considered which is able to flip between an activeand inactive state. In the active state the gene produces protein A,which is degraded by protein B. This example involves intrinsicnoise due to the low abundance of the gene, but also extrinsic noisedue to the distributed initial conditions of protein B. In Figure 4Athe proposed method is illustrated for protein A. Several distributions of bk (gray, dashed) are averaged to obtain the resulting apq~proximate distribution q (gray, solid). The approximate solutionmimics the distributed character of the pseudo reference obtained byMC sampling combined with the SSA, which constitutes a bimodaldistribution with—regarding any approximation approach—a challenging sharp peak for low abundances. The temporal evolution ofthe distributions of protein A and B are shown in Figure 4B and E.The approximation is very similar to the solution obtained by MCsampling combined with the SSA demonstrating the capability tob~Fig. 4. Comparison of performance. (A) Averaging of qk (gray, dashed) yields an approximate solution q (gray, solid) of the CME for protein A at the time point250 s. The pseudo exact solution obtained by MC sampling combined with the SSA is shown in red. For better visualization, the bk are scaled and hence smallerqthan the approximate distribution. The corresponding temporal evolution of the probability distributions for protein A and B are illustrated in (B) and (E). In (C)and (F), the difference in accuracy of the proposed method and the kernel approach is shown for protein A and B. Shades of red indicate superior and white the inferior accuracy of the proposed method (all negative values are marked white). The difference in convergence is illustrated in (D) and (G) for protein A and B witha similar color codeDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i319/3953968by gueston 07 January 2018Efficient simulation of intrinsic, extrinsic and external noise in biochemical systemsi323Table 1. Benchmark model descriptionModelktrueInitial conditionskopti8:8 Á 10À3Gene Model:k1NON0k110À2k2k3NOFF1k210À3geneoff ! geneongeneon ! 1geneon þ A6:6 Á 10À4NA0k35 Á 105:2 Á 10À1NBeN ðl ¼ 103 ; r ¼ 1:5 Á 102 Þk45 Á 10À75:2 Á 10À7NXeN ðl ¼ 2:75 Á 102 ; r ¼ 5Þk13 Á 10À73:1 Á 10À7NA105 (const.)k210À410À4NB2 Á 105 (const.)k3k410À33.510À33.5NE2:5 Á 102k110À410À4NSk4A þ B! 1À1N ðl ¼ 104 ; r ¼ 103 Þk25 Á 10À32:4 Á 10À3À1Schlogl Model (Schlogl, 1972):¨¨k12X þ A! 3Xk2k3B!Xk4Michaelis–Menten-Kinetics(Michaelis and Menten, 1913):k1E þ S ! ESk2k3ES ! P10À1NESNP00k310NVeN ðl ¼ 50; r ¼ 5Þk11:5 Á 10À1Virus Model (Gupta and Rawlings, 2014):k1V !G1:4 Á 10À1k2NG0k22 Á 102:1 Á 10À2k3NM0k35 Á 10À25:1 Á 10À2NP0k419:9 Á 10À1NPCeN ðl ¼ 50; r ¼ 10Þk15 Á 10À15:1 Á 10À1NmRNAG0k255.1NyEGFP0k31NmRNAR0k4G !G þMG ! 2Gk4M !M þPYeast Model (Poovathingal and Gunawan, 2010):k1PC3 ! PC3 þ mRNAGk2mRNAG ! 1k3mRNAG ! mRNAG þ yEGFPk4yEGFP ! 1À21À22:1 Á 10À2À12 Á 10k55 Á 105:3 Á 10À1k6k62 Á 10À12 Á 10À1k7k711k51 ! mRNARNTetR0mRNAR ! 1mRNAR ! mRNAR þ TetRk8k8TetR ! 1qualitatively model intrinsic and extrinsic noise simultaneously.Furthermore, accuracy and convergence were investigated in comparison to the kernel approach. Since a priori the optimal selection of thekernel bandwidth is not clear several bandwidths were tested systematically. In Figure 4C and F the difference of the mean Euclidean distance of the proposed method and the kernel approachEðDKernel Þ À EðDSP Þ is shown for the time point t ¼ 1000 s. If thisterm takes positive values (shades of red) the proposed method outperforms the kernel approach and for negative values (white) the kernel approach excels. The same applies to the difference of thestandard deviation of the Euclidean distance rstd ðDKernel Þ À rstd ðDSP Þ.For protein A, it can be seen that for less than 1:5 Á 103 functionevaluations the accuracy of the proposed method is always higherthan the accuracy of the kernel method. For more function evaluations an optimal bandwidth yields better results at the price of highercomputational costs, see Figure 4C. For protein B the proposedmethod is superior for all bandwidths and function evaluations, seeFigure 4F. Concerning the convergence it was found that the proposedmethod outperforms the kernel approach for both proteins, except forkernel bandwidths much larger than the spread of the underlyingÀ22 Á 102:1 Á 10À2distribution resulting in very low accuracy, see Figure 4D and G.With this example including multimodality, low and high copy numbers of chemical species the strengths of the proposed method hasbeen clearly demonstrated. Note that beforehand the optimal kernelbandwidth is not known and changes with time meaning that this parameter has to be estimated for the optimal representation of everyPDF. The proposed method avoids this computational demandingtask making it easy to handle and applicable.4.2 Application to parameter optimizationHaving shown that the proposed method yields convincing results,we utilize it for optimization of several biochemical reaction networks. In biology it is not possible to measure all parameters directly, which are necessary for computational modeling, leading toparameter estimation problems. The exact simultaneous simulationof intrinsic and extrinsic noise is computational extremely intense,wherefore approximate methods are needed. In this subsection, weuse the proposed method to estimate the unknown rate constants offive different example systems described in Table 1. Therefore, sixDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i319/3953968by gueston 07 January 2018i324reference measurements computed with the SSA and MC samplingwith equal time spacing were used for comparison with the resultsof our approximate method. For the example of gene expressiononly protein A and B are utilized for optimization, whereas for theother examples all chemical species were used. The objective function was the sum of the eucledian distance between the pseudo reference and the approximate solution for all chemical species and timepoints. The optimization was performed with a genetic algorithm,since gradient based algorithms fail for stochastic systems and geteasily stuck in local extrema (Poovathingal and Gunawan, 2010).For all example systems 500 generations with a population size of40 were used. In Table 1 the true parameters ktrue and the optimizedones kopti are provided. For all example systems, the optimized parameters are very close to the true parameters indicating that the proposed method yields excellent results and depicts a promisingtechnique for optimization of stochastic biochemical reaction systems. In the Supplementary Material further comments regardingthe benchmarking can be found.5 DiscussionIn this study, the problem of efficiently simulating biochemical reactionsystems containing intrinsic and extrinsic noise is addressed. We propose a novel algorithm relying on the SP approach and the s-leaping algorithm, which computes approximate solutions of the distributedCME. The method is benchmarked on several examples illustrating itscomputational benefit compared to others. Choosing only 2nH þ 1 SPsdeterministically the proposed method converges very fast to an approximate solution. The accuracy of our approximative approach isbarely dependent on system size but rather on the complexity of the stochastic mapping itself. This can be seen when the estimated PDFs of theSchlogl model are compared to the Virus model (one versus four states).¨Therefore, our results are very likely to also apply for large reaction systems. The strength of the method i.e. fast convergency at reduced sample size, also introduces some of its weaknesses. Since only a fewsamples from the initial distribution are chosen it might be sampled toosparsely. Additionally the choice of PDF kernels is a priori not clear.Although the proposed method might not converge to the exact solution of the CME (see saturation behavior in Fig. 2D) it qualitatively describes important characteristics of the underlying distribution (see Fig.4A, B and E, and also additional illustrations of approximate versuspseudo exact solution in the Supplementary Material).The method may serve as a tool for rapid analysis and optimization of a stochastic system. For precise predictions, we suggest to useour method as a starting point for refinement by means of numerically demanding, but asymptotically exact methods. Although thereis a lot of development making stochastic analysis tools with state ofthe art algorithms and fast implementations (Drawert et al., 2016;Fan et al., 2016; Somogyi et al., 2015) available for a broad community the optimization of stochastic biochemical systems is hardlyperformed due to its challenging computational effort (Poovathingaland Gunawan, 2010). With this study, we demonstrated that theproposed method is capable of describing biochemical systems containing intrinsic and extrinsic noise and it represents a promisingtool suited for optimization and analysis of distributed, stochasticbiochemical reaction systems.FundingThis work was supported by the Federal Ministry of Education and Researchin Germany [031A304 to D.P.].D.Pischel et al.ReferencesAzunre,P. et al. (2011) Mass ﬂuctuation kinetics: analysis and computation ofequilibria and local dynamics. IET Syst. Biol., 5, 325–335.Cao,Y. et al. (2006) Efﬁcient step size selection for the tau-leaping simulationmethod. J. Chem. Phys., 124, 044109.Cha,S.-H. (2007) Comprehensive survey on distance/similarity measures between probability density functions. Int. J. Math. Models Methods Appl.Sci., 1, 300–307.Delvigne,F. et al. (2014) Metabolic variability in bioprocessing: implicationsof microbial phenotypic heterogeneity. Trends Biotechnol., 32, 608–616.Drawert,B. et al. (2016) Stochastic simulation service: bridging the gap between the computational expert and the biologist. PLoS Comput. Biol., 12,e1005220.Fan,S. et al. (2016) Means: python package for moment expansion approximation, inference and simulation. Bioinformatics, 32, 2863–2865.Flassig,R.J., and Sundmacher,K. (2012) Optimal design of stimulus experiments for robust discrimination of biochemical reaction networks.Bioinformatics, 28, 3089–3096.Fu,J. et al. (2013) Time dependent solution for acceleration of tau-leaping.J. Comput. Phys., 235, 446–457.Gillespie,D. et al. (2013) Perspective: stochastic algorithms for chemical kinetics. J. Chem. Phys., 138, 170901.Gillespie,D.T. (1977) Exact stochastic simulation of coupled chemical reactions. J. Phys. Chem., 81, 2340–2361.Gupta,A., and Rawlings,J.B. (2014) Comparison of parameter estimationmethods in stochastic chemical kinetic models: examples in systems biology.AIChE J., 60, 1253–1268.Julier,S. et al. (2000) A new method for the nonlinear transformation of meansand covariances in ﬁlters and estimators. IEEE Trans. Autom. Control, 45,477–482.Kazeev,V. et al. (2014) Direct solution of the chemical master equation usingquantized tensor trains. PLoS Comput. Biol., 10, e1003359.Lakatos,E. et al. (2015) Multivariate moment closure techniques for stochastickinetic models. J. Chem. Phys., 143, 094107.Lee,C.H. et al. (2009) A moment closure method for stochastic reaction networks. J. Chem. Phys., 130, 134107.Fernandes,R.L. et al. (2011) Experimental methods and modeling techniquesfor description of cell population heterogeneity. Biotechnol. Adv., 29,575–599.Marchetti,L. et al. (2016) HRSSA: efﬁcient hybrid stochastic simulation forspatially homogeneous biochemical reaction networks. J. Comput. Phys.,317, 301–317.Michaelis,L., and Menten,M.L. (1913) Die kinetik der invertinwirkung.Biochem. Z., 49, 333–369.Munsky,B., and Khammash,M. (2006) The ﬁnite state projection algorithmfor the solution of the chemical master equation. J. Chem. Phys., 124,044104.Patnaik,P.R. (2006) External, extrinsic and intrinsic noise in cellular systems:analogies and implications for protein synthesis. Biotechnol. Mol. Biol.Rev., 1, 121–127.Poovathingal,S.K., and Gunawan,R. (2010) Global parameter estimationmethods for stochastic biochemical systems. BMC Bioinformatics, 11, 414.Sanft,K.V. et al. (2011) Stochkit2: software for discrete stochastic simulationof biochemical systems with events. Bioinformatics, 27, 2457–2458.Schenkendorf,R. et al. (2009) Optimal experimental design with the sigmapoint method. IET Syst. Biol., 3, 10–23.Schlogl,F. (1972) Chemical reaction models for non-equilibrium phase transi¨tions. Zeitschrift Fur Physik, 253, 147–161.¨Somogyi,E. et al. (2015) LibRoadRunner: a high performance SBML simulation and analysis library. Bioinformatics, 31, 3315–3321.Spiller,D.G. et al. (2010) Measurement of single-cell dynamics. Nature, 465,736–745.Toni,T., and Tidor,B. (2013) Combined model of intrinsic and extrinsic variability for computational network design with application to synthetic biology. PLoS Comput. Biol., 9, e1002960.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i319/3953968by gueston 07 January 2018
5028881986002	PMID28881986	5028881986	https://watermark.silverchair.com/btx252.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881986.main.pdf	Bioinformatics, 33, 2017, i190–i198doi: 10.1093/bioinformatics/btx252ISMB/ECCB 2017Predicting multicellular function throughmulti-layer tissue networksMarinka Zitnik and Jure Leskovec*Department of Computer Science, Stanford University, Stanford, CA 94305, USA*To whom correspondence should be addressed.AbstractMotivation: Understanding functions of proteins in speciﬁc human tissues is essential for insightsinto disease diagnostics and therapeutics, yet prediction of tissue-speciﬁc cellular function remainsa critical challenge for biomedicine.Results: Here, we present OhmNet, a hierarchy-aware unsupervised node feature learning approach for multi-layer networks. We build a multi-layer network, where each layer represents molecular interactions in a different human tissue. OhmNet then automatically learns a mapping ofproteins, represented as nodes, to a neural embedding-based low-dimensional space of features.OhmNet encourages sharing of similar features among proteins with similar network neighborhoods and among proteins activated in similar tissues. The algorithm generalizes prior work, whichgenerally ignores relationships between tissues, by modeling tissue organization with a rich multiscale tissue hierarchy. We use OhmNet to study multicellular function in a multi-layer protein interaction network of 107 human tissues. In 48 tissues with known tissue-speciﬁc cellular functions,OhmNet provides more accurate predictions of cellular function than alternative approaches, andalso generates more accurate hypotheses about tissue-speciﬁc protein actions. We show that taking into account the tissue hierarchy leads to improved predictive power. Remarkably, we alsodemonstrate that it is possible to leverage the tissue hierarchy in order to effectively transfer cellular functions to a functionally uncharacterized tissue. Overall, OhmNet moves from ﬂat networks tomultiscale models able to predict a range of phenotypes spanning cellular subsystems.Availability and implementation: Source code and datasets are available at http://snap.stanford.edu/ohmnet.Contact: jure@cs.stanford.edu1 IntroductionA unified view of human diseases and cellular functions across abroad range of human tissues is essential, not only for understandingbasic biology but also for interpreting genetic variation and developing therapeutic strategies (Greene et al., 2015; GTEx et al., 2015;Okabe and Medzhitov, 2014; Yeger-Lotem and Sharan, 2015). Inparticular, the precise functions of proteins frequently depend on thetissue, and different proteins can have different cellular functions indifferent tissues (Fagerberg et al., 2014; Guan et al., 2012; Hu et al.,2016; Lois et al., 2002; Magger et al., 2012; Rakyan et al., 2008;Yeger-Lotem and Sharan, 2015).While our view of the human protein–protein interaction (PPI)network as a key source for studying protein function is constantlyexpanding, much less is known about networks that form in biologically important environments such as within distinct tissues or inspecific diseases (Yeger-Lotem and Sharan, 2015). Although incredibly influential, current computational methods for extractingfunctional information from protein interaction networks lack tissuespecificity as they assume that cellular function is constant acrossorgans and tissues (Barutcuoglu et al., 2006; Kramer et al., 2014;Mostafavi et al., 2008; Radivojac et al., 2013; Stojanova et al., 2013; Zitnik and Zupan, 2015). In other words, cellular functionsin heart are assumed to be the same as functions in skin. The methods are, hence, less successful in constructing accurate maps of bothwhere and how proteins act. In particular, existing network-basedmethods are probably not the ultimate representation of human tissues for three reasons. (1) First, current methods for cellular function prediction on networks (Mostafavi and Morris, 2009; Radivojac et al., 2013; Vidulin et al., 2016; Zitnik and Zupan,2015) do not model networks with regards to patterns that span tissues, organs, and cellular systems. This means that a complex tissueinvolving a multiscale hierarchy of cellular subsystems is not readilycaptured by current models (Carvunis and Ideker, 2014; Dutkowskiet al., 2012). (2) Second, many genome-scale functional mapsCV The Author 2017. Published by Oxford University Press.i190This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018Predicting multicellular function through multi-layer tissue networks(Costanzo et al., 2016; Kitsak et al., 2016; Kotlyar et al., 2015;Lopes et al., 2011; Rolland et al., 2014; Wang et al., 2016b) are descriptive maps of physical or functional protein connectivity that donot, by themselves, predict cellular function. (3) Third, only fewcomputational approaches (Antanaviciute et al., 2015; Ganegodaet al., 2014; Guan et al., 2012; Magger et al., 2012) used tissuespecific information to identify novel genes and relationships between genes. However, their focus was to leverage tissue specificityto improve prediction of global cellular functions and global genedisease associations. As such, these approaches account for tissuespecificity, but they do not resolve the challenge of predicting gene–function relationships that might be specific to a particular tissue.To be able to predict a range of tissue-specific functions one needsto design scalable multiscale models that can relate tissues to eachother, extract rich feature representations for proteins in each tissuespecific network, and then use the extracted features for tissuespecific cellular function prediction.1.1 Present workWe present OhmNet, an algorithm for hierarchy-aware unsupervised feature learning in multi-layer networks. Our focus is on learning features of proteins in different tissues. We represent each tissueas a network, where nodes represent proteins. Tissue networks actas layers in a multi-layer network, where we use a hierarchy tomodel dependencies between the layers (i.e. tissues) (Fig. 1). Wethen develop a computational framework that learns features ofeach node (i.e. protein) by taking into consideration connections between the nodes within each layer, together with inter-layer relationships between proteins active on different layers. More precisely,our approach embeds each protein in each tissue in a d-dimensionalfeature space such that proteins with similar network neighborhoodsin similar tissues are embedded closely together.In OhmNet, we define an objective function that is independent ofthe downstream prediction task, meaning that the feature representations are learned in a purely unsupervised way. This results in taskindependent features, that, as we show, outperform task-specificapproaches in predictive accuracy. Furthermore, since our featuresare not designed for a specific downstream prediction task, they generalize across a wide variety of tasks and tissues. For example, we usethe learned features to study protein functions across different cellularsystems (e.g. cell types, tissues, organs and organ systems).Fig. 1. A multi-layer network with four layers, where each layer represents atissue-speciﬁc PPI network. The hierarchy M encodes biological similaritiesbetween the tissues at multiple scales. OhmNet embeds each node in a d-dimensional feature space, which we use for tissue-speciﬁc protein functionprediction. For example, layers Gi, Gj, Gk and Gl, might represent brain tissuespeciﬁc interaction networks in cerebrum, hypothalamus, tegmentum andmedullai191OhmNet builds on recent success of unsupervised representationlearning methods based on neural architectures (Grover andLeskovec, 2016; Mikolov et al., 2013). In particular, we develop anew form of structured regularization, which makes OhmNet especially suitable for multi-layer interdependent networks. Our keycontribution lies in modeling the tissue taxonomy constraints byencoding relationships between the tissues in a tissue hierarchy andthen using the structured regularization with the tissue hierarchy(Fig. 1). This way OhmNet effectively learns multiscale feature representations for proteins that are consistent with the tissuehierarchy.Our experiments focus on three tasks defined on a multi-layertissue network: (1) a multi-label node classification task, whereevery protein is assigned zero, one or more tissue-specific cellularfunctions; (ii) a transfer learning task, where we predict cellularfunctions for a protein in one tissue based on classifiers trained onfeatures from other tissues; and (iii) a network-embedding visualization task, where we create meaningful tissue-specific visualizationsthat lay out proteins on a 2D space. Since the multiscale protein feature vectors returned by OhmNet are task-independent, we useOhmNet one time only to learn the features for proteins in every tissue and at every scale of the tissue hierarchy. We can then solve thecellular function prediction task for any tissue using the appropriatetissue-specific protein features.We contrast OhmNet’s performance with that of state-of-the-artapproaches for feature learning (Cannistraci et al., 2013; Groverand Leskovec, 2016; Nickel et al., 2011; Tang et al., 2015;),approaches for tissue-independent cellular function prediction(Mostafavi et al., 2008; Zuberi et al., 2013), and approaches for prioritization of disease-causing genes in tissue-specific protein interaction networks (Guan et al., 2012; Magger et al., 2012), which weadapted for the cellular function prediction task. We experimentwith a multi-layer network having 107 genome-wide tissue-specificprotein interaction layers, and we consider a tissue hierarchydescribing 219 cellular systems in the human body. Experimentsdemonstrate that tissue-specific protein interaction layers providethe necessary protein and tissue context for predicting cellular function. OhmNet outperforms alternative approaches by up to 14.9%on multi-label classification and up to 20.3% on transfer learning.Another notable finding is that OhmNet outperforms alternativeapproaches, which are based on non-hierarchical versions of thesame dataset, alluding to the benefits of modeling hierarchical tissueorganization. We observe that neglecting the existence of tissues oraggregating tissue-specific interaction networks into a single network discards important biological information and affects performance on multi-label classification and transfer learning tasks.Finally, we exemplify the utility of OhmNet for exploring the multiscale structure of tissues. In a case study on nine brain tissue networks, we show that OhmNet’s features inherently encode amultiscale brain organization.The rest of the article is organized as follows. In Section 2, webriefly survey related work in feature learning for networks. We present the technical details of OhmNet in Section 3. In Section 4, wedescribe the multi-layer tissue network and the tissue hierarchy. Weempirically evaluate OhmNet in Section 5 and conclude with directions for future work in Section 6.2 Related workWe have seen in Section 1 that despite the abundance of methods forcellular function prediction, only a few, if any, take into accountDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018i192biologically important contexts given by human tissues. We nowturn our focus to the problem of feature learning in networks.Most approaches for automatic (i.e. non-hand-engineered) feature learning in networks can be categorized into matrix factorization and neural network embedding-based approaches. In matrixfactorization, a network is expressed as a data matrix where theentries represent relationships. The data matrix is projected to alow-dimensional space using linear techniques based on SVD (Tanget al., 2012), or nonlinear techniques based on multi-dimensionalscaling (Belkin and Niyogi, 2001; Hou et al., 2014; Tenenbaumet al., 2000). These methods have two important drawbacks. First,they do not account for important structures typically exhibited innetworks such as high sparsity and skewed degree distribution.Second, matrix factorization methods perform a global factorizationof the data matrix while a local-centric method might often yieldmore useful feature representations (Kramer et al., 2014).Limitations of matrix factorization are overcome by neural network embeddings. Recent studies focused on embedding nodes intolow-dimensional vector spaces by first using random walks to construct the network neighborhood of every node in the graph, andthen optimizing an objective function with network neighborhoodsas input (Grover and Leskovec, 2016; Perozzi et al., 2014; Tanget al., 2015). The objective function is carefully designed to preserveboth the local and global network structures. A state-of-the-art neural network embedding algorithm is the Node2vec algorithm(Grover and Leskovec, 2016), which learns feature representationsas follows: it scans over the nodes in a network, and for every nodeit aims to embed it such that the node’s features can predict nearbynodes, that is, node’s feature predict which other nodes are part ofits network neighborhood. Node2vec can explore different networkneighborhoods to embed nodes based on the principles of homophily (i.e. network communities) as well as structural equivalence(i.e. structural roles of nodes).However, a challenging problem for neural network embeddingbased methods is to learn features in multi-layer networks. Existingmethods can learn features in multi-layer networks either by treatingeach layer independently of other layers, or by aggregating the layersinto a single (weighted) network. However, neglecting the existenceof multiple layers or aggregating the layers into a single network,alters topological properties of the system as well as the importanceof individual nodes with respect to the entire network structure (DeDomenico et al., 2016). This is a major shortcoming of prior workthat can lead to a wrong identification of the most versatile nodes(De Domenico et al., 2015) and overestimation of the importance ofmore marginal nodes (De Domenico et al., 2014). As we shall show,this shortcoming also affects predictive accuracy of the learned features. Our approach OhmNet overcomes this limitation since itlearns features in a multi-layer network in the context of the entiresystem structure, bridging together different layers and generalizingmethods developed for learning features in single-layer networks.In biological domains, measures based on similarities of nodes’extended network neighborhoods are well established for predictingprotein functions. Several approaches use graphlets (Pr ulj, 2007) tozsystematically describe network structure around each node. This isdone by counting how many instances of small subgraph patternsoccur in the network neighborhood of a given node. Graphlet-basedmethods, such as graphlet degree vectors (Hayes et al., 2013), canthus be seen as an alternative approach for extracting feature representations for nodes. In contrast to neural embedding-based methods, such as OhmNet, which learn continuous featurerepresentations, graphlet-based methods return discrete counts ofmotif occurrences. Further, graphlet-based methods in their currentM.Zitnik and J.Leskovecform cannot be applied to multi-layer networks without collapsingthe network layers into one network.Finally, there exists recent work for task-dependent featurelearning based on graph-specific deep network architectures (Liet al., 2015; Xiaoyi et al., 2014; Wang et al., 2016a; Zhai andZhang, 2015). Our approach differs from those approaches in twoimportant ways. First, those architectures are task-dependent, meaning they directly optimize the objective function for a downstreamprediction task, such as cellular function prediction in a particulartissue, using several layers of nonlinear transformations. Second,those architectures do not model rich graph structures, such asmulti-layer networks with hierarchies.3 Feature learning in multi-layer networksWe formulate feature learning in multi-layer networks as a maximum likelihood optimization problem. Let V be a given set of Nnodes (e.g. proteins) fu1 ; u2 ; . . . ; uN g; and let there be K types ofedges (e.g. protein interactions in different tissues) between pairs ofnodes u1 ; u2 ; . . . ; uN . A multi-layer network is a general system inwhich each biological context is represented by a distinct layer i(where i ¼ 1; 2; . . . ; K) of a system (Fig. 1). We use the term singlelayer network (layer) for the network Gi ¼ ðVi ; Ei Þ that indicates theedges Ei between nodes Vi   V within the same layer i. Our analysisis general and applies to any (un)directed, (un)weighted multi-layernetwork.We take into account the possibility that a node uk from layer ican be related to any other node uh in any other layer j. We encodeinformation about the dependencies between layers in a hierarchicalmanner that we use in the learning process. Let the hierarchy be adirected tree M defined over a set M of elements by the parent-childrelationships given by p : M ! M; where pðiÞ is the parent of element i in the hierarchy (Fig. 1). Let T & M be the set of all leaves inthe hierarchy. Let Ti be the set of all leaves in the sub-hierarchyrooted at i. We assume that each layer Gi is attached to one leaf inthe hierarchy. As a result, the hierarchy M has exactly K leaves. Forconvenience, let Ci denote the set of all children of element i in thehierarchy.The problem of feature learning in a multi-layer network is tolearn functions f1 ; f2 ; . . . ; fK , such that each function fi : Vi ! Rdmaps nodes in Vi to feature representations in Rd . Here, d is a parameter specifying the number of dimensions in the feature representation of one node. Equivalently, fi is a matrix of jVi j Â dparameters.We proceed by describing OhmNet, our approach for featurelearning in multi-layer networks. OhmNet has two components:••single-layer network objectives, in which nodes with similar network neighborhoods in each layer are embedded close together,hierarchical dependency objectives, in which nodes in nearbylayers in the hierarchy are encouraged to share similar features.We start by describing the model that considers the layers independently of each other. We then extend the model to encourage nodeswhich are nearby in the hierarchy to have similar features.3.1 Single-layer network objectivesWe start by formalizing the intuition that nodes with similar network neighborhoods in each layer should share similar features. Forthat, we specify one objective for each layer in a given multi-layernetwork. We shall later discuss how OhmNet incorporates thedependencies between different layers.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018Predicting multicellular function through multi-layer tissue networksOur goal is to take layer Gi and learn fi which embeds nodesfrom similar network regions, or nodes with similar structural roles,closely together. In OhmNet, we aim to achieve this goal by specifying the following objective function for each layer Gi. Given a nodeu 2 Vi , the objective function xi seeks to predict, which nodes aremembers of u’s network neighborhood Ni ðuÞ based on the learnednode features fi:xi ðuÞ ¼ log PrðNi ðuÞjfi ðuÞÞ;(1)where the conditional likelihood of every node-neighborhoodnode pair is modeled independently as PrðNi ðuÞjfi ðuÞÞ ¼Qv2Ni ðuÞ Prðvjfi ðuÞÞ. The conditional likelihood is a softmax unitparameterized by a dot product of nodes’ features, which corresponds to a single-layer feed-forward neural network: Prðvjfi ðuÞÞ ¼Pexp ðfi ðvÞfi ðuÞÞ= z2Vi exp ðfi ðzÞfi ðuÞÞ. Given a node u, maximization of xi ðuÞ tries to maximize classification of nodes in u’s networkneighborhood based on u’s learned representation.The objective Xi is defined for each layer i:XXi ¼xi ðuÞ; for i ¼ 1; 2; . . . ; K:(2)u2ViThe objective is inspired by the intuition that nodes with similar network neighborhoods tend to have similar meanings, or roles, in anetwork. It formalizes this intuition by encouraging nodes in similarnetwork neighborhoods to share similar features.We found that a flexible notion of a network neighborhood Ni iscrucial to achieve excellent predictive accuracy on a downstreamcellular function prediction task (Grover and Leskovec, 2016). Forthat reason, we use a randomized procedure to sample many different neighborhoods of a given node u. Technically, the networkneighborhood Ni ðuÞ is a set of nodes that appear in an appropriatelybiased random walk defined on layer Gi and started at node u(Grover and Leskovec, 2016). The neighborhoods Ni ðuÞ are not restricted to just immediate neighbors but can have vastly differentstructures depending on the sampling strategy.Next, we expand OhmNet’s single-layer network objectives toleverage information provided by the tissue taxonomy and this wayinform embeddings across different layers.3.2 Hierarchical dependency objectivesSo far, we specified K layer-by-layer objectives each of which estimatesnode features in its layer independently of node features in other layers.This means that nodes in different layers representing the same entityhave features that are learned independently of each other.To harness the dependencies between the layers, we expandOhmNet with terms that encourage sharing of protein features between the layers. Our approach is based on the assumption thatnearby layers in the hierarchy are semantically close to each otherand hence proteins/nodes in them should share similar features. Forexample, in the tissue multi-layer network, we model the fact thatthe “medulla” layer is part of the “brainstem” layer, which is, inturn, part of the “brain” layer. We use the dependencies among thelayers to define a joint objective for regularization of the learned features of proteins.We propose to use the hierarchy in the learning process by incorporating a recursive structure into the regularization term for every element in the hierarchy M. Specifically, we propose the following form ofregularization for node u that resides in element i of the hierarchy M:1ci ðuÞ ¼ jjfi ðuÞ À fpðiÞ ðuÞjj2 :22(3)i193This recursive form of regularization enforces the features of node uin the hierarchy i to be similar to the features of node u in i’s parentpðiÞ under the Euclidean norm. When regularizing features of allnodes across all elements of the hierarchy, we obtain:Xci ðuÞ; where Li ¼ [j2Ti Vj(4)Ci ¼u2LiIn words, we specify the features for both leaf as well as internal, i.e.non-leaf, elements in the hierarchy, and we regularize the features ofsibling (i.e. sharing the same parent) hierarchy elements toward features in the common parent element in the hierarchy.3.2.1 Node features at multiple scalesIt is important to notice that OhmNet’s structured regularizationallows us to learn feature representations at multiple scales. For example, consider a multi-layer network in Figure 2, consisting of fourlayers that are interrelated by a two-level hierarchy. OhmNet learnsthe mappings fi, fj, fk and fl that map nodes in each layer into a d-dimensional feature space. In addition, OhmNet also learns the mapping f2 representing features for nodes appearing in the hierarchyleaves T2, i.e. Vi [ Vj , at an intermediate scale, and the mapping f1representing features for nodes appearing in the hierarchy leaves T1,i.e. Vi [ Vj [ Vk [ Vl , at the highest scale.The modeling of relationships between layers in a multi-layernetwork has several implications:••••First, the model encourages nodes which are in nearby layers inthe hierarchy to share similar features.Second, the model shares statistical strength across the hierarchyas nodes in different layers representing the same protein sharefeatures through ancestors in the hierarchy.Third, this model is more efﬁcient than the fully pairwise model.In the fully pairwise model, the dependencies between layers aremodeled by pairwise comparisons of nodes across all pairs oflayers, which takes OðK2 NÞ time, where K is the number oflayers and N is the number of nodes. In contrast, OhmNet models inter-layer dependencies according to the parent-child relationships speciﬁed by the hierarchy, which takes only OðjMjNÞtime. Since OhmNet’s hierarchy is a tree, it holds that jMj ( K2 ,meaning that the proposed model scales more easily to largemulti-layer networks than the fully pairwise model.Finally, the hierarchy is a natural way to represent and model biological systems spanning many different biological scales(Carvunis and Ideker, 2014; Greene et al., 2015; Yu et al., 2016).3.3 Full OhmNet modelGiven a multi-layer network consisting of layers G1 ; G2 ; . . . ; GK ,and a hierarchy encoding relationships between the layers, theFig. 2. A multi-layer network with four layers. Relationships between thelayers are encoded by a two-level hierarchy M. Leaves of the hierarchy correspond to the network layers. Given networks Gi and hierarchy M, OhmNetlearns node embeddings captured by functions fiDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018i194M.Zitnik and J.LeskovecOhmNet’s goal is to learn the functions f1 ; f2 ; . . . ; fK that map fromnodes in each layer to feature representations. OhmNet achievesthis goal by fitting its feature learning model to a given multi-layernetwork and a given hierarchy, i.e. by finding the mapping functionsf1 ; f2 ; . . . ; fK that maximize the data likelihood.Given the data, OhmNet aims to solve the following maximumlikelihood optimization problem:XXmaxXi À kCj ;(5)f1 ;f2 ;...;fjMji2Tj2Mwhich includes the single-layer network objectives for all networklayers, and the hierarchical dependency objectives for all hierarchyelements. In Equation (5), parameter k is a user-specified parameterrepresenting the regularization strength. While the optimizationproblem in Equation (5) is non-convex due to the non-convexity ofthe single-layer objective (Grover and Leskovec, 2016), stochasticgradient with negative sampling can be used to efficiently solve theproblem.One appealing property of OhmNet is that by solving the problem in Equation (5) we obtain estimates for functions f1 ; f2 ; . . . ; fKlocated in the leaf elements of the hierarchy (i.e. layers of a givenmulti-layer network), as well as estimates for functions fKþ1 ; fKþ2 ;. . . ; fjMj located in the internal elements of the hierarchy.approach has the advantage that it can easily incorporate the closedform updates developed for the internal objects of the hierarchy(step 11 in Algorithm 1), thereby accelerating the convergence ofOhmNet algorithm. For each leaf object i, OhmNet isolates theterms in the optimization problem in Equation (5) that depend onthe model parameters defining function fi. OhmNet then optimizesEquation (5) by performing one epoch of stochastic gradient descent(SGD1) over fi’s model parameters (step 15 in Algorithm 1).The two phases of OhmNet are executed sequentially. TheOhmNet algorithm scales to large multi-layer networks becauseeach phase is parallelizable and executed asynchronously. Thechoice to model the dependencies between network layers using thehierarchical model requires OðjMjNÞ time instead of the fully pairwise model, which requires OðK2 NÞ time.4 Tissue-specific interactome dataTo construct the human PPI network, tissue-specific network layers,tissue hierarchy and tissue-specific gene–function relationships, wedownloaded and used standard protein, tissue and function information from various reputable data sources.4.1 Tissue hierarchy3.4 The OhmNet algorithmThe pseudocode for OhmNet is given in Algorithm 1.In the first phase, OhmNet applies the Node2vec’s algorithm(Grover and Leskovec, 2016) to construct network neighborhoodsfor each node in every layer. Given a layer Gi and a node u 2 Vi , thealgorithm simulates a user-defined number of fixed length randomwalks started at node u (step 4 in Algorithm 1).In the second phase, OhmNet uses an iterative approach inwhich features associated with each object in the hierarchy are iteratively updated by fixing the rest of the features. The iterativeAlgorithm 1. The OhmNet algorithmInput: Multi-layer network, ðG1 ; G2 ; . . . ; GK Þ with Gi ¼ ðVi ; Ei Þ,Hierarchy, M, Feature representation size, d, Network neighborhood strategy, S, Regularization strength, k1: for i 2 T do2:for u 2 Vi do3:Ni ðuÞ ¼ Node2vecWalkðGi ; u; SÞ (Grover andLeskovec, 2016)4:end for5: end for6: while f1 ; f2 ; . . . ; fjMj not converged do7:for i 2 M do8:if i 2 T then9:for u 2 Vi do10:fi ðuÞ ¼ SGD1ðNi ðuÞ; d; kÞ by Equation (5)11:end for12:else 13:for u 2 [j2Ti Vj  doP14:fi ðuÞ ¼ jCi1fpðiÞ ðuÞ þ c2Ci fc ðuÞjþ115:end for16:end if17:end for18: end while19: return f1 ; f2 ; . . . ; fjMjWe retrieved the mapping of tissues in the Human Protein ReferenceDatabase (HPRD) (Prasad et al., 2009) to tissues in the BRENDATissue Ontology (Chang et al., 2014) from Greene et al. (2015). Thedata is provided as a supplementary dataset in Greene et al. (2015).The hierarchical relationships between tissues were then determinedby the directed acyclic graph structure of the BRENDA TissueOntology. Examples of tissues included: muscle, adrenal cortex,bone marrow and spleen (Fig. 3).4.2 Tissue-specific interaction networksWe took the gene-to-tissue mapping compiled by Greene et al.(2015). Greene et al. mapped genes to HPRD tissues based on lowthroughput tissue-specific gene expression data. The gene-to-tissuemapping was then combined with the human PPI network. The resulting multi-layer tissue network had 107 layers, each layer corresponded to a PPI network specific to a particular tissue. Details areprovided next.Fig. 3. The tissue hierarchy considered in this study. The tissue hierarchy is adirected tree deﬁned over jMj ¼ 219 tissue terms from the BRENDA TissueOntology. Edges in the tree point from children to parents based on ontological relationships: “develops_from”, “is_a”, “part_of” and “related_to”.The K ¼ 107 tissues with tissue-speciﬁc protein interaction networks are theblue leaves in the treeDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018Predicting multicellular function through multi-layer tissue networksi195The human PPI network was collected from Orchard et al.(2013), Rolland et al. (2014), Chatr-Aryamontri et al. (2015),Prasad et al. (2009), Ruepp et al. (2010) and Menche et al. (2015).Considered were physical PPIs with supported by experimental evidence. It should be noted that interactions based on gene expressionand evolutionary data were not considered. The global (unweighted)human PPI network has 21 557 proteins interconnected by 342 353interactions. The reader is referred to Menche et al. (2015) for a detailed description of the data.For each of 107 tissues, a tissue-specific human PPI network wasconstructed based on the global PPI network. For a given tissue,every edge in the global PPI network was labeled as specifically coexpressed in that tissue using the criterion developed by Greeneet al. (2015). Greene et al. labeled each edge as specifically coexpressed if either both proteins are specific to that tissue or oneprotein is tissue-specific and the other is ubiquitous. Lists of specifically co-expressed proteins were retrieved from Greene et al. (2015).Finally, the PPI network specific to a particular tissue is a subnetwork of the global PPI network, induced by the set of specificallyco-expressed edges in that tissue.We evaluate the performance of OhmNet against the followingfeature-learning approaches:4.3 Tissue-specific cellular functions and geneannotationsIn addition, we evaluate the performance of OhmNet against thefollowing tissue-specific/agnostic function prediction approaches:Associations between tissues and cellular functions were retrievedfrom Greene et al. (2015). Greene et al. manually curated biologicalprocesses in the Gene Ontology (GO) (Ashburner et al., 2000) andmapped them to tissues in the BRENDA Tissue Ontology (Changet al., 2014) based on whether a given biological process is specifically active in a given tissue. The data is provided as a supplementardataset in Greene et al. (2015). An example of a cellular functiontissue pair is "low-density lipoprotein particle remodeling" in theblood plasma tissue.All gene annotations were propagated along the ontology hierarchy. Considered are functions with at least 15 annotated proteins(Guan et al., 2012). In total, there are 584 tissue-specific cellularfunctions covering 48 distinct tissues. Each tissue-specific function is assigned to one or more leaves in the tissue hierarchy(Section 4.1).•••••••5 ResultsThe OhmNet’s objective in Equation (5) is independent of anydownstream task. This flexibility offered by OhmNet makes thelearned feature representations suitable for a variety of analyticstasks discussed below.5.1 Prediction of tissue-specific cellular functions5.1.1 Experimental setupWe view the problem of predicting cellular functions as solving amulti-label node classification task. Here, every node (i.e. protein) isassigned one or more labels (i.e. cellular functions from the GO)from a finite set of labels (i.e. all cellular functions in the GO, seeSection 4.3).We apply OhmNet, which for every node in every layer learns aseparate feature vector in an unsupervised way. Thus, for every layerand every function we then train a separate one-versus-all linearclassifier using the modified Huber loss with elastic net regularization. Using cross validation, we observe 90% of proteins and alltheir cellular functions across the layers during the training phase.The task is then to predict the tissue-specific functions for the remaining 10% of proteins.RESCAL tensor decomposition (Nickel et al., 2011): This is a tensorfactorization approach that takes the multi-layer network structureinto account. Given Xi, a normalized Laplacian matrix of layer Gi,matrix Xi is factorized as: Xi ¼ ARi AT , for i ¼ 1; 2; . . . ; K: Here,matrix A contains d-dimensional feature representation for nodes.Minimum curvilinear embedding (Cannistraci et al., 2013): Thisis a non-linear unsupervised framework that embeds nodes in alow-dimensional space. The approach was originally developedfor protein interaction prediction, aiming to embed protein pairsrepresenting good candidate interactions closer to each other. Itutilizes a network denoising method as well as structural information provided by the PPI network topology.LINE (Tang et al., 2015): This approach ﬁrst learns d=2 dimensionsbased on immediate network neighbors of nodes, and then the nextd=2 dimensions based on network neighbors at a 2-hop distance.Node2vec (Grover and Leskovec, 2016): This approach learns ddimensional features for nodes based on a biased random walkprocedure that ﬂexibly explores network neighborhoods of nodes.GeneMania (Zuberi et al., 2013): This is a supervised approachthat takes a multi-layer network as input and directly predictscellular functions in two separate phases. In the ﬁrst phase, it aggregates the layers into one weighted network by weighting thelayers according to their utility for predicting a given function. Itthen uses a label propagation algorithm on the weighted networkto predict the function.Tissue-speciﬁc network propagation (Magger et al., 2012): This approach assigns a prior score to proteins associated with known functions that are phenotypically similar to the query function. This scoreis then propagated through a network in an iterative process. The approach was developed for tissue-speciﬁc disease gene prioritization.Network-based tissue-speciﬁc support vector machine (SVM)(Guan et al., 2012): This approach adopts the network-basedcandidate gene prediction scheme. Essentially, the connectionweights in a network to all positive examples (i.e. genes alreadyknown to be related to a phenotype) are utilized as features forlinear SVM classiﬁcation. The approach was developed fortissue-speciﬁc phenotype and disease gene prioritization.The parameter settings for every approach are determined using internal cross-validation procedure with a grid search over candidateparameter values. Specifically, d ¼ 128 is used in all experiments.Last, we aim to evaluate the benefit of our proposed multi-layerrepresentation of the tissue networks. To this end we also considertwo additional network representations:••Independent layers: This approach learns features for nodes ineach layer by running LINE or Node2vec algorithm on one layerat a time and independently of other layers in the network.Collapsed layers: This approach ﬁrst aggregates the layers into asingle network by connecting nodes representing the same entityin different layers to each other. It then learns feature for nodesin the aggregated network.5.1.2 Experimental resultsTable 1 and Figure 4 give the area under the curve (AUC) scores oftissue-specific protein function prediction.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018i196M.Zitnik and J.LeskovecTable 1. AUROC and area under precision-recall curve (AUPRC)scores for tissue-speciﬁc cellular function predictionApproachTensor decompositionMinimum curvilinear embeddingIndependent LINECollapsed LINEIndependent Node2vecCollapsed Node2vecGeneManiaNetwork-based tissue-speciﬁc SVMTissue-speciﬁc network propagationOhmNet (Section 3)AUROCAUPRC0.674 (60.124)0.674 (60.064)0.642 (60.053)0.663 (60.047)0.649 (60.063)0.697 (60.085)0.683 (60.077)0.701 (60.091)0.675 (60.051)0.756 (60.067)0.235 (60.052)0.248 (60.071)0.261 (60.068)0.271 (60.053)0.283 (60.052)0.298 (60.061)0.274 (60.094)0.281 (60.059)0.265 (60.083)0.336 (60.045)Values in the brackets are halves of the interquartile distance. OhmNet’sresults are statistically signiﬁcant with a P-value of < 0.05Table 2. AUROC scores for transfer learningTarget tissueNatural killer cellPlacentaSpleenLiverForebrainMacrophageEpidermisHematopoietic stem cellBlood plasmaSmooth muscleAverageAUROC (non-transfer)AUROC (transfer)0.834 (60.076)0.830 (60.082)0.803 (60.030)0.803 (60.047)0.796 (60.036)0.789 (60.037)0.785 (60.030)0.784 (60.035)0.784 (60.027)0.778 (60.031)0.7990.776 (60.063)0.758 (60.068)0.779 (60.043)0.741 (60.025)0.755 (60.037)0.724 (60.024)0.749 (60.032)0.744 (60.036)0.703 (60.039)0.729 (60.041)0.746Shown are the scores for ten tissues with best performance on cellular function prediction task. “Non-transfer”: a classiﬁer is trained on a target tissueand then used to predict cellular functions in the same tissue (Section 5.1).“Transfer”: classiﬁers are trained on all non-target tissues and then used topredict cellular functions in the target tissue (Section 5.2).generate more accurate hypotheses about tissue-specific proteinactions.5.2 Transfer of cellular functions to a new tissueFig. 4. Area under ROC curve (AUROC) scores for tissue-speciﬁc cellular function prediction by OhmNet. Numbers in the brackets are counts of tissue-speciﬁc cellular functions per tissueFrom the results, we see how modeling the tissues and their hierarchy spanning multiple biological scales allows OhmNet to outperform other benchmark approaches. OhmNet outperformsGeneMania (Mostafavi et al., 2008; Zuberi et al., 2013) by 10.7%,which can be explained by GeneMania’s inability to weight layers inthe tissue network according to a multiscale tissue organization thatis consistent with the tissue taxonomy constraints. We also compared OhmNet with two other methods (Guan et al., 2012; Maggeret al., 2012) that were so far demonstrated as useful for miningtissue-specific protein relationships. OhmNet has produced more accurate predictions, surpassing other methods by up to 12.0%(AUROC) and up to 26.8% (AUPRC).Independent modeling of the layers showed worse performancethan collapsing the layers into one network. We observed thatCollapsed LINE achieved a gain of 3.3% over Independent LINE,and Collapsed Node2vec achieved a gain of 7.4% over IndependentNode2vec. However, approaches that neglect the existence of tissuesor collapse tissue-specific protein interaction networks into a singlenetwork discard important information about the rich hierarchy ofbiological systems, giving OhmNet a 14.0% gain over CollapsedLINE, and a 8.5% gain over Collapsed Node2vec in AUC scores.This result is a good illustration of how tissue specificity is related tospecialization of protein function (Greene et al., 2015), andapproaches able to directly profile proteins’ distinct interactionneighborhoods in different tissues can leverage this specificity to5.2.1 Experimental setupIn the transfer learning setting, we attempt to transfer knowledgelearned in one or more source layers and use it for prediction in atarget layer.As before, we apply OhmNet to obtain a separate feature vectorfor every node and every layer in an unsupervised way. We then consider, in turn, every tissue as a target layer and all other tissues assource layers. For every function and every source layer, we train aseparate classifier using the same classification model as in Section5.1. We then predict functions for the target layer using only classifiers trained on the source layers. That is, we aim to predict cellularfunctions taking place in the target tissue without having access toany cellular function gene annotation in that tissue, i.e. we pretendthe target tissue has no annotations. Prediction for one node in thetarget layer is the weighted average of predictions of the classifierstrained on source layers. Weights reflect hierarchy-based distancesof source tissues from the target tissue. They are determined by theclosed-form expressions mathematically equivalent to OhmNet’sregularization (details omitted due to space constraints).5.2.2 Experimental resultsTable 2 shows the classification accuracy results for transfer learning based on OhmNet. Since transfer tasks are more difficult thannon-transfer tasks (Section 5.1), it is expected that the AUC scoreswill decrease on transfer tasks. Results in Table 2 confirm these expectations; however, we observe a very graceful degradation in performance leading to an only 7% average decrease in the AUCscores. We get the smallest performance differences for target tissueswith many biologically similar source tissues (i.e. source layers) inthe tissue network. For example, performance difference for theforebrain is only 5.2%, which is due to the fact that there are nineother layers in the tissue network closely related to the forebrain,such as the cerebellum and the midbrain. Considering all 48 tissues with tissue-specific cellular functions, OhmNet outperforms allcomparison methods on most transfer tasks, achieving a gain of upto 20.3% over the closest benchmark in AUC scores (scores notshown). Notice that we exclude GeneMania in the comparison because it is not amenable to transfer learning. This result suggestsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018Predicting multicellular function through multi-layer tissue networksABi197CFig. 5. Visualization of the brain tissue-speciﬁc protein interaction networks. (A) The two-level brain tissue hierarchy as speciﬁed by the BRENDA Tissue Ontology(Chang et al., 2014) and used in the case study in Section 5.3. Leaves of the hierarchy (in blue) represent nine brain tissues each of which is associated with a tissue-speciﬁc protein interaction network. (B) Visualization of the brainstem-speciﬁc networks. The proteins are mapped to the 2D space using the t-SNE packagewith learned features as input. Color of a node indicates the tissue of the protein. (C) Visualization of the brain-speciﬁc networks. The proteins are mapped andcolored using the same procedure as in Bthat considering the relationships between tissues when learningfeatures for proteins has a significant impact on transfer performance.In general, we observed that the transferability of classifiersdecreased when the tree-based distance between the source and thetarget tissue in the tissue hierarchy increased, which is consistent withthe empirical evidence in transfer learning (Yosinski et al., 2014). Thisalso matches our intuition that a source tissue should be most informative for predicting cellular functions in an anatomically close targettissue (e.g. source and target tissues are both part of the same organ).5.3 The multiscale model of brain tissuesWe have seen in Section 4.1 that human tissues have a multi-levelhierarchical organization. The tissue hierarchy categorizes tissuesinto: cell types, groups of cells with similar structure and function;organs, groups of tissues that work together to perform a specific activity; and organ systems, groups of two or more tissues that worktogether for the good of the entire body. We now aim to empiricallydemonstrate this fact and show that OhmNet in fact can discoverembeddings that obey this organization.We first construct a multi-layer brain network by integratingnine brain-specific protein interaction networks (e.g. the cerebellum,frontal lobe, brainstem and other brain tissues). Each of nine brainspecific networks is one layer in the multi-layer network. The layersare organized according to a two-level hierarchy (Fig. 5A). We runOhmNet on this multi-layer network to find node features in apurely unsupervised way. We then map the nodes to the 2D spacebased on the learned features. This way we assign every node inevery layer to a point in the two-dimensional space based solely onthe node’s learned features. We then visualize the points and colorthem based on the layer they belong to.Figure 5B shows the example for the brainstem tissues: substantia nigra, pons, midbrain and medulla oblongata. Laying out thesetissue-specific networks is very challenging as the four brainstem tissues are very closely related to each other in the human body.However, the visualization using OhmNet performs quite well.Notice how points of the same color are closely distributed, andhow well regions of the same color are separated from each other. Inthe brainstem example, this means that OhmNet generates a meaningful layout of the brainstem tissue-specific networks, in which proteins belonging to the same tissues are clustered together.Figure 5C shows the example for the brain, which is located onelevel up from the brainstem in the tissue hierarchy. Again, OhmNetproduces a meaningful layout of the nine brain tissue-specificnetworks.In addition, we repeated this analysis by visualizing protein features learned by running principal component analysis (PCA) ornon-negative matrix factorization (NMF) algorithm on the brainspecific PPI networks. Acknowledging the subjective nature of thisanalysis, we observed that visualizations using PCA or NMF werenot very meaningful, as proteins belonging to the same tissue werenot clustered together (data not shown).OhmNet’s result in Figure 5 is especially appealing because oftwo reasons. First, it shows that OhmNet can learn node featuresthat adhere to a given hierarchy of layers. In the brain example,OhmNet learns the protein features that expose the multiscale tissuehierarchy. Second, it shows that OhmNet can generate meaningfulvisualizations of network embeddings despite the fact thatOhmNet’s objective is independent of the visualization task.6 ConclusionWe presented OhmNet, an approach for unsupervised feature learning in multi-layer networks. We use OhmNet to learn state-of-theart task-independent protein features on a multi-layer network with107 tissues. OhmNet models tissue interdependence up and down atissue hierarchy spanning dozens of biological scales. The learnedfeatures achieve excellent accuracy on the cellular function prediction task, allow us to transfer functions to unannotated tissues, andprovide insights into tissues.There are several directions for future work. Our approach assumes the dependencies between layers are given in the form of ahierarchy. In several biological scenarios, the dependencies are givenin the form of a graph, and we hope to extend the approach to handle graph-based dependencies. As the learned protein features are independent of any downstream task, it would be interesting to seewhether our approach performs equally well for gene–disease association prediction and disease pathway detection.FundingThis research has been supported in part by NSF IIS-1149837, NIH BD2KU54EB020405, DARPA SIMPLEX N66001 and Chan Zuckerberg Biohub.Conflict of Interest: none declared.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018i198ReferencesAntanaviciute,A. et al. (2015) GeneTIER: prioritization of candidate diseasegenes using tissue-speciﬁc gene expression proﬁles. Bioinformatics, 31,2728–2735.Ashburner,M. et al. (2000) Gene Ontology: tool for the uniﬁcation of biology.Nat. Genet., 25, 25–29.Barutcuoglu,Z. et al. (2006) Hierarchical multi-label prediction of gene function. Bioinformatics, 22, 830–836.Belkin,M., and Niyogi,P. (2001) Laplacian eigenmaps and spectral techniquesfor embedding and clustering. In NIPS, vol. 14, MIT Press, Cambridge,pp. 585–591.Cannistraci,C.V. et al. (2013) Minimum curvilinearity to enhance topologicalprediction of protein interactions by network embedding. Bioinformatics,29, i199–i209.Carvunis,A.-R., and Ideker,T. (2014) Siri of the cell: what biology could learnfrom the iPhone. Cell, 157, 534–538.Chang,A. et al. (2014) BRENDA in 2015: exciting developments in its 25thyear of existence. Nucleic Acids Res. 43, D439–D446.Chatr-Aryamontri,A. et al. (2015) The BioGRID interaction database: 2015update. Nucleic Acids Res., 43, D470–D478.Costanzo,M. et al. (2016) A global genetic interaction network maps a wiringdiagram of cellular function. Science, 353, aaf1420.De Domenico,M. et al. (2014) Navigability of interconnected networks underrandom failures. PNAS, 111, 8351–8356.De Domenico,M. et al. (2015) Ranking in interconnected multilayer networksreveals versatile nodes. Nat. Commun., 6, 6868.De Domenico,M. et al. (2016) The physics of spreading processes in multilayernetworks. Nat. Phys., 12, 901–906.Dutkowski,J. et al. (2012) A gene ontology inferred from molecular networks.Nat. Biotechnol., 31, 38–45.Fagerberg,L. et al. (2014) Analysis of the human tissue-speciﬁc expression bygenome-wide integration of transcriptomics and antibody-based proteomics. Mol. Cell. Proteom., 13, 397–406.Ganegoda,G.U. et al. (2014) Prediction of disease genes using tissue-speciﬁedgene-gene network. BMC Syst. Biol., 8, S3.Greene,C.S. et al. (2015) Understanding multicellular function and diseasewith human tissue-speciﬁc networks. Nat. Genet., 47, 569–576.Grover,A., and Leskovec,J. (2016) Node2vec: scalable feature learning for networks. In KDD, pp. 855–864.GTEx,C. et al. (2015) The genotype-tissue expression (GTEx) pilot analysis:multitissue gene regulation in humans. Science, 348, 648–660.Guan,Y. et al. (2012) Tissue-speciﬁc functional networks for prioritizingphenotype and disease genes. PLoS Comput. Biol., 8, e1002694.Hayes,W. et al. (2013) Graphlet-based measures are suitable for biologicalnetwork comparison. Bioinformatics, 29, 483–491.Hou,C. et al. (2014) Joint embedding learning and sparse regression: aframework for unsupervised feature selection. IEEE Trans. Cybernet., 44,793–804.Hu,J.X. et al. (2016) Network biology concepts in complex disease comorbidities. Nat. Rev. Genet., 17, 615–629.Kitsak,M. et al. (2016) Tissue speciﬁcity of human disease module. Sci. Rep.,6, 35241.Kotlyar,M. et al. (2015) Integrated interactions database: tissue-speciﬁc viewof the human and model organism interactomes. Nucleic Acids Res., 44,D536–D541.Kramer,M. et al. (2014) Inferring gene ontologies from pairwise similaritydata. Bioinformatics, 30, i34–i42.Li,Y. et al. (2015) Gated graph sequence neural networks. arXiv:1511.05493.Lois,C. et al. (2002) Germline transmission and tissue-speciﬁc expression oftransgenes delivered by lentiviral vectors. Science, 295, 868–872.Lopes,T.J. et al. (2011) Tissue-speciﬁc subnetworks and characteristics of publiclyavailable human protein interaction databases. Bioinformatics, 27, 2414–2421.M.Zitnik and J.LeskovecMagger,O. et al. (2012) Enhancing the prioritization of disease-causing genesthrough tissue speciﬁc protein interaction networks. PLoS Comput. Biol., 8,e1002690.Menche,J. et al. (2015) Uncovering disease-disease relationships through theincomplete interactome. Science, 347, 1257601.Mikolov,T. et al. (2013) Efﬁcient estimation of word representations in vectorspace. arXiv:1301.3781.Mostafavi,S., and Morris,Q. (2009) Using the gene ontology hierarchy whenpredicting gene function. In UAI, AUAI Press, Corvallis, pp. 419–427.Mostafavi,S. et al. (2008) GeneMANIA: a real-time multiple association networkintegration algorithm for predicting gene function. Genome Biol., 9, 1.Nickel,M. et al. (2011) A three-way model for collective learning on multirelational data. In ICML, ACM, Bellevue, pp. 809–816.Okabe,Y., and Medzhitov,R. (2014) Tissue-speciﬁc signals control reversibleprogram of localization and functional polarization of macrophages. Cell,157, 832–844.Orchard,S. et al. (2013) The MIntAct projectintact as a common curation platform for 11 molecular interaction databases., Nucleic Acids Res. 42,D358–D363.Perozzi,B. et al. (2014) Deepwalk: online learning of social representations. InKDD, ACM, pp. 701–710.Prasad,T.K. et al. (2009) Human protein reference database-2009 update.Nucleic Acids Res. 37, D767–D772.Pr ulj,N. (2007) Biological network comparison using graphlet degree distrizbution. Bioinformatics, 23, e177–e183.Radivojac,P. et al. (2013) A large-scale evaluation of computational proteinfunction prediction. Nat. Methods, 10, 221–227.Rakyan,V.K. et al. (2008) An integrated resource for genome-wide identiﬁcation and analysis of human tissue-speciﬁc differentially methylated regions(tdmrs). Genome Res., 18, 1518–1529.Rolland,T. et al. (2014) A proteome-scale map of the human interactome network. Cell, 159, 1212–1226.Ruepp,A. et al. (2010) CORUM: the comprehensive resource of mammalianprotein complexes-2009. Nucleic Acids Res. 38, D497–D501.Stojanova,D. et al. (2013) Using PPI network autocorrelation in hierarchicalmulti-label classiﬁcation trees for gene function prediction. BMCBioinformatics, 14, 1.Tang,J. et al. (2015) Line: Large-scale information network embedding. InWWW, pp. 1067–1077.Tang,L. et al. (2012) Scalable learning of collective behavior. IEEE Trans.Knowl. Data Eng., 24, 1080–1091.Tenenbaum,J.B. et al. (2000) A global geometric framework for nonlineardimensionality reduction. Science, 290, 2319–2323.Vidulin,V. et al. (2016) Extensive complementarity between gene function prediction methods. Bioinformatics, 32, 3645–3653.Wang,D. et al. (2016a) Structural deep network embedding. In KDD, ACM,pp. 1225–1234.Wang,W. et al. (2016b) Tissue-speciﬁc pathway association analysis usinggenome-wide association study summaries. Bioinformatics, 33, 243–247.Xiaoyi,L., D., Nan,L.H. et al. (2014) A deep learning approach to link prediction in dynamic networks. In SDM, 289–297Yeger-Lotem,E., and Sharan,R. (2015) Human protein interaction networksacross tissues and diseases. Front. Genet., 6, 257.Yosinski,J. et al. (2014) How transferable are features in deep neural networks? In NIPS, Curran Associates, pp. 3320–3328.Yu,M. et al. (2016) Translation of genotype to phenotype by a hierarchy ofcell systems. Cell Syst., 2, 77–88.Zhai,S., and Zhang,Z. (2015) Dropout training of matrix factorization andautoencoder for link prediction in sparse graphs. In SDM, 451–459. Zitnik,M., and Zupan,B. (2015) Data fusion by matrix factorization. IEEETrans. Pattern Anal. Mach. Intell., 37, 41–53.Zuberi,K. et al. (2013) GeneMANIA prediction server 2013 update. NucleicAcids Res., 41, W115–W122.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i190/3953967by gueston 07 January 2018
5028881985002	PMID28881985	5028881985	https://watermark.silverchair.com/btx251.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881985.main.pdf	Bioinformatics, 33, 2017, i311–i318doi: 10.1093/bioinformatics/btx251ISMB/ECCB 2017popFBA: tackling intratumour heterogeneitywith Flux Balance AnalysisChiara Damiani1,2,*,†, Marzia Di Filippo1,3,†, Dario Pescini1,4,Davide Maspero3, Riccardo Colombo1,2 and Giancarlo Mauri1,21SYSBIO Centre of Systems Biology, 20126 Milan, Italy, 2Department of Informatics, Systems and Communication,Department of Biotechnology and Biosciences and 4Department of Statistics and Quantitative Methods,University Milano-Bicocca, 20126 Milan, Italy3*To whom correspondence should be addressed.†The authors wish it to be known that, in their opinion, the ﬁrst two authors should be regarded as Joint First Authors.AbstractMotivation: Intratumour heterogeneity poses many challenges to the treatment of cancer.Unfortunately, the transcriptional and metabolic information retrieved by currently available computational and experimental techniques portrays the average behaviour of intermixed and heterogeneous cell subpopulations within a given tumour. Emerging single-cell genomic analyses arenonetheless unable to characterize the interactions among cancer subpopulations. In this study,we propose popFBA, an extension to classic Flux Balance Analysis, to explore how metabolic heterogeneity and cooperation phenomena affect the overall growth of cancer cell populations.Results: We show how clones of a metabolic network of human central carbon metabolism, sharing the same stoichiometry and capacity constraints, may follow several different metabolic pathsand cooperate to maximize the growth of the total population. We also introduce a method to explore the space of possible interactions, given some constraints on plasma supply of nutrients. Weillustrate how alternative nutrients in plasma supply and/or a dishomogeneous distribution of oxygen provision may affect the landscape of heterogeneous phenotypes. We ﬁnally provide a technique to identify the most proliferative cells within the heterogeneous population.Availability and implementation: the popFBA MATLAB function and the SBML model are availableat https://github.com/BIMIB-DISCo/popFBA.Contact: chiara.damiani@unimib.it1 IntroductionPopulations of tumour cells display considerable phenotypic diversity both at the intertumour and intratumour level. Along with genetic and epigenetic factors, differential trophic supply and variationsin the tumour microenvironment contribute in particular to intratumour metabolic heterogeneity and to the emergence of a complexcancer population architecture (Burrell et al., 2013). Intratumourheterogeneity increases the repertoire of possible cellular responsesto a drug and fosters the adaptive nature of cellular behaviours (Sunand Yu, 2015), compromising the efficacy of cancer therapies.Although single cell-based technologies represent a promising approach for a more in-depth understanding of single cell behaviourwithin solid tumours, cancer populations are composed of both tumour and stromal cells that interact with each other by establishing anetwork of interactions that cannot be deciphered from the analysisof each of these individual components alone (Marusyk et al., 2012).Computational methodologies that allow to identify the possiblecooperations that can be established to enhance the overall growthof the tumour mass and to investigate the mechanisms underlyingthem are therefore desired as they may facilitate the development ofmore effective cancer treatments. In particular, modelling efforts tointegrate different sources of data and to determine the metabolicphenotype of cooperating cells would provide information that cannot be obtained directly from the genotype, transcriptome, proteome nor the metabolome alone (Holmes et al., 2008).Constraint-based modelling and especially Flux Balance Analysis(FBA) represents so far the most applied technique for studying metabolism and has effectively been exploited as a scaffold for ‘omic’data integration (Cazzaniga et al., 2014). In particular, manymethods have been introduced for the integration of transcriptomicdata into constraint-based models of metabolism (Machado andHerrgard, 2014). FBA is performed on a single metabolic network˚CV The Author 2017. Published by Oxford University Press.i311This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i311/3953966by gueston 07 January 2018i312C.Damiani et al.and provides the (optimal) net flux distribution of possibly differentmetabolic populations, hindering the identification of possible metabolic interactions between subpopulations.In Di Filippo et al. (2017), we preliminarily showed that clonesof a constraint-based toy metabolic network can cooperate to maximize the ATP production of a total population. In this study, wepropose a method to explore the space of possible interactions between heterogeneous cell populations within a putative tumour—given some constraints on plasma supply of nutrients—as well as atechnique to identify the most proliferative sub-phenotypes.2 ApproachBecause the classic FBA approach on a single metabolic model predicts the optimal net flux distribution of a population (Di Filippoet al., 2017), the single metabolic model just represents a sort ofblack box of the investigated population, and on its own is unableto inform about the complex interactions occurring inside it.However, cooperation phenomena within some tumours have beenreported, specifically between stromal and cancer cells (Fiaschiet al., 2012; Martinez-Outschoorn et al., 2011; Sanit  et al., 2014;aWhitaker-Menezes et al., 2011), pushing forward the need for computational approaches able to uncover these as well as other possiblekind of interactions between tumour subpopulations. Therefore, wepropose a new methodology aimed at investigating metabolicphenotypes of different subpopulations belonging to the same tumour mass, especially focusing on the relationships among them.Assuming that a tumour mass may be composed of differenttypes of cells (including stromal cells) and that, for reasons of spatialproximity, the communication with the plasma (in terms of nutrients exchange) of the different components may differ with respectto the communication with other cells within the populations, weseparately modelled exchanges with plasma and exchanges with thetumour microenvironment.In particular, the single metabolic model is used as a buildingblock for constructing, in an automatic way, the population modelcharacterized by multiple clones, all having identical stoichiometryand capacity constraints and sharing the plasma supply of nutrients.Exploiting linear programming optimization, we can investigateboth the cooperation among different clones that is consistent withthe achievement of the optimal growth rate of the entire tumourmass, and identify the strategies adopted by the most proliferativeclones.The proposed approach is schematically described in Figure 1and formally defined in the following section.Fig. 1. Graphical representation of popFBA methodology3.1 Metabolic network modelFor the analyses reported in this study we used the model of central carbon metabolism, which we refer to as ‘COREHMR’, extractedfrom the HMR model (Mardinoglu et al., 2013) and introduced inDi Filippo et al. (2016), composed of 243 metabolites and 271 reactions. To adapt the model to popFBA analyses, we set the mitochondrial isocitrate dehydrogenase-catalyzed reactions as reversible; weintroduced a consumption of ATP within the biomass reaction; weintroduced a cell maintenance reaction (ATP ! ADP); and finallywe structurally removed the thermodynamically infeasible loops detected with the algorithm developed in De Martino et al. (2013).A metabolic network is formalized by specifying a set X ¼ fX1 ; . . . ;XM g of metabolites, and the set R ¼ fR1 ; . . . ; RN g of chemicaltransformations taking place among them. Reactions are defined as:3.2 FBA3 Materials and methodspsRj :MjXi¼1aji Xi $MjXbji Xi ;(1)i¼1where Ms and Mp are, respectively, the number of reactants andjjproducts of reaction j—relatively to the case in which the reactionproceeds in the forward direction (from left to right)—and aji ; bji2 N are stoichiometric coefficients associated, respectively, with theith substrate and the ith product of the jth reaction.The assumption underlying FBA is that metabolic networks willreach a steady state: the concentration of each metabolite is assumedconstant: d½Xi  =dt ¼ 0 8Xi 2 X. The stoichiometric constraints leadto a bounded solution space of all feasible flux distributions, whichcan be further restricted by specifying maximum and minimumfluxes through any particular reaction. These bounds on admissiblefluxes allow to define constraints on the reversibility of reactions, toimpose experimentally measured flux ranges, and to set the extentof nutrients supply. The exchange of matter with the environment isDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i311/3953966by gueston 07 January 2018popFBAi313represented as a set E ¼ fE1 ; . . . ; ENext g of Next unbalanced reactions(exchange reactions), enabling a predefined set of metabolites (including the pseudo-metabolite representing biomass) Y ¼ fY1 ; . . . ; YNext g& X to be inserted in or removed from the system, through reactionsas Ej:Ej : Yj $ 1(2)The first step to perform FBA is the derivation of the stoichiometricmatrix S, of size M Â ðN þ Next Þ, whose element sji takes value Àajiif the species Xi is a reactant of reaction Rj, þbji if the species Xi is aproduct of reaction Rj and 0 otherwise.FBA is then applied to determine the rate vi at whicheach reaction in R [ E occurs, that is, the flux distribution~ ¼ ðv1 ; v2 ; . . . ; vNþNext Þ ¼ ðr1 ; . . . ; rN ; e1 ; . . . ; eNext Þ that maximizesvPor minimizes the objective function Z ¼ NþNext wi vi , where wi isi¼1the weight that quantifies the contribution of reaction i, while ri andei are, respectively, the flux value associated to an “internal” or anexchange reaction.In order to simulate tumour growth, in this study we maximizethe flux ebiomass through the biomass exchange reaction, thusZ ¼ ebiomass The optimization problem is postulated as a generalLinear Programming (LP) formulation:maximize or minimize Zsubject to S~ ¼ ~v 0;~Lv~v(3)~Bv~L and ~B are vectors specifying the lower and upper bound, respectvvively, for each flux vi of ~. A negative lower bound indicates thatvflux is allowed in the backward reaction.To solve the above problem we exploited the GLPK solverwithin the COBRA Toolbox (Schellenberger et al., 2011). For amore comprehensive description of FBA, the reader is referred toOrth et al. (2010).3.3 popFBAIn order to investigate the role of cooperation within a populationsharing a common environment, in this study we devised popFBA,an extension to FBA able to cope with the presence of several subpopulations exchanging a defined set of metabolites. Given a metabolicnetwork A defined as A ¼ ðX; R; E Þ, popFBA maximizes the totalbiomass of Npops clones Ac of A, which can cooperate by exchangingnutrients in the tumour microenvironment. For each clone Ac, let X c¼ fXc g be the set of its metabolites, Rc ¼ fRc g the set of its internalijreactions, with j ¼ 1; . . . ; N and c ¼ 1; . . . ; Npops . To correct for thefact that in a population model a metabolite is not removed fromthe systems, but becomes a metabolite in the tumour microenvironment, each reaction Ej 2 E of A is transformed into a cooperation reaction Cc with the formjCc : Yjc $ Y 0jj(4)It is also necessary to define the new set of tumour microenvironment metabolites Y 0 ¼ fY 0i g with i ¼ 1; . . . ; Next , together with anew set of Nblood exchange reactions B ¼ fB1 ; . . . ; BNblood g to allowa subset of metabolites K ¼ fK1 ; . . . ; KNblood g & Y 0 to be exchangedwith the blood supply:Bj : Kj $ 1(5)The population model P is then defined by the union set of the metabolites X P ¼ [c X c [ Y 0 , of the internal reactions RP ¼ [c Rc , ofthe cooperation reactions CP ¼ [c Cc and of the population exchangereactions B.A stoichiometric matrix SP is then built for all reactions in RP ;PC and B and for all metabolites in X P .ÀThe final size of matrix SP is ðNpops Á M þ Nblood Þ Â Npops ÁðN þ Next Þ þ Nblood Þ.To obtain the matrix SP, we implemented a MATLAB functionthat automatically replicates a number of times any (COBRAToolbox compliant—Schellenberger et al., 2011) SBML model toobtain the above defined population model, in a suitable form tothen undergo constraint-based analyses. Linear programming isthen applied as per Equation 3 to determine the flux distributionNN~ ¼ ðv1 ; . . . ; vNpops ÁðNþNext ÞþNblood Þ ¼ ðr1 ; . . . ; r1 ; . . . ; r1 pops ; . . . ; rNpops ; c1 ;v1N1NN. . . ; c1 ext ; . . . ; c1 pops ; . . . ; cNpops ; b1 ; . . . ; bNblood Þ that maximizes the bioNextmass exchange flux bbiomass, with vi representing any flux i of thepopulation model, and for each clone c, rc representing the ith internaliflux, cc representing the ith a cooperation flux and bi a plasma exichange flux.3.4 Sampling in the region of optimal solutionsLinear programming only returns a single optimal solution.However, many alternative optimal flux distributions may exist.Flux Variability Analysis (Mahadevan and Schilling, 2003) has beenefficiently exploited to identify the range of values that a flux cantake across the complete set of optimal solutions. Nevertheless, inorder to analyse the correlation between flux values and the proliferation rates of the model subpopulations, we need punctual solutions. Although methods have been proposed for enumeratingalternative optimal solutions (Reed and Palsson, 2004), an exhaustive enumeration is not practicable for popFBA, due to the interchangeability of the flux distributions of the Npops clones.To cope with this problem, we set the bounds of the biomass exchange flux bbiomass to the optimal value obtained with popFBA andwe sampled the admissible solutions. The dominant algorithm ofchoice to uniformly sampling inside the region of allowed solutionsis the so-called “Hit-and-Run” (HR) (Schellenberger and Palsson,2009), according to which an initial valid point is moved repeatedlyinside the space according to probabilistic rules.In this study, we also exploited a recently proposed alternativeapproach (Bordel et al., 2010; Damiani et al., 2014): the simplexmethod with a random set of objective functions to be maximized.The maximization of each of these objective functions gives a cornerin the space of solutions. In Bordel et al. (2010), random objectivefunctions were generated by selecting random pairs of reactions. Tomaximize variability of sampled solutions, we instead let any number of reactions to take part in the objective function Z as inDamiani et al. (2014). The fraction s of considered reactions is randomly drawn with uniform probability in (0, 1]. To any selected reaction is then assigned a random weight wi uniformly tossed fromthe interval ð0; 1 , where wi takes value 0 with probability s and arandom value with uniform probability in ð0; 1  with probability1 À s.For both methods, we controlled for repetitions in the sampledpoints.3.5 Assessing subpopulations heterogeneityTo assess the heterogeneity of the metabolism of the Npops cloneswithin a given optimal solution, we compared their flux distributions both quantitatively and qualitatively. To avoid taking into account clones that carry no flux, we disregarded the clones that areDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i311/3953966by gueston 07 January 2018i314C.Damiani et al.not cooperating for tumour growth, by filtering out clones that donot have a least one non-zero flux through any of the cooperationreactions.To count the number of quantitatively different subpopulationswe considered clones a and b to be different if they differ by at leastthe value of one flux (rounded at the fourth digit): if 9 i : va 6¼ vb .iiTo count the number of qualitatively different subpopulations,~the counting process is performed, rather than on v0 , on a~~discretized version d0 of vector v0 , whose component d0i is obtainedas follows:8vi > 0>1><0di ¼ À1 vi < 0(6)>>:0vi ¼ 0In this way two clones are considered as different iff they follow atleast one different metabolic path, that is, a different route or a different flux direction.4 Results4.1 popFBA reveals the existence of cooperation andmetabolic heterogeneity within cancer populationmodelsWe applied popFBA to 10 clones (Npops ¼ 10) of the COREHMRmodel. The value 10 was arbitrarily chosen to allow up to 10 possibly different subpopulations to be detected. In a first experimentwe simulated a plasma supply of glucose, glutamine and oxygen, thethree nutrients provided with the same order of molar magnitude, inaccordance with the magnitude found when scanning data in literature. Although the molar concentration of glutamine may be loweras compared with that of glucose and oxygen, in our system glutamine represents the unique nitrogen source, so it is reasonable to increase its uptake flux to account also for other nitrogen sources thatare generally available in the plasma. As a first approximation, weassumed equal bounds for the reactions of the 10 clones, thus disregarding spatial diffusion phenomena. The 10 clones can cooperate,via cooperation reactions described in the Section 3, by exchanginglactate, glutamine, glutamate and ammonia. Although these metabolites can be secreted in the microenvironment compartment, theycannot be disposed (as waste products) in the human plasma. Thisexperimental setting, which we refer to as the reference condition, isbetter detailed in Figure 2A. In this condition, we sampled 2 Â 104different optimal solutions (104 points with the HR sampling methods and 104 with the CB sampling method), that are compatiblewith the same optimal tumour biomass.We assessed the quantitative heterogeneity of the clones in eachof the sampled solutions. Remarkably, in all sampled solutions, weobserved that all 10 clones behave differently. This diversity resultsin a different biomass synthesis flux value (the growth rate) for distinct clones. This heterogeneity may partially be the result of slightlydifferent phenotypes, following the very same metabolic paths butat different rates. We wanted therefore to assess the number ofclones that follow different metabolic paths, that is, that differ in theset of reactions that is active and/or in the direction of the fluxthrough such set of reactions. The distribution of the number ofqualitatively different subpopulations is reported in Figure 2B and itshows that, although subpopulations of quantitatively differentclones may overlap from a qualitative point of view, for both sampling methods, the clones typically all follow different metabolicroutes.Fig. 2. Reference condition. (A) Experimental setting of the bounds imposedon the release/consumption of metabolites in/by the plasma, and on thecooperative reactions. Black or white ﬁlled arrows, respectively, indicateallowed and blocked reactions (bounds set to 0). (B) Histogram relative to thenumber of qualitatively different subpopulations obtained with HR (blue bars)and CB (orange bars) sampling methods. (C) Scatter plots obtained with HR(blue points) and CB (orange circles) sampling methods relative, in clockwiseorder, to the correlation between glutamate exchange and biomass synthesis, between glutamine exchange and biomass synthesis, between NH3 exchange and biomass synthesis and between lactate exchange and biomasssynthesis. Abbreviations: Glc, Glucose; O2, Oxygen; Gln, glutamine; NH3, ammonia; Lac, lactate; Glu, glutamateOnce we established that popFBA is able to highlight the possibleheterogeneity of subpopulations of cells belonging to the same tumour population, we shifted the attention toward a more in-depthinvestigation of which types of interactions are compatible with theachievement of the maximum tumour biomass. The observed heterogeneity results indeed form cooperative behaviours among different subpopulations. In fact, as it can be observed in the scatter plotsin Figure 2C, a secretion (negative values) or consumption (positivevalues) of metabolites in the tumour microenvironment, namely oflactate, glutamate and ammonia emerged among the different subpopulations. Among the four allowed cooperations, glutamine is theonly metabolite that is always just consumed and it is not exchangedwith other subpopulations.4.2 Identification of the most proliferativesubpopulationsTo identify the most proliferative phenotypes among the heterogeneous subpopulations identified above, we computed the PearsonCorrelation Coefficient (q) between the flux of each of the four cooperation reactions (exchange of glutamate, glutamine, lactate andDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i311/3953966by gueston 07 January 2018popFBAammonia via the tumour microenvironment) and the biomass production rate of the corresponding clones, consistently with theachievement of the optimal tumour biomass.We found that the correlation with the biomass synthesis flux, obtained with HR (and CB), is À0.32 (À0.4) for glutamine; þ0.35(þ0.44) for glutamate; À0.55 (À0.65) for ammonia; and À0.96(À0.99) for lactate. Notice that a negative q implies a positive correlation between nutrient consumption and biomass, while a positiveq implies a positive correlation between nutrient secretion and biomass.The q values, along with the corresponding scatter plots (Fig. 2C),clearly indicate that the most proliferative subpopulations are thoseconsuming the lactate available in the tumour microenvironment. Toevaluate whether lactate consuming subpopulations are oxidising thiscarbon source, we also analysed the correlation between the lactateexchange and oxygen consumption fluxes, obtaining a q of þ0.98(þ0.99), confirming that the most proliferative subpopulations areconsuming high levels of oxygen, which is plausibly exploited to oxidize the consumed lactate.i315The observation that the glutamine supplied with plasma is consumed by all 10 popFBA clones, with no clone producing andexchanging it with the tumour microenvironment, was expected.Glutamine represents indeed a unique source of nitrogen supplied byplasma in the above simulations, and nitrogen is mandatory for thesynthesis of amino acids and thus of biomass. The correlation between glutamine and biomass is however modest, because the clonesare able to extract the nitrogen from glutamine and to exchange it inthe form of NH3 or glutamate via the tumour microenvironment.Unexpectedly, the correlation with biomass is indeed negative forNH3, but it is positive for glutamate.We observed that, when the cooperation flux for both NH3 andglutamate is prevented (experimental setting in Fig. 3A and resultsin Fig. 3B and C), the correlation between glutamine and biomassbecomes indeed close to À1 (À0.99 for both the HR and the CBmethod). In this situation, the negative correlation between lactateand biomass is preserved (À0.97 for HR and À0.96 for CB).We also wanted to investigate whether the source of nitrogen inthe plasma supply may affect the possible cooperative behaviours.We tested the situation in which the nitrogen source provided by theplasma to the tumour mass is not represented by glutamine, but bythe ammonia (Fig. 4A), which may account for the nitrogen derivingfrom other amino acids in real cells. In this situation, as shown inFigure 4C, the exchange of glutamine among different subpopulations becomes possible. On the contrary, the exchange of NH3 becomes not possible, as it is now exclusively consumed by the 10clones (data not shown). Notably, we still observed a high correlation between lactate consumption and biomass formation:q ¼À0.89 (À0.98), as per Figure 4C.Once the effect of an alternative nitrogen source on the internaltransport reactions was investigated, we also analysed the possibilityto have an outflow of either glutamate or ammonia or lactate fromthe tumour microenvironment compartment toward the plasma,maintaining the reference experimental setting. We observed that aslight increase in the tumour biomass value is caused by both glutamate (2%) and lactate secretion (2%) in the plasma.Interestingly, we observed that the exit of glutamate from thetumour microenvironment towards the plasma does not prevent anexchange of glutamate among different subpopulations. On the contrary, as compared with the reference condition in Figure 2, this situation results in an enhancement of the correlations betweenglutamate exchange and biomass synthesis rate—from þ0.35 (0.44)to þ0.47 (þ0.61); between glutamate exchange and lactateexchange—from À0.29 (À0.39) to À0.47 (À0.61); and betweenFig. 3. Cooperation reactions variation condition. (A) Experimental setting ofthe bounds imposed on the release/consumption of metabolites in/by theplasma, and on the cooperative reactions. Black or white ﬁlled arrows, respectively, indicate allowed and blocked reactions (bounds set to 0).(B) Histogram relative to the number of qualitatively different subpopulationsobtained with HR (blue bars) and CB (orange bars) sampling methods.(C) Scatter plots obtained with HR (blue points) and CB (orange circles) sampling methods relative to the correlation between glutamine exchange andbiomass synthesis (on the left), and between lactate exchange and biomasssynthesis (on the right)Fig. 4. Changing nitrogen source condition. (A) Experimental setting of thebounds imposed on the release/consumption of metabolites in/by theplasma, and on the cooperative reactions. Black or white ﬁlled arrows, respectively, indicate allowed and blocked reactions (bounds set to 0).(B) Histogram relative to the number of qualitatively different subpopulationsobtained with HR (blue bars) and CB (orange bars) sampling methods.(C) Scatter plots obtained with HR (blue points) and CB (orange circles) sampling methods relative to the correlation between glutamine exchange andbiomass synthesis (on the left), and between lactate exchange and biomasssynthesis (on the right)4.3 The set of nutrients exchanged with plasma affectspossible cooperationsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i311/3953966by gueston 07 January 2018i316glutamate and oxygen—from À0.26 (À0.36) to À0.44 (À0.59).These results indicate that the subpopulations that are responsiblefor a glutamate secretion in the tumour microenvironment arecharacterized by a consumption of both lactate and oxygen.4.4 Comparison of HR and CB sampling methodsIn the scatter diagrams in Figures 2C, 3C and 4C it is evident howthe two sampling methods (HR and CB) differ in the way they explore the space of possible cooperations among metabolic clones.HR uniformly samples within the space of optimal solutions, returning points that are close to one another inside the space of allowedsolution, whereas, as already pointed out in Bordel et al. (2010), theCB method returns solutions corresponding to the corners in the region of allowed flux distributions. This difference in the two methods does not particularly affect the correlation coefficients betweenthe release/consumption of metabolites within the tumour microenvironment and the growth rate of a given sub-population. Itcan be indeed observed in Figure 5C that the qs obtained with thetwo methods are very similar. However, the HR method may insome cases underestimate the number of qualitatively different subpopulations. Although the distribution of the number of qualitatively different subpopulations of the two methods is very similar inthe experiments presented in Figures 2B and 4B, it substantially diverges in the experiment in Figure 3B. A possible explanation of thisphenomenon is that the experiment reported in Figure 3B refers to acase in which the clones have less possibilities to cooperate (glutamate and ammonia cooperation is prevented) and thus tend to bemore similar to one another (indeed also the CB method finds morecases with a lower number of different subpopulations as comparedwith experiments in Figs. 2B and 4B), and that the HR methodamplifies this effect.As expected, the computation time of the two methods growslinearly with the sample size (Fig. 5D) and it is slightly higher forC.Damiani et al.HR because of the fixed initial time to create the warm up points. Inboth cases, the computed qs stabilize for a sample size greater than$4 Â 103 . Also the shape of the distribution of the number of different subpopulations (Fig. 5A and B for HR and CB, respectively) isnot particularly affected by sample size, although some oscillationsare possible especially for the CB case, while the frequency of anynumber of different subpopulations becomes stable for samplesgreater than $8 Â 103 . All in all these considerations confirm thatthe size of the sample chosen for our analysis (104 þ 104 ) wasreasonable.4.5 Simulation of spatial diffusion phenomena withpopFBAThe simulations presented so far assumed equal boundaries for thereactions of the 10 clones. There could be however cases where different biological conditions impose different uptake/secretion capabilities on the different subpopulations. Our approach could still beexploited to represent biologically distinct subpopulations, when information is available to constrain the exchange reactions differently. To prove the viability of popFBA to take into account spatialdiffusion phenomena, we performed an experiment in which, differently from the experimental settings presented above, the 10 subnetworks are not identical in terms of access capability to the plasmasupply.To mimic a simplified oxygen gradient, we set the bounds ofoxygen uptake of the 10 subnetworks according to a linear decreasing function (as illustrated in Fig. 6A). Assuming that subpopulations are radially stratified, subnetworks with a given availability ofoxygen may represent subpopulations of cells at an equal distancefrom the blood vessel.The scatterplots in Figure 6B clearly show that the subpopulations that consume less oxygen produce more lactate, which is thenconsumed by fast growing subpopulations with high oxygen consumption rates.5 ConclusionsWe introduced popFBA, an extension of FBA to take into accountintratumour heterogeneity and interactions among different cellFig. 5. (A) Distribution of the number of qualitatively differentsubpopulations obtained for seven independent samples of different size S(S 2 f100; 500; 1000; 2000; 4000; 8000; 10 000g) obtained with the HR method.Parameter nStepsPerPoint of the sampleCbModel MATLAB function was set todefault value (200); parameter nWarmupPoints was set to 6000; parametersnFiles and nPointsPerFile were set in a way to maintain the ratio between their default values. (B) Distribution of the number of qualitatively different subpopulations obtained for seven independent samples of different sizef100; 500; 1000; 2000; 4000; 8000; 10 000g obtained with the CB method. (C)Values of q between biomass and each of the exchange ﬂuxes with the intratumour microenvironment (Gln, Glu, Lact, O2, NH3), obtained with HR (solid lines)and CB (dashed lines) as a function of sample size. (D) Computation time as afunction of samples size for HR and CBFig. 6. Simulation of oxygen spatial diffusion. (A) Schematic representation ofthe performed experiment. The red-to-blue chromatic scale is used to mimican oxygen gradient through the tumour population, respectively, from an aerobic to a hypoxic environment. The values on the bottom of the ﬁgure referto maximum uptake ﬂux value of oxygen imposed on each clone. (B) Scatterplots obtained with HR sampling methods relative to the correlation betweenlactate exchange and oxygen uptake (on the left), and between biomass synthesis and oxygen uptake (on the right)Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i311/3953966by gueston 07 January 2018popFBApopulations within the same tumour. We applied popFBA to amodel of 10 clones of the metabolic network of human central carbon metabolism, simulating a plasma supply of glucose, glutamineand oxygen, assuming equal bounds for the reactions of the 10clones and an internal exchange of lactate, glutamine, glutamateand ammonia. We sampled different optimal solutions, by usingboth the HR sampling method and the CB sampling method, thatare compatible with the same optimal tumour biomass. We observedthat popFBA reveals the existence of metabolic heterogeneity andcooperation within the population model. Indeed, by assessing thequantitative heterogeneity of the clones in each of the sampled solutions, we observed that all 10 clones behave differently and arecharacterized by a different growth rate. Moreover, by assessing thedistribution of the number of qualitatively different subpopulations,we found that the subpopulations are following different metabolicroutes. This observed heterogeneity is the result of a cooperative behaviour among subpopulations: a consumption or secretion of lactate, glutamine, glutamate and ammonia (the four exchangedmetabolites) emerged indeed among the different subpopulations.Following the observation that popFBA approach is able to pointout, if it is present, a cooperative behaviour among multiple subpopulations of the same population, we investigated which types ofinteractions are compatible with the achievement of the optimal tumour biomass.In order to characterize the metabolism of the most proliferativesubpopulations we assessed the correlation between the exchangesof metabolites within the tumour microenvironment and the growthrate of a given subpopulation. In this regard, we showed that,although the two sampling methods (HR and CB) differ in theway they explore the space of possible solutions, this differencehas no particular effect on the computed correlation coefficients.Therefore, the two methods are equally effective when the goal is todetermine the subpopulation with a propensity for growth.Remarkably, in all of the investigated scenarios, we observedthat the lactate exchange within tumour microenvironment is negatively correlated with respect to the biomass synthesis flux. Thismeans that the most proliferative subpopulations consume the lactate that is secreted in the tumour microenvironment by less proliferative subpopulations, by using it as energy source. Because a highpositive correlation between the lactate exchange and oxygen consumption fluxes emerged, we deduced that the most proliferativesubpopulations are oxidising the consumed lactate.A wide array of studies on clonal cancer populations indicatedthat cancer cells are characterized by high secretion of lactate in themedium and that this effect may be functional for growth (Cantorand Sabatini, 2012; Ward and Thompson, 2012). Conversely, experimental evidence (Fiaschi et al., 2012; Martinez-Outschoornet al., 2011; Sanit  et al., 2014; Whitaker-Menezes et al., 2011) ofathe existence of a stromal-cancer lactate shuttle in human tumours,a phenomenon named as “reverse Warburg effect” due to the factthat tumour stromal cells undergo aerobic glycolysis producing lactate that is used as energy source by the adjacent high proliferativecancer cells has been reported. By simulating scenarios in which thisenergy source is not fully released in the environment but must betaken up by other subpopulations, our approach effectively capturedthe side effects of lactate production on the overall growth of a heterogeneous tumour mass displaying the reverse Warburg effect.Under this assumption, our approach well describes the complex(“symbiotic” but also “parasitic”) metabolic relationship betweencancer and stromal cells in mixed cancer populations.The agreement of our results with the above-mentioned experimental data supports the reliability of our approach and furtheri317confirms the need for computational and experimental approachesable to take into account the specificity of the subpopulations withina tumour rather than observing the average behaviour.We have shown how popFBA may also be applied to simulatescenarios in which different biological conditions impose differentuptake/secretion capabilities on the different subpopulations, as wellas to investigate the metabolic plasticity of the tumour mass withrespect to the adaptation of its components (i.e. different subpopulations belonging to the tumour) to changing external but also internalscenarios.As it has already been pointed out (Resendis-Antonio et al.,2015), the Warburg and reverse Warburg effect may represent onlytwo paradigmatic examples of metabolic interconnection withincancers. Other scenarios may be simulated with popFBA, by exploiting experimental information to specifically constrain the distinctsubpopulations, with particular regard to the information on singlecell transcriptome now enabled by single-cell RNA sequencing(scRNA-seq).Taking inspiration from the plethora of existing methods to integrate transcriptomic data into classic FBA (Machado and Herrgard,˚2014), we plan in the next future to define a method to integrate single cells transcriptomic data into our multiscale model, in order topave the way to the integration of the increasing availability ofscRNA-seq data into computational models. Single-cell flux distributions will be computed as a function of the transcriptome at thecell level, while assuring biomass formation at the population level.Linear programming is computationally inexpensive, making asingle popFBA computation applicable to populations composed ofhundreds of clones and scalable to genome-scale metabolic networks. The computation time will increase linearly with the overallnumber of reactions. Conversely, the proper size of the set ofsampled solutions should be assessed on a case-by-case basis, as it islikely to depend on the specific features of the metabolic networkmodel, as well as on the set of cooperating metabolites. However,random sampling methods involve independent computations andmay thus easily benefit from distributed implementations.FundingThis work is supported with FOE funds from MIUR to SYSBIO Center ofSystems Biology - within the Italian Roadmap for ESFRI ResearchInfrastructures. M.D.F. is supported by a SYSBIO fellowship.Conflict of Interest: none declared.ReferencesBordel,S. et al. (2010) Sampling the solution space in genome-scale metabolicnetworks reveals transcriptional regulation in key enzymes. PLoS Comput.Biol., 6, e1000859.Burrell,R.A. et al. (2013) The causes and consequences of genetic heterogeneity in cancer evolution. Nature, 501, 338–345.Cantor,J.R., and Sabatini,D.M. (2012) Cancer cell metabolism: one hallmark,many faces. Cancer Disc., 2, 881–898.Cazzaniga,P. et al. (2014) Computational strategies for a system-level understanding of metabolism. Metabolites, 4, 1034–1087.Damiani,C. et al. (2014) An ensemble evolutionary constraint-based approachto understand the emergence of metabolic phenotypes. Nat. Comput., 13,321–331.De Martino,D. et al. (2013) Counting and correcting thermodynamically infeasible ﬂux cycles in genome-scale metabolic networks. Metabolites, 3,946–966.Di Filippo,M. et al. (2016) Zooming-in on cancer metabolic rewiring with tissue speciﬁc constraint-based models. Comput. Biol. Chem., 62, 60–69.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i311/3953966by gueston 07 January 2018i318Di Filippo,M. et al. (2017) Constraint-Based Modeling and Simulation of CellPopulations. In: Rossi,F. et al. (eds) Advances in Artiﬁcial Life, EvolutionaryComputation, and Systems Chemistry. WIVACE 2016. Communications inComputer and Information Science, vol 708. Springer, Cham.Fiaschi,T. et al. (2012) Reciprocal metabolic reprogramming through lactateshuttle coordinately inﬂuences tumor-stroma interplay. Cancer Res., 72,5130–5140.Holmes,E. et al. (2008) Metabolic phenotyping in health and disease. Cell,134, 714–717.Machado,D., and Herrgard,M. (2014) Systematic evaluation of methods for˚integration of transcriptomic data into constraint-based models of metabolism. PLoS Comput. Biol., 10, e1003580.Mahadevan,R., and Schilling,C. (2003) The effects of alternate optimal solutionsin constraint-based genome-scale metabolic models. Metab. Eng., 5, 264–276.Mardinoglu,A. et al. (2013) Integration of clinical data with a genome-scalemetabolic model of the human adipocyte. Mol. Syst. Biol., 9, 649.Martinez-Outschoorn,U.E. et al. (2011) Cancer cells metabolically “fertilize”the tumor microenvironment with hydrogen peroxide, driving the Warburgeffect: implications for PET imaging of human tumors. Cell Cycle, 10,2504–2520.Marusyk,A. et al. (2012) Intra-tumour heterogeneity: a looking glass for cancer? Nat. Rev. Cancer, 12, 323–334.C.Damiani et al.Orth,J.D. et al. (2010) What is ﬂux balance analysis?. Nat. Biotechnol., 28,245–248.Reed,J.L., and Palsson,B. (2004) Genome-scale in silico models of E. coli havemultiple equivalent phenotypic states: assessment of correlated reaction subsets that comprise network states. Genome Res., 14, 1797–1805.Resendis-Antonio,O. et al. (2015). Modeling metabolism: a window toward acomprehensive interpretation of networks in cancer. Semin. Cancer Biol.,30, 79–87.Sanit ,P. et al. (2014) Tumor-stroma metabolic relationship based on lactateashuttle can sustain prostate cancer progression. BMC Cancer, 14, 154.Schellenberger,J., and Palsson,B. (2009) Use of randomized sampling for analysis of metabolic networks. J. Biol. Chem., 284, 5457–5461.Schellenberger,J. et al. (2011) Quantitative prediction of cellular metabolismwith constraint-based models: the COBRA Toolbox v2.0. Nat. Protoc., 6,1290–1307.Sun,X-X., and Yu,Q. (2015) Intra-tumor heterogeneity of cancer cells and itsimplications for cancer treatment. Acta Pharmacol. Sin., 36, 1219–1227.Ward,P.S., and Thompson,C.B. (2012) Metabolic reprogramming: a cancerhallmark even warburg did not anticipate. Cancer Cell, 21, 297–308.Whitaker-Menezes,D. et al. (2011) Evidence for a stromal-epithelial “lactateshuttle” in human tumors: MCT4 is a marker of oxidative stress in cancerassociated ﬁbroblasts. Cell Cycle, 10, 1772–1783.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i311/3953966by gueston 07 January 2018
5028881984002	PMID28881984	5028881984	https://watermark.silverchair.com/btx250.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881984.main.pdf	Estimation of time-varying growth, uptakeand excretion rates from dynamicmetabolomics dataEugenio Cinquemani1,*, Valérie Laroute2, Muriel Cocaign-Bousquet2,Hidde de Jong1 and Delphine Ropers11Inria, Centre de Recherche Grenoble – Rhône-Alpes, Montbonnot, France and 2LISBP, Université de Toulouse,CNRS, INRA, INSA, Toulouse, France*To whom correspondence should be addressed.AbstractMotivation: Technological advances in metabolomics have made it possible to monitor the con-centration of extracellular metabolites over time. From these data, it is possible to compute therates of uptake and excretion of the metabolites by a growing cell population, providing preciousinformation on the functioning of intracellular metabolism. The computation of the rate of these ex-change reactions, however, is difficult to achieve in practice for a number of reasons, notably noisymeasurements, correlations between the concentration profiles of the different extracellular me-tabolites, and discontinuties in the profiles due to sudden changes in metabolic regime.Results: We present a method for precisely estimating time-varying uptake and excretion ratesfrom time-series measurements of extracellular metabolite concentrations, specifically addressingall of the above issues. The estimation problem is formulated in a regularized Bayesian frameworkand solved by a combination of extended Kalman filtering and smoothing. The method is shown toimprove upon methods based on spline smoothing of the data. Moreover, when applied to two ac-tual datasets, the method recovers known features of overflow metabolism in Escherichia coli andLactococcus lactis, and provides evidence for acetate uptake by L. lactis after glucose exhaustion.The results raise interesting perspectives for further work on rate estimation from measurementsof intracellular metabolites.Availability and implementation: The Matlab code for the estimation method is available for down-load at https://team.inria.fr/ibis/rate-estimation-software/, together with the datasets.Contact: eugenio.cinquemani@inria.frSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionOver the last two decades powerful new technologies for metabolo-mics enabling the high-throughput quantification of metaboliteshave emerged. These technologies, usually based on mass spectrom-etry (MS) or nuclear magnetic resonance (NMR), may be directedwith high precision at specific classes of metabolites (targetedapproaches) or provide a global scan of the entire metabolome(untargeted approaches) (Patti et al., 2012). Extracellular metabol-ites, accumulating in or disappearing from the growth medium, areparticularly interesting. Their time profiles, while being relativelyeasy to measure, provide a footprint of intracellular physiology (Kellet al., 2005). In particular, the time-varying concentrations of extra-cellular metabolites allow the computation of uptake and excretionrates that can be related to intracellular metabolic fluxes by meansof flux balance models and metabolic flux analysis (Antoniewicz,2013; Mo et al., 2009). A variety of applications exploiting thetime-course profiles of extracellular metabolites can be found in theliterature, increasing our fundamental understanding of the func-tioning of metabolic networks or informing efforts to reengineerthese networks for biotechnological purposes (e.g. Behrends et al.2009; Morin et al. 2016; Taymaz-Nikerel et al. 2016).The estimation of time-varying uptake and excretion rates frommeasurements of extracellular metabolites is a challenging problemfor a number of reasons. First, the available data are noisy, evenwhen taking into account continuous progress in metabolomicsmethods. Second, the time-course profiles of different extracellularVC The Author 2017. Published by Oxford University Press. i301This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i301–i310doi: 10.1093/bioinformatics/btx250ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018metabolites are often strongly correlated. One obvious source ofcorrelation is the proportionality of uptake and excretion rates tothe size of the (growing) population of cells consuming or producingthe metabolites (Stephanopoulos et al., 1998). Third, the time-course profiles of extracellular metabolites are subject to discontinu-ities, due to sudden changes in the functioning of metabolism. Forinstance, in many bacteria catabolite repression leads to the sequen-tial utilization of carbon sources, generally favouring carbon sourcesthat sustain a higher growth rate (Kremling et al., 2015).Addressing the above issues in a principled manner requires anexplicit model relating exchange reactions, concentrations of extra-cellular metabolites and the size of the cell population. Moreover,we need sound statistical methods for the estimation of the ratesfrom measurements of metabolite concentrations and biomass.Most existing approaches assume the population to be in a state ofbalanced exponential growth, in which the cell population accumu-lates at a constant growth rate and in which the rates of exchange re-actions are constant. This much simplifies the problem as it reducesrate estimation to a standard linear regression problem (see Murphyand Young 2013 and references therein).The more general situation in which growth of the microbialpopulation is not balanced has received some attention under theheaders of dynamic metabolic flux analysis and dynamic flux bal-ance analysis (Antoniewicz, 2013; Mahadevan et al., 2002).Existing methods for estimating time-varying uptake and excretionrates under these conditions are mostly based on data smoothingusing moving averages or splines, followed by explicit computationof the rates by differentiation (Herwig et al., 2001; Llaneras andPic o, 2007; Niklas et al., 2011). Unfortunately, these methods sufferfrom high sensitivity to noise. This has motivated input estimationmethods that do not require differentiation, but fit a parameterizedrate function to the concentration data (e.g. Leighty andAntoniewicz 2011). For our purpose, however, these approachescome with a number of drawbacks, in particular the restriction to aspecific class of input functions, no exploitation of informationshared between correlated concentration profiles, and no (auto-mated) detection of dynamic changes in metabolic regimes.The aim of this article is to develop a method for precisely esti-mating time-varying uptake and excretion rates from measurementsof extracellular metabolite concentrations, specifically addressing allof the above issues in a comprehensive manner. In order to achievethis, we exploit the fact that the estimation of rates of exchange re-actions is an instance of more general input estimation problemsthat have been extensively studied in control theory and for whichpowerful solution methods exist (De Nicolao et al., 1997; Pillonettoand Bell, 2007). We follow a regularized Bayesian approach, wherethe unknown rate profiles are modelled as instances of a randomprocess (Rasmussen and Williams, 2006). In order to capture fastchanges in metabolic dynamics, we propose the use of time-varyingstatistical priors on the unknown rate profiles that are adaptivelyand automatically determined by suitable data preprocessing,including detection of metabolite depletion for the identification ofmetabolic regime changes. The resulting estimation problem is thensolved by a dynamical smoothing approach, here developed by thecombination of extended Kalman filtering and smoothing(Jazwinski, 1970; Kailath et al., 2000). Our approach generalizesupon related work in bioreactor process control, where Kalman fil-ters have been used for on-line estimation of growth rate and reac-tion rates (Bastin and Dochain, 1990; Venkateswarlu, 2005). Sincethe data are processed off-line in our case, the additional smoothingstep ensures full exploitation of the data and large improvementsover standard filtering.The test of our extended Kalman smoothing (EKS) method tosynthetic data with realistic noise levels and a representative numberof samples shows excellent performance, superior to results obtainedwith an approach based on spline smoothing. We also apply our ap-proach to datasets of measured time-varying extracellular metabol-ite concentrations in Escherichia coli and Lactococcus lactis. Themethod proves capable of estimating the rates of substrate uptakeand by-product excretion with high precision, uncovering notablyacetate uptake after glucose depletion in a L. lactis fermentationexperiment.The approach developed in this article provides a comprehensivesolution to the three main difficulties of estimating time-varyingrates of exchange reactions from extracellular metabolite data—noise, correlated concentration profiles, and discontinuities—usinga method with a solid mathematical foundation and wide applicabil-ity. An interesting further development would be the generalizationof the approach to measurements of intracellular metabolite concen-trations, for which increasingly powerful methods operating in realtime are becoming available (Link et al., 2015).2 Problem statement2.1 Dynamic model of cellular growth in a bioreactorWe consider experiments where the growth of a cellular populationin a bioreactor (biomass) and the evolution of the concentration of nextracellular metabolites are monitored over time. Let b(t) denotebiomass concentration and ci(t), with i¼1,. . .,n, the concentrationof the ith metabolite at time t. Biomass and metabolite dynamics aremodelled as (Fig. 1)_b tð Þ ¼ l tð Þb tð Þ; (1)_ci tð Þ ¼ ri tð Þb tð Þ; i ¼ 1; . . . ; n; (2)where l(t) denotes microbial growth rate at time t and ri(t) is therate of excretion (if positive) or uptake (if negative) of the ith metab-olite per unit of biomass. Equations (1) and (2) form an unstructuredmodel of a growing cell population, ignoring the functioning of in-ternal metabolism but describing its interactions with the environ-ment (Stephanopoulos et al., 1998). The model is based on theassumption that the only causes of changes in concentrations ci aredue to the uptake and excretion rates, thus leaving aside degradationof extracellular metabolites and inflow and outflow of the mediumin the bioreactor (Bastin and Dochain, 1990).The model of Equations (1) and (2) is a nonlinear system ofnþ1 coupled Ordinary Differential Equations (ODEs), with statevector x ¼ b c1       cn½  T , input vector u ¼ l r1       rn½  T(dependency of the variables on time t is often omitted from nota-tion for brevity), and initial conditions x(t0) at the starting time t0 ofthe experiment. The input profile u( ) is assumed to be piecewisecontinuous, so that the solution of the ODE system is well deter-mined, but not necessarily smooth.We consider that the different quantities xi(t), withi¼1,. . .,nþ1, are measured experimentally at time instants t thatb (t )ri (t )ci (t)Fig. 1. Schematic representation of the model of Equations (1) and (2)i302 E.Cinquemani et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018may differ across i. Let T i be a set of measurement times for xi. Fori ¼ 1; . . . ; nþ 1, measurements yi of xi are modelled asyi tð Þ ¼ xi tð Þ þ ei tð Þ; t 2 T i; (3)where ei(t) denotes random measurement error with mean zero andstandard deviation ri(t)>0. We assume that ei(t) is statistically inde-pendent of ei0 t0ð Þ for any t 2 T i and t0 2 T i0 such that i 6¼ i0 or t 6¼ t0.In a compact notation, the resulting system is_x tð Þ ¼ fx x tð Þ; u tð Þð Þ; t   t0; (4)y tð Þ ¼ Cx tð Þx tð Þ þ e tð Þ; t 2 T ; (5)with fx x; uð Þ ¼ x1   u and T ¼ T 1 [ . . . [ T nþ1. Because the quantitiesobserved at different time instants may not be the same, y(t) is a vectorthat changes size over time t. At a time t such that t 2 T i1 \ . . . \ T i‘ ,with fi1; . . . ; i‘g   f1; . . . ; nþ 1g a set with distinct entries, one hasthat y ¼ yi1       yi‘½  T ; e ¼ ei1       ei‘½  T , and Cx tð Þ is com-posed of rows i1; . . . ; i‘ of an (nþ1)-dimensional identity matrix.2.2 Reconstruction of excretion and uptake ratesLet Y ¼ fY1; . . . ;Ynþ1g be the set of all measurementsYi ¼ fyi tð Þ : t 2 T ig, with i ¼ 1; . . . ;nþ 1. The challenge we ad-dress is the reconstruction of the rate profiles u(t) over a time inter-val of interest given data Y. The problem is per se ill-posed (Bertero,1989; De Nicolao et al., 1997), since infinitely many profiles u(t)may perfectly explain the data for a corresponding choice of initialconditions x t0ð Þ, and the same would hold were x t0ð Þ known. Inparticular, arbitrarily irregular (“wiggly”) profiles u may fit slowlychanging measurements of the xi. To cope with this, methods basedon direct data fitting, such as spline interpolation of every observedprofile Yi, are often used to compute rate estimates by differenti-ation of the fits (Herwig et al., 2001; Llaneras and Pic o, 2007;Niklas et al., 2011). Unfortunately, these methods may produce un-realistic reconstructions as they inappropriately account for meas-urement noise and may loose information carried by the coupling ofthe ODEs through the biomass b.We therefore recast the problem into the framework of regular-ized estimation (Wahba, 1990). In this framework, reconstruction istypically expressed as an optimization problemminu2UQ uð Þ þ kR u;Yð Þ; (6)where U is a convenient class of candidate profiles, Q uð Þ   0 is ameasure of the regularity of the candidate solution u, and R u;Yð Þ  0 quantifies the accuracy by which the state profile predicted by(4) in response to u explains the data (for ease of exposition, here xt0ð Þ is considered fixed). Parameter k   0 trades off regularity of ufor accuracy of the data fit. In practice, existing methods consider aparametric class of profiles U ¼ fuh : h 2 Hg, and (6) is solved interms of the unknown parameters h characterizing the input profile(Schelker et al., 2012). While this approach guarantees well-behavedreconstruction of u, the problem remains challenging due toswitches in metabolic regime following the depletion of a growthsubstrate. In practice, this entails abrupt changes in the uptake, ex-cretion, and growth rates u, i.e. extremely fast dynamics that arehard to detect under the necessary regularity assumptions on u, un-less explicitly accounted for, e.g. by an ad hoc choice of U andrelated definition of Q and k.To address all of these issues, we propose a reconstructionmethod formulated as a Bayesian regularized estimation problem(Pillonetto and Bell, 2007; Rasmussen and Williams, 2006). Themethod is based on automatic detection of the switching times andsubsequent adaptive choice of the regularity of u. The contrastingobjective that this approach is capable to achieve is the reconstruc-tion of slowly-varying rates within a given metabolic regime, to-gether with the detection of abrupt changes in growth, uptake andexcretion due to metabolic switches. Moreover, the solution isnonparametric, i.e. both the definition of a parametric class of can-didate input profiles uh and the corresponding solution of a (typic-ally large) parameter optimization problem are circumventedby means of a dynamic optimization approach. Different from therecent work of Swain et al. (2016) for the estimation of the deriva-tive of an experimental profile, here we address the simultan-eous estimation of several unknown rate profiles, with explicitaccount of nonstationary dynamics, by means of a dynamicalapproach that is naturally suited to a vast class of nonlineardynamics.3 Estimation method3.1 Bayesian statement of the estimation problemIn a Bayesian setting, regularized estimation starts by placing astatistical prior on the unknown profiles that assigns largerprobability to smoother solutions. Consider one entry ui ofu ¼ u1       unþ1½  T . One models the unknown profile ui tð Þ as theoutcome of a random Gaussian process _vi ¼ ciwi and _ui ¼ vi, wherewi is standard white Gaussian noise. Intuitively, modelling ui as thisdouble-integral of white noise implies that ui is (with probability 1)a continuously differentiable profile, with variability (i.e. probabilitydistribution of its derivative) determined by the magnitude of ci > 0.In order to account for rates that may undergo faster changes in spe-cific periods of time (switches in metabolic activity), we let ci be afunction of time, where larger values of ci tð Þ around a time pointallow for rapid changes of ui around that time. Taking this modelfor every i ¼ 1; . . . ;nþ 1, and assuming wi and wi0 (i.e. ui and ui0 ) tobe mutually independent for i 6¼ i0, we get the 2   nþ 1ð Þ-dimen-sional linear system of ODEs_n tð Þ ¼ Ann tð Þ þ Bn tð Þw tð Þ; (7)u tð Þ ¼ Cnn tð Þ (8)where n ¼ v1 u1       vnþ1 unþ1½  T ; w ¼ w1       wnþ1½  Tis a standard Gaussian noise vector process with uncorrelatedentries, and An; Bn tð Þ; Cn are equal to0 01 0. ..0 01 026666666643777777775;c1 tð Þ0. ..cnþ1 tð Þ026666666643777777775;0 1. ..0 126643775;in the same order. With this characterization of the unknown ratevector, estimation of u at any time t given data Y can be formulatedas the computation of the conditional expectation bu tð Þ ¼ E u tð ÞjY½  .In practice, the resulting estimate depends on the choice of the ci.For a constant ci, it can be shown that this approach leads to an esti-mation problem that is equivalent to a Tikhonov regularizationproblem in the form of Equation (6), where the role of theEstimation of time-varying growth, uptake and excretion rates from dynamic metabolomics data i303Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018regularization factor k is played by the relative magnitude of the ciand the ri (De Nicolao et al., 1997; Wahba, 1990). Here, however,we let ci tð Þ vary in time so as to distinguish (long) periods with slowrate changes from (short) periods of steep rate transitions. In the fol-lowing section, we discuss how to (approximately) compute bu tð Þ forassigned functions ci tð Þ. We then discuss how a suitable choice ofci tð Þ is made by appropriate data preprocessing.3.2 Solution via nonlinear Kalman smoothingWe start by considering the stochastic differential equation systemobtained by the composition of Equations (4)–(5) and (7)–(8).Denoting z ¼ xT nT   T , one gets_z tð Þ ¼ f z tð Þð Þ þ x tð Þ; t   t0; (9)y tð Þ ¼ C tð Þz tð Þ þ e tð Þ; t 2 T ; (10)with f zð Þ ¼ fxðx;CnnÞ TðAnnÞTh iTand C tð Þ ¼ Cx tð Þ 0‘ 2  nþ1ð Þ   ,where 0‘ 2  nþ1ð Þ is a zero matrix of dimensions compatible with Cx tð Þ.In addition, x is zero-mean white Gaussian noise with covariance ma-trix Q tð Þ ¼ B tð ÞB tð ÞT , where B tð Þ ¼ 0 nþ1ð Þ  nþ1ð Þ Bn tð ÞTh iT, whilee is a zero-mean random measurement error vector with covariancematrix R tð Þ ¼ diag r2i1 ; . . . ; r2i‘   (recall that i1; . . . ; i‘ are the entries ofx measured at time t).Together with a priori statistics for the initial state x t0ð Þ,Equations (9) and (10) describe z as a continuous-time stochastic dy-namic system with sampled measurements. By virtue of this, giventhat u is part of the system state, computation of bu can be performedby a dynamical smoothing approach. Here, because the system dy-namics are nonlinear, optimal linear Kalman smoothing does notapply, and an approximate solution must be sought. Among manyexisting approaches (Doucet et al., 2001; Julier and Uhlmann,2004), we opt for a smoothing approach based on a forward pass inthe form of an Extended Kalman Filter (EKF, Jazwinski 1970), fol-lowed by a backward correction step in the form of a Bryson-Fraziersmoother (Cox 1964; Kailath et al., 2000).The overall procedure, which we refer to as EKS, works as fol-lows. Let tj, with j ¼ 0; . . . ;m, be the elements of T in increasingorder, i.e. the sequence of measurement times. For j ¼ 0; . . . ;m,let bzj ¼ E z tj  jy t0ð Þ; . . . ; y tj1     ; bzj ¼ E z tj  jy t0ð Þ; . . . ; y tj     andbzþj ¼ E z tj  jy t0ð Þ; . . . ; y tmð Þ   be the optimal Bayesian one-step pre-diction, filtered, and smoothed estimate of z at measurement times tj,in the same order, and let Pj , Pj and Pþj be the corresponding esti-mation error covariance matrices (Jazwinski, 1970). Recall thatz comprises both x and u, i.e. the above quantities provide optimal-prediction, filtered, and smoothed estimates of state x and rates u.Starting from a priori mean bz0 and covariance matrix P0 of the initialstate z t0ð Þ, the following filtering iteration provides approximatecomputation of bzj and bzj for j ¼ 0; 1; . . . ;m:• Measurement update: Computebzj ¼ bzj þGj y tj   C tj  bzj   ;Pj ¼ I GjC tj    Pj I GjC tj    T þGjR tj  GTj ;with Gj ¼ Pj C tj  TS1j and Sj ¼ C tj  Pj C tj  T þR tj  .• Prediction: If j < m, compute bzjþ1 and Pjþ1 as the solutions of_z tð Þ ¼ f z tð Þð Þ; z tj  ¼ bzj; (11)_P tð Þ ¼ F tð ÞP tð Þ þ P tð ÞF tð ÞT þQ tð Þ; P tj  ¼ bPj (12)at time tjþ1, where F(t) is the Jacobian of f(x) evaluated along the so-lution of Equation (11).Note that bzþm ¼ bzm and Pþm ¼ Pm by definition. Then, for j<m, abackward iteration provides the computation of the bzþj from the resultsof the filtering pass with the aid of additional recursively computedquantities kj and Kj. Defining km ¼ C tmð ÞTS1m y tj   C tj  bzm  andKm ¼ C tmð ÞTS1m C tmð Þ, for j ¼ m 1;m 2; . . . ; 0:• Smoothing: Computekj ¼ WTj kjþ1 þ C tj  TS1j y tj   C tj  bzj   ; (13)Kj ¼ WTj Kjþ1Wj þ C tj  TS1j C tj  ; (14)bzþj ¼ bzj þ Pj kj; (15)Pþj ¼ Pj  Pj KjPj ; (16)where Wj ¼ Ujþ1  Ujþ1Pj C tj  TS1j C tj  and Ujþ1 is the Jacobianof the solution at time tjþ1 of Equation (11) with respect to the ini-tial condition z tj  .For every j, Ujþ1 can be calculated by means of so-called sensitiv-ity equations (Khalil, 2002). Because sensitivity equations and thesolution of (12) depend on the solution of (11) in-between timepoints, in practice, the quantities Ujþ1; Pjþ1 and bzjþ1 are simultan-eously calculated (and stored) at every iteration j of the filtering passby the solution of a single augmented ODE system.The forward sweep (filtering and prediction) is a standard imple-mentation of the EKF for continuous dynamic systems with samplemeasurements (Jazwinski, 1970). Yet, the use of a time-varying ma-trix Q(t) exploiting the time-varying smoothing profiles ci tð Þ is non-standard. The backward sweep (smoothing) described here is ageneralization of the method in Cox (1964) to the case of continu-ous dynamics. For the forward pass, which is critical in ensuringconvergence of the approximate solution, EKF showed good per-formance at very little computational expense. Yet, for especiallysparse data sets, a preliminary data interpolation step adapted to thesystem dynamics is also possible (Supplementary Material S1). Notethat the approach is modular, in the sense that more advanced filter-ing schemes (Doucet et al., 2001; Julier and Uhlmann, 2004) couldbe used in place of EKF to generate the forward predictions bzj andPj used in the smoothing sweep to produce the final estimates bzþjand Pþj .The above procedure computes estimates bzþj at measurementtimes. Estimates in-between measurement times are easily obtained byincluding in T additional times of interest, and a simple adaptation ofthe corresponding iterations. The procedure relies on knowledge ofthe measurement uncertainties ri entering matrix R at the variousmeasurement times, of the profiles ci  ð Þ eventually defining the time-varying matrix Q  ð Þ, and on given a priori initial state statistics bz0and P0 . While we assume that the ri are given, in the following sec-tion we discuss how the ci (which are typically not known) as well asbz0 and P0 (which are most often partially known) can be determinedfrom suitable data preprocessing. From now on, we will denote theEKS estimates of u and x at a generic time t as bu tð Þ and bx tð Þ, and thecorresponding estimation error covariance matrices as Px tð Þ andPu tð Þ. From the latter matrices, credibility (i.e. Bayesian confidence)intervals Xai tð Þ and Uai tð Þ for the estimates of x and u, such thatP xi tð Þ 2 Xai tð Þ   ¼ a and P u tð Þ 2 Uai tð Þ   ¼ a, are easily computed.i304 E.Cinquemani et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018For a ¼ 95%, in particular, Xai tð Þ ¼ bxi62  ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiPx tð Þi;i   randUai tð Þ ¼ bui62  ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiPu tð Þð Þi;iq.3.3 Detection of switches and filter tuningIn our approach, the choice of Bayesian priors for the estimatedrates is the result of two steps, the determination of smoothing fac-tors for slow and fast dynamics, and the detection of regions wherefast dynamics take place. In the interest of affordable computationalcomplexity, these two steps are carried out separately and are finallycombined into the definition of the smoothing profiles ci  ð Þ.3.3.1 Calculation of smoothing factors and initial state statisticsHere, we discuss the automated choice of appropriate smoothingprofile ci and quantities z0 and P0 by data preprocessing. This isbased on a (not necessarily accurate) pre-estimation of state xi andrate ui profiles from data Yi, separately for every i. The same proced-ure will also be used later on to benchmark the performance of ourEKS estimation procedure.Consider the case i¼1 first. Given data Y1 at times T 1, a roughestimate of b tð Þ ¼ x1 tð Þ over the time period spanned by T 1 can bedrawn by spline interpolation. We use cubic smoothing splines, sothat our interpolation x1 tjkð Þ depends on a smoothing parameter k.In order to ensure an appropriate choice of k, we resort to the fol-lowing cross-validation procedure. For a candidate k > 0, we parti-tion data Y1 into L groups Yk1 of measurements taken on a set ofT k1   T 1 subsequent times, with k ¼ 1; . . . ;L. For every k, we per-form smoothed spline interpolation using all data Yk01 with k0 6¼ k,and compute  k kð Þ, the sum of squared residuals of the interpolationat the validation times T k1. The resulting index   kð Þ ¼  1 kð Þ þ      þ L kð Þ quantifies the overfitting (lack of predictivity) of the splineinterpolations (the larger the   kð Þ, the worse the interpolation). Wechoose the value of k that optimizes   kð Þ by numerical minimization.By this optimized smoothing parameter, say ~k1, we finally obtainthe optimized smoothing interpolation ~x1 tð Þ ¼ x1 tj~k1   . Pre-estimates of the rate profile u1(t) are then obtained by means ofEquation (1), i.e. ~u1 tð Þ ¼ d~x1 tð Þ=dtð Þ= ~x1 tð Þð Þ.Because of the homogeneity of the smoothing strength over thewhole time span, it is expected that estimates of state and rate pro-files of appropriate regularity are obtained at all times except at thefew rapid transitions from one regime to another, where over-smoothing occurs. By this, ~u1 tð Þ provides us with the necessary in-formation on how to choose the smoothing profile c1 tð Þ. Concretely,this is obtained by the following method of general applicability.Recall that, for EKS purposes, u1 is modelled as a twice-integratedwhite noise process. For a constant c1 over the time intervalt; t þ s½ Þ, the increment u1 t þ sð Þ  u1 tð Þ has mean zero and stand-ard deviation equal to c1   s3=2=ffiffiffi3p. Because this standard deviationdefines the regularity of ~u1, and because the corresponding ~u1 t þ sð Þ~u1 tð Þ are expected to be of the right order in-between metabolicswitches, an appropriate choice of c1 tð Þ within these periods is suchthat the standard deviation of u1 t þ sð Þ  u1 tð Þ equals the averagevalue, say D, of j~u1 t þ sð Þ  ~u1 tð Þj over a grid of times t with sam-pling period s. Upon computation of D from ~u1, this leads to the def-inition c1 tð Þ ¼ c 1, with c 1 ¼ D= s3=2=ffiffiffi3p  , for all times t in-betweenswitches. Finally, in order to capture proportionally faster dynamics,within periods of fast metabolic changes the smoothing factor is setto c1 tð Þ ¼ c11 , with c11 ¼ 103c 1.For i 6¼ 1, a nearly identical procedure is followed. For every i, across-validated spline interpolation is operated on data Yi, obtainingan optimized estimate ~xi tð Þ (with its own parameter ~k i). Then, usingthe previous biomass data interpolation ~x1 tð Þ, rate pre-estimates areobtained with Equation (2) as ~ui tð Þ ¼ d~xi tð Þ=dtð Þ= ~x1 tð Þð Þ. Finally,c i and c1i are set as a function of ~ui as described above.Pre-estimates ~xi and ~ui, with i ¼ 1; . . . ; nþ 1, also allow us to fixthe initial state statistics bz0 and P0 . More precisely, entries of bz0corresponding to xi (resp. to ui) are set equal to the initial value of ~xi(resp. of ~ui), whereas all other entries are set to 0. From this, P0 isset to be a diagonal matrix with diagonal entries defined element-wise by minf bz0  2;Dg, with D big enough. This ensures that priorsare sufficiently weak, in order to favour convergence of the EKSwithout constraining a priori the resulting estimates.3.3.2 Detection of switches and definition of the smoothing profilesSwitching times are automatically detected by direct processing ofthe measurements. We exploit the fact that concentrations ci ofsome metabolites dropping to zero are typically associated withchanges of the metabolic regime. In accordance with this, for everyi, a time tj 2 T i is declared a switching time if yi tj    2   ri tð Þ andthe observations yi tj1  ; yi tj2  ; . . . ; yi tjJ  are above the samethreshold.If a switch at time tj is detected, the time period where ratechanges are expected to be fast is set to tj1; tj   . Over this switchingperiod, because a drop of ci can induce sudden rate changes in allmetabolites, ci0 tð Þ is set to c1i0 for i0 ¼ 1; . . . ;nþ 1. Note that the re-sulting smoothing profiles constitute Bayesian priors that drive theestimation procedure toward estimates with regularity properties ofthe right order. The actual degree of smoothness and steepness of es-timates at sudden metabolic changes is then inherently adjusted bythe EKS procedure.Note that failure to detect a metabolic switch, e.g. due to lack ofmeasurements for critical metabolites, will not spoil the estimationprocedure overall, but will return rate estimates that vary at a slowpace at metabolic changes, prompting for either a manual definitionof switch times, or a more appropriate experiment design.3.4 ImplementationThe estimation method described in the previous section, comprisingdata-driven tuning of the EKS and the EKS itself, has been imple-mented in Matlab. The software takes as input data Y1; . . . ;Ynþ1,measurement times T 1; . . . ;T nþ1, error levels r1; . . . ; rnþ1, and per-forms tuning as well as estimation in a completely automated fash-ion. It returns full state (i.e. biomass, concentration and rate)estimated profiles and estimation error covariances, as well as theestimation settings (notably smoothing factor and switch times).Custom settings (for instance, modifications of the output settings)may be specified as well. A more detailed description of the softwareand its usage is provided in Supplementary Material Section S6.4 Validation on simulated dataIn order to validate the estimation method of Section 3, we now dis-cuss its application to simulated data, so as to compare the esti-mated reaction rates with the actual reaction rates used forgenerating the data. As a concrete example, we will consider thephenomenon of overflow metabolism and diauxic growth. Overflowmetabolism is a recurrent phenomenon in microorganisms occurringin situations where a primary growth substrate is available in excessand inefficiently used by the cells, in the sense that secondary sub-strates are secreted during growth on the primary substrate. Oncethe primary substrate has been depleted, growth continues on the se-cond substrate, often at a lower rate (Kremling et al., 2015; Pacziaet al., 2012). A prototypical example of overflow metabolismEstimation of time-varying growth, uptake and excretion rates from dynamic metabolomics data i305Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018leading to diauxic growth is aerobic growth of E. coli in minimalmedium with glucose, leading to an overflow of acetate that is uti-lized after glucose exhaustion, giving rise to a so-called acetateswitch (Enjalbert et al., 2013; Wolfe, 2005). The simulated experi-ment in this section is much similar to the actual experiments con-sidered in the following section.The dynamics of the growing microbial population in the bio-reactor and of the extracellular metabolite concentrations, the pri-mary and secondary substrates, are described by Equations (1) and(2) with n¼2. The simulated rates are piecewise constant functionsof environmental substrate concentrations. Starting from  b ¼ b 0ð Þ> 0;  c1 ¼ c1 0ð Þ > 0 and  c2 ¼ c2 0ð Þ   0, the primary substrate rater1 tð Þ takes value  r1 < 0 (uptake) until time t¼T1 where c1 tð Þ hits0, and zero afterwards. The secondary substrate rate r2 tð Þ takesvalue  r2 > 0 (excretion) until time T1, then it switches to r2 < 0(uptake) until time t¼T2 where c2 tð Þ hits 0, and zero afterwards.Biomass growth rate takes value  l > 0 until time T1 (growth on firstsubstrate), then switches to l, with  l > l > 0 until time T2 (growthon second substrate), and to 0 afterwards (growth arrest). We simu-lated measurements taken at times T 1 ¼ ftj ¼ j   T; j ¼ 0; . . . ; 3mgfor biomass, and at sparser times T iþ1 ¼ ftj ¼ j   3T; j ¼ 0; . . . ;mg,for the primary and secondary substrates. Random measurementerror is added in accordance with Equation (3) with time-homogeneous standard deviations rb, rc1 ; rc2 . The simulated dataare shown in Figure 2a and also separately in SupplementaryMaterial Section 2.1.Figure 2a and b shows the detected depletion of substrates andthe EKS estimates of x and u obtained with fully automated filtertuning. Detection of switches at times T1 (depletion of the primarysubstrate) and T2 (depletion of the secondary substrate) is visiblycorrect in panel (b), and in absence of further information, poten-tially fast rate changes are authorized in the time interval betweenthe last measurement above and the first below the switching thresh-old. This gives rise to the rate estimates bu displayed in Figure 2b,with a smooth, slowly-varying profile except within the switchingperiods, where transitions are steep as expected.The same rate estimates are also reported in Figure 3b, wherethey are compared with the actual simulated rates and with the rateestimates ~u found by the smoothing spline method of Section 3.3.1.That EKS estimates bu outperform estimates ~u obtained via splinesmoothing is apparent. It is worth remarking how the EKS tuningbased on ~u, which operates on the regularity of bu, does not spoil theEKS estimates themselves (no direct relationship between bu and ~u).Yet, rate estimates over constant regimes show residual fluctuations,presumably due to a slight overestimation of the c i by the automatedtuning step. At the same time, estimated rate transitions are still notas abrupt as in the simulations, and the associated credibility inter-vals are large. This is primarily due to the sparsity of the metabolitemeasurements, causing the estimated transitions to spread over asomewhat longer period of time. In Figure 3b, small manual ad-justments of the settings returned by the procedure yield even betterresults, with sharp changes closely resembling the actual discontinu-ous regime changes. It is important to note that these adjustmentsare driven in a rather intuitive manner by the qualitative analysis ofthe fully automated estimation results, i.e. they can be operated by auser facing the analysis of experimental data.In summary, estimation results for a realistic (noisy) simulateddataset of a diauxic shift experiment show excellent performance ofthe automated method and significant improvements relative to areference approach. Basic manual refinements of the data-drivenEKS tuning allow the results to be even further improved.Comparison of estimation results with those obtained with EKF (theforward pass of the EKS), which is another approach that has beenconsidered in the literature, also witnesses striking improve-ments (see Supplementary Fig. S2). The above results are confirmedby an additional simulation example of fed-batch cultivation inSupplementary Material Section S2.2.5 Applications of rate estimation5.1 Diauxic growth in E. coliThe first application of the method to a real dataset concerns time-series measurements of glucose, acetate and biomass during aerobicgrowth of E. coli in minimal medium with glucose and acetate(Morin et al., 2016). The acetate has accumulated in the medium asa by-product of growth on glucose at a rate exceeding the oxydizingcapacity of central metabolism. The bacteria were cultivated in abioreactor in batch mode and samples were taken every 10–30 minover a period of about 6 h, covering rapid exponential growth onglucose, glucose depletion and continued slow growth on acetate.The samples were analysed by high-performance liquid chromatog-raphy (HPLC) to quantify the concentrations of extracellular(a)(b)Fig. 2. Estimation of exchange rates by applying the EKS method to a datasetobtained by simulating a diauxic growth experiment with overflow me-tabolism. Simulation parameters are reported in Supplementary MaterialSection S2.1, showing the simulated data and their confidence intervals.(a) Simulated data (circles), detected switching times (in-between verticalblue lines) when a substrate concentration drops to 0, and EKS estimates ofbiomass and concentration profiles with their 95% credibility intervals (redcurves and bands, respectively). (b) EKS rate estimates from the fully auto-mated procedure with 95% credibility intervals (red curves and bands,respectively)i306 E.Cinquemani et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018metabolites in the medium (Morin et al., 2016). The resultingmeasurements for one replicate are shown in Figure 4b and inSupplementary Material Section S2.3.The challenge for the analysis of this dataset is to correctly esti-mate the rapid changes of the uptake and secretion rates around thetime of glucose depletion at 0 h, while at the same time provide astable value for the rates during balanced growth on either glucose(before the switching time) or acetate (after the switching time). Thealgorithm described in Section 3.2 was run for the three variablesbiomass (b), glucose concentration (cglc), and acetate concentration(cace), after a first preprocessing round in which appropriate smooth-ing profiles and a priori initial state statistics were obtained by splinesmoothing and generalized cross validation (Section 3.3). The EKSestimates for the rates are shown in Figure 4c.The most striking conclusion drawn from the estimation resultsis the capability of the algorithm to precisely capture the sharp dropin growth rate when the glucose concentration falls to 0, accompa-nied by an equally abrupt arrest of glucose uptake and switch fromacetate excretion to acetate uptake. In contrast, the spline smoothingestimates do not capture this abrupt regime change and, moreover,lead to unstable estimates of the steady-state exchange rates andgrowth rate, visible as oscillations around the steady-state values(Supplementary Fig. S6).The importance of the precision of rate estimation can be illus-trated by testing the consistency of the results with the reaction stoi-chiometry of intracellular metabolism, using a flux balance model ofE. coli (Feist et al., 2007). In particular, we performed a metabolicflux analysis in the manner of Morin et al. (2016) just before theacetate switch, where the smoothing spline and EKS estimates aremost different, and compared estimates for 15 fluxes in central car-bon metabolism obtained by flux variability analysis. Interestingly,the smoothing spline estimates yield flux distributions that are muchless precise and non-intuitive (Supplementary Material Section S4).0 50 100 150 200 250 300 350 400 450−0.0200.02µ (min−1)0 50 100 150 200 250 300 350 400 450−0.0100.01r 1 (AU)0 50 100 150 200 250 300 350 400 450−505x 10−3r 2 (AU)Time (min)(a)0 100 200 300 400 50000.010.020.03µ (min−1)0 100 200 300 400 500−0.0200.02r 1 (AU)0 100 200 300 400 500−202468x 10−3r 2 (AU)Time (min)(b)Fig. 3. Comparison of the results obtained with different rate estimation meth-ods applied to the data set of Figure 2. (a) EKS rate estimates and their 95%credibility intervals obtained after manual adjustment of the automatically deter-mined switching periods and smoothing factors (red curves and bands, respect-ively). The adjustments concern a decrease of the length of the transition periodto 10 min and a decrease of the smoothing factors ðc 1; c 2; c 3Þ ¼ ð1:80; 1:87; 1:12Þ 105 to a uniform value of 10–6 for all i, with ratios c1i =c i unchanged.(b) Comparison of the true rates (dashed black curves) with estimated ratesobtained by the spline smoothing method (solid blue curves), by the fullyautomated EKS method (same as in Fig. 2b; dashed magenta curves), and by aposteriori manual adjustment of the automatic EKS settings (solid red line)(a)(b)(c)Fig. 4. Diauxic growth on glucose and acetate of E. coli. (a) Model schemedefining variables and rates. (b) Data (circles) from Morin et al. (2016) andEKS estimates with credibility intervals (solid curve and shaded band) of bio-mass and concentration profiles. (c) EKS rate estimates and credibility inter-vals (red curve and shaded band) obtained with the fully automatedprocedure described in Section 3. The smoothing parameters found for bio-mass, glucose and acetate, c b ¼ 0:675; c glc ¼ 5:5842 and c ace ¼ 8:4025, wereadjusted by a factor of 101; 101 and 2–1, respectively. For a comparison ofthe EKS estimates with the smoothing spline estimates, see SupplementaryMaterial Section S2.2Estimation of time-varying growth, uptake and excretion rates from dynamic metabolomics data i307Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018For example, the partitioning of the incoming flux of glucose overthe glycolysis, pentose-phosphate, and glycogenolysis pathways isnot appropriately accounted for, since every pathway can be prefer-entially used in some optimal solution. In reality, the major part ofthe incoming glucose flux enters glycolysis (Morin et al., 2016), asobserved when using the EKS estimates for metabolic flux analysis(Supplementary Material Section S4).We found a remarkable consistency between the predicted intra-cellular fluxes with the EKS estimates, when cells are in quasisteady-state growth two hours before glucose exhaustion, withmeasured fluxes in a continuous culture of E. coli on glucose (seeSupplementary Fig. S10).5.2 Production of lactic acid by L. lactisWhen growing on glucose, lactose or other sugars, the energy me-tabolism of L. lactis leads to the excretion of large amounts of lacticacid. In addition to being a catalyst for the production of buttermilk and cheese, the accumulation of lactic acid in the medium in-hibits the growth of microorganisms, including food-borne patho-gens. This has motivated interest in L. lactis for the purpose of foodconservation (Even et al., 2002). The second application of our rateestimation method concerns a study of the effect of lactic acid over-flow on L. lactis growth and the consequences of a glucose pulse onlactic acid production. The bacteria were cultivated in fed-batch in afermenter and 30 samples were taken over 3 days and analysed bymeans of HPLC to quantify glucose, lactic acid and acetate concen-trations in the medium (see Supplementary Material Section S3 forthe experimental protocol). The resulting measurements are shownin Figure 5b and in Supplementary Material Section S2.4.The interest of the dataset is (1) to quantify the effect of the lacticacid produced by L. lactis on the growth rate of the cell populationand (2) to account for rapid changes in the rates of acetate and lacticacid accumulation following the depletion of glucose just before50 h and the supply of a glucose pulse shortly afterwards (Fig. 5b).The detection of glucose exhaustion is similar to the previous appli-cation, but the addition of a pulse of glucose does not strictly fallwithin the modeling framework of Equations (1) and (2), which as-sumes that changes in extracellular metabolite concentrations areonly due to uptake and excretion of the metabolites by the growingcell population (and not by external inflow into the bioreactor).While the models can be straightforwardly generalized to cover thiscase explicitly (see Supplementary Material Section S5), we heresidestep the problem by lumping the rates of glucose uptake and in-flow into a single apparent rate for glucose accumulation. The esti-mation results are summarized in Figure 5c.A first conclusion that can be drawn from inspecting the esti-mated rates is that, due to the growth-inhibitory effect of lactic acidaccumulation, a state of balanced, non-zero growth is never reachedin the first 10 h of the experiment. This problem, well-known in L.lactis cultivation experiments, demonstrates the importance of beingable to compute a time-varying growth rate profile rather than re-port a single value at an arbitrary point along the growth curve.Second, the depletion of glucose just before 50 h is adequately cap-tured by the method, as well as the subsequent uptake of acetate,visible in the negative value of the estimated rate following glucosedepletion. This observation, reminescent of diauxic growth in theprevious example, is interesting but currently not well understood.Just after the glucose pulse at 50.5 h, the acetate excretion rate be-comes slightly positive again, meaning that the bacteria convert theadded glucose into lactic acid and acetate, in the absence of biomassaccumulation since the growth rate does not noticeably change. Thisexample demonstrates the capability of the method to capture subtledynamic changes in uptake and excretion rates from concentrationprofiles of extracellular metabolites.6 DiscussionDynamic estimation problems have become ubiquitous in the era ofhigh-throughput data generation in biology. In the study of gene ex-pression, for example, time-series measurements of the fluorescencesignals emitted by reporter proteins contain information on time-varying promoter activities (Zulkower et al., 2015), while time-series measurements of signal transduction outputs allow the recon-struction of time-varying inputs, like pathway activation by growthhormones (Schelker et al., 2012). The measurement of extracellularmetabolites has become simpler to achieve through ongoing ad-vances in the field of metabolomics and they provide precious(a)(b)(c)Fig. 5. Lactic acid production by L. lactis. (a) Model scheme defining variablesand rates. (b) Data (circles) and EKS estimates with credibility intervals (solidred curve and shaded band) of biomass and concentration profiles. The de-tected switching time lies between the two blue vertical lines. (c) EKS rate es-timates and credibility intervals (red curve and shaded band) obtained withthe fully automated procedure described in Section 3. The smoothing param-eters used for biomass, glucose, lactic acid and acetate are c b ¼ 0:01; c glc¼ 0:1; c lac ¼ 0:1 and c ace ¼ 0:01, respectively. Factors c1i ¼ 103c i , as per de-fault settings, except for c1glc ¼ 104c glc, to cope with glucose additioni308 E.Cinquemani et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018information on intracellular metabolism (Kell et al., 2005). Thesedata are often underexploited though, in the sense that in manystudies the time-varying rates of substrate uptake and by-product se-cretion are not computed. Changes in these rates, however, revealchanges in cellular metabolism and may thus be instrumental in sys-tems biology for better understanding the response of cells to exter-nal perturbations and in biotechnology for dynamically adaptingprocess conditions.One of the reasons that time-series measurements of extracellu-lar metabolites are not fully exploited is the difficulty of estimatingthe uptake and excretion rates in a reliable manner. In particular,we identified three major problems: noisy data, coupling of therates of the exchange reactions, and discontinuities in the concen-tration profiles due to sudden changes in metabolic regime. Inorder to address these problems in a comprehensive and principledmanner, we proposed a Bayesian formulation of the estimationproblem and an EKS method for solving the problem. This ap-proach was seen to perform well on simulated data, in the sensethat the time-varying rates could be accurately reconstructed,much better than by a reference method based on spline smoothingand differentiation. When applied to real data sets, the methodwas able to recover known features of overflow metabolism in twodifferent bacteria, E. coli and L. lactis, and provided evidence foracetate uptake by L. lactis after glucose exhaustion. Moreover, theestimated rates provide tight constraints for metabolic flux ana-lysis, as was seen by combining this information with a stoichiom-etry model of E. coli metabolism.The method presented in this article bears similarity with ex-tended Kalman filtering methods developed for on-line control ofbioreactors (Bastin and Dochain, 1990; Venkateswarlu, 2005). Incomparison with most of these studies, we do not consider the esti-mation problem in an on-line context, but use the entire time-courseof the experiment for reconstructing the rates of the exchange reac-tions, adding a smoothing step to the filtering procedure. The modelof Equations (1) and (2) only covers growth in batch mode, but ourmodels can be straightforwardly extended to account for fed-batchand continuous cultivation by adding terms in the right-hand side ofEquation (2) representing inflow and outflow rates, and possiblyrates of reactions involved in the degradation or gaseous escape ofextracellular metabolites (Bastin and Dochain, 1990). In particular,extending the model with an inflow rate for glucose would more dir-ectly describe the L. lactis application (see Supplementary MaterialSection S5). The Bayesian approach proposed here for the formula-tion of the estimation problem and the dynamical smoothing solu-tion developed in terms of EKS lend themselves to the necessarygeneralizations.Measurement of extracellular metabolites is usually easier toachieve than measurement of intracellular metabolites. Extracellularmetabolites usually accumulate at much higher concentrations andevolve on a slower time scale (Granucci et al., 2015; Kell et al.,2005). Moreover, experimental protocols require less precautionsthan for the quantification of intracellular metabolites, demandingthe rapid quenching of metabolism and adequate separation and ex-traction procedures (van Gulik, 2010). Nevertheless, recent progressin experimental techniques has made high-frequency and high-precision measurement of intracellular metabolites feasible (Linket al., 2015), which suggests interesting extensions of the model and method presented here. In particular, for everymeasured intracellular metabolite the rates of all reactions produc-ing and consuming it need to be estimated. This results in a linearmodel, but with more strongly coupled equations. While the basicprinciple of the Kalman smoothing approach will remain applicable,new theoretical and practical problems are expected to occur, not-ably those related to computational efficiency and observability ofthe unknown inputs (Khalil, 2002).FundingThis study was supported by the Programme d’Investissement d’Avenir underproject Reset (ANR-11-BINF-0005).Conflict of Interest: none declared.ReferencesAntoniewicz,M. (2013) Dynamic metabolic flux analysis—tools for probingtransient states of metabolic networks. Curr. Opin. Biotechnol., 24,973–978.Bastin,G., and Dochain,D. (1990) On-Line Estimation and Adaptive Controlof Bioreactors. Elsevier, Amsterdam.Behrends,V. et al. (2009) Time-resolved metabolic footprinting for nonlinearmodeling of bacterial substrate utilization. Appl. Environ. Microbiol., 75,2453–2463.Bertero,M. (1989) Linear inverse and ill-posed problems. Volume 75 ofAdvances in Electronics and Electron Physics, Academic Press, New York,pp. 1–120.Cox,H. (1964) On the estimation of state variables and parameters for noisydynamic systems. IEEE Trans. Autom. Control., 9, 5–12.De Nicolao,G. et al. (1997) Nonparametric input estimation in physiologicalsystems: problems, methods, and case studies. Automatica, 33, 851–870.Doucet,A. et al. (2001) Sequential Monte Carlo Methods in Practice. Springer,New York.Enjalbert,B. et al. (2013) Physiological and molecular timing of the glucose toacetate transition in Escherichia coli. Metabolites, 3, 820–837.Even,S. et al. (2002) Dynamic response of catabolic pathways to autoacidifica-tion in Lactococcus lactis: transcript profiling and stability in relation tometabolic and energetic constraints. Mol. Microbiol., 45, 1143–1152.Feist,A. et al. (2007) A genome-scale metabolic reconstruction for Escherichiacoli K-12 MG1655 that accounts for 1260 ORFs and thermodynamic infor-mation. Mol. Syst. Biol., 3, 121.Granucci,N. et al. (2015) Can we predict the intracellular metabolic state of acell based on extracellular metabolite data?. Mol. Biosyst., 11, 3297–3304.Herwig,C. et al. (2001) On-line stoichiometry and identification of metabolicstate under dynamic process conditions. Biotechnol. Bioeng., 75, 345–354.Jazwinski,A.H. (1970) Stochastic Processes and Filtering Theory. AcademicPress, New York.Julier,S.J., and Uhlmann,J.K. (2004) Unscented filtering and nonlinear estima-tion. Proc. IEEE, 92, 401–422.Kailath,T. et al. (2000) Linear Estimation. Prentice Hall, Upper Saddle River.Kell,D. et al. (2005) Metabolic footprinting and systems biology: the mediumis the message. Nat. Rev. Microbiol., 3, 557–565.Khalil,H.K. (2002) Nonlinear Systems. Prentice Hall, Upper Saddle River.Kremling,A. et al. (2015) Understanding carbon catabolite repression inEscherichia coli using quantitative models. Trends Microbiol., 23, 99–109.Leighty,R., and Antoniewicz,M. (2011) Dynamic metabolic flux analysis(DMFA): a framework for determining fluxes at metabolic non-steady state.Metab. Eng., 13, 745–755.Link,H. et al. (2015) Real-time metabolome profiling of the metabolic switchbetween starvation and growth. Nat. Methods, 12, 1091–1097.Llaneras,F., and Pic o,J. (2007) A procedure for the estimation over time ofmetabolic fluxes in scenarios where measurements are uncertain and/or in-sufficient. BMC Bioinformatics, 8, 421.Mahadevan,D. et al. (2002) Dynamic flux balance analysis of diauxic growthin Escherichia coli. Biophys. J., 83, 1331–1340.Mo,M. et al. (2009) Connecting extracellular metabolomic measurements tointracellular flux states in yeast. BMC Syst. Biol., 3, 37.Morin,M. et al. (2016) The post-transcriptional regulatory system csr controlsthe balance of metabolic pools in upper glycolysis of Escherichia coli. Mol.Microbiol., 100, 686–700.Estimation of time-varying growth, uptake and excretion rates from dynamic metabolomics data i309Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018Murphy,T., and Young,J. (2013) ETA: robust software for determination of cell spe-cific rates from extracellular time courses. Biotechnol. Bioeng., 110, 1748–1758.Niklas,J. et al. (2011) Quantitative characterization of metabolism and meta-bolic shifts during growth of the new human cell line AGE1.HN using timeresolved metabolic flux analysis. Bioprocess. Biosyst. Eng., 34, 533–545.Paczia,N. et al. (2012) Extensive exometabolome analysis reveals extended over-flow metabolism in various microorganisms. Microb. Cell Fact., 11, 122.Patti,G. et al. (2012) Metabolomics: the apogee of the omics trilogy. Nat. Rev.Mol. Cell. Biol., 13, 263–269.Pillonetto,G., and Bell,B.M. (2007) Bayes and empirical Bayes semi-blind de-convolution using eigenfunctions of a prior covariance. Automatica, 43,1698–1712.Rasmussen,C.E., and Williams,C.K.I. (2006) Gaussian Processes for MachineLearning. MIT Press, Cambridge.Schelker,M. et al. (2012) Comprehensive estimation of input signals and dy-namics in biochemical reaction networks. Bioinformatics, 28, i529–i534.Stephanopoulos,G. et al. (1998) Metabolic Engineering: Principles andMethodologies. Academic Press, San Diego.Swain,P.S. et al. (2016) Inferring time derivatives including cell growth ratesusing gaussian processes. Nat. Commun., 7, 13766.Taymaz-Nikerel,H. et al. (2016) Comparative fluxome and metabolome ana-lysis for overproduction of succinate in Escherichia coli. Biotechnol.Bioeng., 113, 817–829.van Gulik,W. (2010) Fast sampling for quantitative microbial metabolomics.Curr. Opin. Biotechnol., 21, 27–34.Venkateswarlu,C. (2005) Advances in monitoring and state estimation of bio-reactors. J. Sci. Indus. Res., 63, 491–498.Wahba,G. (1990) Spline Models for Observational Data. SIAM, Philadelphia.Wolfe,A. (2005) The acetate switch. Microbiol. Mol. Biol. Rev., 69, 12–50.Zulkower,V. et al. (2015) Robust reconstruction of gene expression pro-files from reporter gene data using linear inversion. Bioinformatics, 31,i71–i79.i310 E.Cinquemani et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i301/3953965by gueston 07 January 2018
5028881983002	PMID28881983	5028881983	https://watermark.silverchair.com/btx249.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881983.main.pdf	A scalable moment-closure approximation forlarge-scale biochemical reaction networksAtefeh Kazeroonian,1,2,3,* Fabian J. Theis1,2 and Jan Hasenauer1,2,*1Institute of Computational Biology, Helmholtz Zentrum München - German Research Center for EnvironmentalHealth, 85764 Neuherberg, Germany, 2Department of Mathematics, Technische Universit€at München, 85748Garching, Germany and 3Institut für Medizinische Mikrobiologie, Immunologie und Hygiene, Fakult€at für Medizin,Technische Universit€at München, 81675 München, Germany*To whom correspondence should be addressed.AbstractMotivation: Stochastic molecular processes are a leading cause of cell-to-cell variability. Their dy-namics are often described by continuous-time discrete-state Markov chains and simulated usingstochastic simulation algorithms. As these stochastic simulations are computationally demanding,ordinary differential equation models for the dynamics of the statistical moments have been de-veloped. The number of state variables of these approximating models, however, grows at leastquadratically with the number of biochemical species. This limits their application to small- andmedium-sized processes.Results: In this article, we present a scalable moment-closure approximation (sMA) for the simula-tion of statistical moments of large-scale stochastic processes. The sMA exploits the structure ofthe biochemical reaction network to reduce the covariance matrix. We prove that sMA yieldsapproximating models whose number of state variables depends predominantly on local proper-ties, i.e. the average node degree of the reaction network, instead of the overall network size. Theresulting complexity reduction is assessed by studying a range of medium- and large-scale bio-chemical reaction networks. To evaluate the approximation accuracy and the improvement in com-putational efficiency, we study models for JAK2/STAT5 signalling and NFjB signalling. Our methodis applicable to generic biochemical reaction networks and we provide an implementation, includ-ing an SBML interface, which renders the sMA easily accessible.Availability and implementation: The sMA is implemented in the open-source MATLAB toolboxCERENA and is available from https://github.com/CERENADevelopers/CERENA.Contact: jan.hasenauer@helmholtz-muenchen.de or atefeh.kazeroonian@tum.deSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionCellular mechanisms are subject to inherent biological noise thatstems from stochastic events such as bursty gene expression. Due tosuch stochasticity, isogenic cells can behave differently under identi-cal conditions (Elowitz et al., 2002), giving rise to heterogeneouscell populations. Rather than being a nuisance, biological noise hasbeen proven to be crucial in the functioning of biological systemssuch as microbial populations and biological tissue (Raj and vanOudenaarden, 2008), e.g. increasing their robustness. Studying thestochasticity of biological processes, therefore, can shed light ontheir underlying mechanisms and is crucial for a better understand-ing of their behaviour.Many biological processes, e.g. gene expression and signal trans-duction, are modelled as networks of chemical species that undergochemical reactions. The dynamics of chemical reaction networks,i.e. the temporal evolution of the counts of individual species, is usu-ally described by continuous-time discrete-state Markov chains(CTMCs). The statistics of CTMCs are described by the ChemicalMaster Equation (CME). As the simulation of the CME is computa-tionally intractable for most processes due to their high- or eveninfinite-dimensional state space, several methods have been pro-posed to approximate the statistical moments, e.g. moment-closureapproximations (MAs) (Engblom, 2006; Lee et al., 2009) andsystem-size expansions (Grima, 2010; van Kampen, 2007). TheseVC The Author 2017. Published by Oxford University Press. i293This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i293–i300doi: 10.1093/bioinformatics/btx249ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i293/3953964by gueston 07 January 2018methods yield ordinary differential equations (ODEs) that ap-proximate the temporal evolution of the statistical moments. TheseODEs are usually lower-dimensional than the CME, rendering theirnumerical simulation more tractable. However, already for the ana-lysis of the mean and covariance of the stochastic process, the sizeof the state space of the approximating models grows quadraticallywith the number of biochemical species. This limits the application ofthese methods to small- and medium-scale biochemical reaction net-works if the calculation of all statistical moments is required.However interestingly, in a range of applications, including parameterestimation (Fröhlich et al., 2016; Munsky et al., 2009), informationabout a subset of statistical moments can be sufficient.In this study, we introduce a scalable second-order moment-closure approximation (s2MA) which is feasible for large-scale bio-chemical reaction networks. The s2MA is designed for the accuratedescription of selected statistical moments, including means andvariances. We introduce an algorithm that exploits the structure ofthe reaction network to select the subset of moments which are mostrelevant for the reliable approximation of means and variances.Using analytical results for toy networks and published biologicalmodels, we show the superior scaling of s2MA over other methodsfor moment approximation, which renders the s2MA tractable forlarge reaction networks. To assess the accuracy and computationalefficiency of s2MA, we simulated several network motifs and mod-els for JAK2/STAT5 and TNF signalling.2 ApproachWe consider a biochemical reaction network of n species, S1; . . . ; Sn,and nr reactions, R1; . . . ;Rnr . The state of this network is denoted byX ¼ X1;X2; . . . ;Xnð ÞT where Xi is the number of molecules of spe-cies Si. Upon the firing of reaction Rr, the state X undergoes the tran-sition X!ar Xþ mr, in which  r and ar Xð Þ denote the stoichiometryand the propensity of reaction Rr, respectively. Due to the stochasticnature of chemical reactions, the state vector X evolves stochastic-ally over time. The probability distribution of X at time t is denotedby p xjtð Þ over all possible states x.The temporal evolution of the statistical moments of p xjtð Þ canbe approximated using MAs of different orders. The order of anMA is the highest order of the statistical moments which are mod-elled. The second-order MA (2MA) is an ODE with n(n þ 3)/2 statevariables which describes the dynamics of the mean m ¼Pxxp xjtð Þand covariance C ¼Px x mð Þ x mð ÞTp xjtð Þ:@mi@t¼Pr ri ar mð Þ þ12Xk;l@2ar@xk@xl    mCkl !;@Cij@t¼Pr  ri rjar mð Þ þXk@ar@xk    m riCjk þ  rjCik   þ12Xk;l@2ar@xk@xl    m ri rjCkl þ  riCjkl þ  rjCikl    ;(1)where Cikl denotes the third-order moment of Xi, Xk and Xl. Due tothe symmetry Cij ¼ Cji only Cij with i   j is considered. As in (1),the evolution equations for second-order moments usually dependon third-order moments. To close the 2MA equations, moment-closure techniques are applied which approximate the third-ordermoments as functions of first- and second-order moments(Hespanha, 2008). The moment closure introduces an approxima-tion error to the otherwise exact moment equations, as it relies onassumptions about p xjtð Þ (e.g. normality or log-normality; Singhand Hespanha, 2006).The 2MA (1) describes the covariances of all pairs of species andthus possesses O n2   state variables. This quadratic scaling with re-spect to the number of species, n, poses a challenge for the applic-ability of 2MA to large biological networks that may contain severalhundreds up to thousands of species. However, it is usually observedthat in large biochemical networks, many pairwise correlations be-tween species are small. This implies a comparably low covarianceand a small contribution to the right-hand side of (1). Consequently,for an approximation of the dynamics of the biochemical network,it may not be necessary to model all covariances.Studying a series of networks, including the JAK2/STAT5 signallingpathway described by Bachmann et al. (2011), we observed that speciesthat directly influence each other via a reaction have a stronger pairwisecorrelation. For the JAK2/STAT5 signalling pathway, depicted in Figure1A, we found that >50% of the correlation coefficients do not exceedan absolute value of 0.1 (Fig. 1B). Furthermore, the correlation coeffi-cients decrease as the distance between species in the network increases(Fig. 1C). Since in many cases biological networks are sparsely con-nected and distances between species are relatively large (Fig. 1D), a sig-nificant portion of the covariances may be negligible.Motivated by this observation, we develop a scalable s2MA thatmodels a subset of covariances. The s2MA is designed to provide agood approximation for means and variances of species, as thosemoments are essential in a range of applications including parameterestimation (Munsky et al., 2009; Fröhlich et al., 2016). Accordingly,the s2MA captures the subset of covariances that are expected to in-fluence the temporal evolution of the means and variances moststrongly. In the simplest case, we only consider the covariances C that have a direct influence on the means and variances, i.e. thosethat appear in their evolution equations for mi and Cii:• Covariances Cik for which a reaction Rr exists with  ri 6¼ 0 and@ar@xk6¼ 0. This is the case if Sk is a modifier or reactant in a reactionproducing or consuming Si.• Covariances Ckl for which a reaction Rr exists with  ri 6¼ 0 and@2ar@xk@xl6¼ 0. This is the case if both, Sk and Sl, are modifiers orreactants in a reaction producing or consuming Si.The remaining covariances are set to zero. The resulting MA ex-ploits the network structure and is similar to a recently proposedMA for spatially distributed systems exploiting the neighbourhoodstructure (Feng et al., 2016). In the following, we present a mathem-atical formulation of the s2MA as well as extensions to control itssize and approximation accuracy.3 Materials and MethodsTo simulate the statistical moments of the trajectories of large-scalestochastic biochemical reaction networks, we introduce scalablemoment-closure approximations (sMAs). These sMA are based onthe afore-mentioned findings and exploit the structure of the bio-chemical reaction network. In the following, we present the requiredgraph characteristics and the derivation of the s2MA.3.1 Graph representation of biochemical reactionnetworksThe s2MA uses the structure of the reaction network to identify thecovariances that are most relevant to accurately approximate themeans and variances of species. To establish a simple structure-based procedure, we exploit the graph structure of the biochemicalreaction networks. This graph structure is best represented usingthe Systems Biology Graphical Notation (SBGN) process diagrami294 A.Kazeroonian et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i293/3953964by gueston 07 January 2018(Le Novère et al., 2009). In essence, SBGN process diagram is agraph which consists of entity nodes representing biochemical species,process nodes representing biochemical reactions and arcs indicatingthe interactions/dependences. The incoming edges to a process nodeindicate all the reactants, as well as the modifiers, of the correspond-ing reaction, while the outgoing edges from a process node mark theproducts. For instance, reaction R2 in Figure 2 is a bimolecular reac-tion where species S2 and S3 react to form species S4. In reaction R3,species S5 acts as a modifier that activates the conversion of S4 into S6and S7. The graph structure is encoded in the propensities and thestoichiometric coefficients and can be easily visualized for SystemsBiology Markup Language (SBML) models using software toolboxessuch as CellDesigner (Funahashi et al., 2008).We use the graph representation to define a dependency matrixD which summarizes direct dependencies between species in the net-work. Following the arguments in Section 2, we say that a species Sjdirectly depends a species Si, if the evolution equations for the meanor the variance of Sj, i.e., mj and Cjj, depend on moments of Si.Accordingly, it can be shown that:• The products of a reaction depend on the reactants and themodifiers.• The reactants of a reaction depend on the other reactants and themodifier.This yields the dependency matrix D,Dij ¼1 if Si directly influences Sj0 otherwise(Note that D is not necessarily symmetric as the defined dependencyis a directed property. In the model depicted in Figure 2, S4 dependson S2 (D24 ¼ 1) but not vice versa (D42 ¼ 0). The dependency ma-trix D encodes the necessary information for the construction of thes2MA.3.2 The scalable s2MAThe exact evolution equations for means m and covariances C (1)can be written as@mi@t¼ Fm;i m;C;Hð Þ; i 2 f1; . . . ;ng@Cij@t¼ FC;ij m;C;Hð Þ; i; jð Þ 2 Iwith I ¼ f i; jð Þ 2 f1; . . . ;ng2ji   jg:(2)where H denotes all moments with orders greater than two.To avoid redundancies caused by the symmetry of the covari-ances, Cij ¼ Cji, we consider only the subset I of covariances.The higher-order moments H result from reactions with non-linear propensities and their temporal evolution is not todescribed by (2). To obtain a closed formulation, the higher-ordermoments H are approximated by functions of lower-ordermoments, H    H m;Cð Þ, using moment closure techniques.Common techniques include zero-cumulant closure (Matis andKiffe, 1999), low-dispersion closure (Hespanha, 2008), andFig. 1. Correlation coefficients in the simulated JAK2/STAT5 signalling pathway. (A) A partial schematic of the JAK2/STAT5 signalling pathway. (B) Maximumabsolute pairwise correlation coefficients found in the simulation of the JAK2/STAT5 signalling pathway. (C) Maximum absolute pairwise correlation coefficientsas function of the distance between species. (D) Frequency distribution of distance between species pairsFig. 2. Illustration of SBGN process diagram of a simple biochemical reactionnetwork. Biochemical species (boxes), biochemical processes (squares) andinteractions/dependencies (arcs) are visualised. Label Si indicates species SiA scalable moment-closure approximation i295Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i293/3953964by gueston 07 January 2018derivative-matching (Singh and Hespanha, 2007). This yields the2MA,@mi@t¼ Fm;i m;C;  H m;Cð Þ   ¼:  Fm;i m;Cð Þ; i 2 f1; . . . ; ng@Cij@t¼ FC;ij m;C;  H m;Cð Þ   ¼:  FC;ij m;Cð Þ; i; jð Þ 2 I:The solution of the 2MA yields an approximation to the momentsof the state of the biochemical reaction network. The quality of thisapproximation depends on the accuracy of the moment closure(Kazeroonian et al., 2016; Schnoerr et al., 2015).The 2MA possesses n nþ 3ð Þ=2 state variables, thus, it growsquadratically with n. The simplest s2MA, the first-degree s2MA, re-duces the growth rate by considering only the covariances on whichthe temporal evolution of the means and variances depends directly.This reduced set of covariances, Cij with i; jð Þ 2 I 1ð Þ, can be deter-mined using the dependency matrix D,I 1ð Þ ¼ i; jð Þ 2 f1; . . . ;ng2    i   j ^ DþDT   ij 6¼ 0  :The covariances Cij with i; jð Þ 2 InI 1ð Þ are not modelled by the first-degree s2MA but can be approximated using the means, the vari-ances and the reduced set of covariances. In this study, we use thelow-dispersion closure, Cij ¼ 0 for i; jð Þ 2 InI 1ð Þ.The approximation quality of the s2MA can be controlledusing the cut-off degree. The second-degree s2MA describes thecovariances that influence the temporal evolution of the meansand variances either directly or via an intermediate step. Moreprecisely, the second-degree s2MA considers the covariances Cij;i; jð Þ 2 I 1ð Þ and the covariances which appear in their evolutionequations. The set of these covariances, Cij; i; jð Þ 2 I 2ð Þ, is definedby the second power of the dependency matrix D2. More gener-ally, we define the dth-degree s2MA (s2MA-d) which describes thereduced set of covariances Cij with i; jð Þ 2 I dð Þ,I dð Þ ¼ i; jð Þ 2 f1; . . . ;ng2    i   j ^ Dd þ Dd   T  ij6¼ 0  :The degree d   1 denotes the maximal intermediate dependencysteps between species pairs (Si, Sj) for which covariances areincluded in the s2MA. For a given d, we obtain the s2MA-d,@mi@t¼  Fm;i m;Cð Þ; i 2 f1; . . . ; ng@Cij@t¼  FC;ij m;Cð Þ; i; jð Þ 2 I dð ÞCij tð Þ ¼ 0; i; jð Þ 2 InI dð Þ:(3)We focus on the case d ¼ 1, in which merely covariances of interact-ing species are considered. To capture long-range interactions, weconsidered d   2, which can improve the approximation accuracy ofthe s2MA in biological systems with complex or highly non-linearkinetics. The potentially enhanced approximation accuracy comesat the cost of higher computational complexity as the number ofstate variables increases with d. In Section 4, we demonstrate thatone can usually find a satisfactory tradeoff between the compu-tational cost and approximation quality for complex biologicalnetworks.3.3 ImplementationWe implemented methods for the construction and simulationof the s2MA in the ChEmical REaction Network Analyzer(CERENA), an open source MATLAB toolbox (Kazeroonianet al., 2016). The advanced version of CERENA supportsautomatic construction of the 2MA and the s2MA using symboliccalculus and allows for a range of moment closure schemes.The proposed construction algorithm circumvents the formulationof the full 2MA to ensure feasibility for large-scale networks.Biochemical reaction networks can be defined in the SBML or in asimple m-file format. For efficient numerical simulation, C-codesimulation files are compiled using the Advanced MATLABInterface for CVODES and IDAS (Fröhlich et al., 2016).This C-code employs sophisticated numerical methods imple-mented in CVODES (Serban and Hindmarsh, 2005), facilitatingthe study of a wide range of models. In addition, simulationusing MATLAB internal ODE solvers is supported. CERENA isfreely available from GitHub (http://cerenadevelopers.github.io/CERENA/) and its functionality is described in a detaileddocumentation.4 ResultsIn the following, we study the properties of the s2MA and illustrateits importance for the study of large-scale biochemical reaction net-works. For this purpose, we analyse various network motifs as wellas published pathway models for which available methods are com-putationally demanding or even infeasible.4.1 Scaling propertiesThe size of the s2MA for a given network as well as its scaling prop-erties depends on network characteristics. To highlight the scalingproperties, we considered reoccurring network motifs and per-formed a general theoretical assessment. As verification, we in-spected published signalling and metabolic pathways with differentnumbers of biochemical species.4.1.1 Theoretical scaling for network motifs and generic networksTo study the scaling properties of s2MA, we considered three differ-ent network motifs illustrated in Figure 3A–C:• A chain of monomolecular reactions as observed in metabolicprocesses (Krumsiek et al., 2011) and delay representations(Bachmann et al., 2011).• A 2D grid of monomolecular reactions as observed in histonemethylation (Zheng et al., 2012).• A sequence of bimolecular reactions with a hub as observedin polymerisation related processes, e.g. prion aggregation(Rubenstein et al., 2007).For these network motifs, we derived the size of the s2MA-1 and -2(see Table 1). For all three motifs, we found a linear scaling of thesize of the s2MA-1 with respect to the number of species n. Thesame holds for the s2MA-2 of the chain of monomolecular reactionsand the 2D grid of monomolecular reactions. The s2MA-2 of the se-quence of bimolecular reactions with a hub is identical to the 2MAas all species are connected via at most one intermediate species (thehub). Accordingly, the analysis of selected motifs suggests that thes2MA allows for a substantial size reduction in the absence of cen-tral hubs.For generic network structures, the scaling of the s2MA dependson the degree distribution P(d) of nodes in the graph representationof the biochemical reaction network (see Section 3.1). By construc-tion, the number of covariances in the s2MA-1 is the sum of nodedegrees over two,i296 A.Kazeroonian et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i293/3953964by gueston 07 January 2018number of covariances in s2MA   1 ¼ 12Xni¼1 di ¼n d2;in which di denotes the degree of node i and the division bytwo is required as covariances are associated to two nodes.Introducing the average node degree,  d ¼ 1nPni¼1 di, the s2MA-1 de-scribes the temporal evolution of n means, n variances and n d2 covari-ances, and thus possesses n2 4 þ  d   state variables. If we assume thatthere are no long-ranged connections in the network and every nodeis only connected to a subset of neighbouring nodes, then we can as-sume that  d is independent of the size of the network n, and s2MA-1will scale linearly with the number of species.The degree distribution in biological systems have been reportedto follow a power-law (Albert, 2005), P dð Þ / d c, with an exponentof 2 < c < 3. Networks with this property are usually referred toas scale-free networks. The expected value of the average node de-gree in scale-free networks isE  d ¼Xni¼1 di ¼Xn 1d¼1 d  P dð Þ ¼Xn 1d¼1 d1 c:Using the lower bound of c and the upper bound on the partial sumsof the harmonic series, we obtainif c > 2 ) E  d < ln n  1ð Þ þ 1ð Þ:Evaluating this upper bound, we notice that even for networks withup to n ¼ 104 species,  d hardly exceeds 10, making it behave like aconstant compared to n. Accordingly, we conclude that the size ofthe s2MA-1 should scale (only slightly worse than) linearly with thenetwork size.4.1.2 Scaling for published biochemical reaction networksTo corroborate the theoretical predictions derived under the as-sumption of scale-free networks, we studied a collection of 50published biochemical reaction networks. These networks were ex-tracted from the BioModels, NetPath and Reactome database. Theyinclude between 17 and 1277 biochemical species and a range ofrate laws. A comprehensive list of the networks is provided inSupplementary Table S1.We used an extension of the MATLAB toolbox CERENA togenerate the s2MAs for the networks and recorded the sizes(Fig. 4). The analysis verified our prediction of a roughly linear re-lation between the size of the s2MA-1 and the number of species.The s2MA-1, on average, possessed only five times more state vari-ables than the reaction rate equations, ensuring the applicability ofthe s2MA-1 to large-scale networks. For the largest network, asize reduction by a factor of >120 was achieved compared to the2MA.As the consideration of pair-wise correlations between reactionpartners might not be sufficient for a particular application, we alsoassessed the scaling of the s2MA-2 and -3. In agreement with the re-sults for the network motifs, we found that the size of the s2MA ofdegree   2 grew stronger than linear, namely with order 1.25 and1.49. This implies that for realistic pathway structures, also the sizeof the s2MA of degree 2 and 3 grows substantially slower than thesize of the 2MA, facilitating the analysis of stochasticity in large-scale networks.4.2 Approximation accuracyThe improved scalability of the s2MA is achieved by merely model-ling a subset of covariances. In the following section, we will assessthe resulting approximation error and its dependence on the degreeFig. 3. Illustration of considered network motifs. (A) Chain of monomolecularreactions (n ¼ 5). (B) 2D grid of monomolecular reactions (n ¼ 25). (C) Chainof bimolecular reactions with a hub (n ¼ 5)Table 1. Comparison of the sizes of the 2MA and the s2MA for dif-ferent network motifsNetwork motif Number of state variables2MA s2MA-1 s2MA-2Chain of monomolecular reactions nðnþ3Þ2 3n  1 4n  32D grid of monomolecular reactions nðnþ3Þ2 4n ffiffiffinp7n  7ffiffiffinpþ 1Chain of bimolecular reactions nðnþ3Þ2 4n  3nðnþ3Þ2Fig. 4. Scaling of different moment-closure approximations for published net-works. Moment-closure approximations for individual networks (markers)and fitted regression curves (lines) are shownA scalable moment-closure approximation i297Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i293/3953964by gueston 07 January 2018of the s2MA. For this analysis, we consider two network motifs andtwo published signalling pathways.4.2.1 Comparison of approximation methods for network motifsFor an initial assessment of the approximation accuracy, we con-sidered the chain of monomolecular reactions (n ¼ 10) and the se-quence of bimolecular reactions with a hub (n ¼ 20) with massaction kinetics (Fig. 3A and C). The initial conditions and parametervalues are reported in Supplementary Tables S2 and S3. As a meas-ure for the approximation accuracy the relative errors in the meansand variances were used, e.g.100%   jCs2MAii tð Þ   C2MAii tð ÞjmaxtC2MAii tð Þ;in which Cs2MAii tð Þ and C2MAii tð Þ denote the time-dependent varianceof species i calculated by s2MA and 2MA, respectively.The numerical simulation revealed a good agreement of meansand variances of 2MA and s2MA-1 (Fig. 5). Neglecting the covari-ances that are not modelled by the s2MA; however, resulted in arelative error <1% for the means and <20% for the variances.Given a size reductions of 55.4 and 66.5%, the low relative errorsupported the validity of the approach.4.2.2 Comparison of approximation accuracy for s2MA ofdifferent degrees on published biochemical reaction networksTo assess the approximation accuracy of s2MAs of different degreesfor realistic pathway topologies, we considered the published mod-els of JAK2/STAT5 signalling and TNF signalling. These modelswere also considered in the scalability analysis (Section 4.1.2).The model of JAK2/STAT5 signalling describes the activity ofthe transcription factor STAT5 in response to Epo treatment(Bachmann et al., 2011). STAT5 regulates cell proliferation, differ-entiation and inflammation. The considered model accounts for 25biochemical species and includes biochemical reactions with non-mass action kinetics. Its 2MA possesses 350 state variables while thes2MA-1 has less than one-third of the state variables, namely 112.Nonetheless, the simulation revealed a good agreement of 2MA ands2MA-1 for the means and variances (Fig. 6A). The means and vari-ances computed using s2MA-2 and s2MA-3 were essentially indis-tinguishable from those computed using 2MA. For all s2MAs, weobserved a reduction in the computation time comparable to the sizereduction.The model of TNF signalling describes the activation of pro- andantiapoptotic factors, i.e. caspases and NFjB, in response to TNFtreatment (Schliemann et al., 2011). Apoptosis is a form of pro-grammed cell death which is relevant, among others, in immuneFig. 5. Approximation accuracy of the s2MA-1 for network motifs. (A) Thechain of monomolecular reactions with n ¼ 10. (B) The sequence of bimolecu-lar reactions with n ¼ 20. (A, B) Means and variances are depicted along withrelative errors in the variances (2MA versus s2MA-1) for several biochemicalspeciesFig. 6. Approximation accuracy of the s2MA for published pathways. (A) TheJAK2/STAT5 signalling pathway. (B) The TNF signalling network. (A, B)Means and variances computed using the 2MA and the s2MA-1, -2 and -3 aredepicted for several biochemical species. For the s2MA of different degrees,the relative error in the variances with respect to the 2MA is providedi298 A.Kazeroonian et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i293/3953964by gueston 07 January 2018response and cancer. The model comprises 47 biochemical species,yielding a 2MA with 1175 state variables. In contrast, the s2MA-1, -2 and -3 possess only 189, 540 and 664 state variables. The numer-ical simulation of the s2MA-1 was more than 25 times faster thanthe numerical simulation of the 2MA. The disagreement betweens2MA-1 and 2MA, which resulted in a relative error of 100% forsome species (Fig. 6B) indicates that also covariances betweens spe-cies which do not interact directly might be required for an accuratedescription of mean and variances. The comparison of the resultsfor s2MA-1, -2 and -3 confirmed that the approximation error de-creases as more covariances are taken into account. For s2MA-3,the relative error is below 15%.In summary, our analysis of network motifs and published net-works revealed that the s2MA yields substantially smaller ODEmodels than the 2MA, indicating a substantial gain in computa-tional efficiency. Moreover, even for models with many species andnon-mass action kinetics, a good approximation accuracy wasachieved.5 DiscussionStochasticity of biochemical reactions is an inherent property of bio-logical processes. It contributes to the establishment of functionalcell-to-cell variability and robust decision-making (Eldar andElowitz, 2010; Raj and van Oudenaarden, 2008). The analysis ofthe stochastic processes is, however, restricted by the available ana-lytical and numerical methods. In this manuscript, we introduce thescalable second-order moment-closure approximation, the firstmethod to enable the simulation of statistical moments of large-scale stochastic processes. The s2MA exploits the network structureto construct approximate evolution equations for selected processstatistics.To assess and illustrate the properties of s2MA, we studied net-work motifs and a large collection of published networks. This com-prehensive evaluation, which sets this study apart from other studiesof moment-closure approximations (e.g. (Feng et al., 2016; Singhand Hespanha, 2006), verified that in practice the size of the first-degree s2MA (s2MA-1) grows linearly with the network size, a scal-ability that is similar to the reaction rate equations. Accordingly, thes2MA enables the assessment of stochastic dynamics on a new scale.The achieved scalability, however, comes at the cost of an approxi-mation error. The approximation quality can be easily controlledvia the degree of the s2MA.Beyond scalable moment-closure approximations for the calcu-lation of means and variances, structured-based approaches mightbe used for the evaluation of third-order moments and conditionalmoments (Hasenauer et al., 2014). Complementarily, an improve-ment might be achieved by tailored moment-closure schemes whichavoid neglecting a large fraction of covariances. A possible formu-lation, for instance, could be based on partial correlations(Krumsiek et al., 2011) or convergent moments (Zhang et al.,2016). All of these methods would benefit from a priori and a pos-teriori error bounds, which are not yet available for moment-closure approximations, such as the s2MA, but are urgentlyneeded.In summary, we presented a scalable moment-closure approxi-mation for the simulation of stochastic chemical kinetics. Thismethod is beneficial for application problems that require numericalsimulations at low computation cost, e.g. parameter estimation(Fröhlich et al., 2016; Munsky et al., 2009). An implementation ofthe method is provided in the open-source MATLAB toolboxCERENA to facilitate its application and further extensions. Thisimplementation, as well as the concept of structure-based reduction,is applicable to a broad range of problems and will help to improvethe analysis of stochastic chemical kinetics.AcknowledgementsWe acknowledge financial support from the Postdoctoral FellowshipProgram (PFP) of the Helmholtz Zentrum München.Conflict of Interest: none declared.ReferencesAlbert,R. (2005) Scale-free networks in cell biology. J. Cell. Sci., 118,4947–4957.Bachmann,J. et al. (2011) Division of labor by dual feedback regulators con-trols JAK2/STAT5 signaling over broad ligand range. Mol. Syst. Biol., 7,516.Eldar,A. and Elowitz,M.B. (2010) Functional roles for noise in genetic circuits.Nature, 467, 1–7.Elowitz,M.B. et al. (2002) Stochastic gene expression in a single cell. Science,297, 1183–1186.Engblom,S. (2006) Computing the moments of high dimensional solutions ofthe master equation. Appl. Math. Comp., 180, 498–515.Feng,C. et al. (2016) Automatic moment-closure approximation of spatiallydistributed collective adaptive systems. In ACM Transactions on Modelingand Computer Simulation, Vol. 26.Fröhlich,F. et al. (2016) Inference for stochastic chemical kinetics usingmoment equations and system size expansion. PLoS Comput. Biol., 12,e1005030.Funahashi,A. et al. (2008) CellDesigner 3.5: A versatile modeling tool for bio-chemical networks. Proc. IEEE, 96, 1254–1265.Grima,R. (2010) An effective rate equation approach to reaction kinetics insmall volumes: Theory and application to biochemical reactions in nonequi-librium steady-state conditions. J. Chem. Phys., 133, (035101.Hasenauer,J. et al. (2014) Method of conditional moments (MCM) for thechemical master equation. J. Math. Biol., 69, 687–735.Hespanha,J. (2008). Moment closure for biochemical networks. In Proceedingsof the 3rd International Symposium on Communications, Control and SignalProcessing, St Julians, 2008, pp. 142–147.Kazeroonian,A. et al. (2016) CERENA: ChEmical REaction NetworkAnalyzer—a toolbox for the simulation and analysis of stochastic chemicalkinetics. PLoS One, 11, e0146732.Krumsiek,J. et al. (2011) Gaussian graphical modeling reconstructs pathwayreactions from high-throughput metabolomics data. BMC Syst. Biol., 5,Le Novère,N. et al. (2009) The Systems Biology Graphical Notation. Nat.Biotechnol., 27, 735–741.Lee,C.H. et al. (2009) A moment closure method for stochastic reaction net-works. J. Chem. Phys., 130, 134107.Matis,H.J. and Kiffe,T.R. (1999) Effects of immigration on some stochastic lo-gistic models: a cumulant truncation analysis. Theor. Popul. Biol., 56,139–161.Munsky,B. et al. (2009) Listening to the noise: random fluctuations revealgene network parameters. Mol. Syst. Biol., 5, (318.Raj,A. and van Oudenaarden,A. (2008) Nature, nurture, or chance: Stochasticgene expression and its consequences. Cell, 135, 216–226.Rubenstein,R. et al. (2007) Dynamics of the nucleated polymerization modelof prion replication. Biophys. Chem., 125, 360–367.Schliemann,M. et al. (2011) Heterogeneity reduces sensitivity of cell death forTNF-stimuli. BMC Syst. Biol, 5, (204.Schnoerr,D. et al. (2015) Comparison of different moment-closure approxi-mations for stochastic chemical kinetics. J. Chem. Phys., 143, (185101.Serban,R. and Hindmarsh,A.C. (2005) CVODES: An ODE solver with sensi-tivity analysis capabilities. ACM T. Math. Softw., 31, 363–396.Singh,A. and Hespanha,J. (2007) A derivative matching approach to mo-ment closure for the stochastic logistic model. Bull. Math. Biol., 69,1909–1925.A scalable moment-closure approximation i299Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i293/3953964by gueston 07 January 2018Singh,A. and Hespanha,J.P. (2006). Lognormal moment closures for biochem-ical reactions. In Proceedings of the 45th IEEE Conference on Decision andControl (CDC), San Diego, CA, 2006, pp. 2063–2068.van Kampen,N.G. (2007). Stochastic Processes in Physics and Chemistry, 3rdedn. North-Holland, Amsterdam.Zhang,J. et al. (2016) A moment-convergence method for stochastic analysisof biochemical reaction networks. J. Chem. Phys, 144, 194109.Zheng,Y. et al. (2012) Total kinetic analysis reveals how combinatorial methy-lation patterns are established on lysines 27 and 36 of histone H3. Proc.Natl. Acad. Sci. USA, 109, 13549–13554.i300 A.Kazeroonian et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i293/3953964by gueston 07 January 2018
5028881982002	PMID28881982	5028881982	https://watermark.silverchair.com/btx248.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881982.main.pdf	Association testing of bisulfite-sequencingmethylation data via a Laplace approximationOmer Weissbrod,1,2,* Elior Rahmani,3 Regev Schweiger,3Saharon Rosset1 and Eran Halperin4,5,*1Statistics Department, Tel Aviv University, Tel Aviv 6997801, Israel, 2Computer Science Department, Technion -Israel Institute of Technology, Haifa 3200003, Israel, 3Blavatnik School of Computer Science, Tel Aviv University,Tel Aviv 6997801, Israel, 4Computer Science Department, University of California Los Angeles, Los Angeles, CA90095, USA and 5Department of Anesthesiology and Perioperative Medicine, University of California Los Angeles,Los Angeles, CA 90095, USA*To whom correspondence should be addressed.AbstractMotivation: Epigenome-wide association studies can provide novel insights into the regulation ofgenes involved in traits and diseases. The rapid emergence of bisulfite-sequencing technologiesenables performing such genome-wide studies at the resolution of single nucleotides. However,analysis of data produced by bisulfite-sequencing poses statistical challenges owing to low and un-even sequencing depth, as well as the presence of confounding factors. The recently introducedMixed model Association for Count data via data AUgmentation (MACAU) can address these chal-lenges via a generalized linear mixed model when confounding can be encoded via a singlevariance component. However, MACAU cannot be used in the presence of multiple variance com-ponents. Additionally, MACAU uses a computationally expensive Markov Chain Monte Carlo(MCMC) procedure, which cannot directly approximate the model likelihood.Results: We present a new method, Mixed model Association via a Laplace ApproXimation(MALAX), that is more computationally efficient than MACAU and allows to model multiple vari-ance components. MALAX uses a Laplace approximation rather than MCMC based approxima-tions, which enables to directly approximate the model likelihood. Through an extensive analysisof simulated and real data, we demonstrate that MALAX successfully addresses statistical chal-lenges introduced by bisulfite-sequencing while controlling for complex sources of confounding,and can be over 50% faster than the state of the art.Availability and Implementation: The full source code of MALAX is available at https://github.com/omerwe/MALAX.Contact: omerw@cs.technion.ac.il or ehalperin@cs.ucla.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionIn recent years, epigenetic variation has proven to be an importantfactor in many traits and diseases (Bird, 2007). One of the most prom-inent sources of epigenetic variation is differential DNA methylation(Jones, 2012). Currently, the predominant technology for measuringmethylation levels is based on methylation arrays, which can probe aspecific list of sites. However, the recently emerging bisulfite-sequenc-ing technology enables measuring methylation levels across the entiregenome (Cokus et al., 2008). In spite of the clear advantages, testingfor associations between methylation patterns and phenotypes viabisulfite-sequencing data is hindered by several challenges.The main challenge in the analysis of bisulfite-sequencing data istheir typically low and uneven sequencing depth. Specifically, theproportion of the number of methylated reads to the total numberof reads is an unreliable measure of the true methylation level, whenthe total number of reads is small. Consequently, naive applicationof regression models for such data leads to loss of power and tospurious results (Sun et al., 2014). To circumvent this difficulty, sev-eral methods proposed treating the phenotype as an explanatoryvariable and the methylation sites as beta-binomial (BB) distributedresponse variables (Dolzhenko and Smith, 2014; Feng et al., 2014;Sun et al., 2014). The use of a BB distribution accounts for bothVC The Author 2017. Published by Oxford University Press. i325This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i325–i332doi: 10.1093/bioinformatics/btx248ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i325/3953963by gueston 07 January 2018the binomial nature of the methylation levels and for their ten-dency to be highly overdispersed relative to a standard binomialdistribution.Although the aforementioned methods successfully account forindependent overdispersion, methylation counts of different individ-uals are often correlated owing to confounding factors such as popu-lation structure, cryptic relatedness or batch effects in the data (Leaet al., 2015). Such confounding factors must be accounted for toprevent spurious findings. Linear mixed models (LMMs) are oftenused to control for confounding in genetic studies (Yang et al.,2014), but similarly to other regression models, LMMs suffer fromloss of power and spurious results in the presence of bisulfite-sequencing data (Lea et al., 2015). To address this challenge, Mixedmodel Association for Count data via data AUgmentation(MACAU) (Lea et al., 2015) combines the idea of treating methyla-tion sites as response variables, with a generalized linear mixedmodel (GLMM) that can control for confounding.A severe limitation of GLMMs is that numerical likelihoodevaluation is computationally infeasible in typical settings. To over-come this limitation, MACAU assigns a prior distribution to themodel parameters and then estimates their posterior distribution viaMCMC. The posterior mean and variance estimates of the effect ofthe phenotype on a certain site are then used to test for association,by treating them as maximum likelihood estimates (MLEs) andusing them in a Wald test context.Although MACAU addresses the computational infeasibilitylimitation, the proposed MCMC approach suffers from two caveats.First and foremost, MACAU cannot be used in the presence of mul-tiple variance components, because it uses an approximation thatspecifically exploits the structure of its probabilistic model whenthere is only a single variance component (in addition to the vari-ance component associated with the identity matrix). In recent yearsit has been shown that it is often beneficial to use multiple variancecomponents to control for multiple sources of confounding. For ex-ample, Widmer et al. (2014) suggested using two variance compo-nents to improve the model fit; Chen et al. (2016) used threevariance components to control for genetic relatedness as well ashousehold and block group membership; Cohen et al. (2016) usedtwo variance components to control for both genetic relatedness andfor experimental variability; Powell et al. (2013) used two variancecomponents corresponding to additive and dominance effects. In thecontext of Epigenome-wide association studies (EWASs), it can bebeneficial to control for genetic similarity as well as methylationsimilarity, which can for example capture confounding due to celltype composition (Zou et al., 2014).A second caveat of the proposed MCMC approach is the need tocarry out convergence diagnostics and fine-tune many parameters,which hinders the use of MCMC methods in practice.Here we present Mixed model Association via a LaplaceApproXimation (MALAX), which directly approximates the likeli-hood of the GLMM used by MACAU via a Laplace approximation(Rasmussen and Williams, 2006). Briefly, MALAX approximatesthe conditional distribution of the logit of the methylation levelsgiven the data as a multivariate normal distribution, by using a se-cond order Taylor expansion. This approximation enables a fastanalytical approximation of the MLEs of all parameters, allowingMALAX to successfully address the caveats above: MALAX can beused with multiple variance components, it does not assume the ex-istence of a prior distribution of the model parameters, and it doesnot require parameter fine-tuning. Additionally, MALAX can beover 50% faster than the state of the art.In order to evaluate MALAX we carry out extensive simulationsof studies with diverse sources of confounding. We additionallydemonstrate the advantages of MALAX in an analysis of 50 ba-boons with multiple variance components, which was not possibleusing previous methods. Our simulations and real data analysis indi-cate that MALAX has high power to discover phenotype-epigeneticassociations, while controlling for diverse sources of confounding.2 Materials and methods2.1 Methods overviewWe begin by providing an overview of association testing in thepresence of bisulfite-sequencing data. Consider a dataset of n indi-viduals with measured phenotypes, covariates, read counts andmethylated read counts. We are interested in testing the null hypoth-esis that the proportion of methylated reads at site j is independentof the phenotype.A naive approach is to treat the observed proportion of methy-lated reads at site j as an additional covariate and test for associ-ation between this covariate and the phenotype via a regressionmodel. Specifically, assuming a quantitative phenotype and denotingx ¼ ½x1; . . . ;xn T as a vector of observed phenotypes, yj ¼ ½yj1; . . . ; yjn Tand r j ¼ ½rj1; . . . ; rjn T as vectors with the number of methylatedreads and the total number of reads for site j, respectively, andW ¼ ½w1; . . . ;wn T as a matrix of covariates including an intercept,(where each element wi is a vector of c covariates for individual i), anaive regression model is defined as follows:x ¼ aþ yj=r j   bþ  : (1)Here,     N 0; r2e I   is a vector of independent normally distributedresiduals with variance r2e , yj=r j   ¼ ½yj1=rj1; . . . ; yjn=rjn T is a vectorof methylation levels, a ¼ ½a1; . . . ; ac T is a vector of fixed effects,and b is the fixed effect of the methylation level. Association testingamounts to testing the null hypothesis H0 : b ¼ 0. The above modelcan be extended to account for complex sources of confounding byusing a LMM, which consists of changing the distribution of   to    Nð0;PVv¼1 r2vKv þ r2e IÞ, where Kv is an n   n matrix describingthe effect of the vth source of confounding, and r2v is the vth vari-ance component.Unfortunately, the observed proportions encoded in the vectoryj=r j are unreliable estimators of the true proportions when r j con-tains small numbers, which can in turn lead to loss of power and tospurious results (Lea et al., 2015). A common solution is to treat thephenotype as a covariate and the observed number of reads as a re-sponse variable, which enables to explicitly model the binomial na-ture of the methylation levels. We now present the statistical modelof MALAX and MACAU, which adopts this approach in the frame-work of a GLMM.2.2 Statistical modelMALAX models the distribution of yji conditional on all the othervariables via a binomial GLMM as follows:yji j rji; xi;wi   Bin rji;pji   pji ¼ 1 þ exp  lið Þð Þ 1l   N Waþ xb;PVv¼1 r2vKv þ r2e I   :(2)Here, pji is the methylation probability of a probe coming from site jin individual i, and l ¼ ½l1; . . . ; ln T is the vector of the logits of thesei326 O.Weissbrod et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i325/3953963by gueston 07 January 2018probabilities. The variance component r2e accounts for independentover-dispersion, which describes the observation that the variance ofmethylation levels across different individuals is often much largerthan expected under a binomial model.A natural interpretation of this model is that every individual i isassociated with a latent random variable li which is affected by hercovariates and genetic variants, such that larger values of li lead tohigher methylation levels. In this respect, the model is similar to thewell-known liability threshold model often employed in case-controlstudies (Weissbrod et al., 2015), wherein every individual is associ-ated with a latent liability value. If Kv is a matrix of inner-productsof normalized genetic vectors, it encodes the assumption that geneticvariants exert a linear effect on li, similarly to the assumption oftenemployed in standard LMMs.Association testing consists of testing the null hypothesisH0 : b ¼ 0, and can be carried out via a Wald test, which requirescomputing the MLE of the model parameters. The log likelihoodfunction is given by:‘ a; b; r21; . . . ; r2V ; r2e   ¼ logðP lð ÞYni¼1 P yji j li; rji   dl; (3)where we omitted conditioning l on the observed variables and onthe model parameters for brevity. Equation (3) demonstrates thatlikelihood evaluation requires numerically evaluating an n-dimen-sional integral, which scales exponentially with n when using stateof the art algorithms, such as adaptive Gauss-Kronrod quadrature(Kahaner et al., 1989). To circumvent this difficulty, MACAUadopts a Bayesian framework by first assigning a prior distributionto the fixed effects and variance components, and then samplingparameter values from their posterior distribution. In contrast,MALAX does not assume that the parameters have a prior distribu-tion, and instead directly approximates the likelihood via a Laplaceapproximation.2.3 Laplace approximationThe underlying idea behind MALAX is that the conditional densityP l jx;W ; yj; r j   can be approximated to follow a multivariateGaussian via a second order Taylor expansion. Under this approxi-mation, the log likelihood can be approximated as follows:‘ a; b; r21; . . . ; r2V ; r2e     12bl  m   Tr logP yjjl   jl¼bl    12log jGAj þ logP yjjbl   :(4)Here, bl ¼ argmaxlP l j x;W ; yj; r j   ; G ¼PVv¼1 r2vKv þ r2e I is theoverall covariance matrix, m ¼ a is the mean of l, r logP yjjl   jl¼bl isthe gradient of logP yjjl   with respect to l, evaluated at l ¼ bl , and A¢ rr log P l jx;W ; yj; r j   jl¼bl is the Hessian of the negative loga-rithm of the conditional density of l with respect to l, evaluated atl ¼ bl .A brief sketch of the derivation of Equation (4) is now provided,with a longer description available in (Rasmussen and Williams,2006). First, we apply a second order Taylor expansion to the loga-rithm of P l; yj jx;W ; r j   aroundbl as follows:logP l; yj j x;W ; r j     logP l ¼ bl ; yj j x;W ; r j     12l  bl   TA l  bl   :(5)Using this approximation, the log likelihood can be approximatedanalytically as follows:‘ a; b; r21; . . . ; r2V ; r2e   ¼ logP yjjx;W ; r j     log ½P l ¼ bl ; yjjx;W ; r j    Ðexp  12l  bl   TA l  bl     dl ¼ log ½P l ¼ bl ; yjjx;W ; r j   2pð Þn=2jAj 12 ¼  12bl  m   TG 1 bl  m     12log jGAj þ logP yjjbl   ¼  12bl  m   Tr logP yjjl   jl¼bl  12log jGAj þ logP yjjbl   :(6)The second equality can be verified by noting that the integral is equalto the reciprocal of the normalizing constant of a multivariate normaldistribution with mean vector bl and covariance matrix A 1. The thirdequality is derived by using the definition of P l ¼ bl ; yjjx;W ; r j   andsome algebra. The fourth equality uses the following equation:r logP l; yjjx;W ; r j   ¼ r logP yjjl    G 1 l  mð Þ; (7)and the fact that the above equation is equal to 0 by definition whenl ¼ bl .The computation of bl can be performed via Newton–Raphson it-erations, which require inverting the Hessian of logP l; yjjx;W ; r j   with respect to l. Additionally, the likelihood approximation re-quires evaluating the determinant of the matrix AG. Both operationsscale cubically with n under standard implementations. Hence, thecomputational complexity of the algorithm is O n3   . The number ofcubic operations required for each methylation site depends on thenumber of iterations required for convergence of the Newton-Raphson algorithm and of the optimization scheme, and was typic-ally around 50 in our implementation.2.4 Testing for associationMALAX tests for association between a phenotype vector x and avector of methylation counts yj by attempting to reject the null hy-pothesis H0 : b ¼ 0 via a Wald test. The test statistic is given byT ¼ b̂2=var b̂   , where b̂ is the MLE of b, and asymptotically fol-lows a v2 distribution with one degree of freedom under the null hy-pothesis. MALAX estimates var b̂   via the diagonal entrycorresponding to b in the inverse of the Hessian of the log likeli-hood. MALAX approximates the Hessian via finite differences.2.5 Gradient computationThe gradient of the approximate log likelihood above is requiredboth for approximating the Hessian and for the maximum likeli-hood estimation procedure. A subtle point that requires consider-ation is that while the quantities G and m depend explicitly on themodel parameters, the quantities bl and A also implicitly depend onthese parameters. We therefore divide the partial derivative accord-ing to each parameter h (which can represent variance componentsor fixed effects) into its explicit and implicit components, by usingthe chain rule as follows:@ logP yjjx;W ; r j   @h¼ @L1@hexplicitþXni¼1@ logP yjjx;W ; r j   @ l̂ i@ l̂ i@h;(8)EWAS via a Laplace approximation i327Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i325/3953963by gueston 07 January 2018where L1 ¼   12 bl  m   TG 1 bl  m   . A full derivation is providedin the Supplementary Material. The gradient computation scales cu-bically with the sample size under standard implementations, simi-larly to the likelihood approximation.2.6 Optimization and implementation detailsThe model parameters were optimized via the L-BFGS-B algorithm(Byrd et al., 1995), using the implementation provided in the SciPypackage (Jones et al., 2001). To begin the optimization procedurewith reasonable initial values, the initial values of the fixed effectswere computed via a BB model. The initial value of each variancecomponent was 0.5. The code was compiled using Cython (Behnelet al., 2011) for efficient computations. When testing each site, indi-viduals with zero reads at this site were excluded from the analysis.3 ResultsWe evaluated the performance of MALAX on synthetic and real data.In all experiments, we compared the following methods: (i) MALAX-2, which uses two variance components, corresponding to both gen-etic kinship and to similarity estimated from methylation data;(ii) MALAX-1g, which uses a single variance component of genetickinship; (iii) MALAX-1m, which uses a single variance component ofmethylation similarity; (iv) BB model can only control for independ-ent over-dispersion; and (v) MACAU (using default settings), whichuses a single variance component of genetic kinship. Specifically,MACAU used 1000 MCMC iterations and 100 burn-in iterations.We verified that increasing these numbers increased the run-time andhad a negligible effect on the results (results not shown).In the synthetic experiments, the genetic kinship matrix wascomputed via normalized single nucleotide polymorphisms (SNPs),and the methylation similarity was based on cell-type composition,as described in the next section. In the real data analysis, the genetickinship matrix was computed as described in Lea et al. (2015), andthe methylation similarity matrix was constructed as described inSection 3.3.1.3.1 Data simulationWe simulated synthetic data with two sources of confounding: gen-etic confounding and confounding due to cell-type composition,which was shown to be a major source of confounding in methyla-tion studies (Jaffe and Irizarry, 2014). Specifically, we simulated in-dividuals with cell-type composition and SNPs. Every individualwas sampled from a mixture of four populations, and the SNP distri-bution of every individual reflected the SNP distributions in the cor-responding populations. The phenotype of every individual wasaffected by the SNPs and by two normally distributed covariates.Finally, the methylation levels were affected by the SNPs, the covari-ates, and possibly also by the cell-type composition and the pheno-type (Supplementary Material).The populations were generated via the Balding Nichols model,which assumes that several populations diverged from a single an-cestral population (Balding and Nichols, 1995). Under this model,the distance between populations can be quantified via the FST meas-ure, where FST ¼ 0.01 corresponds to the typical difference betweenhuman individuals across remote regions in Europe.Unless stated otherwise, in all experiments we simulated 200 in-dividuals with 2 covariates, 60 000 SNPs, a normally distributedphenotype affected by 500 of the SNPs, and 10 000 methylationsites, such that either 0, 25 or 50% of the methylation sites were dif-ferentially methylated (DM) according to five different cell-types,and 500 out of the 10 000 sites were associated with the phenotype.The allele frequencies of each population were generated using FST¼ 0.01. Ten datasets were generated for each unique combination ofevaluated settings. A full description of the simulations procedure isprovided in the Supplementary Material.3.2 Synthetic data experimentsWe performed several experiments to evaluate the performance ofthe evaluated methods. In all experiments, the genetic kinship ma-trix was given by XXT=m, where X is a matrix of normalized SNPsand m is the number of SNPs. The methylation similarity matrixwas similarly given by ZZT=p, where Z is a matrix of normalizedcell-type compositions (which can be estimated using e.g. methyl-omes from purified cell types), and p is the number of cell-types.Our first experiment examined the benefits of using two variancecomponents. To this end, we generated datasets with various de-grees of cell-type composition effects by varying the number of sitesthat are DM across different cell-types. We first measured the ro-bustness of the methods to false positive detections. The false posi-tive rate of MALAX-2 was comparable or superior to that of theother methods under all settings, as evidenced by both QQ plots andby the genomic control inflation factor (Devlin and Roeder, 1999)(Fig. 1). We also evaluated a version of MACAU which used only amatrix of methylation similarities (similarly to MALAX-1m). As ex-pected, this version of MACAU performed very similarly toMALAX-1m (results not shown).Next, we measured the ability to identify truly associated sitesvia detection power (defined as the proportion of top ranked sitesthat are directly associated with the phenotype) of the top 500 sites(corresponding to the number of truly associated sites), as this meas-ure allows a fair comparison between methods with different falsepositives rates. As expected, MALAX-1g and MACAU outper-formed the other methods in the absence of DM sites, but the advan-tage of MALAX-2 and MALAX-1m increased with the proportionof DM sites (Fig. 2). MALAX-2 clearly outperformed the othermethods at distinguishing associated from non-associated sites when50% of the sites were DM. Exact power results are presented inSupplementary Figure S1, which shows very similar results. We fur-ther verified that the results remained similar when modifying someof the simulation parameters such as the number of covariates, theproportions of DM sites and the numbers of populations (results notshown). Additionally, we evaluated the performance of two add-itional recently proposed methods for GLMM approximations(Jiang et al., 2016; Chen et al., 2016), and found that MALAX out-performs both methods (Supplementary Material).To explore the computational efficiency of the methods, wemeasured their run time under varying sample sizes, using a singlecore of a Linux workstation with a 2GHz Xeon CPU. BB was thefastest method, owing to its relatively simple model, while MACAUwas substantially slower than the MALAX methods (Fig. 3).Interestingly, the three MALAX settings had very similar run times,indicating that using additional variance components incurs a negli-gible computational price. We note that MACAU was invoked withthe default parameter of 1000 MCMC iterations, which could makethe advantage of MALAX even greater in practice.Finally, we explored the differences between MALAX-1g andMACAU, which both use a single variance component of genetickinship. The P-values computed by the two methods were highlycorrelated (Fig. 4), indicating that MALAX-1g can be routinely usedat a substantially reduced computational cost compared with thestate of the art.i328 O.Weissbrod et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i325/3953963by gueston 07 January 20183.3 Real data analysisTo demonstrate the differences between the evaluated methods inthe analysis of real data, we investigated a dataset of 50 baboonswith measured relatedness values and methylation levels at 438 311sites, which were tested for association with age. This dataset waspreviously described in Lea et al. (2015), where it was analyzed byMACAU. Here, we reanalyzed this dataset using MALAX withmultiple variance components, which can potentially control forFig. 1. QQ plots of the evaluated methods, computed using only sites not directly associated with the phenotype, under simulated datasets with population struc-ture and with various proportions of DM sites. Each figure aggregates the results of 10 simulated datasets. The 95% CI of the expected null distribution is shadedin gray. The mean and SD of the genomic control inflation factor of each method is shown next to its name. all methods suffer from some degree of inflation inthe presence of severe confounding, but MALAX-2 always controls for type I error as well as or better than the alternative methods. The three methods that donot control for confounding due to methylation similarity become increasingly less calibrated as the percentage of DM sites increasesFig. 2. The detection power of the evaluated methods under simulated datasets with various proportions of DM sites. All results are averaged over 10 simulateddatasets. The three methods that control only for genetic confounding are more powerful than the other ones in the absence of DM sites, but MALAX-2 andMALAX-1m become increasingly more powerful as the percentage of DM sites increasesFig. 3. Box plots describing the running times of the evaluated methods in the presence of simulated datasets with varying proportions of DM sites and samplesizes (n), and with 10 000 sites. The flat boxes at the bottom represent the BB methodEWAS via a Laplace approximation i329Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i325/3953963by gueston 07 January 2018additional sources of confounding, such as cell-type composition(Jaffe and Irizarry, 2014).MALAX-1g used a single variance component associated withgenetic kinship, MALAX-1m used a single variance componentassociated with mehylation similarity, and MALAX-2 used twovariance components associated with the two matrices. The genetickinship matrix was computed via microsatellite data, as describedin Lea et al. (2015). The methylation similarity matrix was com-puted as described in Section 3.3.1, with a selected value of b ¼ 80(indicating that 80 sites were used for estimating methylationsimilarity). The covariates included sex, sample age and efficiencyof the bisulfite conversion rate estimated from the lambda phagespike-in.3.3.1 Construction of methylation-based similarity matrixTo control for sources of confounding that can be captured in thereal methylation data, we applied methodology similar to the onepreviously proposed in (Zou et al., 2014). Namely, we firstcomputed a P-value for every site via a BB model, and then con-structed a similarity matrix using the b sites most associated withthe phenotype.One difficulty in applying this approach in the presence ofbisulfite-sequencing data is that construction of the matrix is notstraightforward, as some sites have many more reads (and thusprovide a greater degree of confidence in their methylation levels)than others. Therefore, naively using the proportion of the numberof methylated to observed reads will lead to highly inaccurateestimates.To address this challenge, we first estimated the (logit of the)methylation level of every individual i in every site k, l̂ik, using theLaplace approximation of MALAX. Afterwards, we defined themethylation similarity between individuals i,j asPk2Tb l̂ikl̂jk=b, whereTb is the set of the b sites with the smallest P-values according to aBB model.The parameter b was determined by maximizing the out of sam-ple log likelihood of the phenotype on held-out data when using thesimilarity matrix described earlier, using a 10-fold cross-validationwith a standard LMM, as in (Zou et al., 2014). Formally, the pheno-type x was modeled as x   Nða; r2lbPk2TbblkblTk þ r2e IÞ; where blk is avector of estimated methylation levels in logit space.The individuals were divided into 10 random equally sized folds.For each evaluated value of b, the parameters a; r2l and r2e were esti-mated via a standard LMM maximum likelihood procedure for eachcombination of 9/10 of the folds, and the likelihood of the left-outfold conditional on the others was computed using the estimatedmodel, under varying values of b. The selected value of b was thevalue that maximized the average out of sample likelihood acrossthe 10 folds. The list of estimated values of b was [1,2,3. . .50,60,70,80. . .250, 275,300,325. . .,500, 600,700. . .,1000].3.3.2 Real data resultsWe first verified that all methods perform comparably with regardsto type 1 error control, indicating a limited degree of genetic con-founding in this data (Fig. 5). Interestingly, MALAX-1g often esti-mated slightly lower P-values than MACAU, despite the closesimilarity of the models used by the two methods. Nevertheless, theP-values computed by MALAX-1g and by MACAU were highly cor-related (Pearson correlation ¼ 0.953, Spearman correlation ¼0.924), leading to very similar rankings of the sites according to thetwo methods.Next, we examined the effect of adding a second variance com-ponent, by comparing the results of MALAX-2 and MACAU. Thecorrelation between the P-values computed by MALAX-2 and byMACAU (Pearson correlation ¼ 0.889, Spearman correlation ¼Fig. 4. The correlation between the P-values computed by MALAX-1g and by MACAU across simulated datasets. The sites are sorted according to the P-valuescomputed by MALAX-1g. The shown values q are the Pearson correlation between the P-values (in log scale)Fig. 5. A QQ plot of the P-values obtained by the evaluated methods in theanalysis of the baboons datai330 O.Weissbrod et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i325/3953963by gueston 07 January 20180.835) was substantially lower than between MALAX-1g andMACAU, as expected based on the difference between their underly-ing models. Additionally, there were several substantial differencesbetween the top ranking P-values computed by MALAX-2 and byMACAU (Fig. 6, Table 1). Namely, 2 of the 10 sites ranked highestby MACAU were ranked substantially lower by MALAX-2, pos-sibly indicating spurious results. Notably, the site ranked highestby MACAU (chr17, locus 8 779 721) obtained a non-significantP-value of 0.15 by MALAX-2. In contrast, the 10 sites ranked high-est by MALAX-2 all received P-values < 8   10 5 by MACAU, andwere among its 700 top ranking sites. Although these results are sug-gestive that inclusion of a second variance component can be benefi-cial, additional in-depth analysis is required to verify the resultsobtained by the two methods.We also evaluated the performance of two recently proposedmethods for GLMM approximations (Chen et al., 2016; Jiang et al.,2016), and found that their reported top sites were substantially dif-ferent from each other and from the top sites reported by MALAXand MACAU, which likely indicates low power on this dataset (re-sults not shown).Finally, we evaluated the run time of the evaluated methods. TheBB model completed the analysis in 4 hours, MALAX-2, MALAX-1g and MALAX-1m completed the analysis in 8.2, 7.45 and 7.77 h,respectively, and MACAU with default parameters competed theanalysis in 23.6 h. These results demonstrate that MALAX can re-duce the running time required to perform an epigenome-wide ana-lysis by over 50%.4 DiscussionWe presented MALAX, a novel method for association testing forcount data, which is especially useful for analysis of data generatedby bisulfite-sequencing. MALAX adopts the probabilistic model ini-tially proposed by MACAU, but unlike MACAU, it can be used inthe presence of multiple variance components representing diversesources of confounding. Additionally, MALAX directly approxi-mates the likelihood function via a Laplace approximation, and isthus both conceptually simpler and computationally faster thanMACAU, which adopts a Bayesian framework and requires expen-sive MCMC-based analysis.Long read-sequencing technologies, such as the Pacific Biosciencesand the Oxford Nanopore platforms (Goodwin et al., 2016), mayeventually replace bisulfite sequencing for EWAS purposes. However,analysis of methylation data via MALAX will remain useful as longas these technologies cannot probe methylation at a very high cover-age at reasonable costs. We further point out that MALAX is a gen-eral technique for GLMM approximation. Consequently, MALAXcan be readily adapted to other settings with non-normally distributedresponses, such as analysis of gene expression data obtained via RNAsequencing (Sun et al., 2016).In recent years, several methods for GLMM approximationshave been proposed in statistical genetics and other domains:GMMAT (Chen et al., 2016) and CARAT (Jiang et al., 2016) aresomewhat similar to MALAX but use simpler approximations,which lead to lower statistical power (Supplementary Material).Variational approximation and Expectation Propagation are twomethods which approximate the likelihood of GLMMs in a differentmanner than MALAX (Nickisch and Rasmussen, 2008), but aremuch slower and do not provide an advantage over MALAX inpractice (results not shown). Finally, INLA (Rue et al., 2009),adopts a Bayesian framework similarly to MACAU, but approxi-mates the likelihood analytically similarly to MALAX. Our resultsindicate that such a Bayesian framework is not required for associ-ation testing, but we note that INLA might be useful should oneFig. 6. A Manhattan plot of the P-values obtained by the evaluated methodsin the analysis of the baboons data. The axis labels for several chromosomesare omitted to improve clarityTable 1. The top 10 sites found by MALAX-2 and MACAU in the analysis of the baboons dataMALAX-2 MACAUrank chr pos P-value alt. rank alt. P-value rank chr pos P-value alt. rank alt. P-value1 10 19 531 653 5.10   10–12 3 1.09   10–10 1 17 8 779 721 9.57   10–12 108 947 1.53E-012 10 76 782 787 8.61   10–11 4 1.30   10–10 2 13 127 550 470 2.23   10–11 5 1.92   10–103 4 43 268 737 1.37   10–10 5 2.92   10–10 3 10 19 531 653 1.09   10–10 1 5.10   10–124 15 4 962 834 1.81   10–10 11 2.18   10–8 4 10 76 782 787 1.30   10–10 2 8.61   10–115 13 127 550 470 1.92   10–10 2 2.23   10–11 5 4 43 268 737 2.92   10–10 3 1.37   10–106 10 13 480 448 3.75   10–10 42 2.93   10–7 6 20 67 348 840 3.57   10–10 14 1.06   10–87 13 118 109 650 6.02   10–10 47 3.62   10–7 7 7 19 570 278 1.27   10–9 11 2.98   10–98 3 33 299 965 1.47   10–9 115 2.59   10–6 8 20 67 035 039 4.88   10–9 50 3.13   10–79 16 48 764 429 2.18   10–9 10 1.63   10–8 9 5 100 757 817 8.39   10–9 8043 2.75   10–310 10 31 702 787 2.27   10–9 696 7.58   10–5 10 16 48 764 429 1.63   10–8 9 2.18   10–9For each top site, we report its chromosome (chr), position (pos), rank and P-value under the main method and its rank and P-value under the alternativemethod.EWAS via a Laplace approximation i331Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i325/3953963by gueston 07 January 2018wish to adopt a Bayesian framework without resorting to expensiveMCMC computations.Although MALAX can be over 50% faster than MACAU, bothmethods have the same computational bottleneck of having to invertor compute determinants of matrices of size n   n, whose computa-tion scales cubically with the sample size under standard implemen-tations. Therefore, both methods cannot currently be used withsamples of thousands of individuals. In the future, we intend to in-vestigate the possibility to accelerate MALAX via low rank matrixapproximations, using techniques such as the Nyström method(Drineas and Mahoney, 2005), incomplete Cholesky factorization(Fine and Scheinberg, 2001) or Bregman matrix divergence kernellearning (Kulis et al., 2009).In this work, we evaluated several implementations of MALAX,which are differentiated by using different combinations of variancecomponents. In practice, the use of MALAX requires domain know-ledge in order to select the set of variance components that can con-trol for all the sources of confounding in a given dataset, similarly tothe use of LMMs in practice.Finally, the experiments in this paper used the technique of (Zouet al., 2014) to construct a matrix of methylation similarities, whichcan potentially capture sources of confounding such as cell-typecomposition. Recently, another technique has been demonstrated toimprove control for cell-type composition by first selecting a subsetof the methylation sites, and then incorporating the top principalcomponents of the selected sites as covariates (Rahmani et al.,2016). However, both methods suffer from the caveat that there iscurrently no analytical proof or empirical evidence that they aresuitable for the analysis of bisulfite-sequencing data. Specifically, amajor difficulty is that computation of methylation similarity matri-ces and of their principal components is unreliable in the presence ofa small number of reads. Adapting one of the above techniques forbisulfite-sequencing data therefore remains a future endeavor.FundingThis research was partially supported by the Edmond J. Safra Center forBioinformatics at Tel Aviv University. E.H., and R.S. and E.R. were sup-ported in part by the Israel Science Foundation [Grant 1425/13], E.H. andR.S. by the US Israel Binational Science Foundation grant 2012304, E.R. byLen Blavatnik and the Blavatnik Research Foundation, R.S. by the ColtonFamily Foundation, and S.R. by the Israeli Science Foundation [grants 1487/12 and 1804/16].Conflict of Interest: none declared.ReferencesBalding,D.J. and Nichols,R.A. (1995) A method for quantifying differenti-ation between populations at multi-allelic loci and its implications for inves-tigating identity and paternity. Genetica, 96, 3–12.Behnel,S. et al. (2011) Cython: the best of both worlds. Comput. Sci. Eng., 13,31–39.Bird,A. (2007) Perceptions of epigenetics. Nature, 447, 396–398.Byrd,R.H. et al. (1995) A limited memory algorithm for bound constrainedoptimization. SIAM J. Sci. Comput, 16, 1190–1208.Chen,H. et al. (2016) Control for population structure and relatedness for bin-ary traits in genetic association studies via logistic mixed models. Am. J.Hum. Genet., 98, 653–666.Cohen,K.A. et al. (2016) Paradoxical hypersusceptibility of drug-resistant m.tuberculosis to b-lactam antibiotics. EBioMedicine, 9, 170–179.Cokus,S.J. et al. (2008) Shotgun bisulphite sequencing of the Arabidopsis gen-ome reveals DNA methylation patterning. Nature, 452, 215–219.Devlin,B. and Roeder,K. (1999) Genomic control for association studies.Biometrics, 55, 997–1004.Dolzhenko,E. and Smith,A.D. (2014) Using beta-binomial regression for high-precision differential methylation analysis in multifactor whole-genomebisulfite sequencing experiments. BMC Bioinformatics, 15, 215.Drineas,P. and Mahoney,M.W. (2005) On the Nyström method for approxi-mating a gram matrix for improved kernel-based learning. J. Mach. Learn.Res., 6, 2153–2175.Feng,H. et al. (2014) A Bayesian hierarchical model to detect differentiallymethylated loci from single nucleotide resolution sequencing data. NucleicAcids Res., 42, e69.Fine,S. and Scheinberg,K. (2001) Efficient SVM training using low-rank kernelrepresentations. J. Mach. Learn. Res., 2, 243–264.Goodwin,S. et al. (2016) Coming of age: ten years of next-generation sequenc-ing technologies. Nat. Rev. Genet., 17, 333–351.Jaffe,A.E. and Irizarry,R.A. (2014) Accounting for cellular heterogeneity iscritical in epigenome-wide association studies. Genome Biol., 15, 1.Jiang,D. et al. (2016) Retrospective binary-trait association test elucidates gen-etic architecture of Crohn disease. Am. J. Hum. Genet., 98, 243–255.Jones,E. et al. (2001). Scipy: Open source scientific tools for Python. http://www. scipy. org/ (30 December 2016, date last accessed).Jones,P.A. (2012) Functions of DNA methylation: islands, start sites, genebodies and beyond. Nat. Rev. Genet., 13, 484–492.Kahaner,D. et al. (1989). Numerical Methods and Software, Vol. 1.Englewood Cliffs, Prentice Hall.Kulis,B. et al. (2009) Low-rank kernel learning with Bregman matrix diver-gences. J. Mach. Learn. Res., 10, 341–376.Lea,A.J. et al. (2015) A flexible, efficient binomial mixed model for identifyingdifferential dna methylation in bisulfite sequencing data. PLoS Genet., 11,e1005650.Nickisch,H. and Rasmussen,C.E. (2008) Approximations for binary Gaussianprocess classification. J. Mach. Learn. Res., 9, 2035–2078.Powell,J.E. et al. (2013) Congruence of additive and non-additive effects ongene expression estimated from pedigree and SNP data. PLoS Genet., 9,e1003502.Rahmani,E. et al. (2016) Sparse PCA corrects for cell type heterogeneity inepigenome-wide association studies. Nat. Methods, 13, 443–445.Rasmussen,C. and Williams,C. (2006). Gaussian Processes for MachineLearning. The MIT Press, Cambridge, MA.Rue,H. et al. (2009) Approximate Bayesian inference for latent Gaussian mod-els by using integrated nested Laplace approximations. J. R. Stat. Soc. B, 71,319–392.Sun,D. et al. (2014) MOABS: model based analysis of bisulfite sequencingdata. Genome Biol., 15, R38.Sun,S. et al. (2016) Differential expression analysis for RNAseq using Poissonmixed models. Nucleic Acids Res., In press.Weissbrod,O. et al. (2015) Accurate liability estimation improves power in as-certained case-control studies. Nat. Methods, 12, 332–334.Widmer,C. et al. (2014) Further improvements to linear mixed models forgenome-wide association studies. Sci. Rep., 4, 6874.Yang,J. et al. (2014) Advantages and pitfalls in the application of mixed-model association methods. Nat. Genet., 46, 100–106.Zou,J. et al. (2014) Epigenome-wide association studies without the need forcell-type composition. Nat. Methods, 11, 309–311.i332 O.Weissbrod et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i325/3953963by gueston 07 January 2018
5028881981002	PMID28881981	5028881981	https://watermark.silverchair.com/btx247.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881981.main.pdf	Bioinformatics, 33, 2017, i234–i242doi: 10.1093/bioinformatics/btx247ISMB/ECCB 2017TITER: predicting translation initiation sites bydeep learningSai Zhang1,†, Hailin Hu2,†, Tao Jiang3,4,5, Lei Zhang2,*and Jianyang Zeng1,*1Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China, 2School ofMedicine, Tsinghua University, Beijing 100084, China, 3Department of Computer Science and Engineering,University of California, Riverside, CA 92521, USA, 4MOE Key Lab of Bioinformatics and Bioinformatics Division,TNLIST/Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China and5Institute of Integrative Genome Biology, University of California, Riverside, CA 92521, USA*To whom correspondence should be addressed.†The authors wish it to be known that these authors contributed equally.AbstractMotivation: Translation initiation is a key step in the regulation of gene expression. In addition tothe annotated translation initiation sites (TISs), the translation process may also start at multiplealternative TISs (including both AUG and non-AUG codons), which makes it challenging topredict TISs and study the underlying regulatory mechanisms. Meanwhile, the advent of severalhigh-throughput sequencing techniques for proﬁling initiating ribosomes at single-nucleotide resolution, e.g. GTI-seq and QTI-seq, provides abundant data for systematically studying the general principles of translation initiation and the development of computational method for TISidentiﬁcation.Methods: We have developed a deep learning-based framework, named TITER, for accurately predicting TISs on a genome-wide scale based on QTI-seq data. TITER extracts the sequence featuresof translation initiation from the surrounding sequence contexts of TISs using a hybrid neural network and further integrates the prior preference of TIS codon composition into a uniﬁed predictionframework.Results: Extensive tests demonstrated that TITER can greatly outperform the state-of-the-art prediction methods in identifying TISs. In addition, TITER was able to identify important sequence signatures for individual types of TIS codons, including a Kozak-sequence-like motif for AUG startcodon. Furthermore, the TITER prediction score can be related to the strength of translation initiation in various biological scenarios, including the repressive effect of the upstream open readingframes on gene expression and the mutational effects inﬂuencing translation initiation efﬁciency.Availability and Implementation: TITER is available as an open-source software and can be downloaded from https://github.com/zhangsaithu/titer.Contact: lzhang20@mail.tsinghua.edu.cn or zengjy321@tsinghua.edu.cnSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionTranslation initiation plays an important role in mRNA translation,in which the methionyl tRNA unique for initiation (Met-tRNAi)identifies the AUG start codon and triggers the downstream translation process (Hershey et al., 2012; Jackson et al., 2010; Sonenbergand Hinnebusch, 2009). As translation initiation is an essential stepin controlling gene expression and protein synthesis, the dysregulation of the initiation process can cause various human diseases,including cancers and metabolic disorders (Hershey et al., 2012;Sonenberg and Hinnebusch, 2009). On the other hand, the mechanisms underlying translation initiation, e.g. the recognition of a translation initiation site (TIS) by the 80S ribosome assembly, are farmore complicated than scientists had initially believed (Gao et al.,2015; Lee et al., 2012). In particular, experimental studies haveshown that the eukaryotic translation is not always initiated at thecanonical AUG start codons (Gao et al., 2015; Kozak, 1989; LeeCV The Author 2017. Published by Oxford University Press.i234This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018Deep learning for predicting translation initiationet al., 2012; Peabody, 1989). In addition to those annotated TISs(aTISs), both upstream (uTISs) and downstream TISs (dTISs) canalso occur at non-AUG codons, which yields alternative open reading frames (ORFs) that can be translated into short peptides or affectthe expression levels of the main ORFs (Barbosa et al., 2013; Calvoet al., 2009; Hershey et al., 2012; Jackson et al., 2010; Sonenbergand Hinnebusch, 2009). This underscores the necessity of a betterunderstanding of the mechanism of TIS recognition.Ribosome profiling (ribo-seq), a high-throughput deep sequencingbased technique that measures the ribosome protected fragmentsin vivo, has become a widely used method to quantify the translationdynamics on a transcriptome (Ingolia et al., 2009, 2012). However,the standard ribo-seq is not suitable for directly detecting TISs (Leeet al., 2012). Based on ribo-seq, additional techniques, e.g. the globaltranslation initiation sequencing (GTI-seq) (Lee et al., 2012) and thequantitative translation initiation sequencing (QTI-seq) (Gao et al.,2015), have been developed for a systematic mapping of the startcodon positions at single-nucleotide resolution in vivo. By far, thesetechniques have provided abundant data for investigating the principles of translation initiation and translational control.A variety of computational methods have been developed to predict alternative TISs or ORFs (Chew et al., 2016; Hatzigeorgiou,2002; Li and Jiang, 2005; Zien et al., 2000; Zur and Tuller, 2013).However, most of these methods only focused on the AUG startcodon and did not consider the widely observed non-AUG TISs. Inaddition, few studies have utilized experimental data generated byGTI-seq or QTI-seq in their works, which limits the empirical predictive power of these methods. Recently, a linear regression-basedapproach, called PreTIS, was proposed to predict non-canonicalTISs by incorporating both AUG and its near-cognate codons (i.e.the codons differing from AUG by one nucleotide), in which TISsidentified by GTI-seq were used to train the prediction model(Reuter et al., 2016). However, only alternative TISs in the 5’ UTR(i.e. uTISs) were considered by PreTIS, and several constraints onthe candidate TISs (e.g. the codon position in the transcript and theexistence of an orthologous mouse sequence) had to be imposed dueto the limitations of their feature engineering. To our best knowledge, our work is the first attempt to predict all possible TISs,including uTISs, aTISs and dTISs, in a unified framework withoutany restriction to the codon site of interest.Recently, deep learning has become one of the most effective andpowerful prediction methods in machine learning (Hinton andSalakhutdinov, 2006; Hinton et al., 2006). It has been widely usedand shown to be able to achieve the state-of-the-art prediction performance on various machine learning tasks, such as speech recognition (Hinton et al., 2012), image classification (Hinton andSalakhutdinov, 2006) and natural language processing (Collobertet al., 2011). In addition, deep learning is gradually gaining its popularity in bioinformatics and has yielded superior performance overconventional learning methods on a variety of biological predictiontasks, such as the predictions of protein–nucleotide binding (Alipanahiet al., 2015; Zhang et al., 2015), functional effects of non-coding sequence variants (Quang and Xie, 2016; Zhou and Troyanskaya,2015) and ribosome stalling (Zhang et al., 2016).In this study, we have developed a deep learning-based framework, named TITER (Translation Initiation siTE detectoR), for accurately predicting TISs based on the available high-throughputsequencing data (Fig. 1). TITER possesses more flexibility than previous methods, and integrates the prior preference of the TIS codoncomposition as well as their surrounding sequence contexts into aunified framework, in which an ensemble of hybrid deep convolutional and recurrent neural networks is implemented to effectivelyi235Fig. 1. Schematic overview of the TITER pipeline. See the main text for moredetailsand robustly capture the sequence features of translation initiation.Extensive validation tests have shown that TITER can greatly outperform the state-of-the-art computational approaches in detectingTISs. In addition, TITER can successfully identify significant sequence motifs for different TIS codons, including a Kozak-sequencelike motif for the AUG TIS codon. By combining gene expressiondata with TITER analysis, we found that the predictions of TITERwell correlated with translation efficiency (TE) of genes, which basically reconfirmed the repressive effect of uORFs on gene expression. Furthermore, our comparative analyses on several importantmutations around TISs showed that the fold changes of TITER prediction scores well conformed to the experimentally verified mutational effects reported in the literature (Calvo et al., 2009; Nodereret al., 2014). These results demonstrated that TITER can offer apowerful tool to model the sequence features of translation initiation and identify potential TISs, which will provide useful insightsinto understanding the underlying mechanisms of translationinitiation.2 Materials and methods2.1 DatasetsWe mainly used the QTI-seq data collected from the HEK293 cellline (Gao et al., 2015) and the annotated TISs retrieved fromEnsembl v84 (Aken et al., 2016) (together denoted by Gao15) totrain and test TITER. Specifically, we first retrieved all transcripts,each containing at least one TIS identified by the QTI-seq experiment. Then we combined the QTI-seq identified TISs among thesetranscripts with the corresponding annotated TISs obtained fromthe Ensembl database, and regarded them as positive samples. Toconstruct a dataset of negative samples that well reflected the imbalance of positive and negative samples in vivo, for each TIS in thepositive dataset, we chose up to 10 codon sites of the same tripletwithin the same transcript as negative samples. Considering theleaky scanning nature of the translation initiation process (Michelet al., 2014), we searched for the negative samples starting from the5’ end of an mRNA, until ten eligible sites were found. Altogether,the Gao15 dataset consisted of 9776 positive samples and 94 899negative samples from 4111 transcripts, among which 400 transcripts were reserved for testing and the others were regardedas our training data (denoted by Gao15_test and Gao15_train,respectively).To further evaluate the prediction performance of TITER in ascenario with an unlimited ratio of positive and negative samples,from the 400 test transcripts, we also constructed another test dataset (denoted by Gao15_test_extended), in which we identified allpossible initiation codon sites before the last TIS of each transcriptwhen searching for negative samples. Notably, as QTI-seq identifiedDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018i236S.Zhang et al.versatile TISs in terms of both codon composition and positions, ourconstructed training and test datasets also reflected this versatility,which thus enabled TITER to capture diverse sequence features oftranslation initiation that exist in vivo. In particular, we found thatin the QTI-seq data derived from Gao et al. (2015), besides the canonical aTISs and the conventional AUG TISs, there also exist alarge number of alternative TISs (i.e. uTISs and dTISs) and nonAUG TISs (e.g. CUG, GUG and UUG) (Fig. 2). This observationunderscores the necessity of modeling the universal rules of translation initiation.2.2 Modeling the preference of codon compositionof TISsIt has been experimentally validated that there is a significant preference of codon composition among TISs (Gao et al., 2015; Lee et al.,2012). For example, the AUG codon generally plays a dominantrole (>50%) at the TISs (Fig. 2a). Interestingly, other codons thatdiffer from AUG at the first nucleotide (nt), including CUG, GUGand UUG, also greatly contribute to translation initiation ($25%;Fig. 2a). Here, we collectively denote these four codons by NUG, inwhich ‘N’ stands for any possible nucleotide. With this prior knowledge on codon preference, we propose to explicitly model the preference of codon composition of TISs by CodonScore ðcÞ ¼ logþ# of c0 s at TISs# of TISs  # of c0 s in background;# of background codons(1)in which CodonScore(c) is defined as the preference of codon composition for a certain codon c, denotes the number of the entities inthe defined category and the background is defined as the upstreamone-third segment of the transcripts, following the same principleas in Gao et al., (2015). In particular, we denote the function between the parentheses on the right-hand side of Equation (1) by f(c).Then we define logþ ðxÞ ¼ log ðxÞ if log ðxÞ > 0; logþ ðxÞ ¼ ax otherwise, where a ¼ minflog ðf ðcÞÞjlog ðf ðcÞÞ > 0; c is a codong. Thisdefinition is introduced as we notice that for those codons that areless similar with the canonical AUG start codon, their abundance inthe QTI-seq dataset is actually lower than the background, leadingto a negative CodonScore value if we only use the log(x) form.Although these codons only account for $10% in the Gao15 dataset, to avoid the over-devaluation of the CodonScore for thesecodons due to the nature of logarithm function, we propose to use alinear model to define the CodonScore for these codons. In particular, the top five codons with the highest CodonScore value are AUG,CUG, GUG, UUG and ACG (Supplementary Fig. S1), which is consistent with the statistics shown in Figure 2a.(a)(b)2.3 Modeling the contextual features of TISsIt has been indicated that the contextual sequences around the TISscan influence the likelihood of translation initiation (Kozak, 1989;Noderer et al., 2014). For example, the upstream and downstreamsequences of the canonical AUG TISs exhibit a consensus motifcalled the Kozak sequence (Kozak, 1989). This observation underlies the rationale of modeling the translation initiation events byencoding the contextual sequence features surrounding TISs.Here, we develop a deep neural network to systematically modelthe sequence features of TISs (Fig. 3(a)). In particular, each TIS is extended both upstream and downstream by 100 nts, which yields thecontextual profile of a translation initiation event and is denoted bys ¼ ðn1 ; . . . ; n203 Þ, where ni denotes the nucleotide at the ith position. To characterize the local motifs of the extended sequence s,we first encode the nucleotides using the one-hot encoding technique(Pedregosa et al., 2011), that is, a nucleotide of a particular type(A, U, C or G) is encoded by a binary vector of length four, in whichthe corresponding position is one while the others are zeros, afterindexing all four types of nucleotides. Then we employ multiple convolution operators (denoted by conv(Á)) to scan the encoded sequence profile and detect the local motifs around each TIS. Afterthat, the pooling operators (denoted by pool(Á)) are used to identifythe activated motifs and also reduce the dimensions of hidden features. Note that the conventional convolution-pooling structures areorder insensitive, as they only detect whether certain motifs exist regardless of their positions or orders. To further characterize themotif order that may also contribute to translation initiation, wealso stack a long short-term memory (LSTM) network (denoted byLSTM(Á)) upon the convolution-pooling module, which takes thepooled feature vectors as input and models the long-term dependencies between different motifs. Finally, the outputs of the LSTM at allpositions are concatenated and fed into a logistic regression layer(denoted by logist(Á)) to compute the probability of translation initiation for the input sequence. Indeed, when considering the LSTMoutputs at all positions, we also implement the attention mechanismthat has been widely used in deep learning (Denil et al., 2012;Larochelle and Hinton, 2010) to leverage the relevance of each position to the final prediction. Altogether, by fully exploiting both sequence composition and motif orders, the hybrid convolutional andrecurrent neural network in TITER computes the following score,ContextScoreðsÞ ¼ logistðLSTMðpoolðconvðencodeðsÞÞÞÞÞ:(2)The sequence profiles of non-TISs (i.e. the negative samples) can bemodeled in the same manner. Note that a similar hybrid neural network architecture has also been implemented in Quang and Xie(2016) and Hassanzadeh and Wang (2016) for different tasks, i.e.predicting the functional effects of non-coding mutations and theDNA-binding protein targets, respectively.2.4 Model training and model selectionGiven the training samples fðsi ; yi Þgi , the loss function of our deeplearning framework is defined as the sum of the negative log likelihoods, i.e.Xloss ¼ Àlog ðyi ContextScoreðsi Þ þ ð1 À yi Þð1 À ContextScoreðsi ÞÞÞ;i(3)Fig. 2. Statistics of the translation initiation sites in the Gao15 dataset.(a) Codon composition of TISs, in which only those codons with a fraction >1% are shown. (b) Fractions of different types of TISswhere yi indicates whether si is the sequence profile of a TIS or not.We use the standard error backpropagation algorithm (Rumelhartet al., 1986) and the stochastic gradient descent method (Bengio,2012) to train the hybrid neural network and search for the networkDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018Deep learning for predicting translation initiation(a)i237groups of bootstrap samples (denoted by Si) from the original imbalanced population S, by randomly selecting samples with replacement. Then for each group Si, we balance the samples bydownsampling, i.e. randomly selecting an equal number of positiveand negative samples from Si, which yields a balanced dataset Bi.After that, a hybrid deep neural network Ni is trained based on eachdataset Bi independently, resulting in an ensemble of binary classifiers fNi g. Given an input sequence s, its final prediction score isaveraged over the prediction scores output by all classifiers, i.e.ContextScoreðsÞ ¼(b)Fig. 3. Schematic illustration of (a) the hybrid deep neural network architecture and (b) the bootstrapping-based technique used in TITER. See the maintext for more detailsweights that minimize the loss function of Equation (3). Severalregularization techniques, including the max-norm constraints onweights (Srebro et al., 2005), dropout (Srivastava et al., 2014) andearly stopping (Bengio, 2012), are also employed to optimize thetraining process and address the overfitting problem.Our hybrid deep neural network architecture and various optimization techniques used in the training process have introducedseveral hyperparameters, e.g. the kernel size, kernel number andmax-norm of weights, that need to be determined. A proper hyperparameter calibration procedure can help yield better solutions tothe optimization problem in Equation (3). Here, we use the treestructured Parzen estimator (TPE) approach (Bergstra et al., 2011)to calibrate the hyperparameters in our model, including kernel size,kernel number and max-norm of weights in the convolution layer,pooling length of the max-pooling layer, output dimension of theLSTM layer, dropout rate, and the optimizer algorithm. In particular, we first use all the positive training samples and an equal number of randomly selected negative training samples to optimize thehyperparameters based on TPE (with 100 evaluations), and thenchoose the hyperparameters that achieve the minimum loss to further train our final models (Supplementary Table S1).We realize that the task of predicting TISs is an imbalanced classification problem (i.e. much more negative samples than positiveones), for which the standard training procedure designed for balanced samples cannot be applied directly. On the other hand, making use of more negative samples in the training process can lead toa more robust model with less variance in prediction (Wallace et al.,2011). To tackle this imbalance problem, here we employ abootstrapping-based technique (Fig. 3b) derived from the theory established in Wallace et al. (2011). Briefly, we first construct severaln1XNi ðsÞ;n i¼1(4)in which n is the total number of the constructed balanced datasetsin fBi g (which is also equivalent to the total number of trained classifiers in fNi g). We apply this bootstrapping-based technique to ourtraining dataset and train 32 independent deep neural networks.After that, their prediction scores are averaged and used as the finalestimated probability of translation initiation for the given input sequence profile.Owing to the nature of non-convex optimization, randomweight initialization can influence the search results of the gradientdescent algorithm (Bengio, 2012). This initialization bias may alsointroduce variance to our modeling and further affect the predictionperformance. In TITER, the aforementioned bootstrapping-basedtechnique can alleviate such an initialization bias in addition to thesample bias, as the network weights have been initialized independently for each balanced sample group before the training process.The hybrid deep neural network of TITER has been implementedusing the Keras library (https://keras.io/), and the Tesla K20c GPUshave been used to speed up the training process. The TPE algorithmfor hyperparameter calibration has been implemented based onHyperas (https://github.com/maxpumperla/hyperas/), a Python libraryfor optimizing hyperparameters of the models implemented based onKeras.2.5 Integrating the preference of codon compositionand the contextual features of TISsAs the neural network in the binary classification scenario mainlydiscriminates the positive and negative samples based on the contextual sequence features, it may be less sensitive to the difference inthe preference of codon composition of TISs. To improve the sensitivity of our method and to make a tradeoff between these two complementary information, here we integrate both CodonScore andContextScore to derive the final score representing the likelihood oftranslation initiation, i.e.TISScoreðsÞ ¼ CodonScoreðsÞ Â ContextScoreðsÞ;(5)where s denotes the sequence profile of a codon site of interest, anda high TISScore is outputted only if both ContextScore andCodonScore are large enough. Basically, the final score is obtainedby weighting between the contributions of the prior preference ofcodon composition and the sequence contexts to translationinitiation.3 Results3.1 TITER accurately predicts TISsHere, we performed extensive tests to show that TITER greatly outperformed the state-of-the-art methods in predicting TISs, includingWRENT (Chew et al., 2016) and PreTIS (Reuter et al., 2016). NoteDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018i238that WRENT cannot compute the initiation scores for those nonAUG codons, while PreTIS only focuses on the 5’ UTR for the nearcognate codons (i.e. the codons differing from AUG by onenucleotide).After hyperparameter calibration, we first tested our methodusing a five-fold cross-validation procedure and evaluated its prediction performance based on both the areas under the receiver-operating characteristic (AUROC) and the precision recall (AUPR) curves.We found that without using the recurrent layer or thebootstrapping-based technique in our framework, the predictionperformance dropped dramatically compared to that of the proposed model (Supplementary Fig. S2), especially for the AUPR score(with drop in AUPR by $10%), demonstrating the important contributions of these techniques to reduce the modeling variance andboost the prediction accuracy. Also, our model yielded better prediction performance on the AUG codons than non-AUG ones(Supplementary Fig. S2). This indicated that the sequence features ofAUG TISs may provide more predictive information in our framework than those of non-AUG TISs. In addition, we observed thateven for the AUG sites, our method greatly outperformed WRENT,with increases in AUROC and AUPR scores by 20.9 and 47.0%,respectively.We also performed additional tests on an independent datasetinvolving 400 transcripts derived from the Gao15 dataset (seeSection 2). We first validated the prediction performance of TITERon the Gao15_test dataset, in which the positive and negative samples were selected in the same way as in the construction of ourtraining data, resulting in 935 TISs and 9098 non-TISs. Test resultsrevealed comparable or even superior prediction performance onthis dataset compared to the previous cross-validation results (Fig.4a and b, Supplementary Fig. S2), which further validated that ourmodel did not suffer from the overfitting problem. Similarly, ourmethod also achieved better prediction performance on AUG sitesthan non-AUG ones, and greatly outperformed WRENT on AUGcodons with increases in AUROC and AUPR scores by 17.4 and46.1%, respectively (Fig. 4a and b). Furthermore, we tested TITERon a more natural and realistic setting, in which we considered allnegative samples (i.e. non-TISs) surrounding the positive ones (i.e.TISs) in a transcript. By mainly focusing on those codons with relatively high probability of translation initiation (i.e. CodonScore >1,including AUG, CUG, GUG, UUG and ACG), we selected all thenon-TISs of the same triplet before the last TISs for each transcriptas negative samples to construct the Gao15_test_extended dataset,which resulted in 767 positive and 9914 negative samples in total.Test results showed that TITER also yielded excellent predictionperformance with AUROC and AURP scores of 89.1 and 61.8%, respectively (Fig. 4a and b). Specifically, the prediction performancewas improved after incorporating the prior knowledge on the preference of codon composition of TISs (i.e. CodonScore), with an increase of AUPR score by 9.3% (Fig. 4a and b), which thusdemonstrated the necessity of our integrative modeling.To further validate the generalization of our framework across different datasets and organisms, we additionally evaluated the prediction performance of TITER on an additional dataset of mouse(denoted by Gao15_mouse), which was also derived from Gao et al.(2015). All the data preprocess procedures were the same as those forthe human data (i.e. the Gao15 dataset). In particular, we randomlyselected 360 transcripts as the test data and used the remaining transcripts as our training data. The final prediction performance wasevaluated on the test data. Based on the same hyperparameter valuescalibrated on the Gao15 dataset (Supplementary Table S1), wefound that the prediction performance of TITER on the mouse dataS.Zhang et al.was comparable or even superior to that on the human data, demonstrating the generalization capacity of our framework (SupplementaryFig. S4).To facilitate the comparison between TITER and PreTIS, wealso constructed a separated uTIS test dataset (denoted byCalviello16). Specifically, this dataset included AUG TISs identifiedby RiboTaper (Calviello et al., 2016), a statistical method to definethe ORFs through the three-nucleotide periodicity of the ribosomeprofiling data. The TISs from the transcripts and their isoforms usedfor training either TITER or PreTIS were excluded, resulting in 383available transcripts. Furthermore, since the feature engineering ofPreTIS required the transcript harboring the TIS of interest to havean orthologous mouse sequence and to be at least 99 nts downstream from its transcript start site, we also excluded all the samplesthat failed to meet these requirements ($164 transcripts), leaving253 transcripts available for the test. The positive and negative samples were labeled using the same method as described in Reuter et al.(2016), resulting in an imbalanced dataset containing 227 positiveand 864 negative samples. Tests on the Calviello16 dataset showedthat TITER still greatly outperformed both WRENT and PreTIS inthis independent dataset, with increases in AUROC by 19.0 and3.9%, respectively, and in AUPR by 17.6 and 3.8%, respectively(Fig. 4c and d). Note that here the comparison only focused on theAUG codons since RiboTaper only identified the ORFs starting withAUG. In addition, TITER yielded excellent prediction performanceon the Gao15 dataset when considering uTISs (still outperformingPreTIS) and dTISs separately (Supplementary Fig. S5).3.2 TITER captures the sequence motifs of different TIScodonsThe hybrid deep neural network of TITER can be easily extended togenerate the sequence motifs of different TIS codons. Based on thesimilar idea to that used in Simonyan et al. (2013), here we generated the sequence motifs of a particular TIS codon by optimizing thefollowing problem:arg min ðContexScoreðsl   c   sr Þ À 1Þ2 ;sl ;sr(6)in which sl, sr and c stand for the left sequence, the right sequenceand the TIS codon of interest, respectively, and   represents the sequence concatenation operation. Basically, the above formulationminimizes the difference between the predicted and the positivelabels, and finds the optimal sequence with the highest probabilityof being a TIS for that particular codon. By fixing the weights of atrained neural network, we can optimize the above problem usingthe gradient descent technique.Here, we mainly focused on the motif of a region covering bothupstream and downstream 10 nts from the TIS codon of interest. Inparticular, for the canonical AUG start codon, our generated motifwas quite similar to the Kozak sequence, i.e. (gcc)gccRccAUGG,which was previously validated to be consensus for aTISs (Kozak,1989) (Fig. 5a). In the Kozak sequence, a lower case indicates a commonly occurred base, a upper case denotes a highly conserved base,and the sequence in a bracket is of uncertain significance. However,simply depending on the Kozak sequence or similar methods likeWRENT cannot accurately identify the experimentally-verifiedAUG TISs (Fig. 4), as the prior knowledge on the sequence motifsurrounding the annotated AUG TISs may lead to a biased estimation, which may also explain the slight difference between our generated sequence motif and the Kozak sequence (Fig. 5a). Note thatthe Kozak sequence and the position weight matrix (PWM)-basedDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018Deep learning for predicting translation initiation(a)(b)i239(c)(d)Fig. 4. Prediction performance on different test datasets. (a, b) Comparison of prediction performance between different methods on the Gao15 dataset evaluatedby (a) ROC and (b) PR curves, respectively. (c, d) Comparison of prediction performance between different methods on the Calviello16 dataset evaluated by (c)ROC and (d) PR curves, respectively. ‘preTITER’ denotes a preliminary version of our deep learning framework that only considered the context features of TISs(a)(b)(c)(d)Fig. 5. The sequence motifs generated by TITER for (a) AUG (ATG), (b) CUG (CTG), (c) GUG (GTG) and (d) UUG (TTG) TIS codons, respectively. The ﬁnal positionweight matrix (PWM) for each TIS codon was calculated by averaging the optimal input sequences computed by the ensemble of 32 deep neural networks inTITER. The base sequence motifs were visualized using Seq2Logo v2.0 (Thomsen and Nielsen, 2012). All the sequence motifs were visualized in the cDNA settingmethods (e.g. WRENT) simply assume the independence betweendifferent base positions, which largely simplifies the sequence features contributing to translation initiation. In contrast, the deep neural network of TITER can capture more abundant information, suchas the correlation between bases in distinct positions, which is generally also relevant to the prediction of translation initiation.We also generated the sequence motifs for other three NUG TIScodons, including CUG, GUG and UUG. Interestingly, we found thatthese codons own specific motifs different from that of AUG, especially in the upstream region, indicating that there may exist a different mechanism for alternative translation initiation (Fig. 5b–d).Moreover, we observed that these three codons exhibit an (AU)-richmotif in their local downstream regions. As the (AU)-rich regions ofmRNAs are commonly associated with high free energy and weak secondary structure (Lehninger et al., 2008; Waterman and Smith,1978), this implied that the unstructured regions may assist the translation initiation and the following translation elongation process,which can also be supported by known evidence from the previousstudy (Chew et al., 2016).3.3 Prediction of TITER correlates with TEPrevious studies have shown that translation initiation at aTISs anduTISs can play important functional roles in regulating gene expression (Calvo et al., 2009; Chew et al., 2016; Ferreira et al., 2013;Hinnebusch et al., 2016). In particular, it has been believed that thestrength of translation initiation signals at aTISs can positively correlate with TE, while the occurrence of uTISs may repress the expressionof the main ORFs (Calvo et al., 2009; Chew et al., 2016; Ferreiraet al., 2013; Hershey et al., 2012; Hinnebusch et al., 2016; Jacksonet al., 2010; Sonenberg and Hinnebusch, 2009). Here, we were particularly interested in investigating the contributions of the predictedTISScore values at aTISs and uTISs to the TE of the main ORFs.Here, we defined the score of TE as the logarithm of the proteinexpression level divided by the corresponding mRNA expressionlevel. Specifically, the tandem mass spectrometry (MS/MS) data andthe mRNA-seq data of HEK293 cell line were obtained from thepreviously published studies (Geiger et al., 2012; Nam et al., 2014)and were used to derive the levels of protein and mRNA expression,respectively. The iBAQ normalized intensity and the RPKM valuewere averaged among different replicates for protein and mRNA expression, respectively. We only considered those proteins that weredetected in at least two out of three replicates in the MS/MS data.The Uniprot IDs of the genes measured by mass spectrometry werematched to the Ensembl transcript IDs by the Uniprot Retrieve/IDmapping interface (Consortium, 2015). As the current tandem massspectrum technique cannot accurately distinguish isoforms, our analysis was carried out on the gene level as in Calvo et al. (2009).Specifically, for a Uniprot ID that was mapped to multiple transcripts, we selected the transcript with the largest expression valuemeasured by mRNA-seq. The final dataset of matched proteins andmRNAs contained 5752 genes in total. Consistent with the previous report (Lundberg et al., 2010), we also observed a certainlevel of correlation between protein and mRNA expression levels(Spearman’s correlation coefficient r ¼ 0.67), validating the qualityof this dataset to some extent. For uTISs, we followed the same definition of uORF as described in Calvo et al. (2009), in which auORF is defined as a continuous segment of codons that has a startcodon in the 5’ UTR, a stop codon before the end of the main ORFand a minimum length of nine nts (including the stop codon).We first employed a linear regression model (implemented basedon the scikit-learn library (Pedregosa et al., 2011)) to predict the TEvalue from the TISScore of aTIS for each transcript. We applied a10-fold cross-validation procedure to evaluate the correlation between the predicted and the experimentally derived values. Test results showed that the predicted TE values based on the TISScorevalues of aTISs computed by TITER well correlated with the experimentally derived TE values (Spearman’s correlation coefficientr ¼ 0.234; Table 1), indicating that TISScore can provide predictivepower to estimate the strength of translation initiation. Note thatDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018i240S.Zhang et al.there are a variety of biological processes between translation initiation and the final translated protein products that were notincluded in our modeling, e.g. translation elongation and proteinfolding, which may explain the weakness of the observed correlationin this test. We further integrated the TISScore values of those uTISsthat were confidently predicted to form a uORF and withCodonScore >1 and ContextScore >0.95 into the regression model.Interestingly, after integrating the TISScore values of uTISs, the correlation between the predicted and experimentally derived TE valuesincreased from 0.234 to 0.245 (Table 1). In particular, the featurecoefficients in the regression model for the TISScore values of eligible uTISs were negative, indicating the repressive effect of uORFon the protein expression. Such an increase in correlation was limited, which was consistent with the previous report that the repressive effect of uORF on protein expression level is relatively weak inhuman (Chew et al., 2016). To further validate the effectiveness ofthe TISScore, we also performed the same prediction task by replacing the TISScore values of uTISs with the number of AUG codons inthe 5’ UTR. We found that with this feature replacement, the difference in correlation was almost negligible (Table 1), which thus demonstrated the necessity of considering the contextual sequencefeatures of TISs in the prediction model. We also considered thelength of 5’ UTR (which is presumably related to the number of undetected uORFs) in the regression model to test whether those undetected uORFs are influencing TE other than the detected TISs.Test results showed that this additional feature did not affect theprediction performance largely (Table 1), implying that very unlikely our method can suffer from the false negative issue.3.4 TITER quantifies mutational effects on translationinitiationPreviously, several studies have identified mutations that are putative to affect TE through the alteration in the sequence contexts ofTISs (Kozak, 2002; Noderer et al., 2014; Wolf et al., 2011) or theintroduction of extra uORFs (Barbosa et al., 2013; Calvo et al.,2009; Hinnebusch et al., 2016). As TITER has been shown to beable to accurately predict bona fide TISs in human transcriptome,we further explored its ability to quantify the mutational effects thatmay be related to the physiological and pathological conditions. Inparticular, we selected two sets of mutations that have been validated through different quantitative reporter assays in Noderer et al.(2014) and Calvo et al. (2009), respectively, and then evaluated howthe changes of the predicted scores associated with the mutationscan reflect their functional effects in vivo.Through flow cytometry, Noderer et al. (2014) quantitativelymeasured the effects of seven mutations derived from the COSMICdatabase (Forbes et al., 2015) and observed consistent effects withother known tumor expression patterns (Fig. 6a). Specifically, theyemployed a dual fluorescence vector with a GFP reporter under thecontrol of a specific TIS context as well as an independent IRESdriven RFP reporter as the internal standard, and the final result wasreported with the GFP/RFP ratio. As the experiments were performedby expressing the reporter from a plasmid, we fed the plasmid sequences containing each TIS into the TITER framework, and calculated the changes of the predicted scores (i.e. ContextScore) alongwith the mutations. As expected, the changes of ContextScore valueswere in good agreement with the changes of the experimentally measured GFP/RFP ratios (Fig. 6b; Pearson’s correlation coefficientr ¼ 0.83), indicating that TITER was able to capture the TIS contextsthat are related to TE, even though the mutation information was notincluded in our training data.In another study, Calvo et al. (2009) carried out a series of luciferase assays to demonstrate the effects of the additionally introduced non-overlapping uORFs on protein expression of clottingfactor XII (FXII). Their study involved six sequence variants thatwere associated with the non-overlapping uORFs (Fig. 6c). Similarto the above analysis, the sequence contexts of individual TISs,including both aTISs and uTISs, were fed into the TITER frameworkas input. We observed a good correlation even when only considering the changes of the prediction scores for aTISs (Pearson’s correlation coefficient r ¼ 0.75). Notably, the correlation was furtherimproved (Pearson’s correlation coefficient r ¼ 0.85), when theContextScore values of uTISs were included through a linear leakyscanning model proposed by Ferreira et al. (2013) (Fig. 6d), i.e.dContextScore a ¼ ð1 À k Â ContextScoreu Þ Â ContextScorea ;(7)dwhere ContextScore stands for the calibrated ContextScore, ‘a’ and‘u’ denote the ‘annotated’ and ‘upstream’ TISs, respectively, and k isthe model parameter. Note that here we set k ¼ 0.86 based on theprevious result of the synthetic reporter assay reported in Ferreiraet al. (2013). Together with our previous genome-wide analysis,these results demonstrated the ability of TITER to quantitativelyevaluate the repressive effects of uTIS/uORF on protein expression.To further confirm that the signal differences presented abovewere not due to the experimental bias and truly reflected the physiological or pathological effects, we also performed two additionalanalyses where the input sequences were changed to the corresponding sequences of the real transcripts. As expected, the changes ofContextScore for the transcript sequences still maintained a goodconsistency with the changes of experimental signals in both studies,with only a moderate fluctuation in the correlation, which thus further validated the biological relevance of our results (SupplementaryFig. S3).4 DiscussionThe prediction of TISs has long been considered an important taskin the studies of gene expression regulation (Jackson et al., 2010;Table 1. Results on estimating translation efﬁciency (TE) from the TITER prediction scores, using a linear regression model with differentcombinations of featuresFeature setaTISaTISþuTISaTISþAUGaTISþuTISþlofUTRFeature coefficients in regression (mean6SD)Spearman’s correlation r1.83360.0341.77460.040, –0.12660.0051.70960.038, –0.02960.0051.74460.037, –0.12260.005, –0.000163.634e–50.2340.2450.2340.236P valueMSE1.077eÀ723.801eÀ792.712eÀ722.228eÀ733.0813.0633.0783.063‘MSE’ denotes the mean square error. ‘aTIS’ represents the predicted TISScore for the aTIS, ‘uTIS’ represents the sum of TISScore values of all the eligibleuTISs, ‘AUG’ represents the number of AUGs in the upstream region of the aTIS, and ‘lofUTR’ represents the length of the 5’ UTR. The mean and the standard deviation (SD) of the feature coefﬁcients in the regression model in a 10-fold cross-validation procedure were calculated.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018Deep learning for predicting translation initiation(a)i241(b)(d)(c)Fig. 6. The prediction scores of TITER correlate with the experimentally measured mutational effects. (a, c) Illustrations of different mutations in the tests derivedfrom studies (Noderer et al., 2014) and (Calvo et al., 2009), respectively, in which the base sequences were shown in the cDNA setting. The single nucleotide variants were underlined, and the wild-type ATGs and the emerging ATGs were colored in red and blue, respectively. (b, d) The correlations between the predictionscores of TITER and the experimentally measured mutational effects in the previous studies (Noderer et al., 2014) and (Calvo et al., 2009), respectivelyHershey et al., 2012; Sonenberg and Hinnebusch, 2009). For years,progress has been made on the identification of the contextual sequence features that can prompt translation initiation, with thefocus on the AUG start codon of the main ORFs (Chew et al., 2016;Hatzigeorgiou, 2002; Kozak, 1989; Zien et al., 2000; Zur andTuller, 2013). However, the existence of alternative initiationcodons and multiple initiation positions in vivo further complicatethis prediction problem. Herein, we have developed a deep learningbased framework, called TITER, that can automatically extract intrinsic sequence features from the experimentally identified TISs. Byintegrating both the preference of codon composition and the contextual sequence information, our unified framework was able to accurately predict various types of TISs. The subsequent motif analysishas expanded our current understanding of the sequence contexts offavored TIS codons. Furthermore, additional analyses of gene expression and mutations showed that TITER can be applied to accurately estimate the probability of translation initiation in variousbiological scenarios.Recently, Reuter et al. proposed a linear model, called PreTIS,that was trained based on ribosome profiling data and can predictboth AUG and non-AUG TISs in the 5’ UTR (Reuter et al., 2016).However, the application of this method is limited by the codon position (i.e. at least 99 nts downstream from the transcript start siteand in the 5’ UTR) and the existence of mouse orthologous. SinceTITER does not rely on any explicit feature engineering, it possessesthe generality of using any input sequence for prediction. Moreover,the extensive tests have shown that TITER can greatly outperformPreTIS, further demonstrating the superiority of our method.Previously, a number of techniques based on ribosome profilinghave been developed to identify and characterize the TISs. In particular, Lee et al. (2012) developed the GTI-seq technique, which employed an initiation-specific small molecule ribosome inhibitor, calledlactimidomycin, to capture TISs with both AUG and alternativecodons on a genome-wide scale. As an updated version of thismethod, Gao et al. proposed a dual inhibition technique, called QTIseq, to address the limitation of the previous method regarding theamplification of ribosome signals on TISs and the inflation of signalsat the 5’ ends of transcripts, which achieved a quantitative profiling ofinitiating ribosomes (Gao et al., 2015). Moreover, they also appliedstringent mapping protocol and statistical test to increase the fidelityof the identified TISs. Therefore, in our study we mainly chose thedataset generated from Gao et al. (2015) to facilitate the training andtest of TITER. With the careful selection of data source and the accurate prediction performance on various test settings, we believe thatTITER will be useful for the community to investigate probable TISsand further expand our understanding of the mechanisms underlyingtranslation initiation.FundingThis work was supported in part by the National Basic Research Program ofChina Grant 2011CBA00300 and 2011CBA00301, the National NaturalScience Foundation of China Grant 61033001, 61361136003 and 61472205,the US National Science Foundation Grant DBI-1262107 and IIS-1646333,the China’s Youth 1000-Talent Program, and the Beijing AdvancedInnovation Center for Structural Biology.Conflict of Interest: none declared.ReferencesAken,B.L. et al. (2016) The Ensembl gene annotation system. Database, 2016.Alipanahi,B. et al. (2015) Predicting the sequence speciﬁcities of DNA- andRNA-binding proteins by deep learning. Nat. Biotech., 33, 831–838.Barbosa,C. et al. (2013) Gene expression regulation by upstream open readingframes and human disease. PLOS Genet., 9, e1003529.Bengio,Y. (2012). Neural Networks: Tricks of the Trade. In: PracticalRecommendations for Gradient-Based Training of Deep Architectures, 2ndedn. Springer, Berlin, Heidelberg, pp. 437–478.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018i242Bergstra,J.S. et al. (2011). Algorithms for hyper-parameter optimization. InShawe-Taylor, J. et al., eds. Advances in Neural Information ProcessingSystems 24. Curran Associates, Inc, pp. 2546–2554.Calviello,L. et al. (2016) Detecting actively translated open reading frames inribosome proﬁling data. Nat. Methods, 13, 165–170.Calvo,S.E. et al. (2009) Upstream open reading frames cause widespread reduction of protein expression and are polymorphic among humans. Proc.Natl. Acad. Sci., 106, 7507–7512.Chew,G.-L. et al. (2016) Conservation of uORF repressiveness and sequencefeatures in mouse, human and zebraﬁsh. Nat. Commun., 7, 11663.Collobert,R. et al. (2011) Natural language processing (almost) from scratch.J. Mach. Learn. Res., 12, 2493–2537.UniProt Consortium (2015) Uniprot: a hub for protein information. NucleicAcids Res., 43, D204–D212.Denil,M. et al. (2012) Learning where to attend with deep architectures forimage tracking. Neural Comput., 24, 2151–2184.Ferreira,J.P. et al. (2013) Tuning gene expression with synthetic upstreamopen reading frames. Proc. Natl. Acad. Sci., 110, 11284–11289.Forbes,S.A. et al. (2015) COSMIC: Exploring the world’s knowledge of somatic mutations in human cancer. Nucleic Acids Res., 43, D805–D811.Gao,X. et al. (2015) Quantitative proﬁling of initiating ribosomes in vivo.Nat. Methods, 12, 147–153.Geiger,T. et al. (2012) Comparative proteomic analysis of eleven common celllines reveals ubiquitous but varying expression of most proteins. Mol. Cell.Proteomics, 11,Hassanzadeh,H.R., and Wang,M.D. (2016). DeeperBind: enhancing prediction of sequence speciﬁcities of DNA binding proteins. In: IEEEInternational Conference on Bioinformatics and Biomedicine, BIBM 2016,Shenzhen, China, December 15–18, 2016, pp. 178–183.Hatzigeorgiou,A.G. (2002) Translation initiation start prediction in humancDNAs with high accuracy. Bioinformatics, 18, 343–350.Hershey,J.W. et al. (2012) Principles of translational control: an overview.Cold Spring Harbor Perspect. Biol., 4, a011528.Hinnebusch,A.G. et al. (2016) Translational control by 5’-untranslated regions of eukaryotic mRNAs. Science, 352, 1413–1416.Hinton,G. et al. (2012) Deep neural networks for acoustic modeling in speechrecognition: The shared views of four research groups. IEEE Signal ProcessMag., 29, 82–97.Hinton,G.E., and Salakhutdinov,R.R. (2006) Reducing the dimensionality ofdata with neural networks. Science, 313, 504–507.Hinton,G.E. et al. (2006) A fast learning algorithm for deep belief nets. NeuralComput., 18, 1527–1554.Ingolia,N.T. et al. (2009) Genome-wide analysis in vivo of translation with nucleotide resolution using ribosome proﬁling. Science, 324, 218–223.Ingolia,N.T. et al. (2012) The ribosome proﬁling strategy for monitoringtranslation in vivo by deep sequencing of ribosome-protected mRNA fragments. Nat. Protoc., 7, 1534–1550.Jackson,R.J. et al. (2010) The mechanism of eukaryotic translation initiationand principles of its regulation. Nat. Rev. Mol. Cell. Biol., 11, 113–127.Kozak,M. (1989) Context effects and inefﬁcient initiation at non-AUG codonsin eucaryotic cell-free translation systems. Molecular and Cellular Biology,9, 5073–5080.Kozak,M. (2002) Emerging links between initiation of translation and humandiseases. Mamm. Genome, 13, 401–410.Larochelle,H., and Hinton,G. (2010). Learning to combine foveal glimpseswith a third-order boltzmann machine. In Lafferty, J. et al. eds. Advances inNeural Information Processing Systems, Curran Associates, Inc., RedHook, NY 12571, Vol. 23, pp. 1243–1251.S.Zhang et al.Lee,S. et al. (2012) Global mapping of translation initiation sites in mammalian cells at single-nucleotide resolution. Proc. Natl. Acad. Sci., 109,E2424–E2432.Lehninger,A. et al. (2008). Lehninger Principles of Biochemistry. W. H.Freeman, New York.Li,H., and Jiang,T. (2005) A class of edit kernels for SVMs to predict translation initiation sites in eukaryotic mRNAs. J. Comput. Biol., 12, 702–718.Lundberg,E. et al. (2010) Deﬁning the transcriptome and proteome in threefunctionally different human cell lines. Mol. Syst. Biol., 6, 450.Michel,A.M. et al. (2014) Computational approach for calculating the probability of eukaryotic translation initiation from ribo-seq data that takes intoaccount leaky scanning. BMC Bioinform., 15, 380.Nam,J.-W. et al. (2014) Global analyses of the effect of different cellular contexts on MicroRNA targeting. Mol. Cell, 53, 1031–1043.Noderer,W.L. et al. (2014) Quantitative analysis of mammalian translationinitiation sites by FACS-seq. Mol. Syst. Biol., 10, 748.Peabody,D.S. (1989) Translation initiation at non-AUG triplets in mammaliancells. J. Biol. Chem., 264, 5031–5035.Pedregosa,F. et al. (2011) Scikit-learn: machine learning in Python. J. Mach.Learn. Res., 12, 2825–2830.Quang,D., and Xie,X. (2016) DanQ: a hybrid convolutional and recurrentdeep neural network for quantifying the function of DNA sequences.Nucleic Acids Res., 44, e107.Reuter,K. et al. (2016) PreTIS: a tool to predict non-canonical 5’ UTR translational initiation sites in human and mouse. PLOS Comput. Biol., 12,e1005170.Rumelhart,D.E. et al. (1986) Learning representations by back-propagatingerrors. Nature, 323, 533–536.Simonyan,K. et al. (2013). Deep inside convolutional networks: visualisingimage classiﬁcation models and saliency maps. Workshop at InternationalConference on Learning Representations, CoRR, abs/1312.6034.Sonenberg,N., and Hinnebusch,A.G. (2009) Regulation of translation initiation in eukaryotes: mechanisms and biological targets. Cell, 136, 731–745.Srebro,N. et al. (2005). Maximum-margin matrix factorization. Adv. NeuralInform. Process. Syst., 17, 1329–1336.Srivastava,N. et al. (2014) Dropout: a simple way to prevent neural networksfrom overﬁtting. J. Mach. Learn. Res., 15, 1929–1958.Thomsen,M.C.F., and Nielsen,M. (2012) Seq2Logo: a method for constructionand visualization of amino acid binding motifs and sequence proﬁles includingsequence weighting, pseudo counts and two-sided representation of aminoacid enrichment and depletion. Nucleic Acids Res., 40, W281–W287.Wallace,B. et al. (2011). Class imbalance, redux. In: 2011 IEEE 11thInternational Conference on Data Mining, pp. 754–763.Waterman,M., and Smith,T. (1978) RNA secondary structure: a completemathematical analysis. Math. Biosci., 42, 257–266.Wolf,A. et al. (2011) Single base-pair substitutions at the translation initiationsites of human genes as a cause of inherited disease. Hum. Mutat., 32,1137–1143.Zhang,S. et al. (2015) A deep learning framework for modeling structural features of RNA-binding protein targets. Nucleic Acids Res., 44, e32.Zhang,S. et al. (2016). ROSE: a deep learning based framework for predictingribosome stalling. bioRxiv.Zhou,J., and Troyanskaya,O.G. (2015) Predicting effects of noncoding variants with deep learning-based sequence model. Nat. Methods, 12, 931–934.Zien,A. et al. (2000) Engineering support vector machine kernels that recognize translation initiation sites. Bioinformatics, 16, 799–807.Zur,H., and Tuller,T. (2013) New universal rules of eukaryotic translation initiation ﬁdelity. PLOS Comput. Biol., 9, e1003136.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i234/3953962by gueston 07 January 2018
5028881980002	PMID28881980	5028881980	https://watermark.silverchair.com/btx246.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881980.main.pdf	Alignment of dynamic networksV. Vijayan1, D. Critchlow1,2 and T. Milenkovi c1,*1Department of Computer Science and Engineering, ECK Institute for Global Health, and Interdisciplinary Centerfor Network Science and Applications (iCeNSA), University of Notre Dame, Notre Dame, IN 46556, USA and2Department of Physics and Astronomy, Austin Peay State University, Clarksville, Tennessee, TN 37044, USA*To whom correspondence should be addressed.AbstractMotivation: Network alignment (NA) aims to find a node mapping that conserves similar regionsbetween compared networks. NA is applicable to many fields, including computational biology,where NA can guide the transfer of biological knowledge from well- to poorly-studied speciesacross aligned network regions. Existing NA methods can only align static networks. However,most complex real-world systems evolve over time and should thus be modeled as dynamic net-works. We hypothesize that aligning dynamic network representations of evolving systems willproduce superior alignments compared to aligning the systems’ static network representations, asis currently done.Results: For this purpose, we introduce the first ever dynamic NA method, DynaMAGNAþþ. Thisproof-of-concept dynamic NA method is an extension of a state-of-the-art static NA method,MAGNAþþ. Even though both MAGNAþþ and DynaMAGNAþþ optimize edge as well as nodeconservation across the aligned networks, MAGNAþþ conserves static edges and similarity be-tween static node neighborhoods, while DynaMAGNAþþ conserves dynamic edges (events) andsimilarity between evolving node neighborhoods. For this purpose, we introduce the first evermeasure of dynamic edge conservation and rely on our recent measure of dynamic node conserva-tion. Importantly, the two dynamic conservation measures can be optimized with any state-of-the-art NA method and not just MAGNAþþ. We confirm our hypothesis that dynamic NA is superior tostatic NA, on synthetic and real-world networks, in computational biology and social domains.DynaMAGNAþþ is parallelized and has a user-friendly graphical interface.Availability and implementation: http://nd.edu/ cone/DynaMAGNAþþ/.Contact: tmilenko@nd.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionNetworks can be used to model complex real-world systems in avariety of domains (Boccaletti et al., 2006). Network alignment(NA) compares networks with the goal of finding a node mappingthat conserves topologically or functionally similar regions betweenthe networks. NA has been used in many domains and applications(Emmert-Streib et al., 2016). In computer vision, it has been used tofind correspondences between sets of visual features (Duchenneet al., 2011). In online social networks, NA has been used to matchidentities of people who have different account types (e.g. Twitterand Facebook) (Zhang et al., 2015). In ontology matching, NA hasbeen used to match concepts across ontological networks (Bayatiet al., 2013). Computational biology is no exception. In this domain,NA has been used to predict protein function (including the role ofproteins in aging), by aligning protein interaction networks (PINs)of different species, and by transferring functional knowledge froma well-studied species to a poorly-studied species between the spe-cies’ conserved (aligned) PIN regions (Elmsallati et al., 2016; Faisalet al., 2015a,b; Guzzi and Milenkovi c, 2017; Meng et al., 2016b).Also, NA has been used to construct phylogenetic trees of speciesbased on similarities of their PINs or metabolic networks (Kuchaievet al., 2010; Kuchaiev and Pr zulj, 2011).NA methods can be categorized as local or global (Guzzi andMilenkovi c, 2017; Meng et al., 2016b). Local NA typically findshighly conserved but consequently small regions among comparednetworks, and it results in a many-to-many node mapping. On theother hand, global NA typically finds a one-to-one node mappingbetween compared networks that results in large but consequentlysuboptimally conserved network regions. Clearly, each of local NAand global NA has its (dis)advantages (Guzzi and Milenkovi c, 2017;Meng et al., 2016a,b). In this paper, we focus on global NA, but ourideas are applicable to local NA as well. Also, NA methods can beVC The Author 2017. Published by Oxford University Press. i180This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i180–i189doi: 10.1093/bioinformatics/btx246ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018categorized as pairwise or multiple (Faisal et al., 2015b; Guzzi andMilenkovi c, 2017; Vijayan and Milenkovi c, 2016). Pairwise NAaligns two networks while multiple NA can align more than twonetworks at once. While multiple NA can capture conserved net-work regions between more networks than pairwise NA, which maylead to deeper biological insights compared to pairwise NA, multipleNA is computationally much harder than pairwise NA since thecomplexity of the NA problem typically increases exponentiallywith the number of networks. This is why in this paper we focus onpairwise NA, but our ideas can be extended to multiple NA as well.Henceforth, we refer to global and pairwise NA simply as NA.Existing NA methods can only align static networks. This is be-cause in many domains and applications, static network representa-tions are often used to model complex real-world systems,independent of whether the systems are static or dynamic. However,most real-world systems are dynamic, as they evolve over time.Static networks cannot fully capture the temporal aspect of evolvingsystems. Instead, such systems can be better modeled as dynamicnetworks (Holme, 2015). For example, a complex system such as asocial network evolves over time as friendships are made and lost.Static networks cannot model the changes in interactions betweennodes over time, while dynamic networks can capture the times dur-ing which the friendships begin and end. Other examples of systemsthat can be more accurately represented as dynamic networks in-clude communication systems, human or animal proximity inter-actions, ecological systems, and many systems in biology that evolveover time, including brain or cellular functioning. In particular, re-garding the latter, while cellular functioning is dynamic, currentcomputational methods (including all existing NA methods) for ana-lyzing systems-level molecular networks, such as PINs, deal with thenetworks’ static representations. This is in part due to unavailabilityof experimental dynamic molecular network data, owing to limita-tions of biotechnologies for data collection. Yet, as more dynamicmolecular (and other real-world) network data are becoming avail-able, there is a growing need for computational methods that arecapable of analyzing dynamic networks (Przytycka and Kim, 2010;Przytycka et al., 2010), including methods that can align suchnetworks.The question is: how to align dynamic networks, when the exist-ing NA methods can only deal with static networks? To allow forthis, we generalize the notion of static NA to its dynamic counter-part. Namely, we define dynamic NA as a process of comparing dy-namic networks and finding similar regions between such networks,while exploiting the temporal information explicitly (unlike staticNA, which ignores this information). We hypothesize that aligningdynamic network representations of evolving real-world systemswill produce superior alignments compared to aligning the systems’static network representations, as is currently done. To test this hy-pothesis, we introduce the first ever method for dynamic NA.Our proposed dynamic NA method, DynaMAGNAþþ, is aproof-of-concept extension of a state-of-the-art static NA method,MAGNAþþ (Vijayan et al., 2015). Saraph and Milenkovi c (2014)and Vijayan et al. (2015) compared MAGNAþþ to state-of-the-artstatic NA methods at the time: IsoRank (Singh et al., 2007), MI-GRAAL (Kuchaiev and Pr zulj, 2011), and GHOST (Patro andKingsford, 2012). The comparisons were made on synthetic as wellas real-world PINs, in terms of both topological and functionalalignment quality. MAGNAþþ resulted in higher-quality align-ments than the other methods in all of the comparison tests. Morerecently, Meng et al. (2016b) compared in the same mannerMAGNAþþ to additional newer static NA methods: NETAL(Neyshabur et al., 2013), GEDEVO (Ibragimov et al., 2013),WAVE (Sun et al., 2015), and L-GRAAL (Malod-Dognin andPr zulj, 2015). On synthetic networks, only MAGNAþþ and WAVEproduced high-quality alignments across all comparison tests, unlikethe other methods. On real-world PINs, the best method varied de-pending on the comparison test, but overall, MAGNAþþ andL-GRAAL produced the highest-quality alignments in most of thetests. Hence, MAGNAþþ was the only top-performing method forboth synthetic and real-world networks. This is exactly why wehave chosen to extend MAGNAþþ rather than some other staticNA method to its dynamic counterpart. However, as any futurestatic NA methods are developed (Mamano and Hayes, 2017) thatare potentially superior to MAGNAþþ, our ideas on dynamic NAwill be applicable to such methods too. Section 2 describes themethod, and Section 3 confirms our hypothesis that dynamic NA issuperior to static NA, under fair comparison conditions, on bothsynthetic and real-world networks, and on data from both computa-tional biology and social network domains.2 Materials and methodsWe first describe MAGNAþþ and then its DynaMAGNAþþextension.2.1 MAGNA11Static networks and static NA. A static network G(V, E) consists ofa node set V and an edge set E. An edge ðu; vÞ 2 E is an interactionbetween nodes u and v. There can only be a single edge between thesame pair of nodes. Given two static networks G1ðV1;E1Þ andG2ðV2;E2Þ, assuming without loss of generality that jV1j   jV2j, astatic NA between G1 and G2 is a one-to-one node mappingf : V1 ! V2, which produces the set of aligned node pairs fðv; f ðvÞÞjv 2 V1g (Fig. 1a).Static edge conservation. Given an NA between two static net-works, an edge in one network is conserved if it maps to an edge inthe other network, and an edge in one network is non-conserved if itmaps to a non-adjacent node pair (i.e. a non-edge) in the other net-work (Fig. 1a). A good static NA is a node mapping that conservessimilar network regions. That is, a good static NA should have alarge number of conserved edges and a small number of non-conserved edges. In this context, we measure the quality of a staticNA using the popular symmetric substructure score (S3) edge conser-vation measure (Saraph and Milenkovi c, 2014).S3 is defined as follows. Formally, the number of conserved edges isNc ¼Xðu;vÞ2V1 V11½ðu; vÞ 2 E1 ^ ðf ðuÞ; f ðvÞÞ 2 E2 ;and the number of non-conserved edges isNn ¼Xðu;vÞ 2 V1   V11½ ððu; vÞ 2 E1 ^ ðf ðuÞ; f ðvÞÞ 62 E2Þ _ððu; vÞ 62 E1 ^ ðf ðuÞ; f ðvÞÞ 2 E2Þ  ¼ jE1j þ jE02j   2Nc;where G02ðV 02; E02Þ is the subgraph of G2 induced by V 02 ¼ ff ðuÞju2 V1g; 1½p  ¼ 1 if p is true and 1½p  ¼ 0 if p is false, and U  V is theCartesian product of sets U and V.Then, S3 ¼ NcNcþNn. Supplementary Algorithm S1 describes our S3implementation that has OðjE1j þ jE2jÞ time complexity.Static node conservation. A good static NA should also conservethe similarity between aligned node pairs, i.e. node conservation.Node conservation accounts for similarities between all pairs ofAlignment of dynamic networks i181Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018nodes across the two networks. Node similarity can be defined in away that depends on one’s goal or domain knowledge. In this work,we use a node similarity measure that is based on graphlets, asfollows.Graphlets (in the static setting) are small, connected, inducedsubgraphs of a larger static network (Milenkovi c and Pr zulj, 2008).Graphlets can be used to describe the extended network neighbor-hood of a node in a static network via the node’s graphlet degreevector (GDV). The GDV generalizes the degree of the node, whichcounts how many edges are incident to the node, i.e. how manytimes the node touches an edge (where an edge is the only graphleton two nodes), into the vector of graphlet degrees (i.e. GDV), whichcounts how many times the node touches each of the graphlets onup to n nodes, accounting in the process for different topologicallyunique node symmetry groups (automorphism orbits) that mightexist within the given graphlet. In this work, we use all graphletswith up to four nodes, which contain 15 automorphism orbits,when calculating the GDV of a node, per recommendations of theexisting studies (Hulovatyy et al., 2015, 2014). Hence, the GDV ofa node has 15 dimensions containing counts for the 15 orbits.Given GDVs of all nodes in two static networks G1ðV1; E1Þ andG2ðV2;E2Þ, where xu is the GDV of node u, we calculate similaritys(u, v) between nodes u 2 V1 and v 2 V2 by relying on an existingGDV-based measure of node similarity that was used by Hulovatyyet al. (2015). The measure works as follows. First, to extract GDVdimensions that contain the most relevant information about the ex-tended network neighborhood of the given node, the measure re-duces dimensionality of each GDV via principal component analysis(PCA). PCA is performed on the vector set fxwjw 2 V1 [ V2g, wherewe keep as few of the first k PCA components as needed to accountfor at least 99% of variance in the vector set. Let us denote by yu thedimensionality-reduced vector of xu that contains the k PCA compo-nents. Second, we define node similarity s(u, v) as the cosine similar-ity between yu and yv. Third, given a static NA f, we define our nodeconservation measure asPu2V1sðu;f ðuÞÞjV1 j .Objective function and optimization process (or search strategy).MAGNAþþ is a search-based algorithm that finds a static NA bydirectly maximizing both edge and node conservation. Namely, itmaximizes the objective function aSE þ ð1   aÞSN, where SE is theS3 measure of static edge conservation described above, SN is thegraphlet-based measure of static node conservation described above,and a is a parameter between 0 and 1 that controls for the two meas-ures. In several studies, it was shown that a of 0.5 yields the best re-sults (Meng et al., 2016b; Vijayan et al., 2015), which is the a valuewe use in this study, unless otherwise noted. Given an initial popula-tion of random static NAs, MAGNAþþ evolves the population ofalignments over a number of generations while aiming to maximizeits objective function. MAGNAþþ then returns the alignment fromthe final generation that has the highest value of the objectivefunction.2.2 DynaMAGNA11Dynamic networks. A dynamic network H(V, T) consists of a nodeset V and an event set T, where an event is a temporal edge (Fig. 1b).An event is a 4-tuple ðu; v; ts; teÞ, where nodes u and v interact fromtime ts to time te. An event is active at time t if ts   t   te. The dur-ation of an event is the time during which an event is active, i.e.,te   ts. There can be multiple events between the same two nodes inthe dynamic network, but no two events between the same twonodes may be active at the same time. In fact, if there are two eventsbetween the same two nodes that are active at the same time, thenthey must be combined into a single event.In the above representation of a dynamic network that our NAmethod uses, time is captured in a continuous manner (i.e. eachevent has a duration). However, dynamic network data is often pro-vided in a different representation, as a discrete temporal sequenceof static network snapshots G1ðV1; E1Þ; . . . ;GkðVk; EkÞ. We can eas-ily convert the static snapshot-based representation of a dynamic net-work into our event duration-based representation (i.e. into H(V, T)as defined above). We do this as follows: if there is an edgeFig. 1. (a) Two static networks G1ðV1; E1Þ and G2ðV2; E2Þ (where edges be-tween nodes in the same network are denoted by solid lines), and a static NAbetween them (in this case, ui maps to vi for i ¼ 1; . . . ; 4, as shown by the dot-ted arrows). An edge is conserved if it maps to another edge (e.g. edge (u1,u2) maps to edge (v1, v2)). An edge is non-conserved if it maps to a non-adja-cent (disconnected) node pair (e.g. edge (u1, u4) maps to a disconnected nodepair (v1, v4)). All conserved edges are shown in green, and all non-conservededges are shown in red. (b) Two dynamic networks H1ðV1; T1Þ and H2ðV2; T2Þ.If two nodes interact at least once during the network’s lifetime (i.e. if there isat least one event between the nodes), there is a solid line between the nodes.A given solid line can capture multiple events. Each event is represented as(ts, te), where ts is its start time, and te is its end time. For example, the eventbetween u3 and u4 is active from start time 8 to end time 10. A dynamic NA isa node mapping between the two networks (in this case, ui maps to vi fori ¼ 1; . . . ; 4, as shown by the dotted arrows). (c) Illustration of the conservedevent time (CET) and non-conserved event time (NCET) of the mapping ofnode pair (u1, u2) to node pair (v1, v2). On the left are node pair (u1, u2) fromdynamic network H1ðV1; T1Þ and node pair (v1, v2) from dynamic networkH2ðV2 ; T2Þ, where (u1, u2) maps to (v1, v2). For each of the two node pairs (u1,u2) and (v1, v2), the event times of the given node pair are visualized by theplot on the right, where the given solid line in the plot indicates the start timeto the end time of the given event (i.e. the period of time during which thegiven node pair is active). Given the above, the CET between (u1, u2) and (v1,v2) is the amount of time during which both (u1, u2) and (v1, v2) are active atthe same time (the green area). Similarly, the NCET between (u1, u2) and (v1,v2) is the amount of time during which exactly one of (u1, u2) or (v1, v2) is ac-tive (the red area). We calculate the CET between these two illustrated nodepairs as follows. Since the events ðu1; u2; 1; 4Þ and ðv1; v2; 2; 5Þ are both activefrom time 2 to time 4 for a duration of 4  2 ¼ 2, the events ðu1; u2; 8; 11Þ andðv1; v2; 7; 10Þ are both active from time 8 to time 10 for a duration of10  8 ¼ 2, and the events ðu1; u2; 13; 18Þ and ðv1; v2; 14; 17Þ are both activefrom time 14 to time 17 for a duration of 17  14 ¼ 3, the total CET between(u1, u2) and (v1, v2) is 2þ 2þ 3 ¼ 7. We calculate the NCET between these twoillustrated node pairs as follows. We know that (u1, u2) is active during timeperiods 1 to 4, 8 to 11, and 13 to 18, totaling a duration ofð4  1Þ þ ð11  8Þ þ ð18  13Þ ¼ 11, and that (v1, v2) is active during time peri-ods 2 to 5, 7 to 10, and 14 to 17, totaling a duration ofð5  2Þ þ ð10  7Þ þ ð17  14Þ ¼ 9. Since NCET is the amount of time duringwhich (u1, u2) is active, or (v1, v2) is active, but not both, we need to add upthe time during which either node pair is active, and subtract the time duringwhich both node pairs are active (making sure to subtract twice to avoid dou-ble counting, because of the “but not both” constraint). Since the time duringwhich both node pairs are active is the CET, the NCET between (u1, u2) and(v1, v2) is 11þ 9  2 7 ¼ 6i182 V.Vijayan et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018connecting two nodes in the tth snapshot of the snapshot-based rep-resentation, then there is an event between the two nodes that is ac-tive from time t to time tþ1 in the event duration-basedrepresentation. In other words, we combine the node sets of thesnapshots into a single node set V ¼ V1 [ . . . [ Vk. Then, for eachsnapshot Gt, t ¼ 1; . . . ; k, we convert each edge ðu; vÞ 2 Et into anevent between nodes u 2 V and v 2 V in the dynamic network H(V,T) with start time t and end time tþ1, i.e., the event ðu; v; t; t þ 1Þ.This allows us to use the snapshot-based representation of a dy-namic network in our study.Dynamic NA. Given two dynamic networks H1ðV1; T1Þ andH2ðV2;T2Þ, assuming without loss of generality that jV1j   jV2j, adynamic NA between H1 and H2 is a one-to-one node mappingf : V1 ! V2, which produces the set of aligned node pairs fðv; f ðvÞÞjv 2 V1g (Fig. 1b). Note the similarity between the definitions ofstatic NA and dynamic NA (although the process of finding the ac-tual alignments is different). This makes static NA and dynamic NAfairly comparable.Dynamic edge (event) conservation. First, given node pair (u1,u2) in H1 that maps to node pair (v1, v2) in H2 (Fig. 1c), we extendthe notion of a conserved or non-conserved edge from static NA todynamic NA by accounting for the amount of time that the mappingof (u1, u2) to (v1, v2) is conserved or non-conserved (defined below).That is, we extend the notion of a conserved or non-conserved staticedge to the amount of a conserved or non-conserved dynamic edge(event), as follows.We define the amount of a conserved event as follows.Similar to how an edge ðu01;u02Þ in static network G1ðV1; E1Þ isconserved if it maps to an edge ðv01; v02Þ in static networkG2ðV2;E2Þ (and vice versa), the mapping of (u1, u2) to (v1, v2) isconserved at time t if both (u1, u2) and (v1, v2) are active at time t.We refer to the entire amount of time during which this mapping isconserved as the conserved event time (CET) between (u1, u2) and(v1, v2). In other words, it is the amount of time during which both(u1, u2) and (v1, v2) are active at the same time. Formally, let Tu1u2be the set of events between u1 and u2, and let Tv1v2 be the setof events between v1 and v2. Then, the CET between (u1, u2) and(v1, v2) isCETððu1; u2Þ; ðv1; v2ÞÞ ¼Xe2Tu1u2Xe02Tv1v2ctðe; e0Þ;where the conserved time ctðe; e0Þ ¼ maxð0; minðte; t0eÞ   maxðts; t0sÞÞis the amount of time during which events e ¼ ðu1; u2; ts; teÞ ande0 ¼ ðv1; v2; t0s; t0eÞ are active at the same time, i.e., ctðe; e0Þ is thelength of the overlap of the intervals ½ts; te  and ½t0s; t0e .We define the amount of a non-conserved event as follows.Similar to how an edge ðu01;u02Þ in G1 is non-conserved if it maps toa disconnected node pair ðv01; v02Þ in G2 (or vice versa), the mappingof (u1, u2) to (v1, v2) is non-conserved at time t if exactly one of (u1,u2) or (v1, v2) is active at time t. We refer to the entire amount oftime during which this mapping is non-conserved as the non-con-served event time (NCET) between (u1, u2) and (v1, v2). In otherwords, it is the amount of time during which (u1, u2) is active, or(v1, v2) is active, but not both are active at the same time. Formally,the NCET between (u1, u2) and (v1, v2) isNCETððu1;u2Þ; ðv1; v2ÞÞ¼Xe2Tu1u2dðeÞ þXe02Tv1v2dðe0Þ   2Xe2Tu1u2Xe02Tv1v2ctðe; e0Þ;where d(e) is the duration of event e, i.e., the amount of time duringwhich e is active. We make sure to subtract twice the amount oftime during which (u1, u2) and (v1, v2) are both active due to theabove “but not both are active at the same time” constraint.Second, given these definitions of CET and NCET between twonode pairs (u1, u2) and (v1, v2), we extend the S3 measure of staticedge conservation to a new dynamic S3 (DS3) measure of dynamicedge (event) conservation. To define DS3, we need to introduce thenotion of CET between all node pairs across the entire alignment(rather than between just two aligned node pairs), henceforth simplyreferred to as alignment CET, which is the sum of CET between allnode pair mappings between H1 and H2. Also, we need to define thenotion of alignment NCET, which is the sum of NCET between allnode pair mappings between H1 and H2. Alignment CET measuresthe amount of event conservation of the entire alignment and align-ment NCET measures the amount of event non-conservation of theentire alignment. A good dynamic NA is a node mapping that con-serves similar evolving network regions. That is, a good dynamicNA should have high alignment CET and low alignment NCET,which is what DS3 aims to capture. Formally, alignment CET isTc ¼Xðu;vÞ2V1 V1CETð ðu; vÞ; ðf ðuÞ; f ðvÞÞ Þand alignment NCET isTn ¼Xðu;vÞ2V1 V1NCETð ðu; vÞ; ðf ðuÞ; f ðvÞÞ Þ:Then, DS3 ¼ TcTcþTn. Supplementary Algorithm S2 describes ourDS3 implementation that has OðjT1j þ jT2jÞ time complexity.We note that there are many real-world networks that containevents with durations that are significantly less than the entire timewindow of the network, called “bursty” events. Examples of net-works containing bursty events are e-mail communication networks,economic networks that model transactions, and brain networksconstructed from oxygen level correlations as measured by fMRIscanning, each of whose events last much less than a second whilethe networks’ time windows span minutes to hours (Holme, 2015).Since bursty events are so short, small perturbations in the eventtimes can greatly affect the resulting dynamic edge (event) conserva-tion value. Thus, in order to allow our DS3 measure to be more ro-bust to perturbations of up to Dt in the event times, one may simplyextend the duration of each event in the network by 2Dt. This is dueto the following. Given two events ðu1; u2; t; tÞ and ðv1; v2; t0; t0Þ withdurations of 0, where t0 ¼ t þ Dt, the conserved time ctð ;  Þ betweenthe two events is 0. Thus, if we want to consider the two events asconserved, we can increase the durations of both events by 2Dt tocreate the modified events ðu1; u2; t   Dt; t þ DtÞ andðv1; v2; t0   Dt; t0 þ DtÞ, which results in a conserved time of Dt forthe two modified events. While we do not use this technique in ourwork since we do not use networks with bursty events, others mightin the future, and if so, this needs to be considered when performingdynamic NA.Dynamic node conservation. Just as for static NA, a good dy-namic NA method should also conserve the similarity betweenaligned node pairs, i.e. node conservation. To take advantage of thetemporal information encoded in dynamic networks that are beingaligned and also to make dynamic NA as fairly comparable as pos-sible to static NA, in this work, we rely on a measure of node simi-larity based on dynamic graphlets, as follows.Dynamic graphlets are an extension of static graphlets (Section2.1) to the dynamic setting. While static graphlets can be used tocapture the static extended network neighborhood of a node, dy-namic graphlets can be used to capture how the extendedAlignment of dynamic networks i183Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018neighborhood of a node changes over time. To define dynamicgraphlets, we first present the notion of a Dt-time-respecting pathand a Dt-connected network. A Dt-time-respecting path is a se-quence of events that connect two nodes such that for any two con-secutive events in the sequence, the end time of the earlier event andthe start time of the later event are within Dt time of each other (i.e.,are Dt-adjacent). A dynamic network is Dt-connected if for each pairof nodes in the network, there is a Dt-time-respecting path betweenthe two nodes. Then, just as a static graphlet is an equivalence classof isomorphic connected subgraphs (Section 2.1), a dynamic graph-let is an equivalence class of isomorphic Dt-connected dynamic sub-graphs, where two graphlets are equivalent if they both have thesame relative temporal order of events. We use Dt ¼ 1, per recom-mendations by Hulovatyy et al. (2015). Just as the GDV of a node ina static network is a topological descriptor for the extended neigh-borhood of the node, there exists the dynamic GDV (DGDV) of anode in a dynamic network, which describes how the extendedneighborhood of a node changes over time. Specifically, just as theGDV of a node counts how many times the node touches each staticgraphlet at each of its automorphism orbits, the DGDV of a nodecounts how many times the node touches each dynamic graphlet ateach of its orbits. Dynamic graphlets have a similar notion of orbitsas static graphlets do, which now depend on both topological andtemporal positions of a node within the dynamic graphlet. To makethings fairly comparable to static NA, and per recommendations byHulovatyy et al. (2015), we use dynamic graphlets with up to fournodes and six events, which contain 3727 automorphism orbits.Hence, the DGDV of a node has 3727 dimensions containing countsfor the 3727 orbits.Given the DGDVs of all nodes in two dynamic networks H1ðV1;T1Þ and H2ðV2; T2Þ, just as in Section 2.1, we calculate similaritys(u, v) between nodes u 2 V1 and v 2 V2 and then rely on cosinesimilarities between the PCA-based dimensionality-reduced DGDVsto obtain the total dynamic node conservation.Objective function and optimization process (also known assearch strategy). DynaMAGNAþþ is a search-based algorithm thatfinds a dynamic NA by directly maximizing both dynamic edge(event) and node conservation. Namely, DynaMAGNAþþ maxi-mizes the objective function aST þ ð1   aÞSN, where ST is the DS3measure of dynamic edge conservation described above, SN is theDGDV-based measure of dynamic node conservation describedabove, and a is a parameter between 0 and 1 that controls for thetwo measures. To make DynaMAGNAþþ fairly comparable toMAGNAþþ, here we also use MAGNAþþ’s best a value of 0.5,unless otherwise noted. Given an initial population of random dy-namic NAs, DynaMAGNAþþ evolves the population of alignmentsover a number of generations while aiming to maximize its objectivefunction. DynaMAGNAþþ then returns the alignment from thefinal generation that has the highest value of the objective function.Time complexity. To align two dynamic networks H1ðV1; T1Þand H2ðV2; T2Þ, DynaMAGNAþþ evolves a population of p align-ments over N generations. It does so by using its crossover function(see Saraph and Milenkovi c (2014) for details) to combine pairs ofparent alignments in the given population into child alignments, foreach generation. For each generation, the dynamic edge (event) con-servation, dynamic node conservation, and crossover of O(p) align-ments are calculated. Since dynamic edge conservation takesOðjT1j þ jT2jÞ to compute, dynamic node conservation takes OðjV1jÞtime to compute, crossover takes OðjV2jÞ time to compute, andjV1j   jV2j, the time complexity of DynaMAGNAþþ isOðNpjV2j þNpðjT1j þ jT2jÞÞ. Note that the calculation of dy-namic edge and dynamic node conservation in DynaMAGNAþþ isparallelized. This allows DynaMAGNAþþ to be run on multiplecores, which empirically results in close to linear speedup.Other parameters. Given an initial population of dynamic NAs,DynaMAGNAþþ evolves the population for up to a specified num-ber of generations or until it reaches a stopping criterion. For eachgeneration, DynaMAGNAþþ keeps an elite fraction of alignmentsfrom the current generation’s population for the next generation’spopulation. In addition to the dynamic edge and node conservationmeasures, and the a parameter that controls for the contribution ofthe two measures, the remaining parameters of DynaMAGNAþþare (i) the initial population, (ii) the size of the population, (iii), themaximum number of generations, (iv) the elite fraction, and (v) thestopping criterion. For DynaMAGNAþþ, we use a population of15 000 alignments initialized randomly, as in the originalMAGNAþþ paper. We specify a maximum of 10 000 generations,since the alignments that we test all converge by 10 000 generations.The elite fraction is 0.5, as in the original MAGNAþþ paper. Thealgorithm stops when the highest objective function value in thepopulation has increased less than 0.0001 within the last 500 gener-ations, since the alignments that we test do not increase by a signifi-cant amount after this point.To fairly compare DynaMAGNAþþ against MAGNAþþ, weaim to set the parameters of both methods to be as similar as pos-sible. So, other than MAGNAþþ’s edge and node conservationmeasures, the remaining parameters of MAGNAþþ are the same asfor DynaMAGNAþþ. This way, any differences that we see be-tween results of DynaMAGNAþþ and results of MAGNAþþ willbe the consequence of the differences of the two methods’ edge andnode conservation measures, i.e. of accounting for temporal infor-mation in the network with DynaMAGNAþþ and ignoring this in-formation with MAGNAþþ. In other words, any differences thatwe see between results of DynaMAGNAþþ and results ofMAGNAþþ will fairly reflect differences between dynamic NA andstatic NA.3 Results and discussionSince there are no other dynamic NA methods to compare against,we compare DynaMAGNAþþ to the next best option, namely itsstatic NA counterpart. That is, we compare DynaMAGNAþþwhen it is used to align two dynamic networks, to MAGNAþþwhen it is used to align static versions of the two dynamic networks.By “static versions”, we mean that we “flatten” or “aggregate” adynamic network into a static network that will have the same set ofnodes as the dynamic network and a static edge will exist betweentwo nodes in the static network if there is at least one event betweenthe same two nodes in the dynamic network. This network aggrega-tion simulates the common practice where network analysis oftime-evolving systems is done in a static manner, by ignoringtheir temporal information (Holme, 2015). We evaluateDynaMAGNAþþ and MAGNAþþ on synthetic and real-world dy-namic networks, as follows.3.1 Evaluation using synthetic networksMotivation. A good NA approach should be able to produce high-quality alignments between networks that are similar and low-quality alignments between networks that are dissimilar (Yavero gluet al., 2015). In this test on synthetic networks, “similar” means net-works that originate from the same network model, and “dissimi-lar” means networks that originate from different network models.So, we refer to this test as network discrimination. Thus, in thisi184 V.Vijayan et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018section, we evaluate the network discrimination performance ofDynaMAGNAþþ and MAGNAþþ.Data. We perform this evaluation on a set of biologicallyinspired synthetic networks. Specifically, we generate 20 dynamicnetworks using four biologically inspired network evolution models(or versions of the same model with different parameter values) thatsimulate the evolution (i.e. growth) of PINs, resulting in five net-works per model (Hulovatyy et al., 2015). The four models that weuse are (i) GEO-GD with p¼0.3, (ii) GEO-GD with p¼0.7, (iii)SF-GD with p¼0.3 and q¼0.7, and (iv) SF-GD with p¼0.7 andq¼0.6, where GEO-GD is a geometric gene duplication model withprobability cut-off and SF-GD is a scale-free gene duplication model(Pr zulj et al., 2010). Hulovatyy et al. (2015) generalized the staticversions of these models to their dynamic counterparts, and we relyon the same model networks as those used by Hulovatyy et al.(2015) (see their paper for details). Intuitively, each of the 20 syn-thetic networks is represented as a sequence of snapshots, wheresnapshot size increases with time, as illustrated in SupplementaryFigure S1. The final (largest) snapshot of each synthetic network has1000 nodes, and the number of edges varies depending on the differ-ent parameter values of the considered network models.Representative statistics describing the final snapshot of each of thesynthetic networks, such as their sizes or degree distributions, areshown in Supplementary Table S1 and Supplementary Figure S2.To illustrate generalizability of dynamic NA to other domains,we also perform this evaluation on a set of social synthetic networksgenerated using different parameter values of a social network evo-lution model (due to space constraints, we refer the reader toSupplementary Section S1.1, Supplementary Table S2, andSupplementary Figs S3–S4 for details).Evaluation measures. We calculate performance of each methodas follows. We align all possible pairs of the synthetic networks. Thehigher the alignment quality between pairs of similar networks (i.e.networks coming from the same model) and the lower the alignmentbetween pairs of dissimilar networks (i.e. networks coming from dif-ferent models), the better the NA method. Here, by alignment qual-ity between two networks that the given method identifies, we meanthe method’s objective function value for the alignment of the twonetworks that is returned by the method (Section 2). Given the align-ment quality values for all network pairs, we summarize the givenmethod’s performance using precision-recall and receiver operatingcharacteristic (ROC) frameworks. For some given threshold r,a good NA method should result in alignment quality greater thanr for pairs of similar networks and in alignment quality smaller thanr for pairs of dissimilar networks. So, for a given threshold r, wecompute accuracy in terms of precision, the fraction of networkpairs that are similar and with alignment quality greater than r outof all network pairs with alignment quality greater than r, and re-call, the fraction of network pairs that are similar and with align-ment quality greater than r out of all similar network pairs. Varyingthe threshold r for all r 0 (i.e. for r between 0 and the maximumobserved alignment quality value, in increments of the smallest dif-ference between any pair of observed alignment quality values) givesus the precision-recall curve. Then, we compute the area under theprecision-recall curve (AUPR), the F-score (harmonic mean of preci-sion and recall) at which precision and recall cross and are thusequal (F-scorecross), and the maximum F-score over all threshold rvalues (F-scoremax). For a given threshold r, we also computemethod accuracy in terms of sensitivity, which is the same as recall,and specificity, the fraction of network pairs that are dissimilar andwith alignment quality less than r out of all network pairs that aredissimilar. Varying the threshold r for all r 0 gives us the receiveroperating characteristic (ROC) curve. Then, we compute the areaunder the ROC curve (AUROC).Results. First, we aim to test whether optimizing both dynamicedge (event) conservation and dynamic node conservation inDynaMAGNAþþ is better than optimizing either dynamic edgeconservation alone or dynamic node conservation alone, since it wasshown for MAGNAþþ that optimizing both static edge conserva-tion and static node conservation performs better than optimizingany one of static edge conservation or static node conservation alone(Meng et al., 2016b; Vijayan et al., 2015). So, we compare three dif-ferent versions of DynaMAGNAþþ that differ in their optimizationfunctions. The three versions optimize: (i) a combination of dynamicedge conservation and dynamic node conservation (correspondingto a¼0.5, named DynaMAGNAþþ (EþN)), (ii) dynamic edgeconservation only (corresponding to a¼1, named DynaMAGNAþþ (E)), and (iii) dynamic node conservation only (corresponding toa¼0, named DynaMAGNAþþ (N)) (Section 2). We find that whileDynaMAGNAþþ (N) performs the best for biological synthetic net-works (Table 1, Supplementary Fig. S5, and Supplementary TableS3), it performs the worst for the social synthetic networks (Table 2,Supplementary Fig. S6, and Supplementary Table S4). Similarly,DynaMAGNAþþ (E) performs the best for synthetic social net-works (Table 2) but the worst for biological synthetic networks(Table 1). On the other hand, DynaMAGNAþþ (EþN) consist-ently performs well (though not the best) in both cases. Because ofthis, and because DynaMAGNAþþ (EþN) is the best of all threeversions for all analyzed real-world networks (as we show in Section3.2), in the main paper, we only report results for DynaMAGNAþþ(EþN) and refer to it simply as DynaMAGNAþþ. We report re-sults for DynaMAGNAþþ (E) and DynaMAGNAþþ (N) in theSupplement (Supplementary Section S1.2). This way, we fairly com-pare DynaMAGNAþþ and MAGNAþþ, both using a¼0.5.Second, and most importantly, we test whether dynamic NA is su-perior to static NA, by comparing the network discrimination perform-ance of DynaMAGNAþþ and MAGNAþþ. Indeed, for biologicalsynthetic networks, DynaMAGNAþþ outperforms MAGNAþþ withrespect to all considered NA quality measures (Fig. 2, Table 3,Supplementary Fig. S5, and Supplementary Table S3). Similar results(superiority of DynaMAGNAþþ over MAGNAþþ) also hold for thesocial synthetic networks (Supplementary Fig. S6 and SupplementaryTable S4).In summary, under fair comparison conditions, we demonstratethat dynamic NA is superior to static NA for synthetic dynamicnetworks.3.2 Evaluation using real-world networksMotivation. Here, we still evaluate whether the given method pro-duces high-quality alignments for similar networks and low-qualityFig. 2. Network discrimination performance of DynaMAGNAþþ andMAGNAþþ for biological synthetic networks with respect to (a) precision-re-call curve and (b) ROC curveAlignment of dynamic networks i185Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018alignments for dissimilar networks. However, the notion of similar-ity that we use here is different than for the synthetic networksabove, because for real-world networks, we do not know which net-work models they belong to (Yavero glu et al., 2015). Specifically,we align an original real-world network to its randomized (noisy)versions (see below), where we vary the noise level. The larger thenoise level, the more dissimilar the aligned networks are, and thus,the lower the alignment quality should be.Zebra network. Since there is a lack of available dynamic mo-lecular networks (Section 1), we begin our evaluation of real-worldnetworks on an alternative biological network type — an ecologicalnetwork. The original real-world network that we use is the Grevy’szebra proximity network (Rubenstein et al., 2015), which containsinformation on interactions between 27 zebras in Kenya over 58days. The data was collected by driving a predetermined route eachday while searching for herds. There are 779 events in the network.We also report results for another animal proximity network thatcontains information on interactions between 28 onagers, a speciesthat is closely related to the Grevy’s zebra, mostly in the Supplementdue to space constraints (Supplementary Section S1.3). The onagerproximity network contains 28 nodes and 522 events.Since the difference between dynamic NA and static NA is thatthe former accounts for the temporal aspect of the data more expli-citly than the latter, to properly validate results for dynamic NA, asstrict randomization scheme as possible should be used when creat-ing randomized (noisy) versions of the original dynamic networkthat will be aligned to the original network. By “as strict as pos-sible”, we mean that we want to use a randomization scheme thatpreserves as much structure (i.e. topology) as possible of thedynamic network and randomizes only the temporal aspect of thenetwork. This way, the only difference observed betweenDynaMAGNAþþ’s and MAGNAþþ’s performance will be theconsequence of considering the temporal aspect of the data. For thisreason, we randomize the original network using the followingmodel per recommendations by Holme (2015). In order to random-ize the original dynamic network H(V, T) to a certain noise level,first, we arbitrarily number all m events in the network asT ¼ fe1; e2; . . . ; emg. Then, for each event ei, with probability p(where p is the noise level) we randomly select another eventej; j 6¼ i, and swap the time stamps of the two events. Since we onlyswap the time stamps, this randomization scheme conserves the totalnumber of events and the structure of the flattened version of theoriginal dynamic network. We study 10 different noise levels, from0% to 100% in smaller increments initially and larger incrementstoward the end (clearly, at the 0% noise level, the aligned networksare identical). For each noise level, we generate five randomized ver-sions of the original network and report results averaged over thefive randomization runs.We evaluate DynaMAGNAþþ and MAGNAþþ’s performanceas follows. First, for a good method, alignment quality should de-crease as the noise level increases, since the original network and itsrandomized version become more dissimilar with this increase. As inSection 3.1, one measure of alignment quality that we use is eachmethod’s objective function. Another measure that we use is nodecorrectness. Node correctness of an alignment is the fraction of cor-rectly aligned node pairs (according to the ground truth node map-ping) out of all aligned node pairs. Given that our original networkand its randomized versions have the same set of nodes, we knowwhich nodes in the original network correspond to which nodes inthe given randomized network. That is, we know the ground truth(or perfect) mapping between the aligned networks. Hence, we canmeasure node correctness between the networks. Thus, we evaluateeach method’s alignment quality using the method’s objective func-tion as well as node correctness, with the expectation that for agood method, alignment quality should decrease with increase in thenoise level.Second, since we know the perfect alignment between the ori-ginal network and each of its randomized versions, we compute the“ideal” alignment quality — the quality of the perfect alignment, asmeasured by DynaMAGNAþþ’s objective function. The expect-ation is that a good method’s alignment quality should mimic wellthe “ideal” quality.Third, we expect DynaMAGNAþþ’s alignment quality to besuperior to MAGNAþþ’s alignment quality with respect to nodecorrectness for lower (meaningful) noise levels, if it is indeed truethat dynamic NA is superior to static NA. We do not expect this su-periority for higher noise levels, since at such noise levels, networksbeing aligned are highly randomized and thus a good method shouldproduce low-quality alignments.Indeed, our results confirm all three of the above expectations(Fig. 3 and Supplementary Fig. S7). Specifically, first,DynaMAGNAþþ’s alignment quality indeed decreases with the in-crease in the noise level with respect to both its objective function(Fig. 3a) as well as node correctness (Fig. 3b). On the other hand,MAGNAþþ’s alignment quality stays constant with increase in thenoise level. In other words, MAGNAþþ produces alignments of thesame quality for low noise levels (where network structure is mean-ingful) as it does for high noise levels (where network structure israndom). Second, DynaMAGNAþþ’s alignment quality followsclosely the quality of the perfect alignments, while MAGNAþþdoes not (Fig. 3a). Third, DynaMAGNAþþ achieves higher nodeTable 1. Network discrimination performance of DynaMAGNAþþ,while optimizing both dynamic edge and node conservation(EþN), dynamic edge conservation alone (E), and dynamic nodeconservation alone (N), for biological synthetic networks, with re-spect to the area under the precision-recall curve (AUPR), F-scoreat which precision and recall cross and are thus equal (F-scorecross),maximum F-score (F-scoremax), and the area under the ROC curve(AUROC)NA method AUPR F-scorecross F-scoremax AUROCDynaMAGNAþþ (EþN) 0.865 0.700 0.771 0.950DynaMAGNAþþ (E) 0.742 0.550 0.762 0.919DynaMAGNAþþ (N) 0.994 0.950 0.962 0.998Note: In each column, the highest score is bolded.Table 2. Equivalent of Table 1 for social synthetic networksNA method AUPR F-scorecross F-scoremax AUROCDynaMAGNAþþ (EþN) 0.908 0.800 0.839 0.959DynaMAGNAþþ (E) 1.000 1.000 1.000 1.000DynaMAGNAþþ (N) 0.751 0.600 0.723 0.883Note: In each column, the highest score is bolded.Table 3. Network discrimination performance ofDynaMAGNAþþ and MAGNAþþ, for biological synthetic net-works, with respect to the same measures as in Table 1NA method AUPR F-scorecross F-scoremax AUROCDynaMAGNAþþ 0.865 0.700 0.771 0.950MAGNAþþ 0.711 0.550 0.645 0.863Note: In each column, the highest score is bolded.i186 V.Vijayan et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018correctness than MAGNAþþ at lower (meaningful) noise levels.This is not a surprise, since DynaMAGNAþþ explicitly uses thetemporal information in the aligned networks, while MAGNAþþdoes not. Thus, in summary, dynamic NA outperforms static NA.We observe similar results for the onager network (SupplementaryFig. S8).The above complete failure of MAGNAþþ to produce align-ments of decreasing quality as the noise level increases is due to thestrict randomization scheme that we use to create the noisy versionsof the original network, which conserves all structure of the flat-tened version of the original dynamic network. Recall that we usethe strict randomization scheme to ensure that the results ofDynaMAGNAþþ are meaningful. Yet, to give as fair advantage aspossible to static NA, we produce a different set of noisy versions ofthe original network using a somewhat more flexible randomizationscheme that does not conserve the structure of the flattened versionof the original dynamic network, per recommendations of Holme(2015). This randomization scheme works as follows. In order torandomize the original dynamic network H(V, T) to a certain noiselevel, first, we arbitrarily number all m events in the network asT ¼ fe1; e2; . . . ; emg. Then, for each event ei, with probability p(where p is the noise level) we randomly select an event ei0 , and werewire the two events. That is, given ei ¼ ðu; v; ts; teÞ andei0 ¼ ðu0; v0; t0s; t0eÞ, we either set ei ¼ ðu; v0; ts; teÞ and ei0 ¼ ðu0; v; ts; teÞwith probability 0.5, or we set ei ¼ ðu; u0; ts; teÞ and ei0 ¼ ðv; v0; ts; teÞwith probability 0.5. If the rewiring creates a loop (i.e., an eventfrom a node to itself) or a multiple link (i.e., duplicate events be-tween the same nodes), then we undo it and randomly select anotherevent ei0 . This randomization scheme conserves the entire set of timestamps of the original network, but it does not preserve the structureof the flattened network. We study 10 different noise levels (from0% to 100% in smaller increments initially and larger incrementstoward the end). For each noise level, we generate five randomizedversions of the original network and report results averaged over thefive randomization runs. While now MAGNAþþ’s alignment qualityalso decreases with increase in the noise level and also MAGNAþþclosely follows the quality of the perfect alignments, as it should (Fig.4a and Supplementary Fig. S9), DynaMAGNAþþ is still superior toMAGNAþþ with respect to node correctness (Fig. 4b), which againimplies that dynamic NA is superior to static NA.Because the results are consistent independent of the randomiza-tion scheme that is used to produce noisy networks, and since thestrict scheme should be used to correctly evaluateDynaMAGNAþþ’s correctness, henceforth, we report results onlyfor the strict randomization scheme.Yeast network. Since there is a lack of available experimentaldynamic molecular networks, we continue our evaluation of real-world networks on the next best available dynamic molecularnetwork option. Namely, we create a dynamic yeast PIN from anartificial temporal sequence of static yeast PINs. Here, the staticPINs that are used as snapshots of the dynamic PIN are all real-world networks, it is just their temporal sequence that is artificial.The sequence consists of six static PIN snapshots: a high-confidenceS. cerevisiae (yeast) PIN with 1004 proteins and 8323 interactions,and five lower-confidence yeast PINs constructed by adding to thehigh-confidence PIN 5%, 10%, 15%, 20%, or 25% of lower-confidence interactions; the interactions are added in order ofdecreasing confidence. Clearly, the five lower-confidence PINs havethe same 1004 nodes as the high-confidence PIN, and the largest ofthe five lower-confidence PINs has 25% more edges than the high-confidence PIN, i.e. 10 403 of them. This network set has been usedin many existing static NA studies (Kuchaiev et al., 2010;Milenkovi c et al., 2010; Kuchaiev and Pr zulj, 2011; Meng et al.,2016b; Saraph and Milenkovi c, 2014; Vijayan and Milenkovi c,2016). When we use the six static PINs as snapshots to form a dy-namic network, we order the six networks from the smallest one interms of the number of edges (i.e. the one of the highest confidence)to the largest one in terms of the number of edges (i.e. the one of thelowest confidence). Since each static PIN contains the same set ofnodes, this simulates a dynamic network that is growing as itevolves, with more and more interactions being added to the net-work over time. When we align the resulting (original) dynamicyeast network to its randomized versions, we find that just as for thezebra network, DynaMAGNAþþ’s alignment quality decreaseswith increase in the noise level, with respect to both its objectivefunction (Fig. 5a) as well as node correctness (Fig. 5b), whileMAGNAþþ’s alignment quality does not change. Further,DynaMAGNAþþ again matches more closely the quality of the per-fect alignments than MAGNAþþ does (Fig. 5a). Finally,DynaMAGNAþþ again produces higher node correctness thanMAGNAþþ for the lower (meaningful) noise levels. Thus, dynamicNA is superior to static NA for the yeast network as well.Enron network. To demonstrate DynaMAGNAþþ’s generaliz-ability on non-biological networks, we continue our evaluation on asocial network. The original network that we use is the Enrone-mail communication network (Priebe et al., 2005), which is basedon e-mail communications of 184 employees in the Enron corpor-ation from 2000 to 2002, made public by the Federal EnergyRegulatory Commission during its investigation. The entire two-year time period is divided into two-month periods so that if there isat least one e-mail sent between two people within a particular two-month period, then there exists an event between the two peopleduring that period. There are 5539 events in the Enron network.Fig. 3. Alignment quality of DynaMAGNAþþ and MAGNAþþ as a function ofnoise level when aligning the original Grevy’s zebra network to randomized(noisy) versions of the original network. Here, the randomization is as strictas possible, as it conserves all structure of the flattened version of the originaldynamic network and only randomly “shuffles” the given percentage (noiselevel) of its event time stamps. Alignment quality is shown with respect to (a)each method’s objective function, and (b) node correctness. “Ideal” in panel(a) shows the quality of perfect alignments (see the text)Fig. 4. Alignment quality of DynaMAGNAþþ and MAGNAþþ for the Grevy’szebra network. The figure can be interpreted in the same way as Figure 3, ex-cept that here, the randomization used to create the noisy networks does notconserve the structure of the flattened version of the original dynamicnetworkAlignment of dynamic networks i187Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018When we align the original network to its randomized versions, wefind that just as for the zebra and yeast networks,DynaMAGNAþþ’s alignment quality decreases with increase inthe noise level, while MAGNAþþ’s alignment quality does notchange (Fig. 6). Further, DynaMAGNAþþ again matches moreclosely the quality of the perfect alignments (Fig. 6a). So, these re-sults again indicate that dynamic NA is superior to static NA.Interestingly, for this network, MAGNAþþ also produces high-quality alignments with respect to node correctness for the low noiselevels, just like DynaMAGNAþþ does (Fig. 6b).On optimizing edge versus node conservation for real-worldnetworks. Here, we briefly come back to the discussion ofwhich of DynaMAGNAþþ (EþN), DynaMAGNAþþ (E), orDynaMAGNAþþ (N) is superior. For all analyzed real-world net-works, DynaMAGNAþþ (EþN) is superior to the other two ver-sions (Table 4), which justifies our choice to mainly report theresults of DynaMAGNAþþ (EþN) throughout the paper.Running time. Recall that the time complexity ofDynaMAGNAþþ is linear with respect to the number of events inthe aligned networks (Section 2.2), while the time complexity ofMAGNAþþ is linear with respect to the number of edges in thealigned networks (Section 2.1). Because there are typically moreevents in a dynamic network than edges in its flattened version, andbecause more computations are involved when calculating eventconservation than when calculating edge conservation,DynaMAGNAþþ is expected to be slower than MAGNAþþ (yet,it is this ability of DynaMAGNAþþ to capture temporal event in-formation that makes it more accurate than MAGNAþþ). Whenusing eight cores to align the yeast network to its 0% randomizedversion, DynaMAGNAþþ takes 2.0 hours, with 2% of this timespent on counting dynamic graphlets, while MAGNAþþ takes0.8 hours, with 11% of this time spent of counting static graphlets(faster implementations for static graphlet counting exist (Ho cevarand Dem sar, 2014)). This makes DynaMAGNAþþ 2.5 times slowerthan MAGNAþþ. Nonetheless, the somewhat slower (yet still verypractical) runtime of DynaMAGNAþþ is justified byDynaMAGNAþþ’s superiority over MAGNAþþ in terms of align-ment quality.DynaMAGNA11’s availability. We implement a friendlygraphical user interface (GUI) for DynaMAGNAþþ (Fig. 7) foreasy use by domain (e.g., biological) scientists. Also, we provide thesource code of DynaMAGNAþþ so that computational scientistsmay potentially extend the work (http://nd.edu/ cone/DynaMAGNAþþ/).4 ConclusionWe introduce the first ever dynamic NA method. We show that ourmethod, DynaMAGNAþþ, produces superior alignments com-pared to its static NA counterpart due to its explicit use of availabletemporal information in dynamic network data. DynaMAGNAþþis a search-based NA method that can optimize any alignment qual-ity measure. In this work, we propose an efficient temporalinformation-based alignment quality measure, DS3, thatDynaMAGNAþþ partly optimizes in order to find good align-ments. DynaMAGNAþþ can be extended in two ways: by optimiz-ing future, potentially more efficient alignment quality measureswith the current search strategy, or by optimizing its current align-ment quality measures with a future, potentially superior searchstrategy (Crawford et al., 2015). DynaMAGNAþþ can also be ex-tended into an “online” version to allow for dealing with constantly“arriving” temporal data (Albers, 2003). Namely, afterDynaMAGNAþþ has produced an alignment of two dynamicFig. 5. Alignment quality of DynaMAGNAþþ and MAGNAþþ for the yeastnetwork. The figure can be interpreted in the same way as Figure 3Fig. 6. Alignment quality of DynaMAGNAþþ and MAGNAþþ for the Enronnetwork. The figure can be interpreted in the same way as Figure 3Table 4. Alignment quality of DynaMAGNAþþ optimizing both dy-namic edge and node conservation (EþN), dynamic edge conser-vation alone (E), and dynamic node conservation alone (N), interms of node correctness, when each network is aligned to itself(corresponding to the 0% noise level)NA method \ Network Zebra Yeast EnronDynaMAGNAþþ (EþN) 0.800 0.920 1.000DynaMAGNAþþ (E) 0.704 0.635 1.000DynaMAGNAþþ (N) 0.793 0.891 0.996Note: Each score is an average over five runs. In each column, the highestscore is bolded.Fig. 7. GUI to DynaMAGNAþþ. The only required parameters are the twonetworks to be aligned, and the output directory/file name information. WhileDS3 is the only currently implemented dynamic edge (event) conservationmeasure, other future dynamic edge conservation measures can be easilyadded. Any dynamic node similarity measure can be used by selecting a filecontaining pairwise similarities between nodes of the two networks. The aparameter (Section 2.2) can be set to any desired value. Other advanced par-ameters can also be user-specified. The default values are set according tothe parameter values used in this work (Section 2.2)i188 V.Vijayan et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018networks, if new events are added to either of the networks,DynaMAGNAþþ can be modified to allow for updating the currentalignment into a new one that will account for the new events.We demonstrate applicability of DynaMAGNAþþ and dynamicNA in general in multiple domains: biological networks (ecologicalnetworks and PINs) and social networks. Given the impact thatstatic NA has had in computational biology, as more PIN and othermolecular dynamic network data are becoming available, dynamicNA and thus our study will continue to gain importance. The sameholds for other domains in which increasing amounts of real-worlddynamic network data are being collected. So, we have justscratched the tip of the iceberg called dynamic NA.FundingThis work was supported by the Air Force Office of Scientific Research [YIPFA9550-16-1-0147]; the National Science Foundation [CCF-1319469]; andthe Notre Dame’s Center for Research Computing.Conflict of Interest: none declared.ReferencesAlbers,S. (2003) Online algorithms: a survey. Math. Program., 97, 3–26.Bayati,M. et al. (2013) Message-passing algorithms for sparse network align-ment. ACM Trans. Knowl. Discov. Data, 73, 1–3. 31.Boccaletti, S. et al. (2006) Complex networks: structure and dynamics. Phys.Rep., 424, 175–308.Crawford, J. et al. (2015) Fair evaluation of global network aligners. Algorith.Mol. Biol., 10.Duchenne,O. et al. (2011) A tensor-based algorithm for high-order graphmatching. Pattern Anal. Machine Intel., IEEE Trans., 33, 2383–2395.Elmsallati,A. et al. (2016) Global alignment of protein-protein interaction net-works: a survey. IEEE/ACM Trans. Comput. Biol. Bioinform., 13,689–705.Emmert-Streib,F. et al. (2016) Fifty years of graph matching, network align-ment and network comparison. Info. Sci., 346 (C, ), 180–197.Faisal,F. et al. (2015a) Global network alignment in the context of aging.IEEE/ACM Trans. Comput. Biol. Bioinform., 12, 40–52.Faisal,F. et al. (2015b) The post-genomic era of biological network alignment.EURASIP J. Bioinform. Systems Biol., 2015, 1–19.Guzzi,P.H. and Milenkovi c,T. (2017) Survey of local and global biologicalnetwork alignment: the need to reconcile the two sides of the same coin.Brief. Bioinform., doi: 10.1093/bib/bbw132.Holme,P. (2015) Modern temporal network theory: a colloquium. Eur. Phys.J. B, 88, 1–30.Ho cevar,T. and Dem sar,J. (2014) A combinatorial approach to graphletcounting. Bioinformatics, 30, 559–565.Hulovatyy,Y. et al. (2014) Revealing missing parts of the interactome via linkprediction. PLOS One, 9, e90073.Hulovatyy,Y. et al. (2015) Exploring the structure and function of temporalnetworks with dynamic graphlets. Bioinformatics, 31, 171–180.Ibragimov,R. et al. (2013). GEDEVO: an evolutionary graph edit distance al-gorithm for biological network alignment. In GCB, pages 68–79.Kuchaiev,O. and Pr zulj,N. (2011) Integrative network alignment reveals largeregions of global network similarity in yeast and human. Bioinformatics,27, 1390–1396.Kuchaiev,O. et al. (2010) Topological network alignment uncovers biologicalfunction and phylogeny. J. R Soc. Interf., 7, 1341–1354.Mamano,N. and Hayes,W.B. (2017) SANA: simulated annealing far outper-forms many other search algorithms for biological network alignment.Bioinformatics, doi: 10.1093/bioinformatics/btx090.Malod-Dognin,N. and Pr zulj,N. (2015) L-GRAAL: Lagrangian graphlet-based network aligner. Bioinformatics, 31, 2182–2189.Meng,L. et al. (2016a). IGLOO: integrating global and local biological net-work alignment. In Proc. of Workshop on Mining and Learning withGraphs (MLG) at the Conference on Knowledge Discovery and DataMining (KDD).Meng,L. et al. (2016b) Local versus global biological network alignment.Bioinformatics, 32, 3155–3164.Milenkovi c,T. and Pr zulj,N. (2008) Uncovering biological network functionvia graphlet degree signatures. Cancer Inform., 6, 257–273.Milenkovi c,T. et al. (2010) Optimal network alignment with graphlet degreevectors. Cancer Inform., 9, 121–137.Neyshabur,B. et al. (2013) NETAL: a new graph-based method for globalalignment of protein-protein interaction networks. Bioinformatics, 29,1654–1662.Patro,R. and Kingsford,C. (2012) Global network alignment using multiscalespectral signatures. Bioinformatics, 28, 3105–3114.Priebe,C.E. et al. (2005) Scan statistics on Enron graphs. Comput. Math.Organ. Theory, 11, 229–247.Pr zulj,N. et al. (2010). Geometric evolutionary dynamics of protein interactionnetworks. In Proc. of the Pacific Symposium Biocomputing, pages 4–8.Przytycka,T.M. and Kim,Y.-A. (2010) Network integration meets networkdynamics. BMC Bioinform., 8.Przytycka,T.M. et al. (2010) Toward the dynamic interactome: it’s abouttime. Brief. Bioinform., 11, 15–29.Rubenstein,D.I. et al. (2015) Similar but different: dynamic social networkanalysis highlights fundamental differences between the fission-fusion soci-eties of two equid species, the onager and Grevy’s zebra. PLOS One, 10,e0138645.Saraph,V. and Milenkovi c,T. (2014) MAGNA: Maximizing Accuracy inGlobal Network Alignment. Bioinformatics, 30, 2931–2940.Singh,R. et al. (2007). Pairwise global alignment of protein interaction net-works by matching neighborhood topology. In: Research in ComputationalMolecular Biology. Springer, Oakland, CA, USA, pp. 16–31.Sun,Y. et al. (2015). Simultaneous optimization of both node and edge conser-vation in network alignment via WAVE. In Proc. of Workshop onAlgorithms in Bioinformatics (WABI), Atlanta, GA, USA, pp. 16–39.Vijayan,V. and Milenkovi c,T. (2016). Multiple network alignment viamultiMAGNAþþ. In Proc. of Workshop on Data Mining inBioinformatics (BIOKDD) at the Conference on Knowledge Discovery andData Mining (KDD).Vijayan,V. et al. (2015) MAGNAþþ: Maximizing Accuracy in GlobalNetwork Alignment via both node and edge conservation. Bioinformatics,31, 2409–2411.Yavero glu, €O. et al. (2015) Proper evaluation of alignment-free network com-parison methods. Bioinformatics, 31, 2697–2704.Zhang,Y. et al. (2015). COSNET: connecting heterogeneous social networkswith local and global consistency. In Proc. ACM SIGKDD Int. Conf. onKnowledge Discovery and Data Mining, pages 1485–1494.Alignment of dynamic networks i189Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i180/3953961by gueston 07 January 2018
5028881979002	PMID28881979	5028881979	https://watermark.silverchair.com/btx245.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881979.main.pdf	Bioinformatics, 33, 2017, i341–i349doi: 10.1093/bioinformatics/btx245ISMB/ECCB 2017Identification of associations betweengenotypes and longitudinal phenotypesvia temporally-constrained group sparsecanonical correlation analysisXiaoke Hao1, Chanxiu Li1, Jingwen Yan2,3, Xiaohui Yao2,3,Shannon L. Risacher2, Andrew J. Saykin2, Li Shen2,3,*,Daoqiang Zhang1,* and for the Alzheimer’s Disease NeuroimagingInitiative†1School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing211106, China, 2Department of Radiology and Imaging Sciences, School of Medicine and 3School of Informaticsand Computing, Indiana University, Indianapolis, IN 46202, USA*To whom correspondence should be addressed.†Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database(www.loni.usc.edu/ADNI). As such, the investigators within the ADNI contributed to the design and implementation of ADNIand/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can befound at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.AbstractMotivation: Neuroimaging genetics identiﬁes the relationships between genetic variants (i.e., thesingle nucleotide polymorphisms) and brain imaging data to reveal the associations from genotypes to phenotypes. So far, most existing machine-learning approaches are widely used to detectthe effective associations between genetic variants and brain imaging data at one time-point.However, those associations are based on static phenotypes and ignore the temporal dynamics ofthe phenotypical changes. The phenotypes across multiple time-points may exhibit temporal patterns that can be used to facilitate the understanding of the degenerative process. In this article, wepropose a novel temporally constrained group sparse canonical correlation analysis (TGSCCA)framework to identify genetic associations with longitudinal phenotypic markers.Results: The proposed TGSCCA method is able to capture the temporal changes in brain from longitudinal phenotypes by incorporating the fused penalty, which requires that the differences between two consecutive canonical weight vectors from adjacent time-points should be small. A newefﬁcient optimization algorithm is designed to solve the objective function. Furthermore, we demonstrate the effectiveness of our algorithm on both synthetic and real data (i.e., the Alzheimer’sDisease Neuroimaging Initiative cohort, including progressive mild cognitive impairment, stableMCI and Normal Control participants). In comparison with conventional SCCA, our proposedmethod can achieve strong associations and discover phenotypic biomarkers across multiple timepoints to guide disease-progressive interpretation.Availability and implementation: The Matlab code is available at https://sourceforge.net/projects/ibrain-cn/ﬁles/.Contact: dqzhang@nuaa.edu.cn or shenli@iu.eduCV The Author 2017. Published by Oxford University Press.i341This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018i342X.Hao et al.1 IntroductionIntegrating neuroimaging and molecular genetics technology holdgreat promising to use brain imaging as quantitative phenotypes toinvestigate the role of genetic variations. These imaging quantitativetraits (QTs) serve as intermediate phenotypes with rich information,which bridge the gap between genetic factors and phenotypic outcomes (Glahn et al., 2007; Gottesman and Gould, 2003; Haririet al., 2006) and may lead to a better understanding of the complexbiological mechanism underlying neurodegenerative diseases [e.g.,mild cognitive impairment (MCI), the prodromal stage ofAlzheimer’s disease (AD)].In prior imaging genetic studies, pairwise univariate analysisstrategies have been performed to identify the associations betweensingle nucleotide polymorphisms (SNPs) and neuroimaging QTs.The most comprehensive studies focused on scanning the entirebrain and the entire genome (Shen, et al., 2010; Stein et al., 2010).In recent studies, taking into account the inherent structure amonggenotype or phenotype data, some researchers have developed generalized multivariate linear regression models (Hibar et al., 2011;Kohannim et al., 2011, 2012; Vounou et al., 2010; Wang et al.,2012a) and structured bi-multivariate models (Chi et al., 2013; Linet al., 2014; Yan et al., 2014) to identify multi-SNP-multi-QT associations. Those methods have sufficient power to discover structuredphenotypic imaging markers associated with disease-relevant SNPs.However, examining genetic influence on the longitudinal profilesof imaging phenotype is still an under-explored topic in imaginggenetics. Specifically, a straight forward approach such as conventional sparse canonical correlation analysis (SCCA) (Chi et al.,2013; Witten et al., 2009; Witten and Tibshirani, 2009), which doesnot take into account the valuable information conveyed by the longitudinal pattern of phenotypic input, is to perform multi-SNPmulti-QT associations at one time-point. In fact, the phenotypesacross multiple time-points may exhibit temporal patterns that canbe used to describe the degenerative process. Some studies haveinvestigated on prediction of memory impairment and cognitive assessments with longitudinal magnetic resonance imaging (MRI) data(Jie et al., 2016; Wang et al., 2016).So far, only a few machine-learning strategies have been proposed to examine how the phenotypic changes are that affected bySNPs. Recently, Wang et al. (2012b) proposed a novel task-correlated longitudinal sparse regression model to study the associationbetween phenotypic imaging markers and the genotypes by takinginto account the temporal structure of the longitudinal imagingdata. More specifically, they used L21-norm for regression coefficient matrix to jointly select imaging markers that have common effects across all time-points. However, the task-correlatedlongitudinal sparse regression model assumed that longitudinalimaging markers were related to all candidate SNPs as a taskcorrelated constraint, which might not hold in real applications.More recently, Vounou et al. (2010) have proposed a two-stepframework based on sparse reduced-rank regression to solve theimaging genetics problem for the genome-wide detection of markersassociated with voxel-wise longitudinal changes in brain (Vounouet al., 2012). They first pre-selected the disease relevant voxel levelimaging phenotypes with high-classification performance betweenAD and NC group by penalized linear discriminant analysis, andthen identified the SNPs associated with the multivariate imagingphenotypes from the first step. This approach might be inadequateto capture the dynamics of phenotypic trajectories and thus unableto detect the underlying temporal patterns. Therefore, how to identify the longitudinal phenotypes across consecutive time-pointsFig. 1. Schematic illustration of TGSCCA for imaging geneticsassociated to the disease sensitive SNPs is still an important topic inimaging genetic studies.With these observations, the motivation of this study is to identify associations between risk genotypes and longitudinal phenotypes, where we aim to design a powerful model to simultaneouslymaximize progression-relevant imaging genetic associations andcapture the consecutive changes in brain between adjacent timepoints. Accordingly, as shown in Figure 1, we propose a novel temporally constrained group sparse canonical correlation analysis(TGSCCA) framework that incorporates the group sparsity constraint and fused penalty to identify the associations between geneticfactors and longitudinal phenotypes. In particular, it is promising tofind the consecutive patterns that are robust to noises or outliers viaconsidering both joint selection and the fused information in imaging phenotypes from adjacent time-points.In this study, to evaluate the effectiveness and efficiency of ourproposed method, we perform experiments on both synthetic and realdata. For real data, using the Alzheimer’s Disease NeuroimagingInitiative (ADNI) cohort (Mueller et al., 2005), we examine imaginggenetic associations between SNPs nearby the apolipoprotein E(APOE) gene and region of interest (ROI) measures extracted fromlongitudinal structural MRI. The empirical results show that ourmethod not only yields clearly improved association performanceunder the metrics of correlation coefficient but also detects relevantrisk SNP loci and imaging ROI markers.The rest of this article is organized as follows. Section 2 introduces the TGSCCA method. The performances of the proposedmethod are evaluated through both simulations and real data analysis in Section 3. The last section concludes the study.2 Materials and methods2.1 Sparse Canonical Correlation Analysis (SCCA)We first describe relevant notations to present imaging genetic association analysis. We use lowercase letters to denote vectors,and uppercase letters to denote the matrices. Let X ¼½x1 ; . . . ; xn ; . . . ; xN  T 2 RNÂp be the SNP genotype data,½y1 ; . . . ; yn ; . . . ; yN  T 2 RNÂq be the phenotype data, where N is thenumber of participants, and p and q are the feature number of SNPsand imaging data, respectively.Canonical correlation analysis (CCA) is a powerful associationmethod that seeks linear transformations of two data sets X and Yto achieve the maximal correlation between Xu and Yv (Hotelling,1935), which can be formulated as:Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018max uT XT Yvw;vIdentification of associations between genotypes and longitudinal phenotypess:t: uT XT Xu ¼ 1; vT Y T Yv ¼ 1;(1)where we assume that the columns of X and Y are standardized tohave zero mean and unit variance, and u and v are canonicalweights, reflecting the contribution of each feature in the identifiedcanonical correlation. However, in imaging genetics applications,the traditional CCA model tends to overfit and does not yield desirable results as the dimension of the data is much higher than thesample size. In addition, the CCA outcome could spread nontrivialeffects across all features, which are not desirable for applicationsneeding to identify relevant features. To address these issues, sparseversion of CCA (SCCA) (Chi et al., 2013; Witten et al., 2009;Witten and Tibshirani, 2009) has been proposed by introducing penalties with L1 regularization for variable selection (Tibshirani,2011) as follows:max uT XT Yvw;vs:t: jjXujj2 ¼ 1; jjYvjj2 ¼ 1; jjujj122c1 ; jjvjj1c2(2)where the constraints jjujj1c1 and jjvjj1c2 are regularizationterm for the objective function, and c1 and c2 is the correspondingregularization parameters. In imaging genetics applications, theweight vectors u and v measure the relative contributions of the SNPloci and imaging phenotype ROIs. For easy computation, the variance matrix of X and Y is treated as diagonal matrix, which hasshown to be effective and efficient for high-dimensional data(Grellmann et al., 2015; Witten et al., 2009).2.2 Temporally constrained group sparse canonicalcorrelation analysis (TGSCCA)In clinical practice, imaging phenotypes affected by genetic factorschanges over time. To investigate the association between genotypesand longitudinal imaging phenotypes, in this article, we considerhow to perform bi-multivariate association analysis across the consecutive time-points. Assume that we have N training subjects, andeach subject has imaging data derived from T different time-points.Given the genotype SNPs data X ¼ ½x1 ; . . . ; xn ; . . . ; xN  T 2 RNÂp andlongitudinal imaging phenotypes Yt ¼ ½y1t ; . . . ; ynt ; . . . ; yNt  T 2RNÂq at time-point t (1tT) as input in the association model,where N is the number of participants, p and q are the numbers offeature dimensionalities (i.e., number of SNP loci and brain imagingROIs). As described in Section 1 and Figure 1, our aim is to discoverthose longitudinal brain imaging markers associated with geneticfactors across different time-points. Task-correlated longitudinalanalysis model has recently been successfully investigated andapplied to regression problems (Wang et al., 2012a, b), which areinspired by using multi-task learning framework (Liu et al., 2009;Obozinski et al., 2006) in machine-learning community. Followingtheir previous work, we induce the joint penalty term L21-norminto the Equation (2) and then develop group sparse canonical correlation analysis (GSCCA) model as follows:min Àu;VTXregularization terms, respectively. V ¼ ½v1 ; . . . ; vt ; . . . ; vT   2 RqÂT isthe weight matrix whose row vi is the vector of coefficients assignedPto the -th feature across different time-points, and jjV jj2;1 ¼ di¼1jjV i jj2 is to penalize all coefficients in the same row of matrix V forjoint feature selection. It is worth noting that the L21 regularizationterm can be coupled over time dimension and this “group-sparsity”regularizer forces only a small number of features being selected(Yuan and Lin, 2006). In other words, the longitudinal imaging features across all time-points will be identified.To further take into account detecting temporally-constrainedimaging genetic associations, we expect to develop our model to explore the association between baseline SNPs and longitudinal imaging phenotypes for a better understanding of underlying progressivemechanism specific to the disease. More specifically, motivated bythe existing work (Jie et al., 2016), we induce a new regularizationterm called fused least absolute shrinkage and selection operator(Lasso) (Liu et al., 2010) in machine-learning community and thenformulate the TGSCCA model as follows:min Àu;VTXuT XT Yt vt þ ku jjujj1 þ kv jjV jj2;1 þ ktt¼1T À1Xjjvtþ1 À vt jj1t¼1s:t: jjXujj2 ¼ 1; jjYt vt jj2 ¼ 122t¼1(3)(4)where the weight vectors u and vt measure the relative contributionsof the SNP loci and imaging phenotype ROIs at time-point t. Theweight vector vtþ1 and vt are from adjacent time-points. ku , kv andkt denote control parameters of the regularization terms, respectively. The fused Lasso regularization term tends to constrain the differences between two successive canonical weight vectors fromadjacent time-points to be small, that is the smoothness of weightvectors encourages neighboring features to be selected together. Dueto the two regularization terms in Equation (3), it is promising tofind the better solution that is robust to noises or outliers via considering both joint selections and the fused information inherent inimaging genetic associations.2.3 Optimization algorithmIn this section, we introduce the algorithm to obtain u and V fromEquation (4). The objective function is convex with respect to uwhen V is fixed and vice versa. So the iteration procedures mainlycontain two steps as follows:min ÀuTXuT XT Yt vt þ ku jjujj1t¼1s:t: jjXujj2 ¼ 12min ÀVTXuT XT Yt vt þ kv jjV jj2;1 þ ktt¼1(5)TÀ1Xjjvtþ1 À vt jj1t¼1s:t: jjYt vt jj2 ¼ 12uT XT Yt vt þ ku jjujj1 þ kv jjV jj2;1s:t: jjXujj2 ¼ 1; jjYt vt jj2 ¼ 122i343(6)We can use the Lagrange multiplier and write the penalties intothe matrix form for Equation (5), thus the new objective function isas follows:where the weight vector u and vt measure the relative importance ofthe SNP loci and imaging phenotype ROIs at time-point t(1tT). ku and kv denote control parameters of theDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018min ÀuTXt¼1huT XT Yt vt þ ku jjujj1 þ jjXujj222(7)i344X.Hao et al.where ku and h are the model parameters. In this solution, a smoothapproximation has been estimated for L1 term by including an extremely small value. Take the derivative regarding ku and let it be 0.The solution for u in each iteration step is as follows:!  À1 XTkuTTu¼ X Xþ DX Yt v t(8)ht¼1where D is a diagonal matrix with the kth element as1=2jjuk jj1 ðk 2 ½1; p Þ.To obtain V when u is fixed for Equation (6), we follow the previous work (Fang et al., 2016; Witten et al., 2009) and assume thatthe variance matrix of Yt is treated as diagonal matrix, which hasshown to be effective and efficient for optimization. The solution forV in each iteration step is as follows:minTXjjvt jj2 ¼12Àzt T vt þ kv jjV jj2;1 þ ktt¼1T À1Xjjvtþ1 À vt jj1(9)t¼1where zt ¼ YtT Xu, is given by jjvt jj2 ¼ 1, where V is the optimum of2minVTX1t¼12jjvt À zt jj2 þ kv jjV jj2;1 þ kt2T À1Xjjvtþ1 À vt jj1(10)t¼1It is straightforward to verify that Equation (10) is convex butnon-smooth because of L21-norm and Fused Lasso regularizationterm. The basic idea to solve this problem is to use a smooth function to approximate the original non-smooth objective function. Inthis study, we use the Nesterov’s accelerated proximal gradient(APG) algorithm (Beck and Teboulle, 2009; Chen et al., 2009)to solve our optimization problem, which is shown in theAlgorithm 1.First, we separate Equation (10) into a smooth part Equation(11) and a non-smooth part Equation (12) as follows:f ðV ÞTX1t¼12jjvt À zt jj22gðV Þ ¼ kv jjV jj2;1 þ ktTÀ1Xjjvtþ1 À vt jj1(11)(12)t¼1We define the approximation function Equation (10) as follows,which is composited by the above smooth part and non-smooth one:lXðV; Vi Þ ¼ f ðVi Þþ < V À Vi ; rf ðVi Þ > þ jjV À Vi jj2 þ gðV ÞF2(13)where jj Á jj2 denotes the Frobenius norm, rf ðVi Þ denotes the gradiFent of f ðV Þ on point Vi at the ith iteration, and l is the step size.Finally, the update step of Nesterov’s APG is defined as:11Viþ1 ¼ arg min jjV À W jj2 þ gðV ÞFV 2l(14)where W ¼ Vi À 1 rf ðVi Þ. The key of APG algorithm is how tolsolve the update step efficiently. In addition, according to the technique used in Chen et al. (2009), instead of performing gradient descent based on Vi , we compute the search point as:(15)Qi ¼ Vi þ ai ðVi À ViÀ1 Þpﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1þ 1þ4q2iÀ1where ai ¼and qi ¼. For more details about the so2lution of L21-norm and Fused Lasso problem, please refer to theprevious work (Jie et al., 2016; Liu et al., 2010).qiÀ1 À1qiAlgorithm 1Input: SNPs X ¼ ½x1 ; . . . ; xn ; . . . ; xN  T 2 RNÂp ; longitudinalimagingphenotypes Yt ¼ ½y1t ; . . . ; ynt ; . . . ; yNt  T 2NÂqRat time-point t (1tT); parametersku > 0, h > 0, kv > 0, kt > 0.Initialization: l ¼ l0 ¼ 1, V0 ¼ V1 ¼ 0, q0 ¼ 1.While not converge do1: Calculate the diagonal matrix D, where the k-th element is 1=2jjuk jj1 ;2: Update u by Equation (8);3: Scale u so that jjXujj2 ¼ 1;24: Computed the search point Qi according to Equation(15);5:Find thesmallestl ¼ liÀ1 ; 2liÀ1 ; . . .sothatXðViþ1 ; Qi Þf ðViþ1 Þ þ gðViþ1 Þ, where Viþ1 is computed by Equation (14);6: Set li ¼ l;7: Scale vt so that jjYt vt jj2 ¼ 1.2End whileOutput: canonical vector u 2 RpÂ1 ; V ¼ ½v1 ; . . . ; vt ; . . . ; vT  2 RqÂT .3 Results and discussions3.1 Results on simulation dataIn this section, we present a simulation study to evaluate the potential power of our proposed TGSCCA method. The procedure ofsimulation generation is similar to that in Chen et al. (2012)andFang et al. (2016). We first generated one canonical vector u with p’non-zero entries and successive canonical vector vk with q’ nonentries, where vkþ1 ¼ vk þ Dv ðDv $ Nð0; 0:1Þ and ðk ¼ 1; 2; 3ÞÞ.Each non-zero variable in u and v1 was sampled independently froma uniform distribution in the range of ½À2; À0:5  [ ½0:5; 2 . Andthen, we randomly generated a latent variable h with normal distribution Nð0; rh Þ for each sample, where rh is the signal to noiselevel. For the data matrix X and Y, the features were simulated fromGaussian distribution Nðuh; re Ip Þ and Nðvk h; re Iq Þ, respectively.We set N ¼ 100, p ¼ 100, q ¼ 50, p’¼30, q’¼20, rh ¼ 0:1. To validate the effects on the performance, we varied the noise level re from0.1 to 0.5 to generate simulation data 1 and simulation data 2,respectively.In our experiments, 5-fold cross-validation strategy is adopted toevaluate the effectiveness of our proposed method. All the regularization parameters are optimally tuned using a grid search from therange of {0.01, 0.02, 0.05, 0.08, 0.1, 0.2, 0.5, 0.8, 1} by anothernested 5-fold cross-validation on the training set.We compare SCCA (denoted as sparse canonical correlation analysis to detect associations between SNP loci and imaging features ateach time-point), GSCCA (denoted as group sparse canonical correlation analysis to detect associations between SNP loci and longitudinal imaging phenotypic features jointly across all time-points onlyvia L21-norm), and TGSCCA (denoted as temporally-constrainedgroup sparse canonical correlation analysis to detect associations between SNP loci and longitudinal imaging features jointly across alladjacent time-points via L21-norm and fused penalty).The performance on each dataset is assessed with correlation coefficient between X and Y, which are widely used in measuring performances of association analysis. The average results of correlationDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018Identification of associations between genotypes and longitudinal phenotypesi345coefficients on 5-fold testing data are calculated respectively onSimulation 1 and Simulation 2. As shown in Figure 2, joint longitudinal association methods (including GSCCA and TGSCCA) outperform SCCA consistently and significantly in the metrics ofcorrelation coefficients on both simulations. It is worth noting thatTGSCCA is comparable with GSCCA due to the low noises inSimulation 1, while TGSCCA is more robust to the data with highnoises in Simulation 2. Furthermore, we show the estimated canonical weights from different methods. As shown in Figure 3, the overall profiles of the estimated u and v values from TGSCCA areconsistent with the ground truth on both Simulation 1 andSimulation 2, whereas SCCA is only capable of identifying inconsistent signals at different time-points. Although GSCCA can almostcapture the same signals on u as TGSCCA, from the unsmooth patterns across the longitudinal case, it may be affected by noises andthen draw false discoveries on v without the induced temporalconstraint. From the above results, it is observed that TGSCCA canidentify not only the signal locations but also strong correlations,which has certain superiority compared with other methods.3.2 Results on real imaging genetic dataFig. 2. The averaged correlation coefﬁcients on 5-fold test data using differentmethods on simulations. (a) Results on simulation data 1. (b) Results onsimulation data 23.2.1 ADNI datasetReal imaging genetics data used in the preparation of this articlewere obtained from the ADNI database (adni.loni.usc.edu).Fig. 3. The estimated weights of u and v from average 5-fold cross-validation test on simulation data are shown in the left ﬁve panels and right ﬁve panels.Ground truth of w and v are shown in the most left in the two parts, respectively. The estimated u values and v values are shown in the remaining panels, corresponding to different methods. (a) Results on simulation data 1. (b) Results on simulation data 2Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018i346X.Hao et al.Table 1. Demographic characteristics of the studied population (thevalues are denoted as mean 6 standard deviation)SubjectsGender (M/F)AgeEducationMMSE(BL)MMSE(M06)MMSE(M12)MMSE(M24)ADAS-Cog(BL)ADAS-Cog(M06)ADAS-Cog(M12)ADAS-Cog(M24)pMCI (n ¼ 15)sMCI (n ¼ 41)NC (n ¼ 58)8/771.7565.9216.3363.5426.9361.9126.0762.6925.4762.7222.8064.0020.6465.5122.9168.4824.3366.5726.9568.0726/1573.4067.5916.2262.8627.5961.5027.5961.7627.5661.9127.6162.2415.4565.8015.5265.7715.2065.9316.1166.2831/2775.7164.7416.3862.8529.2160.9929.0361.0329.3860.8329.1261.098.9263.698.9563.757.5864.058.4364.43Note: NC ¼ Normal Control, pMCI ¼ progressive Mild CognitiveImpairment, sMCI ¼ stable Mild Cognitive Impairment, MMSE ¼ MiniMental State Examination, ADAS-Cog ¼ Alzheimer’s Disease AssessmentScale-Cognitive Subscale.The ADNI was launched in 2003 as a public–private partnership,led by Principal Investigator Michael W. Weiner, MD. The primarygoal of ADNI has been to test whether serial MRI, positron emissiontomography (PET), other biological markers, and clinical andneuropsychological assessment can be combined to measure the progression of MCI and early AD. For up-to-date information, seewww.adni-info.org.The genotyping and longitudinal imaging data of 114 nonHispanic Caucasian participants were downloaded from the ADNIwebsite. The demographic information is summarized in Table 1.Specifically, the time points examined in this study for MRI T1weighted imaging and cognitive assessments (i.e., MMSE andADAS-Cog) included baseline (BL), Month 06 (M06), Month 12(M12) and Month 24 (M24). We aligned the preprocessed imagingdata [i.e., voxel based morphometry (VBM)] to each participant’ssame visit scan, and then created normalized gray matter densitymaps from MRI data in the standard Montreal NeurologicalInstitute (MNI) space as 2 Â 2 Â 2 mm3 voxels SPM software package (Ashburner and Friston, 2007). One hundred and sixteen ROIlevel measurements of mean gray matter densities were further extracted based on the MarsBaR AAL atlas (Tzourio-Mazoyer et al.,2002). After removal of cerebellum, the imaging measures of 90ROIs were used as phenotypes in our experiments. For the genotyping data, we included 85 SNPs within 620k base pairs of the APOEgene boundary based on the ANNOVAR (http://annovar.openbioinformatics.org) annotation. For input in this association study, eachSNP value was coded in an additive fashion as 0, 1, 2, indicating thenumber of minor alleles.3.2.2 Improved association between risk SNP loci and longitudinalimaging ROI markersIn the real data experiments, we also use 5-fold cross-validationstrategy to evaluate the effectiveness of our proposed method.Similar to the previous simulation study, we determine the values ofregularization parameters by nested 5-fold cross-validation on thetraining set. The parameters are tuned in the range of {0.01, 0.02,0.05, 0.08, 0.1, 0.2, 0.5, 0.8, 1}.In current studies, we compare our proposed joint longitudinalimaging genetic strategies (including GSCCA and TGSCCA) withconventional SCCA method. For measuring the association performance of the compared methods, the average values of PearsonFig. 4. The averaged correlation coefﬁcients on 5-fold test data using differentmethods on ADNIcorrelation coefficients on 5-fold test sets are calculated to eliminatethe bias. As shown in Figure 4, the performances of longitudinalstrategies with joint detections (including GSCCA and TGSCCA)are more stable than the conventional SCCA, which treats imaginggenetic associations at each time-point independently. As expected,TGSCCA can achieve the best correlation coefficients so that it consistently outperform SCCA and GSCCA. These results demonstratethat the usage of temporal information across adjacent time-pointscan help improve the performances of association between genotypes and longitudinal imaging phenotypes.3.2.3 Identification of risk SNP lociBesides improving association performance, one major goal of thisstudy is to identify some vital SNP loci and imaging phenotypicmarkers for disease progression in MCI research. Therefore, findinggenetic risk factors and imaging ROIs helps scientists better understand how the disease develops and identify possible treatments tostudy. We aim to present the selected features on the SNP loci andimaging ROIs, whose annotations are shown on the X-axis from topand bottom panels in Figure 5. It shows all comparisons of absoluteweight maps for top 10 loci from APOE SNPs associated to top 10brain ROIs with longitudinal analysis respect to different methods.For detecting genetic factors, as shown on top panels in Figure 5,the locus rs76692773 and rs2075649 (Lin et al., 2016) are the tophits by all methods. However, compared with SCCA, the joint longitudinal detections (including GSCCA and TGSCCA) can discoverconsistent and clear patterns across all time-points, which indicateour proposed method performs stable in longitudinal imaging genetic associations. It is worth noting that the best-known risk geneticloci rs429358 has not been identified by all methods (including ourproposed TGSCCA). It warrants further investigation to confirmwhether the eminent risk factor rs429358 is truly not associatedwith longitudinal VBM phenotypes in the MCI progression. In addition, as the association solution relies on the linear combination ofall loci, an individual one might not have a direct influence to thecorrelation, i.e., it might modulate the influence of another locus.Consequently, these genetic factors selected in this association studyshould also warrant further investigation for replication in independent and larger cohorts.3.2.4 Identification of longitudinal imaging ROI markersFor detecting brain imaging ROIs, as shown on bottom panels inFigure 5, the conventional SCCA method identifies some irregularimaging ROIs at different time-points, which are not able to servescreening target over the course of MCI progression. While usingjoint longitudinal detection strategies, we can obtain clear patternson the feature panels. For example, right parahippocampal gyrusDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018Identification of associations between genotypes and longitudinal phenotypesi347Fig. 5. The estimated weights of u (top panels) and v (bottom panels) from average 5-fold cross-validation test on ADNI data using the different methodsand right superior parietal gyrus are the two top hits in joint detections (including GSCCA and TGSCCA), which are in accordancewith previous findings (Jacobs et al., 2012; Shen et al., 2010). Inaddition, the top ROIs selected for progression across all timepoints by TGSCCA can be treated as increasingly stable markers,which are also accordance with the fact that the grey matter atrophyof these ROIs is severe in MCI (Driscoll et al., 2009). Therefore,compared with SCCA and GSCCA, our proposed TGSCCA can tolerate noises to some extent so that the weight maps of each selectedROI across different time-points are very smooth. This further indicates the advantage of using the temporal-constrained smoothnessregularization.4 ConclusionIn this article, we propose a novel TGSCCA framework to detectrisk genetic factors and their correlated longitudinal phenotypemarkers for a progressive disease (i.e., MCI). This approachexplicitly captures the consecutive changes between phenotypesfrom adjacent time-points by incorporating the group sparsity constraint and fused penalty into the objective function. We also presentan effective iterative algorithm to solve the optimization problem.We apply the proposed method on the simulation data and ADNIcohort [including progressive mild cognitive impairment (pMCI),stable MCI (sMCI) and NC participants]. The experimental resultsshow that our proposed TGSCCA model can identify stronger associations than conventional SCCA and GSCCA. Besides the improvedassociation performance, in real imaging genetic data, our modelcan also detect the risk SNP loci and clearly consistent brain ROIsacross all time-points, which provides valuable information to helpunderstand the genetic basis of brain structural change over the progression of MCI and AD.As an interesting future direction, this TGSCCA method can beapplied to investigate the potential mechanism of other imagingphenotypes (e.g., fluorodeoxyglucose positron emission tomography(FDG-PET) and Florbetapir F 18 amyloid PET data) (Hao et al.,2016) and biomarkers such as cerebrospinal fluid and plasma fromDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018i348longitudinal perspective (Fagan et al., 2014). Therefore, all kinds ofbiomarker outcomes learned from trajectories across the course ofdisease can be evaluated, and the findings may have the potential tohelp with neurodegenerative assessments in clinical practices.In this initial study, the proposed TGSCCA can be successfullyapplied for longitudinal imaging genetics study on a candidate geneset. However, when the datasets contain more features, it becomesmore challenging to identify truly relevant ones. Thus, an interestingfuture direction could be to develop an improved TGSCCA modelby exploring non-convex penalty terms that have been shown to bemore effective than L1 based terms in terms of feature selection viasparse learning. In addition, this general framework can be extendedand applied to some other interesting fields such as brain-perceivedanalysis (Connolly et al., 2016) and gene expression analysis onmultiple data sources (Allahyar and de Ridder, 2015; Bunte et al.,2016; Omranian et al., 2016), allowing for generating new insights.AcknowledgementsData collection and sharing for this project was funded by the Alzheimer’sDisease Neuroimaging Initiative (ADNI) (National Institutes of Health GrantU01 AG024904) and DOD ADNI (Department of Defense award numberW81XWH-12-2-0012). ADNI is funded by the National Institute on Aging,the National Institute of Biomedical Imaging and Bioengineering, and throughgenerous contributions from the following: Abbott, Alzheimer’s Association;Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.;Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Eisai Inc.; ElanPharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-LaRoche Ltd and its afﬁliated company Genentech, Inc.; Fujirebio; GEHealthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research &Development, LLC.; Johnson & Johnson Pharmaceutical Research &Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso ScaleDiagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; NovartisPharmaceuticals Corporation; Pﬁzer Inc.; Piramal Imaging; Servier; TakedaPharmaceutical Company; and Transition Therapeutics. The CanadianInstitutes of Health Research is providing funds to support ADNI clinical sitesin Canada. Private sector contributions are facilitated by the Foundation forthe National Institutes of Health (www.fnih.org). The grantee organization isthe Northern California Institute for Research and Education, and the studyis coordinated by the Alzheimer’s Disease Cooperative Study at the Universityof California, San Diego. ADNI data are disseminated by the Laboratory forNeuro Imaging at the University of Southern California.FundingThis study has been supported by the National Natural Science Foundation ofChina (Nos. 61422204, 61473149, 61501230), the NUAA FundamentalResearch Funds (No. NE2013105) in China. At Indiana University, this studywas supported by National Institute of Health R01 EB022574, R01LM011360, U01 AG024904, RC2 AG036535, R01 AG19771, P30AG10133, UL1 TR001108, R01 AG 042437, and R01 AG046171; theUnited States Department of Defense W81XWH-14-2-0151, W81XWH-131-0259, and W81XWH-12-2-0012; National Collegiate Athletic Association14132004; and CTSI SPARC Program.Conﬂict of Interest: none declared.ReferencesAllahyar,A., and de Ridder,J. (2015) FERAL: network-based classiﬁer withapplication to breast cancer outcome prediction. Bioinformatics, 31,i311–i319.X.Hao et al.Ashburner,J., and Friston,K. (2007) Voxel-based morphometry, StatisticalParametric Mapping: The Analysis of Functional Brain Images, 92–98.Beck,A., and Teboulle,M. (2009) A fast iterative shrinkage-thresholding algorithm for linear inverse problems. Siam J Imaging Sci, 2, 183–202.Bunte,K. et al. (2016) Sparse group factor analysis for biclustering of multipledata sources. Bioinformatics, 32, 2457–2463.Chen,X. et al. (2012) Structured sparse canonical correlation analysis.AISTATS. pp. 199–207.Chen,X. et al. (2009) Accelerated gradient method for multi-task sparse learning problem, IEEE Data Mining, 746–751.Chi,E.C. et al. (2013) Imaging genetics via sparse canonical correlation analysis. Proc IEEE Int Symp Biomed Imaging, 2013, 740–743.Connolly,A.C. et al. (2016) How the human brain represents perceived dangerousness or "predacity" of animals. J. Neurosci., 36, 5373–5384.Driscoll,I. et al. (2009) Longitudinal pattern of regional brain volume changedifferentiates normal aging from MCI. Neurology, 72, 1906–1913.Fagan,A.M. et al. (2014) Longitudinal change in CSF biomarkers inautosomal-dominant Alzheimer’s disease. Sci. Transl. Med., 6, 226ra230.Fang,J. et al. (2016) Joint sparse canonical correlation analysis for detectingdifferential imaging genetics modules. Bioinformatics, 32, 3480–3488.Glahn,D.C. et al. (2007) Neuroimaging endophenotypes: Strategies for ﬁndinggenes inﬂuencing brain structure and function. Hum. Brain Mapp., 28,488–501.Gottesman,I.I., and Gould,T.D. (2003) The endophenotype concept in psychiatry: etymology and strategic intentions. Am. J. Psychiatry, 160, 636–645.Grellmann,C. et al. (2015) Comparison of variants of canonical correlationanalysis and partial least squares for combined analysis of MRI and geneticdata. Neuroimage, 107, 289–310.Hao,X.K. et al. (2016) Identifying multimodal intermediate phenotypes between genetic risk factors and disease status in Alzheimer’s disease.Neuroinformatics, 14, 439–452.Hariri,A.R. et al. (2006) Imaging genetics: perspectives from studies of genetically driven variation in serotonin function and corticolimbic affective processing. Biol. Psychiatry, 59, 888–897.Hibar,D.P. et al. (2011) Multilocus genetic analysis of brain images. Front.Genet., 2, 73.Hotelling,H. (1935) The most predictable criterion. J. Educ. Psychol., 26,139.Jacobs,H.I.L. et al. (2012) Parietal cortex matters in Alzheimer’s disease: anoverview of structural, functional and metabolic ﬁndings. Neurosci.Biobehav. R, 36, 297–309.Jie,B. et al. (2016) Temporally-constrained group sparse learning for longitudinal data analysis in Alzheimer’s disease, IEEE Trans. Biomed. Eng., 64,238–249.Kohannim,O. et al. (2011) Boosting power to detect genetic associations inimaging using multi-locus, genome-wide scans and ridge regression. I SBiomed. Imag., 48,1855–1859.Kohannim,O. et al. (2012) Discovery and replication of gene inﬂuences onbrain structure using LASSO regression. Front. Neurosci. Switz., 6, 115.Lin,D. et al. (2014) Correspondence between fMRI and SNP data by groupsparse canonical correlation analysis. Med. Image Anal., 18, 891–902.Lin,R. et al. (2016) Association of common variants in TOMM40/APOE/APOC1 region with human longevity in a Chinese population. J. Hum.Genet., 61, 323–328.Liu,J. et al. (2009) Multi-task feature learning via efﬁcient l 2, 1-norm minimization. Proceedings of the twenty-ﬁfth conference on uncertainty in artiﬁcial intelligence. AUAI Press, pp. 339–348.Liu,J. et al. (2010) An efﬁcient algorithm for a class of fused lasso problems.Proceedings of the 16th ACM SIGKDD international conference onKnowledge discovery and data mining. ACM, pp. 323–332.Mueller,S.G. et al. (2005) The Alzheimer’s disease neuroimaging initiative.Neuroimag. Clin. N. Am., 15, 869–877.Obozinski,G. et al. (2006) Multi-task feature selection, statistics department.UC Berkeley, Technical Report 2.Omranian,N. et al. (2016) Gene regulatory network inference using fusedLASSO on multiple data sets. Sci. Rep. UK, 6, 20533Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018Identification of associations between genotypes and longitudinal phenotypesShen,L. et al. (2010) Whole genome association study of brain-wide imagingphenotypes for identifying quantitative trait loci in MCI and AD: a study ofthe ADNI cohort. Neuroimage, 53, 1051–1063.Stein,J.L. et al. (2010) Voxelwise genome-wide association study (vGWAS).Neuroimage, 53, 1160–1174.Tibshirani,R. (2011) Regression shrinkage and selection via the lasso: a retrospective. J. R. Stat. Soc. B, 73, 273–282.Tzourio-Mazoyer,N. et al. (2002) Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRIsingle-subject brain. Neuroimage, 15, 273–289.Vounou,M. et al. (2010) Discovering genetic associations with highdimensional neuroimaging phenotypes: a sparse reduced-rank regression approach. Neuroimage, 53, 1147–1159.Vounou,M. et al. (2012) Sparse reduced-rank regression detects genetic associations with voxel-wise longitudinal phenotypes in Alzheimer’s disease.Neuroimage, 60, 700–716.Wang,H. et al. (2012a) From phenotype to genotype: an association study oflongitudinal phenotypic markers to Alzheimer’s disease relevant SNPs.Bioinformatics, 28, i619–i625.i349Wang,H. et al. (2012b) Identifying quantitative trait loci via group-sparsemultitask regression and feature selection: an imaging genetics study of theADNI cohort. Bioinformatics, 28, 229–237.Wang,X. et al. (2016) Prediction of memory impairment with MRI Data:a longitudinal study of Alzheimer’s disease. International Conference onMedical Image Computing and Computer-Assisted Intervention. Springer,pp. 273–281.Witten,D.M. et al. (2009) A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis.Biostatistics, 10, 515–534.Witten,D.M., and Tibshirani,R.J. (2009) Extensions of sparse canonical correlation analysis with applications to genomic data. Stat. Appl. Genet. Mol.,8, 1–27.Yan,J. et al. (2014) Transcriptome-guided amyloid imaging genetic analysisvia a novel structured sparse learning algorithm. Bioinformatics, 30,i564–i571.Yuan,M., and Lin,Y. (2006) Model selection and estimation in regression withgrouped variables. J. R. Stat. Soc. B, 68, 49–67.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i341/3953960by gueston 07 January 2018
5028881978002	PMID28881978	5028881978	https://watermark.silverchair.com/btx244.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881978.main.pdf	Bioinformatics, 33, 2017, i170–i179doi: 10.1093/bioinformatics/btx244ISMB/ECCB 2017Incorporating interaction networks into thedetermination of functionally related hitgenes in genomic experiments withMarkov random fieldsSean Robinson1,2,3,4,5,*, Jaakko Nevalainen4,6, Guillaume Pinna7,Anna Campalans8,9,10,11, J. Pablo Radicella8,9,10,11and Laurent Guyon1,2,3,*1CEA, BIG, Biologie   Grande Echelle, F-38054 Grenoble, France, 2Universite Grenoble-Alpes, F-38000 Grenoble,a´France, 3INSERM, U1038, F-38054 Grenoble, France, 4Department of Mathematics and Statistics, University ofTurku, Turku, Finland, 5Industrial Biotechnology, VTT Technical Research Centre of Finland, Turku, Finland, 6Schoolof Health Sciences, University of Tampere, Tampere, Finland, 7Plateforme ARN Interference (PArI), DSV/ISVFJ/´SBIGEM/UMR 9198 I2BC, CEA Saclay, F-91191 Gif-sur-Yvette, France, 8Institute of Molecular and CellularRadiobiology, CEA, F-92265 Fontenay-aux-Roses, France, 9INSERM, U967, F-92265 Fontenay-aux-Roses, France,10Universite Paris Diderot, U967, F-92265 Fontenay-aux-Roses, France and 11Universite Paris Sud, U967, F-92265´´Fontenay-aux-Roses, France*To whom correspondence should be addressed.AbstractMotivation: Incorporating gene interaction data into the identiﬁcation of ‘hit’ genes in genomic experiments is a well-established approach leveraging the ‘guilt by association’ assumption to obtaina network based hit list of functionally related genes. We aim to develop a method to allow formultivariate gene scores and multiple hit labels in order to extend the analysis of genomic screening data within such an approach.Results: We propose a Markov random ﬁeld-based method to achieve our aim and show that theparticular advantages of our method compared with those currently used lead to new insights inpreviously analysed data as well as for our own motivating data. Our method additionally achievesthe best performance in an independent simulation experiment. The real data applications we consider comprise of a survival analysis and differential expression experiment and a cell-based RNAinterference functional screen.Availability and implementation: We provide all of the data and code related to the results in thepaper.Contact: sean.j.robinson@utu.ﬁ or laurent.guyon@cea.frSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionHigh-throughput genomic experiments allow for measurements tobe taken on thousands of genes relating to particular biologicalprocesses such as gene expression or exhibition of a phenotype ofinterest. Such experiments generally concern an overly large numberof genes and where ‘hit’ genes in the experiment, those with significant expression or scores, are subsequently identified for furtheranalysis. The hit gene list is a smaller and more easily analysablesubset of genes that is used to inform follow up studies and as ageneral aide for interpretation of the biological question of interest.One classical strategy is to decipher mechanisms of action betweenhit genes in the form of modelled cellular pathways (Wang et al.,2011).Based on a mathematical graph object, genomic networks aremade up of vertices corresponding to genes and edges between vertices possibly corresponding to physical, regulatory or signalling information, for example (Kim et al., 2016). Protein–proteininteraction (PPI) networks, where there is an edge between verticesCV The Author 2017. Published by Oxford University Press.i170This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018Determination of functionally related hit genes with Markov random fieldsif the corresponding genes are known or inferred to have proteinsthat interact, have been previously used as the basis of network analysis of genomic screening data, including from RNA interference(RNAi) screens (Hao et al., 2013; Kumar et al., 2013).Although there is a large body of work on biological networkanalysis in general (Pavlopoulos et al., 2011), overlaying additionalgenomic data leads to further considerations in the analysis. For example, an initial approach is to investigate the distribution of scoresor hits within the PPI network to find structures or areas of interest(Kumar et al., 2013). Network-based tests for differential expression(Jacob et al., 2012) and extensions of Fisher exact tests for enrichment or depletion of functional/gene ontology (GO) annotations(Dong et al., 2016) have also been proposed, along with methods toquantify the clustering of functional/GO annotations in the network(Cornish and Markowetz, 2014). Further sophisticated approachesinclude network inference and gene functional prediction (Ma et al.,2014) and using a PPI network as the basis of a meta-analysis ofmultiple screening data sets (Amberkar and Kaderali, 2015; Haoet al., 2013; Kumar et al., 2013).Our aim is to incorporate PPI networks and the observed genomic data to determine a network based hit list by making use of the‘guilt by association’ assumption (Cornish and Markowetz, 2014;Ma et al., 2014; Wang et al., 2009). It has been commented on thatby visualizing a PPI network overlaid with hit results from an RNAiscreen, genes that are just below a simple hit threshold but connected to many other hit genes in the network could be fruitfullyconsidered as a hit themselves (Kumar et al., 2013). Besides, to facilitate the interpretation of gene hit lists, taking into accountknown or inferred functional relations between proteins would helpin deciphering gene relationships important in the phenotype ofinterest (Markowetz, 2010). There are a number of proposedapproaches to turn this ad hoc notion into a mathematically formulated network determination of hit genes concerning RNAi screening data (Cornish and Markowetz, 2014; Jiang et al., 2015; Wanget al., 2009) as well as gene expression data (Beisser et al., 2010;Dittrich et al., 2008) among others.The Knode (Cornish and Markowetz, 2014), NePhe (Wanget al., 2009) and NEST (Jiang et al., 2015) methods all calculateadditional network based scores for each gene. Table 1 lists the proposed ways in which the PPI network is incorporated into thesemethods. Then the NePhe and NEST scores are simply calculated bysumming or averaging the original scores weighted by the similaritymatrix. For example, considering the ‘shortest paths’ similarity matrix and ‘average’ summation, the network score for each gene is theaverage of all the other original scores weighted by the inverse distance of the shortest path between the vertices. The summing procedure for the Knode method is based on a network adaptation ofRipley’s K-function (Ripley, 2004) and hence ‘Knode’. A hit list inthese cases is a certain number of genes with the highest networkbased scores.The BioNet method (Beisser et al., 2010; Dittrich et al., 2008)aims to find a subgraph of hits. Compared to similar approaches(Chuang et al., 2007), BioNet is guaranteed to find the maximumweight connected subgraph in the network. This subgraph is anticipated to be generally made up of the genes with the most significantscores but genes with non-significant scores may also be containedwithin the subgraph. That is, even though individual genes with nonsignificant scores will themselves contribute a suboptimal weight inthe subgraph, they may allow for other more significantly scoredgenes to be included through their edges, which can give a more optimal overall weight. The list of genes in the maximum-weight connected subgraph can then be taken as the hit list of interest.i171Table 1. The proposed similarity matrices for calculating the Knode(Cornish and Markowetz, 2014), NePhe (Wang et al., 2009) andNEST (Jiang et al., 2015) scoresKnodeAdjacencyCommon neighboursMean steps betweenShortest pathDiffusion kernelNESTXXXXXNePheXXXOur proposed approach is based on Markov random fields(MRFs), mathematical models where associations in the data can beconsidered in an efficient way. Such models have been previouslyused for network-based classification of gene expression data(Stingo and Vannucci, 2011), finding differentially expressed genesin specified pathways (Wei and Li, 2007) and modelling gene expression over a network (Wei and Pan, 2008, 2010). Instead of aiming to model genomic data and the underlying network, we broadlyconsider the same approach to using MRFs for digital image segmentation (Blake et al., 2011; Robinson et al., 2015). That is, thegenomic data are not modelled over the network but rather ‘hit’ and‘non-hit’ genes are labelled taking the network into account. In thisway, we determine a network based hit list comparable to previously proposed methods (Beisser et al., 2010; Cornish andMarkowetz, 2014; Dittrich et al., 2008; Jiang et al., 2015; Wanget al., 2009).We consider a number of different data sets including from asimulated experiment, a lymphoma study with measurements basedon differential expression and survival analysis, and from our ownmotivating RNAi screen. We compare to previously proposed methods to show that our MRF based method performs the best in asimulation study and is able to provide increased pathway enrichment and a greater determination of the hit genes in the previouslyanalysed lymphoma study. For our motivating RNAi screening data,the MRF method is able to find pertinent network hits that areotherwise not discovered by thresholding the multivariate scores,suggesting useful possibilities for further analysis. We show that themajor advantages of our MRF based method are that multivariatescores for genes as well as multiple hit labels are easily available inthe method.2 Materials and methods2.1 Network scoring of hit genesConsider that we have a collection of genes with an associated genenetwork and that each gene is indexed by a scalar i. Let the vertexset V be the set of gene indices and let the edge set E ¼ feij ! 0 j eij¼ eji ; for all i; j 2 Vg be the set of all edges between every pair ofvertices where genes i and j are neighbours in the network if andonly if eij > 0. Let the degree of gene/vertex i beX@i ¼eij :j2VNote that if the graph is not weighted (that is eij 2 f0; 1g for alli; j 2 V), then @i is just the number of neighbours of vertex i.Let the (possibly multivariate) random variable Zi be the dataderived from the genomic experiment for gene i and let the randomvariable Xi be the unobserved label of interest. In the most generalcase, the labels will be ‘hit’ and ‘non-hit’ but it is also possible toDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018i172S.Robinson et al.have more than 2 hit labels. Let the collection of these random variables be a conditional MRF with the associated energy functionXXXXXEðxÞ ¼ui;l Ifxi ¼ lg þwij;lk Ifxi ¼ l; xj ¼ kg (1)ilði;jÞlkfor vertices i and pairs of vertices (i, j) with labels l and k. The minimum energy labels arebx ¼ argmin EðxÞ:x(Fig. 1b). The vertices 1; 2; . . . ; 10 on the left of Figure 1a are true‘blue’ vertices while the vertices 11; 12; . . . ; 20 on the right are true‘red’ vertices. The colour of the vertices in Figure 1a corresponds tothe observed value from the associated distribution.Figure 1c shows the minimum energy labels for different valuesof b. When b ¼ 0 there is no edge information in the energy functionand each vertex is labelled as either ‘blue’ or ‘red’ based only on itsobserved value. That is, whether the observed value is above orbelow the intercept point of the two densities at 0 (Fig. 1b). Forincreasing values of b, the minimum energy labelling increasinglyThe unary potentials ui;l are defined to beui;l ¼ Àlog ðpl ðzi ÞÞ(2)(a)where zi is the observed data for gene i and pl is the probability density function corresponding to label l. The pairwise potentials wij;lkare defined to be(eij if l 6¼ k(3)wij;lk ¼ b0 if l ¼ k:When b ¼ 0, the minimum energy labels are simply given by theunary potentials. That is, each gene i has minimum energy labelbased on the observed data zi and unary potentials (Equation (2))only, independent of the network. This is equivalent to labelling thegenes based on thresholding the scores at the interception points ofthe densities corresponding to each label. When b > 0, the pairwisepotentials (Equation (3)) impose a penalty in the energy function between pairs of neighbouring vertices without the same label. Henceneighbouring vertices are impelled to have the same label in orderthat the total labelling has the minimum possible energy. Althoughthis is balanced against the unary potentials, there exists a value b*above which all of the vertices have the same minimum energy label.This ‘dominant’ label is the label such that the energy is minimizedwhen all vertices are given this label compared to all other labels. ThatPPis, the dominant label l is the label such that i2V ui;l < i2V ui;k forall other labels k. Note that a toy example is presented below.In practice we need to set the value of b as well as the probabilitydensity function pl for each label l. We first consider setting thevalue of b, while setting the underlying densities pl for each label l isbconsidered in Section 2.3. Let x ðbÞ be the minimum energy labelsfor a given value of b. We define the MRF score for vertex i underlabel l asXsi;l ¼ @iIfbi ðbÞ ¼ lgx(4)(b)(c)b2Bwhere B ¼ f0; ð1=nÞbÃ ; ð2=nÞbÃ ; . . . ; bÃ g, n is the resolution and b*is the minimum value of b such that the minimum energy labelling isthe dominant label for all vertices. That is, the score for each vertex,for each label, is the number of values of b for which the vertex is assigned the label, scaled by the degree of the vertex. Since the value ofb is bounded by 0 and b*, and the score is defined as a summationover a range of values of b between these bounds, we are notrequired to actually set the value of b.(d)2.2 Toy exampleIn order to explain the proposed score, Figure 1 presents a toyexample. The graph seen in Figure 1a is made up of two completesubgraphs of nine vertices, each with an additional leaf vertex and asingle edge bridging the two subgraphs. In this case, all of the edgeshave the same weight. The unobserved labels in the network are‘blue’ and ‘red’, each with an associated Gaussian distributioncentred at À1 and 1, respectively, with a standard deviation of 2Fig. 1. Toy example. (a) Original observations and underlying label (‘blue’ or‘red’) for each vertex. The vertices 1; 2; . . . ; 10 on the left are true ‘blue’ vertices while the vertices 11; 12; . . . ; 20 on the right are true ‘red’ vertices, indicated in the vertex labels. The observed value for the vertex is indicated inthe colour of the vertex itself. (b) Densities corresponding to both the ‘blue’ or‘red’ labels. (c) Minimum energy labels for a range of values of b. (d) The MRFscoresDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018Determination of functionally related hit genes with Markov random fields‘smooths out’ up to the point where every vertex is assigned the ‘red’label when b ¼ bÃ ¼ 1.Figure 1d shows the scaled difference in MRF scores(si;‘red’ À si;‘blue’ ) for each vertex. Comparing the final scores to theminimum energy labels for any single value of b, the MRF scoresallow for greater determination of the labelling. For example, whenb ¼ 0:9 the vertices 1; 2; . . . ; 10 have the correct minimum energylabel ‘blue’, but we can see in the final MRF scores that vertices 4and 6 are the strongest ‘blue’. That is, rather than requiring thevalue of b be determined, we simultaneously sidestep this problemand increase the labelling information for each vertex.In this toy example the densities associated to each label are symmetric and so it is just by chance that the dominant label is ‘red’. ThatPPis, it happened to be the case that 20 ui;‘red’ < 20 ui;‘blue’ . Wei¼1i¼1can see that the leaf vertex 20 holds out from the dominant ‘red’ labelmuch longer than the initially ‘blue’ vertex 13 with a higher observedvalue. This is because vertex 13 is much more connected than 20,which is on the periphery of the graph and will therefore hold onto itsinitial label much longer with increasing b. It is this tendency that weaccount for by scaling the MRF score by vertex degree. Hence boththe leaf vertices 1 and 20 are labelled ‘blue’ for as many values of b asvertices 8 or 9 (Fig. 1c) but do not have as high a final MRF scoresince they are less connected in the graph. Although we consideredother graph centrality measures such as betweenness, closeness, harmonic and eigenvector centrality (Pavlopoulos et al., 2011), none ofthem was as consistently suitable or were as computationally feasibleas simply using vertex degree.2.3 Setting the underlying densitiesIn the toy example of Figure 1, we knew the true densities associatedto each label and so were able to use them whereas this is not thecase in practice. However, in all of our applications the observeddata are associated to a P-value as derived from a screen or other experiment. Hence for the ‘non-hit’ label, we use a standard uniformdensity and for the ‘hit’ label we use an exponential density. Wechose an exponential density as it is peaked at 0 but does not asymptote at the y axis and decays much slower than a truncated Gaussiandensity. In the general ‘hit’ and ‘non-hit’ scenario we suggest an exponential density that intercepts the standard uniform density at 0.3(Supplementary Fig. S1).3 Results and discussion3.1 The MRF method allows for the network analysisof multivariate RNAi data and finds pertinentfunctional/GO enrichmentWe consider data from an RNAi screen aiming to identify genesimplicated in DNA repair after induction of oxidative DNA damage(Guyon et al., 2015). Briefly, in the context of this screen, HeLa cellsspecifically engineered to express OGG1 [the initiating enzyme ofthe base excision repair (BER) of oxidized guanine] fused to aGreen Fluorescent Protein (GFP), were systematically transfectedwith siRNAs targeting the ‘druggable’ subset of genes from humangenome (3 siRNAs/gene; 7218 target genes). Three days posttransfection, 8-oxo-7,8-dihydro-guanine (8-oxoG) DNA base lesions were induced by cell exposure to potassium bromate (KBrO3)and the recruitment of chromatin-bound OGG1-GFP was quantified(subsequently the fluorescence intensity is averaged in each nucleus)by computer-assisted imaging performed on an automated epifluorescence microscope (Operetta, Perkin Elmer).i173Each observation was converted to a U-score (Guyon et al.,2015), which has a standard Gaussian distribution under the nullhypothesis that we observe no phenotypic difference in the geneknock-down condition. We only consider the negative phenotypeand hence a low P-value is associated to a negative siRNA score corresponding to a decrease of chromatin bound OGG1 upon siRNAtransfection. For each gene, we converted either the known RefSeqgene ID or gene symbol to Entrez gene ID using bioDBnet(Mudunuri et al., 2009) (latest release bioDBnet 2.1; May 6, 2015),which was subsequently converted to STRING ID (Szklarczyk et al.,2014), resulting in 4006 genes. Hence our RNAi screening data setis made up of 4006 genes each with 3 scores/P-values (hereafterOGG1 data).The STRING database (Szklarczyk et al., 2014) compiles knownand inferred PPI information from experimental sources as well astext mining the literature. The strength of evidence for each interaction is calculated from these multiple sources (Von Mering et al.,2005) and is given as a weight on the corresponding edge. Table 2gives the proportion of edges in the 4006 vertex ‘combined’ PPI network for the OGG1 data that have a contribution from each source.Also shown are the proportion of edges that have a unique contribution from a source although note that otherwise Table 2 does notgive the extent of contribution, just that the source contributed.Previous analysis has shown that a confounding factor betweenthe way gene annotation is carried out and the construction of PPInetworks has resulted in a ‘circular’ way of optimizing and validating algorithms for gene function prediction (Gillis and Pavlidis,2011). In this case, considering functional/GO annotation enrichment will be problematic due to an ‘annotation bias’ (Gillis et al.,2014), where highly studied genes both have more known proteininteractions (edges in the PPI network) as well as more functional/GO annotations.Table 2 shows that ‘text mining’ is the most prominent source ofinteraction evidence, which is also the most likely source to be associated to an annotation bias. In this case, PPI evidence is automatically extracted from abstracts in the literature, which likely furthersthe issue that well studied genes with many known functional/GOannotations have many known or inferred protein interactions andhence edges in the network. Not only does ‘text mining’ contributeto $93% of all edges, but it is the unique source of information for$54% of edges in the ‘combined’ network. Both ‘experimental’ and‘database’ concern interaction information gathered from other PPIdatabases which are also likely to be affected by an ‘annotation bias’among others (Gillis et al., 2014).The ‘co-expression’ evidence concerns genes found to have beenco-expressed in a variety of experiments and across a number of different species (Stuart et al., 2003; Von Mering et al., 2005).Conservation of co-expressed genes over multiple species implies aselective advantage and hence that the genes are functionally relatedTable 2. Proportion of edges with contributions from different sources in the overall ‘combined’ PPI network for the OGG1 dataContributeText miningExperimentalCo-expressionDatabaseNeighbourhoodCo-occurrenceFusionDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018Uniquely contribute0.92680.32940.11340.07220.01610.01190.00050.53640.02950.00950.02360.00060.00150.0000i174(Xulvi-Brunet and Li, 2010). This is one source in the STRING database that we expect not to be influenced by an annotation bias(Gillis et al., 2014) and with a low unique contribution, this evidence additionally conforms to that from the other sources.Supplementary Figure S2 shows the vertex degree and the number of functional/GO annotations from DAVID (Huang et al.,2009a, b) for the 4006 genes in both the ‘combined’ and ‘co-expression’ networks. In the combined network we can see a high linearcorrelation while in the co-expression network vertex degree doesnot seem to be correlated to the number of terms at all. Although itis known that some functional/GO annotation terms are inferredfrom expression patterns, this potential issue does not appear to bepresent in the same way as the ‘annotation bias’ (Supplementary Fig.S2) for the OGG1 data. Note that each edge in both networks isweighted by the strength of evidence for that interaction and hencevertex degree is not simply the number of neighbours. Hence, weuse the co-expression PPI network for the network analysis of theOGG1 data. The network is made up of 4006 vertices and 43 097weighted edges (density ¼ 0.0054).Recall that for the OGG1 data there are three scores and hencethree P-values for each gene. However, there is no natural or intrinsic way to order the siRNAs for each gene so that the data can beconsidered as trivariate. Correspondent trivariate data are achievedby ordering the three P-values so that the first data dimension is thelowest P-value, the second data dimension is the median P-valueand the third data dimension is the highest P-value for each gene.Figure 2 presents the density schematic for the OGG1 data. Here,there are four labels of interest, a given number of negative siRNAs,as well as the dominant ‘no negative siRNAs’ label. Note that thedensities in each dimension intercept at increasingly significant Pvalues. This is necessary as due to the ordering of the P-values, thelowest are skewed towards 0 and require a lower intercept so thatthe ‘no negative siRNAs’ label remains the dominant label. Hence agene is guaranteed to have a non-zero MRF score for the ‘1 negativesiRNA’ label if its lowest P-value is below 0.1, the ‘2 negativesiRNAs’ label if its lowest P-value is below 0.1 and its median Pvalue is below 0.2, and the ‘3 negative siRNAs’ label if its lowest Pvalue is below 0.1, its median P-value is below 0.2 and its highest Pvalue is below 0.3. It is the consideration of multiple siRNAs foreach gene that aims to address the variability of the P-values. Thenwhen we consider our MRF score, we consider all dimensions sothat ‘hits’ have either 2 or 3 negative siRNAs.Figure 3 shows the log10(P-values) for Fisher exact tests of enrichment or depletion in functional/GO annotation terms for the top200 hits for the MRF method (si;‘2 negative siRNAs’ þ si;‘3negative siRNAs’ )against the top 200 hits obtained by ordering the genes by medianP-value. These hit lists are comparable in that if the median P-valueis below a certain threshold, at least 2 of the P-values are below thethreshold and hence we consider both the ‘2 negative siRNAs’ and‘3 negative siRNAs’ labels from the MRF method. A greater functional/GO enrichment can be generally observed in the MRF basedhit lists.Supporting the pertinence of the network based hits, we considerthose with the increased enriched ‘nuclear lumen (GO:0031981)’annotation (Fig. 3), where BER takes place. Nuclear lumencorresponds to the localization of genes comprised in the whole volume inside the nuclear inner membrane, including ‘nuclear chromosome (GO:0000228)’, ‘nucleoplasm (GO:0005654)’ and ‘nucleolus(GO:0005730)’. Figure 4 shows TOP3A and its neighbours that arealso present in the MRF hit list. Genes with the ‘nuclear lumen(GO:0031981)’ annotation have diamond vertices. Neither TOP3Awith P-values (0.03, 0.03, 0.05), nor POLR1C with P-values (0.01,S.Robinson et al.MaximumP-valueMedianP-valueMinimumP-valueP-valueFig. 2. Schematic diagram of the construction of the trivariate densities corresponding to each label for the OGG1 data. In each dimension there are twopossibilities and an associated univariate density: ‘null case’ (U(0, 1)) and‘negative siRNA’ (gÁ ). Trivariate densities are deﬁned for each of four labels ofinterest and are schematically represented by encompassing a singlepossibility in each dimension. The four labels are: ‘3 negative siRNAs’ (3 lowP-values), ‘2 negative siRNAs’ (2 low P-values), ‘1 negative siRNA’ (1 lowP-value) and ‘no negative siRNAs’ (no low P-values). Note that the ordering ofthe P-values limits the possible combinations of trivariate densities acrossthe three dimensions in this caseFig. 3. Log10(P-values) for Fisher exact tests of enrichment or depletion infunctional/GO annotations for the hits lists obtained from the MRF methodand median P-value. The annotation term ‘nuclear lumen (GO:0031981)’ hasbeen highlighted0.05, 0.12) are strong hits when ordered by their median P-valuewith ranks 500 and 576 respectively. However, both of these genescan be seen to be likely hits based on a consensus judgment of theirP-values and are much more highly ranked with the MRF method.Neighbours of TOP3A also in the MRF hit list include otherDNA repair proteins such as RAD50 and ERCC4, which belongs toa pathway previously shown to be involved in the repair of oxidizedbases (Parlanti et al., 2012). The presence of TOP3A and POLR1Cin this cluster, both proteins involved in transcription, supports thelink between transcription and the repair of the oxidized guanine.Moreover, ERCC4 is known to participate in the repair of activelytranscribed genes and more globally in transcription, consistent withDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018Determination of functionally related hit genes with Markov random fieldsFig. 4. TOP3A and neighbours present in the top 200 MRF hits. Vertices arecoloured based on median score/P-value. Diamond vertices correspond tothe genes with the ‘nuclear lumen (GO:0031981)’ annotation. The networkwas visualized in Cytoscape 3.3.0 (Shannon et al., 2003) using the yFilesOrganic layoutthe hypothesis of a preferential repair of 8-oxoguanine present in active regions of the genome (Amouroux et al., 2010).We additionally investigated the functional/GO enrichment forhit lists based on vertex degree. Supplementary Table S1 showsthat the vertices with the highest degree are indeed the most enriched in exceptional terms of interest such as ‘Pathways in cancer(hsa05200)’. However, for the co-expression network degree, theseexceptional terms are not the most enriched but rather a number ofterms associated with nucleotide and nucleoside binding in particular. This suggests that our choice to use the co-expression PPInetwork was well founded and allows us to avoid the previouslyidentified annotation bias clearly present in the combined PPInetwork.To investigate the extent of the influence of the network on theMRF hit list we permuted the gene scores (maintaining the coexpression PPI network structure) and considered which genes arepresent in the MRF hit list. This was carried out 100 times andSupplementary Figure S3 shows the proportion of times that eachgene was present in the hit list plotted by its vertex degree. It appearsthat genes with a higher degree are generally more often present inthe hit list and in the extreme case, genes with no neighbours willnever be in the hit list. However, it is clear that simply because a vertex has a high degree, the corresponding gene is not automaticallypresent in the hit list.Here we have demonstrated the flexibility of our MRF based network scoring method that allows for multivariate densities corresponding to multiple labels of interest to be readily defined. Weconsidered functional/GO enrichment as a guide for interpretationthat has allowed for the formation of further relevant hypotheses forspecifically identified genes. We took particular care to account forthe previously observed annotation bias and also investigated thedirect influence of the PPI network on our results.3.2 The MRF method finds greater pathway enrichmentand allows for more detailed determination ofhit genes in a lymphoma studyWe consider data from a study of diffuse large B-cell lymphoma previously analysed using both the BioNet and Knode methods. Foreach gene, risk association was obtained from a survival analysisand differential expression was measured between the lymphomasubtypes ABC and GCB (Beisser et al., 2010; Dittrich et al., 2008).Hence there are 2 P-values for each gene corresponding to survivalanalysis (S) and differential expression (T). A method for combiningi175multiple P-values using order statistics was also proposed withBioNet (Dittrich et al., 2008; Beisser et al., 2010). As can be seen inSupplementary Figure S4, the low combined P-values are those thathave both low S and T P-values and if one of the P-values is high,then the combined P-value is high.The PPI network for the lymphoma data was sourced from theHuman Protein Reference Database (Prasad et al., 2009), which isno longer maintained. We could have considered obtaining anotherPPI network from a more contemporary source and also attemptedto account for the previously identified ‘annotation bias’. However,the aim of this section is not to analyse the lymphoma data for itsown sake but rather to compare the MRF output to the previouslypublished output from the BioNet and Knode methods. The PPI network is made up of 2034 vertices and 15 512 unweighted edges(density ¼ 0.0038).We consider the lymphoma data to show a general advantage ofthe MRF method in that we both find greater pathway enrichmentand are able to more generally determine the hit genes as against theBioNet analysis. In this experiment, there are many genes with onehigh and one low P-value resulting in a high combined P-value(Supplementary Fig. S4). Assuming that interest lies not only whenboth P-values are low but when one or the other is, we can achievesuch output with the MRF method by defining bivariate densitiesfor these labels. Figure 5 shows the construction of the bivariate densities where the 4 labels are ‘S and T hit’, ‘S hits only’, ‘T hits only’and ‘no hits’. The P-values in the T dimension are generally muchlower (Supplementary Fig. S4b) and approximately half of them arebelow 0.2. In order that the ‘no hits’ label remains dominant, we setthe intercept of the exponential density and the standard uniform inthe T dimension to be 0.2 (Fig. 5).Recall that the BioNet output is a binary labelling for each geneand the hit list is the list of genes in the maximum-weight connectedsubgraph. Since the subgraph obtained from BioNet contains 46genes, following the Knode analysis (Cornish and Markowetz,2014), we also consider the top 46 genes as the hit list for each othermethod. In this case, we obtain 3 hit lists from the MRF outputbased on ranking the genes for each of the three labels of interest, ‘Sand T hit’, ‘S hits only’ and ‘T hits only’. Figure 6 shows the overlapin the top 46 genes for each method. Firstly, there is no overlap between the top 46 genes for each of the lists from the MRF method asexpected. Similarly, the BioNet hit list does not intersect with eitherthe ‘S hit only’ or ‘T hit only’ lists but does overlap with the ‘S and Thits’ list. This again makes sense since the ‘S and T hits’ label is theclosest to BioNet since both P-values being low is necessary for thecombined P-value used by BioNet to be low (Supplementary Fig.S4). There is reasonable overlap between the vertex degree hit listand all the other lists except ‘S hits only’. This makes sense for theMRF hit lists as vertex degree is explicitly present the MRF score(Equation (4)). This is also not surprising for BioNet since verticeswith high degree are likely to be included in the maximum-weightconnected subgraph since they allow access to the most number ofother vertices through their edges.Also shown in Figure 6 are genes associated with the carcinogenic NFjB pathway (Hoesel and Schmid, 2013), originally used toevaluate the BioNet output for the lymphoma data (Dittrich et al.,2008). We can see that most of the NFjB genes contained in theBioNet list are also present in the ‘S and T hits’ list, while there areadditional such genes in the ‘T hits only’ list. The ‘S hits only’ listdoes not have any NFjB genes (although it is not significantlydepleted) which is interesting in itself. Although the vertex degreehit list has eight NFjB genes in total, only two of them are exclusiveand there are an additional five genes the MRF lists contain that areDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018i176S.Robinson et al.PPA general advantage of the MRF method that it readily allows formore than a single hit list for multiple labels of interest. The additional information available in this way is not possible to determineafter the P-values have been combined as was done for input into theBioNet method. The output from Knode (Supplementary Fig. S5)generally conforms to the above discussion. Although multivariateP-values do not appear to be a problem in principle for either theBioNet or Knode methods, it is not clear how multiple labels beyondbinary ‘hit’ and ‘non-hit’ could also be achieved. The output of theNePhe and NEST methods, as well as simply ordering by combinedP-value is also provided in the supplementary MATLAB code.3.3 The MRF method performs the best in anindependent simulation experimentFig. 5. Schematic diagram of the construction of the bivariate densities corresponding to each label for the lymphoma data. In each dimension there aretwo possibilities and an associated univariate density: ‘non-hit’ (U(0, 1)) and‘hit’ (fÁ ). Bivariate densities are deﬁned for each of four labels of interest andare schematically represented by encompassing a single possibility in eachdimension. The four labels are: ‘S and T hits’ (low P-value in both dimensions), ‘S hits only’ (low P-value in S dimension), ‘T hits only’ (low P-value inT dimension) and ‘no hits’ (no low P-value in either dimension)Fig. 6. Venn diagram of the hit lists for the lymphoma data overlaid withgenes associated with the NFjB pathway. There are 46 genes in each hit list(black) along with the genes associated with the NFjB pathway (grey andlisted at the sides in blue). The NFjB annotation was obtained from KEGG(Kanehisa et al., 2016). Below each method is the P-value for the Fisher exacttext for enrichment or depletion of the NFjB terms in the hit list. The Venndiagram is based on the layout from http://bioinformatics.psb.ugent.be/webtools/Venn/not in the degree list. Hence we are able to find additional enrichment in the NFjB pathway as previously reported for BioNet whilegaining further insight into the lymphoma study by demarcatingsuch genes as ‘S and T hits’ or ‘T hit only’, as well as that we find noNFjB genes for ‘S hit only’.We consider the simulation experiment used to evaluate the Knodemethod against BioNet (Cornish and Markowetz, 2014). A scalefree network with 1000 vertices was simulated with 3 ‘clusters’ ofdesignated hits resulting in a total of 30 ‘true hit’ vertices. The hitvertices have P-values simulated from a truncated Gaussian distribution centred at 0 while the non-hit vertices have P-values simulated from a standard uniform distribution. Rather than usingthe original standard deviation of 10À6 for the hit distribution, weconsider a standard deviation of 0.05. This greatly increased value isnecessary because under the original simulation set-up, the density was too peaked at 0 and simply taking the top 30 verticesordered by lowest P-value finds the highest proportion of true hits(Supplementary Fig. S6).For the MRF method, we used our general exponential densityscheme (Supplementary Fig. S1), rather than the known densitiessince this information is not available for the other methods we arecomparing to. For the BioNet method, the input FDR parameterwas set to 0.8 in order that the maximum-weight connected subgraph could be found. The similarity matrix used for the Knodemethod was the diffusion kernel, the same as was used originally(Cornish and Markowetz, 2014). We considered the NePhe scorewith the shortest path similarity matrix and average summationmethod, the best performing pair of options. We additionally considered the NEST score (Jiang et al., 2015) along with simply ordering the vertices by vertex degree (highest) and P-value (lowest). Thehit lists are the top 30 vertices obtained from each method.Figure 7 shows box plots of the proportion of true hits verticesfor each method obtained over 1000 simulation runs. Since it isknown that there are 30 true hits, as originally presented (Cornishand Markowetz, 2014), we consider the proportion of true hits ineach hit list of size 30. We can see that simply ordering the verticesby degree performs very poorly, while ordering by P-value is betterbut still relatively poor. The NEST, NePhe and Knode scores perform quite similarly while BioNet has a lower median performanceand a much greater spread. The MRF method clearly gives the bestperformance and when using the known true hit density rather thanour general exponential scheme, the MRF method gives even betterresults, with a median proportion just below 0.8 and a similarspread (not shown).There were 30 simulated runs where BioNet obtained a proportion correct >0.8 and for 12 of those runs, the proportion correctwas 1. However, for the BioNet method there is no guarantee thatthe maximum-weight connected subgraph contains only 30 verticesand in these cases, the minimum number of vertices in the returnedsubgraph was 43 while the median was 91. Overall, the median sizeof the BioNet hit list was 27 vertices (mean 39.53) with a maximumsize of 251 vertices. So even without accounting for the falseDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018Determination of functionally related hit genes with Markov random fieldspositives in the BioNet hit lists, the number of true positives found isstill generally worse than other methods, which was also the case inthe original simulation experiment (Supplementary Fig. S6).Simulating a biological network is particularly difficult as evencharacterizing network topologies is a challenging problem(Pavlopoulos et al., 2011). Here, the simulated scale free networkswere trees whereas this is an unlikely attribute of a 1000 vertex PPInetwork. Additionally, simulating genomic data are equally difficultand recent tools, for example to simulate gene expression based onRNA sequencing (Benidt and Nettleton, 2015; Frazee et al., 2015),do not then further consider the simulated data over a network. Wehave just used an independent simulation experiment to compareour MRF score to a number of other previously proposed methods.Such a comparison with known ‘true hits’ is otherwise not possiblefor any real data application.3.4 Model fitting discussionThe intercept of the underlying densities in our general set-up(Supplementary Fig. S1) has an interpretation as the P-value belowwhich genes are guaranteed to have a non-zero MRF score. Wefound that an intercept of 0.3 generally gave reasonable results. Thisvalue is near the maximum possible intercept point while still beingreasonably peaked at 0 (Supplementary Fig. S1). Under this generalscheme, all genes with P-values <0.3 will therefore be a ‘hit’ at b ¼ 0and hence will have a non-zero MRF score. Genes with P-values >0.3 may be a hit for some other value of b dependent on theirP-value and neighbours, it is just not guaranteed they will have anon-zero score. The consideration here is that the intercept shouldbe as high as possible so that many genes are scored, while maintaining that the ‘non-hit’ label is the dominant label and so the finalMRF scores are therefore a measure of the strength of the hit. Notethat the hit density is not a model for the observed ‘true hit’ P-valuesbut simply a discriminative element in our MRF based method.A general trade-off in a network based hit list is the loss of information relating to screening data that does not conform to the network. In the most extreme case, if a vertex does not have anyneighbours then the corresponding gene cannot be a network basedhit. This is equally true for any of the other previously proposedi177methods we have compared to. Hence the network based hit listshould never be considered by itself but rather as an additional network ‘grouped’ or ‘smoothed’ hit list that is considered in relation tothe original threshold hit list. In our case, by using a co-expressionPPI network for the OGG1 data, our network based hit list favoursfunctionally related (Stuart et al., 2003; Xulvi-Brunet and Li, 2010)moderate to strong hits at the expense of strong isolated hits. Thisdirectly facilitates the investigation of mechanisms of interest for theselected phenotype. The full ‘combined’ PPI network may also be ofinterest to investigate well studied groups of genes in other biological contexts. For example, co-expression PPI networks may not include important interactions such as those involving house-keepinggenes. It would also be of interest to investigate PPI networksinferred from single specific studies (Huttlin et al., 2015; Rollandet al., 2014), rather than inferred from a collection of many disparate studies such as the PPI data collated by STRING.PPI networks are the most common genomic networks utilizedin such analysis. However, in general there are known issues withthe accuracy of PPI network data (Mahdavi and Lin, 2007; Panet al., 2015) as well as the fact that proteins that may not physicallyinteract but that still act on the same cell or molecular function arenot taken into consideration. Furthermore, as opposed to regulatorynetworks, it is not known whether any given interaction represents apositive or negative effect, for example. It is possible that such additional information could be incorporated within a network basedanalysis. More generally, such an approach could also be consideredwith directed network data from other sources, for example signalling or metabolic pathway information, and this could be a fruitfulsubject for future work.3.5 Implementation in MATLAB and computationalexpenseThe MATLAB code to reproduce the results presented in the paperis given as Supplementary Material. Our MRF based network scoring method was implemented in MATLAB on a mid-2012 MacBookPro with a 2.6 GHz Intel Core i7 processor (quad-core) and 16 GBof RAM. The minimum energy labels were found using the a-expansion algorithm (Boykov et al., 2001; Boykov and Kolmogorov,2004; Kolmogorov and Zabih, 2004). Running over n ¼ 1000 valuesof b took $1.3 s for the simulated data (1000 vertices, 2 labels),$20 s for the lymphoma data (2034 vertices, 4 labels) and $70 s forthe OGG1 data (4006 vertices, 4 labels). Due to the heterogeneousnature of the genomic data and networks, our MRF-based method iscurrently not suitable for a graphical user interface. For example,the flexibility to consider multivariate P-values as well as multiplelabels of interest beyond just binary ‘hit’ and ‘non-hit’ is bestachieved using a command line input as available in MATLAB.4 ConclusionFig. 7. Box plots of the proportion of true hit vertices identiﬁed for eachmethod in the ‘3 cluster’ Knode simulation scheme (1000 simulation runs)(Cornish and Markowetz, 2014)We have proposed a network based method to determine hit genesusing an MRF and have shown the effectiveness of the method usinga broad range of different data sets. The multiple advantages of theMRF method are that it easily allows for multivariate gene scores inaddition to multiple labels of interest beyond binary ‘hit’ and ‘nonhit’. The intercept of the underlying densities can be considered as aparameter with an interpretation and for which we have providedgeneral advice on setting. We have provided the MATLAB code anddata to reproduce the results presented in the paper, which is freelyavailable for modification.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018i178We have shown that our MRF method gave the best results in asimulation experiment originally used to evaluate Knode andBioNet. We have additionally shown that we were able to findgreater pathway enrichment as well as further determine hit genes ina lymphoma study concerning survival analysis and gene expression,whereas the multiple observed P-values for each gene were combined for input into the BioNet method. For the OGG1 screeningdata we have shown that the MRF method allows for the analysis ofthe multivariate scores and is able to find relevant functional/GOannotation. For the study of genomic data with an associated network where the ‘guilt by association’ assumption is appropriate, wehave shown that our proposed MRF based method gives an advantageous way to determine network based hit genes as against previously proposed methods.We also noted a major pitfall using functional/GO enrichmentfor validation when it is strongly correlated with vertex degree inthe PPI network and confounded by an ‘annotation bias’. We considered a network of only co-expression PPI evidence to account forthis issue and also took this into consideration when evaluating ourresults. This issue does not appear to have been given enough attention in the literature, especially when there are so many papers utilizing both PPI networks as well as functional/GO enrichmentanalysis.AcknowledgementsThe authors wish to thank Xavier Gidrol for useful discussion and suggestions. The authors would also like to thank the editor and three anonymousreviewers for valuable suggestions.FundingS.R. is the beneﬁciary of a CEA-Industry thesis contract and a VTT thesis contract. This work was supported by a grant from the Association pour laRecherche contre le Cancer (PJA 20151203141 to J.P.R.) and from Universite´Grenoble-Alpes (AGIR-PEPS ABC_ARNi to L.G.).Conﬂict of Interest: none declared.ReferencesAmberkar,S.S., and Kaderali,L. (2015) An integrative approach for a networkbased meta-analysis of viral RNAi screens. Algorithms Mol. Biol., 10, 1.Amouroux,R. et al. (2010) Oxidative stress triggers the preferential assemblyof base excision repair complexes on open chromatin regions. Nucleic AcidsRes., 38, 2878–2890.Beisser,D. et al. (2010) Bionet: an R-package for the functional analysis of biological networks. Bioinformatics, 26, 1129–1130.Benidt,S., and Nettleton,D. (2015) Simseq: a nonparametric approach to simulation of RNA-sequence datasets. Bioinformatics, 31, 2131–2140.Blake,A. et al. (ed.) (2011). Markov Random Fields for Vision and ImageProcessing. The MIT Press, Cambridge, MA.Boykov,Y., and Kolmogorov,V. (2004) An experimental comparison of mincut/max-ﬂow algorithms for energy minimization in vision. IEEE Trans.Pattern Anal. Mach. Intell., 26, 1124–1137.Boykov,Y. et al. (2001) Fast approximate energy minimization via graph cuts.IEEE Trans. Pattern Anal. Mach. Intell., 23, 1222–1239.Chuang,H.-Y. et al. (2007) Network-based classiﬁcation of breast cancer metastasis. Mol. Syst. Biol., 3, 140.Cornish,A.J., and Markowetz,F. (2014) Santa: quantifying the functional content of molecular networks. PLOS Comput. Biol, 10, e1003808.Dittrich,M.T. et al. (2008) Identifying functional modules in protein–proteininteraction networks: An integrated exact approach. Bioinformatics, 24,i223–i231.S.Robinson et al.Dong,X. et al. (2016) Lego: a novel method for gene set over-representationanalysis by incorporating network-based gene weights. Sci. Rep., 6, 18871.Frazee,A.C. et al. (2015) Polyester: simulating RNA-seq datasets with differential transcript expression. Bioinformatics, 31, 2778–2784.Gillis,J., and Pavlidis,P. (2011) The impact of multifunctional genes on “guiltby association” analysis. PLOS One, 6, e17258.Gillis,J. et al. (2014) Bias tradeoffs in the creation and analysis of protein–protein interaction networks. J. Proteomics, 100, 44–54.Guyon,L. et al. (2015) U-score: A cell-to-cell phenotypic scoring method forsensitive and selective hit discovery in cell-based assays. Sci. Rep., 5, 14221.Hao,L. et al. (2013) Limited agreement of independent RNAi screens forvirus-required host genes owes more to false-negative than false-positive factors. PLOS Comput. Biol., 9, 1003235.Hoesel,B., and Schmid,J.A. (2013) The complexity of NF-jB signaling in inﬂammation and cancer. Mol. Cancer, 12, 1.Huang,D.W. et al. (2009a) Bioinformatics enrichment tools: paths toward thecomprehensive functional analysis of large gene lists. Nucleic Acids Res.,37, 1–13.Huang,D.W. et al. (2009b) Systematic and integrative analysis of large genelists using DAVID bioinformatics resources. Nat. Protoc., 4, 44–57.Huttlin,E.L. et al. (2015) The BioPlex network: a systematic exploration of thehuman interactome. Cell, 162, 425–440.Jacob,L. et al. (2012) More power via graph-structured tests for differentialexpression of gene networks. Ann. Appl. Stat., 6, 561–600.Jiang,P. et al. (2015) Network analysis of gene essentiality in functional genomics experiments. Genome Biol., 16, 10.Kanehisa,M. et al. (2016) Kegg as a reference resource for gene and protein annotation. Nucleic Acids Res., 44, D457–D462.Kim,Y.-A. et al. (2016) Understanding genotype-phenotype effects in cancervia network approaches. PLOS Comput. Biol., 12, e1004747.Kolmogorov,V., and Zabih,R. (2004) What energy functions can be minimized via graph cuts?. IEEE Trans. Pattern Anal. Mach. Intell., 26,147–159.Kumar,P. et al. (2013) Screensifter: analysis and visualization of RNAi screening data. BMC Bioinform., 14, 290.Ma,X. et al. (2014) Integrative approaches for predicting protein function andprioritizing genes for complex phenotypes using protein interaction networks. Brief. Bioinform., 15, 685–698.Markowetz,F. (2010) How to understand the cell by breaking it: network analysis of gene perturbation screens. PLOS Comput. Biol., 6, e1000655.Mahdavi,M.A., and Lin,Y.-H. (2007) False positive reduction in protein–protein interaction predictions using gene ontology annotations. BMCBioinform., 8, 262.Mudunuri,U. et al. (2009) bioDBnet: the biological database network.Bioinformatics, 25, 555–556.Pan,A. et al. (2015) Computational analysis of protein interaction networksfor infectious diseases. Brief. Bioinform., 17,517–526.Parlanti,E. et al. (2012) The cross talk between pathways in the repair of 8oxo-7, 8-dihydroguanine in mouse and human cells. Free Radic. Biol. Med.,53, 2171–2177.Pavlopoulos,G.A. et al. (2011) Using graph theory to analyze biological networks. BioData Mining, 4, 1.Prasad,T.K. et al. (2009) Human protein reference database – 2009 update.Nucleic Acids Res., 37(suppl 1), D767–D772.Ripley,B.D. (2004). Spatial Statistics. John Wiley & Sons, Hoboken, NJ.Robinson,S. et al. (2015) Segmentation of image data from complex organotypic 3D models of cancer tissues with Markov random ﬁelds. PLOS One,10, e0143798.Rolland,T. et al. (2014) A proteome-scale map of the human interactome network. Cell, 159, 1212–1226.Shannon,P. et al. (2003) Cytoscape: a software environment for integratedmodels of biomolecular interaction networks. Genome Res., 13,2498–2504.Stingo,F.C., and Vannucci,M. (2011) Variable selection for discriminant analysis with Markov random ﬁeld priors for the analysis of microarray data.Bioinformatics, 27, 495–501.Stuart,J.M. et al. (2003) A gene-coexpression network for global discovery ofconserved genetic modules. Science, 302, 249–255.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018Determination of functionally related hit genes with Markov random fieldsSzklarczyk,D. et al. (2014) String v10: protein–protein interaction networks,integrated over the tree of life. Nucleic Acids Res., 43, D447–D452.Von Mering,C. et al. (2005) STRING: known and predicted protein–proteinassociations, integrated and transferred across organisms. Nucleic AcidsRes., 33(suppl 1), D433.Wang,L. et al. (2009) A network-based integrative approach to prioritize reliable hits from multiple genome-wide RNAi screens in drosophila. BMCGenomics, 10, 220.Wang,X. et al. (2011) HTSanalyzeR: an R/Bioconductor package for integratednetwork analysis of high-throughput screens. Bioinformatics, 27, 879–880.i179Wei,P., and Pan,W. (2008) Incorporating gene networks into statistical testsfor genomic data via a spatially correlated mixture model. Bioinformatics,24, 404–411.Wei,P., and Pan,W. (2010) Network-based genomic discovery: applicationand comparison of Markov random-ﬁeld models. J. R. Stat. Soc. Ser. C(Appl. Stat.), 59, 105–125.Wei,Z., and Li,H. (2007) A Markov random ﬁeld model for network-basedanalysis of genomic data. Bioinformatics, 23, 1537–1544.Xulvi-Brunet,R., and Li,H. (2010) Co-expression networks: graph propertiesand topological comparisons. Bioinformatics, 26, 205–214.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i170/3953959by gueston 07 January 2018
5028881977002	PMID28881977	5028881977	https://watermark.silverchair.com/btx243.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881977.main.pdf	Bioinformatics, 33, 2017, i225–i233doi: 10.1093/bioinformatics/btx243ISMB/ECCB 2017Denoising genome-wide histone ChIP-seq withconvolutional neural networksPang Wei Koh1,2,†, Emma Pierson1,† and Anshul Kundaje1,2,*1Department of Computer Science and 2Department of Genetics, Stanford University, Stanford, CA 94305, USA*To whom correspondence should be addressed.†These authors contributed equally to this work.AbstractMotivation: Chromatin immune-precipitation sequencing (ChIP-seq) experiments are commonlyused to obtain genome-wide proﬁles of histone modiﬁcations associated with different types offunctional genomic elements. However, the quality of histone ChIP-seq data is affected by many experimental parameters such as the amount of input DNA, antibody speciﬁcity, ChIP enrichmentand sequencing depth. Making accurate inferences from chromatin proﬁling experiments that involve diverse experimental parameters is challenging.Results: We introduce a convolutional denoising algorithm, Coda, that uses convolutional neuralnetworks to learn a mapping from suboptimal to high-quality histone ChIP-seq data. This overcomes various sources of noise and variability, substantially enhancing and recovering signalwhen applied to low-quality chromatin proﬁling datasets across individuals, cell types and species.Our method has the potential to improve data quality at reduced costs. More broadly, this approach—using a high-dimensional discriminative model to encode a generative noise process—isgenerally applicable to other biological domains where it is easy to generate noisy data but difﬁcultto analytically characterize the noise or underlying data distribution.Availability and implementation: https://github.com/kundajelab/coda.Contact: akundaje@stanford.edu1 IntroductionDistinct combinations of histone modifications are associated withdifferent classes of functional genomic elements such as promoters,enhancers and genes (Consortium et al., 2015). Chromatin immunoprecipitation followed by sequencing (ChIP-seq) experiments targeting these histone modifications have been used to profilegenome-wide chromatin state in diverse populations of cell typesand tissues (Consortium et al., 2015), allowing us to better understand the mechanisms of development (Bernstein et al., 2006) anddisease (Gjoneska et al., 2015).However, the quality of histone ChIP-seq experiments is affectedby a number of experimental parameters including antibody specificity and efficiency, library complexity and sequencing depth (Junget al., 2014). Achieving optimal experimental parameters and comparable data quality across experiments is often difficult, costly oreven impossible, resulting in low sensitivity and specificity of measurements especially in low input samples such as rare populations ofprimary cells and tissues (Acevedo et al., 2007; Brind’Amour et al.,2015; Cao et al., 2015). For example, Brind’Amour et al., (2015)found that single mouse embryos do not provide enough cells to profile using conventional ChIP-seq techniques. Similarly Acevedoet al., (2007) notes that tumor biopsies, fractionated cell populationsand differentiating embryonic stem cells provide very small numbersof cells to use as input populations. Furthermore, the high sequencing depths (>50–100M reads) required for saturated detection of enriched regions in mammalian genomes for several broad histonemarks (Jung et al., 2014) are often not met due to cost and materialconstraints. Suboptimal and variable data quality significantly complicate and confound integrative analyses across large collections ofdata.To overcome these limitations, we introduce a convolutionaldenoising algorithm, called Coda, that uses convolutional neuralnetworks (CNNs) (Jain and Seung, 2009; Krizhevsky et al., 2012) tolearn a generalizable mapping between ‘suboptimal’ and highquality ChIP-seq data (Fig. 1). Coda substantially attenuates threeprimary sources of noise—due to low sequencing depth, low cell input and low ChIP enrichment—enhancing signal in low-quality samples across individuals, cell types and species. Our approach isconceptually related to the existing literature on structured signal recovery, in particular supervised denoising in images (Jain and Seung,2009; Mousavi et al., 2015; Xie et al., 2012) and speech (Maas andLe, 2012). It complements other efforts to impute missing genomicCV The Author 2017. Published by Oxford University Press.i225This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018i226data, such as ChromImpute (Ernst and Kellis, 2015), which predictprofiles for a missing target mark in a target cell type (e.g.H3K4me3 in embryonic stem cells) by leveraging other availablemarks in the target cell type (e.g. H3K27ac in embryonic stem cells)and target mark datasets in other reference cell types (e.g. H3K4me3in hundreds of other cell types). In contrast, our models take in lowquality signal of multiple target marks in a target cell type anddenoise them all (e.g. using low-quality H3K27ac and H3K4me3signal from a given cell population to produce higher-qualityH3K27ac and H3K4me3 signal in that same cell population).Neural networks have been successfully used to reduce noise inimage data (Jain and Seung, 2009) and speech data (Amodei et al.,2016; Maas and Le, 2012), and there are several reasons to believethat neural networks could similarly denoise histone ChIP-seq data.First, histone marks have regular structure: peaks in each mark, forexample, might tend to have certain widths and certain shapes. Thismeans that a noisy signal can be denoised by a model that encodesprior expectations of what a clean signal should look like, just ashumans use the regular structure in speech to decode noisy speechsignals. Second, histone marks are correlated; thus, one noisy markcan be denoised using information from other noisy marks. Third,neural networks excel at flexibly learning complex non-linear relationships when given large amounts of data, making themideal for genome-wide applications. Indeed, neural networks haverecently been successfully applied to many biological domains(Angermueller et al., 2016b): for example, they have been used topredict regulatory sequence determinants of DNA- and RNA-binding proteins (Alipanahi et al., 2015; Zhou and Troyanskaya, 2015),chromatin accessibility (Kelley et al., 2015) and methylation status(Angermueller et al., 2016a).Fig. 1. Overall model. Coda learns a transformation from noisy histone ChIPseq data to a set of clean signal tracks and accurate peak calls. Top: a noisysignal track derived from 1M ChIP-seq reads per histone mark on the lymphoblastoid cell line GM12878. Bottom: a high-quality signal track derived from100 þ M ChIP-seq reads per histone mark from the same experiment. S, signal; P, peak callsP.W.Koh et al.2 Materials and methods2.1 ModelCoda takes in a pair of matching ChIP-seq datasets of the same histone modifications in the same cell type—one high-quality and theother noisy—and uses CNNs to learn a mapping from the noisy tothe high-quality ChIP-seq data. The noisy dataset used in trainingcan be derived computationally (e.g. by subsampling the highquality data) or experimentally (e.g. by conducting the same ChIPseq experiment with fewer input cells). Once this mapping has beenlearned, the same mapping can then be applied to new, noisy data inany other cellular context with the same underlying noise structure.For each type of noise (e.g. due to low cell numbers, sequencingdepth or enrichment) and each target histone mark, we train twoseparate CNNs to accomplish two tasks: a regression task to predicthistone ChIP signal (i.e. the fold enrichment of ChIP reads over input DNA control) and a binary classification task to predict thepresence or absence of a significant histone mark peak (Fig. 2). Intotal, if a given experiment has M marks, then we train 2M modelsseparately (one regression and one classification model for eachmark). Each individual model makes use of the noisy ChIP-seq datafrom all available marks but outputs only one target histone mark.This allows us to learn separate features for each mark and taskwhile still leveraging information from multiple input histonemarks; we find empirically that this improves performance.For computational efficiency, we first bin the genome into 25 bpbins, averaging the signal in each bin. Let L be the number of bins inthe genome (i.e. the length of the genome divided by 25). Each individual model takes in an M Â L input matrix X and returns a 1 Â Loutput vector Y representing the predicted high-quality signal (in theregression setting) or peak calls (in the classification setting). It doesthis by feeding the noisy data through a first convolutional layer, arectified linear unit (ReLU) layer, a second convolutional layer, andthen a final ReLU or sigmoid layer (for regression or classification,respectively). For the first convolutional layer, we use 6 convolutional filters, each 51 bins in length; for the second convolutionalFig. 2. Model architecture. Coda learns two separate convolutional neural networks (CNN) for each target histone mark, one for regression (signal track reconstruction) and the other for classiﬁcation (peak calling). All networksshare the same architecture. Here, we show a schematic of a model trained tooutput a denoised signal track for H3K27ac. To make a prediction on a singlelocation, we take in 25 025 bp of data from all available histone marks centered at that location and pass it through two convolutional layersDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018Denoising genome-wide histone ChIP-seq with convolutional neural networksi227layer, we use a single filter of length 1001. Effectively, this meansthat a prediction at the ith bin is a function of the noisy data at a25 025 bp window centered on the ith bin. We chose the number ofconvolutional filters and the length of each filter using crossvalidation; for optimal performance on other datasets, crossvalidation could again be used to select hyperparameters.The convolutional nature of our models (and the lack of maxpooling layers commonly seen in neural network architectures forcomputer vision) enables us to do efficient genome-wide prediction,as 98% of the computation required for predicting signal at the ithbin is shared with the computation required for predicting theði þ 1Þth bin. In particular, to compute the prediction at the ith bin,the network needs to perform 6 Â 1001 Â 51 operations at the firstconvolutional layer and 6 Â 1001 operations at the second convolutional layer. To compute the prediction at the ði þ 1Þth bin, the network needs to perform only 6 Â 51 more operations at the firstconvolutional layer and 6 Â 1001 operations at the second convolutional layer, saving 6 Â 1001 Â 50 operations. Other models, especially non-linear models such as random forests, would require acompletely separate set of computations for each bin and are therefore significantly more computationally expensive when it comes tomaking predictions across the entire genome.results reported in this paper are on the entire (unbalanced) genome.We used the Keras package (Chollet, 2015) for training andAdaGrad (Duchi et al., 2011) as the optimizer, stopping training ifvalidation loss did not improve for three consecutive epochs. We didnot observe overfitting with our models (train and test error werecomparable), and therefore opted not to use common regularizationtechniques such as dropout (Srivastava et al., 2014).We chose model hyperparameters and architecture through holdout validation on the low-sequencing-depth denoising task withGM12878 as the training cell line (Kasowski et al., 2013), holdingout a random 20% subset of the training data for validation; this taskwill be discussed in more detail in the next section. The model architecture described above (6 convolutional filters each 51 bins in lengthin the first layer, and 1 convolutional filter of length 1001 in the second layer) yielded optimal validation performance out of the configurations we tried (varying the number of convolutional filters and thelengths of the filters by up to an order of magnitude). Adding an additional layer to the neural network brought a modest increase in performance at the cost of more computation time and complexity. Tobe sure that our model architecture generalized, we used the samearchitecture and hyperparameters for all denoising tasks without anyfurther tuning.2.2 Training and evaluation3 ResultsWe applied Coda to three distinct sources of noise: low sequencingdepth, low cell input, and low ChIP enrichment. In all cases, the inputs to our model were noisy signal measurements of multiple histone marks (see Section 5 for more details), and we trained separatemodels to predict the high-quality signal and peak calls for each target mark. For all tasks, we test the model on a different cell line orindividual than we train it on.For the regression tasks (predicting signal), we evaluated performance by computing the Pearson correlation and mean-squarederror (MSE) between the predicted and measured high-quality foldenrichment signal profiles after an inverse hyperbolic sine transformation, which reduced the dominance of outliers. We comparedthis with the baseline performance obtained by directly comparingthe noisy and high-quality signal profiles of the target mark (afterthe same inverse hyperbolic sine transformation).For the classification tasks (predicting presence or absence of apeak), we compared our model’s output to peaks called by theMACS2 peak caller (Feng et al., 2012) on the high-quality signal forthe target mark. As our dataset is unbalanced—peaks only make upa small proportion of the genome—we evaluated performance bycomputing the area under the precision-recall curve (AUPRC), astandard measure of classification performance for unbalanced datasets (Davis and Goadrich, 2006). We compared the AUPRC of ourmodel with a baseline obtained by comparing MACS2 peaks on thenoisy data for the target mark to those obtained from the highquality data for the target mark (see Section 5 for further details ondataset preparation).We trained our models on 50 000 positions randomly sampledfrom peak regions of the genome and 50 000 positions sampledfrom non-peak regions, sampling from each autosome with equallikelihood. We defined peak regions using the output mark of interest and with the high-quality data. Further increasing dataset sizedid not increase performance; as each sample covered 25 025 bp,100 000 samples provided good coverage of the entire genome. Weselected the training dataset to be balanced because a uniformlydrawn dataset would have had very few peaks, making it difficultfor the model to learn to predict at peak regions; however, the test3.1 Removing noise from low sequencing depth dataA minimum of 40–50M reads is recommended for optimal sensitivity for histone ChIP-seq experiments in human samples targetingmost canonical histone marks (Jung et al., 2014). As adhering to thisstandard can often be infeasible due to cost and other limitations, asubstantial proportion of publicly available datasets do not meetthese standards. Motivated by these constraints, we tested whetherour model could recover high-read depth signal from low-read depthexperiments.3.2 Training and testing on the same cell type acrossdifferent individualsWe evaluated Coda on lymphoblastoid cell lines (LCLs) derivedfrom six individuals of diverse ancestry [European (CEU), Yoruba(YRI), Japanese, Han Chinese, San] (Kasowski et al., 2013). Weused the CEU-derived cell line (GM12878) to train our model to reconstruct the high-depth signal (100Mþ reads per mark; exact numbers in Data Availability and Processing) from a simulated noisysignal derived by subsampling 1M reads per mark. On the other fivecell lines, Coda significantly improved Pearson’s correlation between the full and noisy signal (Fig. 3A, left) and the accuracy ofpeak calling (Fig. 3A, right). Using just 1M reads per mark, the output of our model was equivalent in quality to signal derived from15Mþ reads (H3K27ac) to 25Mþ reads (H3K36me3) (Fig. 3B).Figure 4 shows how Coda can accurately reconstruct histone modification levels at the promoter of the PAX5 gene, a master transcription factor required for differentiation into the B-lymphoid lineage(Nutt et al., 1999).We confirmed Coda was not simply memorizing the profile ofthe training cell line (GM12878) and copying it to the test cell linesby examining differential regions, called by DESeq (Anders andHuber, 2010), between GM12878 and the other cell lines (Kasowskiet al., 2013). Coda improved correlation and peak-calling even inthose regions (Table 1). Similarly, it also improved correlation onthe regions of the genome with enriched signal, i.e. called as statistically significant peaks (Table 2).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018i228P.W.Koh et al.3.3 Training and testing on different cell types acrossdifferent individualsWe next assessed if Coda could be trained on one cell type in one individual and used to denoise low-sequencing-depth data from a different cell type in a different individual. As above, the model wastrained to output high-depth data (30M reads) from low-depth data(1M reads). We used histone ChIP-seq data spanning T-cells (E037),monocytes (E029), mesenchymal stem cells (MSCs, E026), andfibroblasts (E056) from the Roadmap Epigenomics Consortium(Consortium et al., 2015). Coda substantially improved the qualityof the low-depth signal on the test cell type for all pairs of cell types(Table 3), illustrating that it can denoise low-depth data on a celltype even if high-depth training data for that cell type is notavailable.3.4 Coda outperforms linear baselinesWe compared Coda with a linear and logistic regression baseline forsignal denoising and peak calling, respectively. In both cases, weused an input region of the same size as Coda (i.e. 25 025 bp centered on the location to be predicted, binned into 25 bp bins). Asnoted above, the desire for computational efficiency in makinggenome-wide predictions across multiple marks limits the complexity of models that would be practically useful in genome-wideprediction.Fig. 4. Genome browser tracks for low-sequencing-depth experiments. Wecompare noisy signal and peak calls obtained from 1M reads per mark (top)with Coda’s output (middle) and the target, high-quality signal and peak callsobtained from 100Mþ reads per mark (bottom) at the PAX5 promoter. Codasuccessfully cleans up signal across all histone marks and correctly calls theH3K27ac, H3K36me3, and H3K4me1 peaks (missed in the noisy data) whileremoving the spurious H3K27me3 peak calls. Note that we show the noisypeak calls to allow for comparisons; Coda uses only the noisy signal, not thepeak calls, as input. The signal tracks are in arcsinh units, with the followingy-axis scales: H3K27ac: 0–160, H3K27me3: 0–20, H3K36me3 and H3K4me1:0–40, H3K4me3: 300. The shading of the peak tracks that the model outputsrepresent the strength of the peak call on a scale of 0–1Table 1. Denoising differential regions (diff. reg.) between test cellline GM18526 and training cell line GM12878MSE (diff. reg.) Pearson’s R (diff. reg.) AUPRC (diff. reg.)H3K4me1 285% (4.01, 0.57)H3K4me3 275% (2.88, 0.70)H3K27ac 286% (3.43, 0.48)H3K27me3280% (0.78, 0.15)159% (0.37, 0.59) 103% (0.93, 0.97)114% (0.63, 0.72) 111% (0.78, 0.87)139% (0.55, 0.77) 106% (0.90, 0.96)1106% (0.14, 0.30)–Performance reported is improvement of the denoised model over baseline(original, subsampled reads) on the test cell line. In parentheses, we report thebaseline results followed by the denoised results. Peak-calling results onH3K27me3 are omitted due to the lack of peak calls in differential regions; allresults on H3K36me3 are omitted due to low number of differential regions.Table 2. Denoising peak regions between test cell line GM18526and training cell line GM12878MSE (peaks)Fig. 3. Coda removes noise from low-sequencing-depth experiments on lymphoblastoid cell lines derived from different individuals. (A) Compared withthe signal from subsampled reads (blue), the denoised signal (green) showsgreater correlation with the full signal (left) and more accurate peak-calling(right) across all cell lines. The model was trained on GM12878 and tested ondifferent cell lines; within each column in the plot, each point is a single testcell line. (B) With 1M reads per mark, the denoised H3K27ac data are equivalent in quality to a dataset with 15Mþ reads per mark, and the H3K36me3data are equivalent in quality to a dataset with 25Mþ reads per mark. Similarresults hold for other marks. These results are from training on GM12878 andtesting on GM18526H3K4me1H3K4me3H3K27acH3K27me3H3K36me3Pearson’s R (peaks)286% (3.69, 0.49)283% (2.93, 0.50)287% (3.36, 0.43)290% (2.20, 0.21)293% (3.78, 0.25)156% (0.44, 0.70)111% (0.78, 0.87)128% (0.65, 0.83)1103% (0.18, 0.36)1120% (0.32, 0.70)Performance reported is improvement of the denoised model over baseline(original, subsampled reads) on the test cell line. In parentheses we report thebaseline results followed by the denoised results.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018Denoising genome-wide histone ChIP-seq with convolutional neural networksWhen evaluated in the same cell type, different individual setting, Coda achieved 3Â lower MSE on peak regions and 2Â lowerMSE on differential regions, with similar (very slightly better) MSEand correlation across the whole genome. This implies that Coda isbetter able to learn to match the exact values of the signal tracks on‘difficult’ regions (i.e. where there is the greatest deviation from thetraining signal), even though the linear model matches the roughshape. These regions are important to predict well because they cangive insight into the differences between individuals and cell types.We note that many forms of smoothing can be represented vialinear regression. For example, a standard Gaussian filter can be interpreted as taking a linear combination of surrounding points withfixed coefficients. The comparison against a linear regression baseline therefore sets an upper bound for the performance of simplesmoothing measures on this task (assuming no overfitting, which wedo not observe in our case).3.5 Comparisons to denoising and imputationNext, we studied Coda’s performance in two additional settings:pure denoising (using the noisy target mark as the only input mark)and imputation from noise (using all noisy histone marks but thetarget mark as the input marks). This is in contrast to the standardsetting described above, where we use all noisy histone marks,including the noisy version of the target mark, to recover a highquality version of the target mark.In the denoising case, Pearson’s correlation dropped by 0.03points and AUPRC dropped by 0.05, on average, compared withwhen all marks were used as input. Thus, additional marks providedsome information, but the denoised signal was still substantially better than the original subsampled signal.In the imputation case, performance dropped somewhat on thenarrow marks (H3K4me1, H3K4me3, H3K27ac; –0.12 correlation,–0.13 AUPRC) and dropped more on the broad marks (H3K27me3,H3K36me3; –0.29 correlation, –0.30 AUPRC). The gap in correlation was even larger within peak regions. Thus, having a noisyversion of the target mark substantially boosts recovery of the highquality signal.3.6 Removing noise from low cell inputConventional ChIP-seq protocols require a large number of cells toreach the necessary sequencing depth and library complexity(Brind’Amour et al., 2015; Cao et al., 2015), precluding profilingwhen input material is limited. Several ChIP-seq protocols were recently developed to address this problem. We studied ULI-NChIPseq (Brind’Amour et al., 2015) and MOWChIP-seq (Cao et al.,2015), which use low cell input (102 À 103 cells) to generate signalthat is highly correlated, when averaged over bins of size 2–4 kb,i229with experiments with high cell input. However, at a finer scale of25bp, the low-input signals from both protocols are poorly correlated with the high-input signals (Table 4).We thus used Coda to recover high-resolution, high-cell-inputsignal from low-cell-input signal specific to each protocol. For ULINChIP-seq, we used a single mouse embryonic stem cell dataset(Brind’Amour et al., 2015). For MOWChIP-seq, we trained on datafrom the human LCL GM12878 and tested on hematopoietic stemand progenitor cells (HSPCs) from mouse fetal liver (Cao et al.,2015). Coda successfully denoised the low-input signal from bothprotocols (Table 4). Figure 5 illustrates our model denoisingMOWChIP-seq signal across the Runx1 gene, a key regulator ofHSPCs (North et al., 2002); the results of peak calling were toonoisy, even on the original 10 000-cell data, to allow for any qualitative judgment of improvement.We note that the Pearson correlations between the low cell inputand high cell input in the original ULI-NChIP-seq (Brind’Amouret al., 2015) and MOWChIP-seq (Cao et al., 2015) papers are significantly higher than the ones we report here. We report lower correlations because we use a smaller bin size for the genome, as notedabove; we look at correlation across the whole genome, instead ofonly at transcription start sites; and we compute correlation after anarcsinh transformation to prevent large peaks from dominating thecorrelation. Therefore, while the original low-cell-input data is suitable for studying histone ChIP-seq signal at a coarse-grained leveland around genetic elements like transcription start sites, thedenoised data is more accurate at a fine-grained level and across thewhole genome.3.7 Removing noise from low-enrichment ChIP-seqHistone ChIP-seq experiments use antibodies to enrich for genomicregions associated with the target histone mark. When an antibodyTable 4. Low-cell-input experimentsMSEPearson’s RAUPRCULI-NChIPH3K4me3 261% (1.39, 0.54) 1208% (0.13, 0.41) 161% (0.24, 0.38)H3K9me3 246% (0.51, 0.27) 128% (0.41, 0.53) 132% (0.28, 0.36)H3K27me3 241% (0.68, 0.40) 157% (0.34, 0.54) 132% (0.34, 0.45)MOWChIPH3K4me3 242% (1.18, 0.68) 1122% (0.14, 0.31) 134% (0.19, 0.25)H3K27ac 221% (1.44, 1.14) 1159% (0.09, 0.24) 166% (0.15, 0.24)We report improvement of the denoised model output over baseline (original low-input experiments), when compared with high-input experiments.In parentheses we report the baseline results followed by the denoised results.Table 3. Cross cell-type experimentsMonocytesPearson’s RT-cellsMonocytesMSCsAUPRCT-cellsMonocytesMSCsMSCsFibroblasts133% (0.51, 0.67)––158% (0.44, 0.70)159% (0.44, 0.70)–178% (0.36, 0.65)179% (0.36, 0.65)181% (0.36, 0.66)1116% (0.31, 0.66)––1136% (0.31, 0.72)1139% (0.31, 0.73)–194% (0.35, 0.69)194% (0.35, 0.69)1100% (0.35, 0.71)Rows are train cell type, while columns are test cell type. In parentheses, we report the baseline results followed by the denoised results, averaged across all histone marks used.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018i230P.W.Koh et al.Fig. 5. Genome browser tracks for low-cell-input experiments. We comparenoisy signal obtained from 100 cells (top) with Coda’s output (middle) and thetarget, high-quality signal obtained from 10 000 cells (bottom) at the Runx1gene in mouse hematopoietic stem and progenitor cells. The model wastrained on MOWChIP-seq data generated from human LCL (GM12878) andcaptures two strong peaks at the promoters of the two isoform classes,removing much of the intervening noise. The signal tracks are in arcsinhunits, with a scale of 0–40 for both histone marksTable 5. Low-enrichment experimentsMSEPearson’s RAUPRCH3K4me1 275% (0.35, 0.09) 142% (0.64, 0.91) 1215% (0.29, 0.92)H3K4me3 286% (0.44, 0.06) 154% (0.58, 0.91) 194% (0.49, 0.95)H3K27ac 270% (0.37, 0.11) 137% (0.65, 0.90) 1121% (0.43, 0.94)H3K27me3 261% (0.27, 0.10) 188% (0.42, 0.78) 1242% (0.14, 0.49)H3K36me3 282% (0.36, 0.06) 147% (0.65, 0.95) 1168% (0.36, 0.98)We report improvement of the denoised model output over baseline (lowenrichment experiments), when compared with high-enrichment experiments.In parentheses we report the baseline results followed by the denoised results.with low specificity or sensitivity for the target is used, the resultingChIP-seq data will be poorly enriched for the target mark. This is amajor source of noise (Landt et al., 2012). We simulated resultsfrom low-enrichment experiments by corrupting GM12878 andGM18526 LCL data (Kasowski et al., 2013). For each histone markprofiled in those cell lines, we kept only 10% of the actual reads andreplaced the other 90% with reads taken from the control ChIP-seqexperiment, which was done without the use of any antibody; thissimulates an antibody with very low specificity.This corruption process significantly degraded the genome-widePearson’s correlation and the accuracy of peak calling (Table 5).This shows that recovering the true signal from the corrupted datacannot be achieved by simply linearly scaling the signal (e.g. multiplying the empirical fold enrichment by 10 since only 10% of the actual reads were kept), as if that were the case, the correlation wouldbe unchanged. In contrast, when trained on GM12878 and tested onGM18526, Coda accurately recovered high-quality, uncorruptedsignal from the corrupted data (Table 5). Figure 6 shows a comparison of Coda’s output versus the corrupted and uncorrupted data atthe promoter of the EBF1 gene, another key transcription factor ofthe B-lymphoid lineage. (Nechanitzky et al., 2013)To further validate Coda’s output, we examined aggregate histone ChIP-seq signal around known biological regions of interest. Inparticular, we used the fact that H3K4me1 and H3K27ac, knownenhancer marks, are enriched at DNase I hypersensitive sites(DHSs), whereas H3K27me3 is depleted at DHSs. (Shu et al., 2011)For each of those marks, we compared the average uncorrupted signal, the average denoised signal, and the average low-enrichmentFig. 6. Genome browser tracks for low-enrichment ChIP-seq experiments. Wecompare noisy signal and peak calls obtained from the corrupted data with10% enrichment (top) with Coda’s output (middle) and the target, high-qualitysignal and peak calls obtained from the uncorrupted data (bottom) at theEBF1 promoter. Coda signiﬁcantly improves the signal-to-noise ratio and correctly calls the H3K27ac, H3K36me3, H3K4me1 and H3K4me3 peaks that weremissed in the noisy data while removing a spurious H3K27me3 peak call.Note that we show the noisy peak calls to allow for comparisons; Coda usesonly the noisy signal, not the peak calls, as input. The signal tracks are in arcsinh units, with the following y-axis scales: H3K27ac: 0–60, H3K27me3,H3K36me3 and H3K4me1: 0–40, H3K4me3: 100. The shading of the peaktracks that the model outputs represent the strength of the peak call on ascale of 0–1signal within 5000 bp of the summits of DNase I hypersensitivepeaks in GM12878 from ENCODE data (Bernstein et al., 2012). Asexpected, the corrupted, low-enrichment signal was biased by thereads from the control experiment and had significantly lower foldenrichment of H3K4me1 and H3K27ac at DHSs, compared to theuncorrupted signal. In contrast, the denoised signal was significantlymore enriched at DHSs than the corrupted signal, more closelyresembling the uncorrupted signal. Conversely, the corrupted signalhad higher levels of H3K27me3 at DHSs, whereas the denoisedsignal had low levels of H3K27me3 throughout the DHS, similar tothe uncorrupted signal though without a dip at the peak summit(Fig. 7).4 ConclusionWe describe a convolutional denoising algorithm, Coda, that usespaired noisy and high-quality samples to substantially improve thequality of new, noisy ChIP-seq data. Our approach transfers information from generative noise processes (e.g. mixing in control readsto simulate low-enrichment, or performing low-input experiments)to a flexible discriminative model that can be used to denoise newdata. We believe that a similar approach can be used in other biological assays, e.g. ATAC-seq and DNase-seq (Buenrostro et al., 2013;Crawford et al., 2006), where it is near impossible to analyticallycharacterize all types of technical noise or the overall dataDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018Denoising genome-wide histone ChIP-seq with convolutional neural networksFig. 7. Aggregate histone ChIP-seq signal at DNase I hypersensitive sites. Wecompare the average uncorrupted signal (full), the average denoised signal(denoised) and the average corrupted signal (low enrichment) at DNase Ihypersensitive sites. Across all histone marks, the denoised signal is signiﬁcantly more similar to the uncorrupted signal than the corrupted signal isi231cells, or the level of ChIP enrichment. In some cases (e.g. the lowsequencing-depth and low-cell-input settings) this is true, but inothers (e.g. the low-enrichment setting) it may not be. An importantdirection for future work is to make Coda more robust; for example,training a single model over various settings of the noise parametersand various cell types could improve the generalizability of themodels.To further improve performance, more complex neural networkarchitectures could also be explored. Possibilities include using recurrent neural networks (Sutskever et al., 2014) to explicitly modellong-range spatial correlations in the genome; multi-tasking acrossoutput marks instead of training separate models for each mark; orusing deeper networks.Another avenue for future work is using more than just the noisyhistone ChIP-seq data at test time. In this work, we use only thenoisy data at test time, training our models to transform it into highquality data. In reality, at test time we might have access to otherdata; for example, we might also have the DNA sequence of the testsample or access to high-quality ChIP-seq data on a closely relatedcell type. Other work has used DNA sequence to predict transcription factor binding (Alipanahi et al., 2015; Zhou and Troyanskaya,2015), chromatin accessibility (Kelley et al., 2015), and methylationstatus (Angermueller et al., 2016a). A natural next step would be tocombine the ideas from these methods with ours, e.g. by having aseparate convolutional module in our neural network that incorporates sequence information and joins with the ChIP-seq module at anintermediate layer. Others have also used high-quality ChIP-seqdata from closely related cell types for imputation (Ernst and Kellis,2015); combining this with our denoising approach could help toavoid a potential pitfall of these imputation approaches, namely theloss of cell-type-specific signal, while improving the accuracy of ourdenoised output.Below, we provide a link to a script that trains a model for lowsequencing-depth noise using the LCL data described above. Sincethe type of noise can vary from context to context, we also providethe code for the general Coda framework to allow for developers ofnew protocols (e.g. new low-cell-count techniques) or core facilitiesthat have high throughput to train Coda with data specific to theircontext.5. Data availability and processing5.1 Datasetsdistribution but possible to generate noisy versions of high-qualitysamples through experimental or computational perturbation. Thiscan significantly reduce cost while maintaining or even improvingquality, especially in high-throughput settings or when dealing withlimited amounts of input material (e.g. in clinical studies).An important caveat to our work is that Coda’s performance depends strongly on the similarity of the noise distributions and underlyingdata distributions in the test and training sets. For example, Coda expectsthat the relationships between different histone marks are conserved between the test and training set. Thus, applying Coda to test data that isvery different from its training data is unlikely to work, and it is important to assess the reliability of the denoised output. We suggest examiningwhether the denoised signals near regions of interest (e.g. DNase hypersensitive sites) match the expected patterns (Fig. 7). Assessing whetherthe QC metrics for the noisy data (e.g. sampling depth) fall within theranges discussed in this paper provides another check.We also assume that the noise parameters in the test data areknown in advance, e.g. the sequencing depth, the number of inputWe used the following publicly-available GEO datasets in this work:1.2.3.4.GSE50893 for ChIP-seq data on LCLs (Kasowski et al., 2013)GSE63523 for ULI-NChIP-seq data (Brind’Amour et al., 2015)GSE65516 for MOWChIP-seq data (Cao et al., 2015)GSM736620 for DNase I hypersensitive peaks (Bernstein et al.,2012)For the low-sequencing-depth experiments, the full depth forGM12878 (training set) was 171M (million reads) for H3K4me1,168M for H3K4me3, 328M for H3K27ac, 265M for H3K27me3and 123M for H3K36me3. The full depth for GM18526 (test set)was 120M for H3K4me1, 136M for H3K4me3, 125M forH3K27ac, 138M for H327me3 and 223M for H3K36me3.For the cross-cell-type experiments, we used the consolidatedRoadmap Epigenomics data (Consortium et al., 2015), which is publicly available from http://egg2.wustl.edu/roadmap/data/byFileType/alignments/. Each mark is downsampled to a maximum of 30M readsto maximize consistency across marks; we used this as the full depthDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018i232P.W.Koh et al.data, and downsampled to 1M reads for the noisy data. A detailed description of this dataset is available in Roadmap Epigenomics Project(2015).NChIP-seq data had matching low- and high-input experimentsonly for a single cell type, so we divided it into chr5–19 for training,chr3–4 for validation and chr1–2 for testing.5.2 Dataset preparationCode, data and browser track availabilityOur code is available on Github at https://github.com/kundajelab/coda, including a script that downloads pre-processed data and replicates the low-sequencing-depth experiments described above, aswell as code for processing new data.The figures of browser tracks (Figs 4–6) shown above were takenfrom the Wash U Epigenome Browser (Zhou and Wang, 2012).Links to the entire browser tracks are as follows:5.2.1 Fold change signal profiles and peak callingFor each experiment, we used align2rawsignal (Kundaje, 2013) togenerate signal tracks and MACS2 (Feng et al., 2012) to call peaks,as implemented in the AQUAS package (Lee and Kundaje, 2016).For the signal track, we used fold change relative to the expecteduniform distribution of reads after an inverse hyperbolic sine transformation (Hoffman et al., 2012). We used the gappedPeaks outputfrom MACS2 as the peak calls. For computational efficiency, webinned the genome into 25 bp segments, averaging the signal in eachsegment.We evaluated our peak calling on a bin-by-bin basis, i.e. ourmodel output one number for each bin representing the probabilitythat bin was a true peak, and we treated each bin as a separate example for the purposes of computing AUPRC, our metric for peakcalling performance. To get ground truth data for our peak callingtasks, we labeled each bin as ‘peak’ or ‘non-peak’ based on whetherthat bin was part of a peak called by MACS2 on the high-qualitydata.Computing AUPRC requires predictions to be ranked in order ofconfidence. For our model, we used the output probabilities for eachbin to calculate the ranking. MACS2 outputs both a peak P-valuetrack, assigning a P-value to each genomic coordinate and a set ofbinary peak calls. To measure baseline performance on the noisydata, we ranked each bin by the maximum peak P-value assigned byMACS2 to a genomic coordinate in that bin, unless that bin did notintersect with any of the binary peak calls, in which case it was assigned a P-value of Àinf (i.e. ranked last). We did this to ensure thatthe high-quality peak track had an AUPRC of 1; empirically, thisalso improved performance of the noisy MACS2 baseline.•••Figure 4, low-sequencing-depth experiments on LCL GM12878:http://epigenomegateway.wustl.edu/browser/?genome¼hg19&session¼KZvYzGBt03&statusId¼107864126Figure 5, low-cell-count experiments on mouse HSPCs: http://epigenomegateway.wustl.edu/browser/?genome¼mm9&session¼PJUr7vAwEh&statusId¼1611801659Figure 6, low-enrichment experiments on LCL GM12878: http://epigenomegateway.wustl.edu/browser/?genome¼hg19&session¼3hDZdGiGmF&statusId¼1913128468AcknowledgementsWe thank Jin-Wook Lee for his assistance with the AQUAS pipeline and KyleLoh, Irene Kaplow and Nasa Sinnott-Armstrong for their helpful feedbackand suggestions.FundingE.P. acknowledges support from a Hertz Fellowship and an NDSEGFellowship. This work was also supported by NIH grants DP2-GM-123485and 1R01ES025009-01.Conﬂict of Interest: none declared.5.2.2 Histone marks usedWe used different sets of input and output histone marks for different experiments depending on which marks each dataset provided.For the same cell type, different individual experiments (usingLCLs), we trained and tested on H3K4me1, H3K4me3, H3K27ac,H3K27me3 and H3K36me3; we used the same data for the lowChIP-enrichment experiments. For the different cell type, differentindividual experiments [using the uniformly processed RoadmapEpigenomics Consortium datasets (Consortium et al., 2015)], wetrained and tested on H3K4me1, H3K4me3, H3K9me3, H3K27ac,H3K27me3 and H3K36me3. For all of the above experiments, wealso used data from the control experiments (no antibody) as input.Lastly, for the low-cell-input experiments, we used H3K4me3,H3K9me3 and H3K27me3 from the ULI-NChIP-seq dataset andH3K4me3 and H3K27ac from the MOWChIP-seq dataset.5.2.3 Low-cell-input datasetsThe ULI-NChIP-seq (Brind’Amour et al., 2015) and MOWChIP-seq(Cao et al., 2015) papers provided several datasets corresponding todifferent numbers of input cells used. For each protocol, we used thedatasets with the lowest number of input cells as the noisy inputdata (ULI-NChIP-seq: 103 cells for H3K9me3 and H3K27me3,5Â103 cells for H3K4me3; MOWChIP-seq: 102 cells) and the datasets with the highest number of input cells as the gold-standard,high-quality data (ULI-NChIP-seq: 106 cells for H3K9me3, 105 cellsfor H3K4me3 and H3K27me3; MOWChIP-seq: 104 cells). The ULI-ReferencesAcevedo,L.G. et al. (2007) Genome-scale ChIP-chip analysis using 10,000human cells. BioTechniques, 43, 791–797. http://www.ncbi.nlm.nih.gov/pubmed/18251256http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid¼PMC2268896.Alipanahi,B. et al. (2015) Predicting the sequence speciﬁcities of DNA- andRNA-binding proteins by deep learning. Nat. Biotechnol., 33, 831–838.Amodei,D. et al. (2016) Deep speech 2: end-to-end speech recognition inEnglish and Mandarin. In: International Conference on Machine Learning,pp.173–182.Anders,S., and Huber,W. (2010) Differential expression analysis for sequencecount data. Genome Biol., 11, R106. http://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-10-r106.Angermueller,C. et al. (2016a) Accurate prediction of single-cell DNA methylation states using deep learning. Technical report. http://biorxiv.org/lookup/doi/10.1101/055715.Angermueller,C. et al. (2016b) Deep learning for computational biology. Mol.Syst. Biol., 12, 878. http://msb.embopress.org/lookup/doi/10.15252/msb.20156651.Bernstein,B.E. et al. (2012) An integrated encyclopedia of DNA elements inthe human genome. Nature, 489, 57–74.Bernstein,B.E. et al. (2006) A bivalent chromatin structure Marks key developmental genes in embryonic stem cells. Cell, 125, 315–326.Brind’Amour,J. et al. (2015) An ultra-low-input native ChIP-seq protocol forgenome-wide proﬁling of rare cell populations. Nat. Commun., 6, 6033.http://www.nature.com/ncomms/2015/150121/ncomms7033/full/ncomms7033.html.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018Denoising genome-wide histone ChIP-seq with convolutional neural networksBuenrostro,J.D. et al. (2013) Transposition of native chromatin for fast andsensitive epigenomic proﬁling of open chromatin, DNA-binding proteinsand nucleosome position. Nat. Methods, 10, 1213–1218. http://www.nature.com/doiﬁnder/10.1038/nmeth.2688.Cao,Z. et al. (2015) A microﬂuidic device for epigenomic proﬁling using 100cells. Nat. Methods, 12, 959–962. http://www.nature.com.laneproxy.stanford.edu/nmeth/journal/v12/n10/full/nmeth.3488.html.Consortium,R.E. et al. (2015) Integrative analysis of 111 reference human epigenomes. Nature, 518, 317–330.Crawford,G.E. et al. (2006) Genome-wide mapping of DNase hypersensitivesites using massively parallel signature sequencing (MPSS). Genome Res.,16, 123–131. http://www.ncbi.nlm.nih.gov/pubmed/16344561http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid¼PMC1356136.Davis,J., and Goadrich,M. (2006) The relationship between precision-recalland roc curves. In Proceedings of the 23rd International Conference onMachine Learning, ICML’06, pp. 233–240. ACM, New York, NY, USA.http://doi.acm.org/10.1145/1143844.1143874.Duchi,J. et al. (2011) Adaptive subgradient methods for online learning andstochastic optimization. J. Mach. Learn. Res., 12, 2121–2159. http://www.jmlr.org/papers/v12/duchi11a.html.Ernst,J., and Kellis,M. (2015) Large-scale imputation of epigenomic datasetsfor systematic annotation of diverse human tissues. Nat. Biotechnol., 33,364–376.Feng,J. et al. (2012) Identifying ChIP-seq enrichment using MACS. Nat.Protocols, 7, 1728–1740.Chollet,F. (2015) Keras. https://github.com/fchollet/keras.Gjoneska,E. et al. (2015) Conserved epigenomic signals in mice and humansreveal immune basis of Alzheimer’s disease. Nature, 518, 365–369.Hoffman,M.M. et al. (2012) Unsupervised pattern discovery in human chromatin structure through genomic segmentation. Nat. Methods, 9, 473–476.Jain,V., and Seung,S. (2009) Natural image denoising with convolutional networks. In Advances in Neural Information Processing Systems, pp. 769–776. http://papers.nips.cc/paper/3506-natural-image-denoising-with-convolutional-networks.Jung,Y.L. et al. (2014) Impact of sequencing depth in ChIP-seq experiments.Nucleic Acids Res., 42, e74. http://nar.oxfordjournals.org/content/early/2014/03/05/nar.gku178.Kasowski,M. et al. (2013) Extensive variation in chromatin states acrosshumans. Science (New York, N.Y.), 342, 750–752. http://www.sciencemag.org/content/342/6159/750.Kelley,D.R. et al. (2015) Basset: learning the regulatory code of the accessiblegenome with deep convolutional neural networks. Technical report. http://biorxiv.org/content/early/2016/02/18/028399.abstract.Krizhevsky,A. et al. (2012) ImageNet classiﬁcation with deep convolutionalneural networks. In Advances in Neural Information Processing Systems,pp.1097–1105. http://papers.nips.cc/paper/4824-imagenet-classiﬁcation-w.i233Kundaje,A. (2013) align2rawsignal. https://code.google.com/archive/p/align2rawsignal/.Landt,S.G. et al. (2012) ChIP-seq guidelines and practices of the ENCODEand modENCODE consortia. Genome Res., 22, 1813–1831. http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid¼3431496{\&}tool¼pmcentrez{\&}rendertype¼abstract.Lee,J.-W., and Kundaje,A. (2016) AQUAS TF ChIP-seq pipeline. https://github.com/kundajelab/TF_chipseq_pipeline.Maas,A., and Le,Q. (2012) Recurrent Neural Networks for Noise Reductionin Robust ASR. INTERSPEECH, pp. 3–6. https://research.google.com/pubs/pub45168.html.Mousavi,A. et al. (2015) A Deep Learning Approach to Structured SignalRecovery. http://arxiv.org/abs/1508.04065.Nechanitzky,R. et al. (2013) Transcription factor EBF1 is essential for themaintenance of B cell identity and prevention of alternative fates in committed cells. Nat. Immunol., 14, 867–875. http://www.nature.com/doiﬁnder/10.1038/ni.2641.North,T.E. et al. (2002) Runx1 expression marks long-term repopulatinghematopoietic stem cells in the midgestation mouse embryo. Immunity, 16,661–672. http://www.cell.com/article/S1074761302002960/fulltext.Nutt,S.L. et al. (1999) Commitment to the B-lymphoid lineage depends on thetranscription factor Pax5. Nature, 401, 556–562.Roadmap Epigenomics Project (2015) http://egg2.wustl.edu/roadmap/web_portal/processed_data.html (25 April 2017, date last accessed).Shu,W. et al. (2011) Genome-wide analysis of the relationships betweenDNaseI HS, histone modiﬁcations and gene expression reveals distinctmodes of chromatin domains. Nucleic Acids Res., 39, 7428–7443. http://www.ncbi.nlm.nih.gov/pubmed/21685456http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid¼PMC3177195.Srivastava,N. et al. (2014) Dropout: a simple way to prevent neural networksfrom overﬁtting. J. Mach. Learn. Res., 15, 1929–1958.Sutskever,I. et al. (2014) Sequence to sequence learning with neural networks.In Proceedings of the 27th International Conference on Neural InformationProcessing Systems, NIPS’14, pp. 3104–3112. MIT Press, Cambridge, MA,USA. http://dl.acm.org/citation.cfm?id¼2969033.2969173.Xie,J. et al. (2012) Image denoising and inpainting with deep neural networks.In Advances in Neural Information Processing Systems, pp. 341–349. http://papers.nips.cc/paper/4686-image-denoising.Zhou,J., and Troyanskaya,O.G. (2015) Predicting effects of noncoding variants with deep learning-based sequence model. Nat. Methods, 12, 931–934.Zhou,X., and Wang,T. (2012) Using the wash U epigenome browser to examine genome-wide sequencing data. Curr. Protoc. Bioinform. http://www.ncbi.nlm.nih.gov/pubmed/23255151http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid¼PMC3643794.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i225/3953958by gueston 07 January 2018
5028881976002	PMID28881976	5028881976	https://watermark.silverchair.com/btx242.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881976.main.pdf	Increasing the power of meta-analysis ofgenome-wide association studies todetect heterogeneous effectsC. H. Lee1, E. Eskin2,3 and B. Han1,*1Department of Convergence Medicine, University of Ulsan College of Medicine & Asan Institute for LifeSciences, Asan Medical Center, Songpa-gu, Seoul 138-736, Korea, 2Department of Computer Science and3Department of Human Genetics, University of California, Los Angeles, CA 90095, USA*To whom correspondence should be addressed.AbstractMotivation: Meta-analysis is essential to combine the results of genome-wide association studies(GWASs). Recent large-scale meta-analyses have combined studies of different ethnicities, envi-ronments and even studies of different related phenotypes. These differences between studies canmanifest as effect size heterogeneity. We previously developed a modified random effects model(RE2) that can achieve higher power to detect heterogeneous effects than the commonly used fixedeffects model (FE). However, RE2 cannot perform meta-analysis of correlated statistics, which arefound in recent research designs, and the identified variants often overlap with those found by FE.Results: Here, we propose RE2C, which increases the power of RE2 in two ways. First, we general-ized the likelihood model to account for correlations of statistics to achieve optimal power, usingan optimization technique based on spectral decomposition for efficient parameter estimation.Second, we designed a novel statistic to focus on the heterogeneous effects that FE cannot detect,thereby, increasing the power to identify new associations. We developed an efficient and accuratep-value approximation procedure using analytical decomposition of the statistic. In simulations,RE2C achieved a dramatic increase in power compared with the decoupling approach (71% vs.21%) when the statistics were correlated. Even when the statistics are uncorrelated, RE2C achievesa modest increase in power. Applications to real genetic data supported the utility of RE2C. RE2C ishighly efficient and can meta-analyze one hundred GWASs in one day.Availability and implementation: The software is freely available at http://software.buhmhan.com/RE2C.Contact: buhm.han@amc.seoul.krSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionGenome-wide association studies (GWASs) have identified numer-ous single-nucleotide polymorphisms (SNPs) that are associatedwith human traits (Manolio, 2010; Welter et al., 2014). For manydiseases, however, the identified variants explain only part of theknown heritability, which indicates the existence of undetected vari-ants with small effects (Evangelou and Ioannidis, 2013; Manolio,2013). To scale up genetic discovery, meta-analysis of GWASs hasbecome a popular tool to augment the sample size (Evangelou andIoannidis, 2013; Fleiss, 1993; Zeggini and Ioannidis, 2009).Recently, the use of meta-analysis in GWASs has expanded to newresearch designs, such as combining different related diseases(Kiryluk et al., 2012; Lee et al., 2014; Perry et al., 2012), popula-tions (Liu et al., 2015), environments (Kang et al., 2014), tissues(Sul et al., 2013) and cancer types (Bhattacharjee et al., 2012;Petersen et al., 2010). These differences between studies can mani-fest as heterogeneity, which refers to effect-size differences. Whenheterogeneity exists, the commonly used fixed effects model (FE) isnot optimal. The traditional random effects model (RE)(DerSimonian and Laird, 1986) is also conservative and is notpowerful (Han and Eskin, 2011). To overcome this challenge, we re-cently developed a modified RE (RE2) that has higher power undercondition of heterogeneity (Han and Eskin, 2011). RE2 has beenused widely in cross-population human disease analyses (ChimusaVC The Author 2017. Published by Oxford University Press. i379This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i379–i388doi: 10.1093/bioinformatics/btx242ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018et al., 2014; Keller et al., 2014; Sapkota et al., 2014), cross-environment mouse trait analyses (Kang et al., 2014), cross-condition expression quantitative trait loci (eQTL) analyses (Sulet al., 2013; Ye et al., 2014), and cross-feature neuroimaging ana-lyses (Hibar et al., 2012; Stein et al., 2012).However, RE2 has some limitations. First, RE2 cannot performmeta-analysis of correlated statistics. Although the traditional as-sumption of independence of statistics has been valid in conven-tional study designs, it can be invalidated in new research designs.For example, in cross-disease meta-analyses, it is common that somecontrols are used in more than one study, which can cause correl-ations of statistics (Dichgans et al., 2014; Kar et al., 2016;Moskvina et al., 2013). Thus, in cross-disease analyses, both hetero-geneity and correlations can occur. In a cross-tissue eQTL analysis(Sul et al., 2013), the intra-individual similarity of gene expressionlevels between different tissues can cause the correlations of statis-tics. To account for these correlations, Lin and Sullivan extended FE(Lin and Sullivan, 2009). However, for RE methods, no solutionshave been suggested. Recently, Han et al. developed a decouplingapproach that makes the statistics independent (Han et al., 2016).The transformed data can be used for RE2. However, the optimalityof this approach has not been evaluated yet. The second limitationof RE2 is that the identified variants by RE2 and FE overlap sub-stantially. This is because RE2 is designed as a stand-alone methodthat captures variants with and without heterogeneity. However, inmost of the meta-analyses of GWASs, it is essential to apply FE be-fore applying RE2, because detecting variants with homogeneous ef-fects is of primary interest. To the best of our knowledge, allinvestigators who employed RE2 for meta-analyses of GWASs usedRE2 coupled with FE. Considering this practical situation, the cur-rent implementation of RE2 could be suboptimal.In the present study, we propose a new method, called RE2C,which increases the power of RE2 in two ways. First, we generalizedthe likelihood model of RE2 to account for correlations of statisticsand to achieve optimal power. To estimate the maximum likelihoodestimators of parameters efficiently, we developed an optimizationprocedure based on spectral decomposition of the variance-covariance matrix. Second, we modified the statistic to focus on theheterogeneous effects that cannot be detected by FE. This modifica-tion increased the power to identify new associations after the appli-cation of FE. The statistic does not follow a known asymptoticdistribution; therefore, we developed an efficient and accurate P-valueapproximation procedure using analytical decomposition of the statis-tic. In our simulations, RE2C achieved a dramatic increase in powercompared with competing approaches, such as the decoupling ap-proach (71% vs. 21%) when the statistics were correlated. Evenwhen the statistics were uncorrelated, RE2C achieved a modest in-crease in power. Applications to real genetic data demonstrated thatRE2C improved the significances of the associated variants. RE2C isefficient and can meta-analyze one hundred GWASs within one day.The software is available at http://software.buhmhan.com/RE2C.2 Materials and methods2.1 Existing meta-analysis methods for independentstatistics2.1.1 Fixed effects modelThe FE method assumes that the magnitude of the true effect is com-mon or fixed in every study in the meta-analysis. The inverse-variance-weighted effect-size method (Cochran, 1954; de Bakkeret al., 2008; Fleiss, 1993; Mantel and Haenszel, 1959) and theweighted sum-of-z-scores method (de Bakker et al., 2008; Han andEskin, 2011; Zaykin, 2011) are used widely. We only describethe former, because the two methods are approximately equivalent(Lee et al., 2016). Let X1; . . . ; XN be the effect-size estimates, suchas log odds ratios or regression coefficients, in N independent studies.Under the FE model, the observed effect Xi of study i is the sumof the true common effect l and the within-study error ei:Xi ¼ l þ ei:If the sample sizes of the studies are sufficiently large, Xi is normallydistributed. Let SE(Xi) be the standard error of Xi and letVi ¼ SE(Xi)2. It is common practice to use the estimated sample vari-ance for Vi. Let Wi ¼ Vi 1 be the inverse variance. The inverse-variance-weighted effect-size estimator is the sum of Xi weightedwith weights Wi:XFE ¼PWiXiPWi: (1)The variance of XFE isVFE ¼1PWi:It follows that the standard error of XFE is SE XFEð Þ ¼PWið Þ 1=2.Note that SEðXFEÞ is minimized only if the weights are inverse vari-ances, which explains the method’s name (Cochran, 1954; Greene,2012; Lee et al., 2016). We can then build a summary z-score,ZFE ¼XFESE XFEð Þ¼PWiXiffiffiffiffiffiffiffiffiffiffiffiffiPWip ;which follows N 0;1ð Þ under the null hypothesis of no associationðH0 : l ¼ 0Þ. The P-value can be calculated aspFE ¼ 2U   ZFEj jð Þ;where U is the cumulative density function of the standard normaldistribution.2.1.2 Random effects model (traditional)In contrast to FE, the RE method models heterogeneity explicitlyand assumes that the true value of the effect size li of each study issampled from an underlying distribution. Suppose that the distribu-tion has mean l and variance s2. The observed effect Xi is then thesum of the common effect l and the deviation of the ith study’sobserved effect from l, say di ¼ ðli   lÞ þ ei (Cochran, 1954) suchthatXi ¼ l þ di;where the within-study error ei is uncorrelated with the true effectsizes li. The variance in Xi is the sum of the between-study varianceand the within-study variance (Western and Bloome, 2009),V dið Þ ¼ Wi 1 þ s2:The most popular approach to estimate s2 is the method of momentsproposed by DerSimonian and Laird (DerSimonian and Laird,1986, 2015). Given the estimated between-study variance bs2, the REeffect size is calculated similarly to Equation (1):XRE ¼Pw i XiPw i;where the weights are now w i ¼ Wi 1 þbs2    1instead of Wi.Note that SE XREð Þ ¼Pw i    1=2.i380 C.H.Lee et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018Similarly to FE, we can construct a z-score statisticZRE ¼XRESE XREð Þ;and the P-value ispRE ¼ 2U   ZREj jð Þ :The traditional RE approach is equivalent to a likelihood ratio testthat assumes the same heterogeneity under both the null and the al-ternative hypotheses (Han and Eskin, 2011). This assumption canbe conservative in GWASs; therefore, RE has limited power inGWASs (Han and Eskin, 2011).2.1.3 RE2 (Han and Eskin)Han and Eskin proposed a modified RE method (RE2) that has bet-ter power than RE or FE under conditions of effect size heterogen-eity (Han and Eskin, 2011). The key difference between RE andRE2 is that the latter assumes no heterogeneity under the null hy-pothesis. This assumption is appropriate in many situations ofGWASs where we expect that the effect sizes are all zero under thenull hypothesis. The method is a likelihood ratio test that has thefixed parameters l ¼ 0 and s2 ¼ 0 under the null hypothesis, asfollows:Lo ¼Yi1ffiffiffiffiffiffiffiffiffiffi2pVip exp   X2i2Vi  ; (2)L1 ¼Yi1ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi2p Vi þ s2ð Þp exp   Xi   lð Þ22 Vi þ s2ð Þ !: (3)The roots of the partial derivatives of the equation (3) are not in aclosed form; therefore, the maximum likelihood (ML) estimates bland bs2 must be determined by using an iterative procedure. Hardyand Thompson suggested a simple and efficient procedure based onthe Newton–Raphson method (Han and Eskin, 2011; Hardy andThompson, 1996). Given bl and bs2, the likelihood ratio statistic canbe constructed as follows:SRE2 ¼XlogViVi þbs2  þXX2iVi X Xi   blð Þ2Vi þbs2:The value of s2 is restricted to be non-negative; therefore, as shownby Self and Liang (Self and Liang, 1987), the statistic follows a50:50 mixture of v21 and v22 asymptotically. Thus, the asymptotic P-value isp RE2 ¼ 0:5   P v21   SRE2   þ 0:5   P v22   SRE2   In practice, because of the small number of studies (N), a tabulatedcorrection is necessary for an accurate P-value. We pre-calculatedthe P-value table and the P-value ispRE2 ¼ kðN; SRE2Þ   p RE2where kðN; SRE2Þ is the small sample correction factor.2.2 Existing meta-analysis methods for correlatedstatistics2.2.1 The Lin-Sullivan methodHistorically, meta-analysis methods focused mainly on summarizingindependent estimates. However, in recent research design, the stat-istics are often correlated, for example, because of overlapping sub-jects, which is common in cross-disease meta-analysis. Lin andSullivan (Lin and Sullivan, 2009) developed a meta-analysis solutionto account for these correlations. First, they showed that the correl-ations of statistics could be calculated analytically. For example, ina case/control design, the correlation between statistics of studies iand j is approximated ascij   nij0ffiffiffiffiffiffiffiffiffiffiffiffini1nj1ni0nj0rþ nij1ffiffiffiffiffiffiffiffiffiffiffiffini0nj0ni1nj1r  =ffiffiffiffiffiffiffiffiffiffiffiffininj   qwhere ni, nj and nij are the total number of ith and jth studies andthe number of overlapping subjects between the two (ith and jth), re-spectively. Subscripts 1 and 0 denote the case and control status. LetC ¼ rij  NN be the correlation matrix of X ¼ X1; . . . ; XNf g. GivenC, one can easily calculate the variance-covariance matrix, X: Linand Sullivan suggested a statistic:XLS ¼~e 0X 1X~e 0X 1~ewhere ~e is an N  1 vector with ones. The variance is Var XLSð Þ¼ ð~e0X 1~eÞ 1: Therefore, one can obtain a z-score as well as aP-value (Lin and Sullivan, 2009). This method does not assume het-erogeneity; therefore, it can be considered as an extension of FE toaccount for correlations.2.2.2 The decoupling methodRecently, Han et al. (Han et al., 2016) proposed a methodcalled "decoupling" that can transform correlated data into independ-ent data. As Lin and Sullivan showed, in many situations, the correl-ation matrix C can be approximated analytically before the meta-analysis. Han et al. calculate a transformed covariance structure:Xdecoupled ¼ diag ~e 0 diag sð Þ   C   diag sð Þð Þ 1    1where s is the vector of standard errors, and diag(s) is a diagonal ma-trix whose diagonals are s. The updated standard errors then becomeSEdecoupled;i ¼ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiXdecoupled½i; i qwhere Xdecoupled½i; i  denotes the ith diagonal element of Xdecoupled.The data become independent, and thus can be used for RE2 as wellas FE. Han et al. showed that when the decoupled data are used forFE, the method is analytically equivalent to the Lin-Sullivan method.Han et al. also showed that under conditions of heterogeneity, RE2with decoupling (Decoupling-RE2) shows a higher power than FEwith decoupling. However, the optimality of Decoupling-RE2 hasnot been evaluated.2.3 RE2CIn the present study, we propose RE2C, a powerful random effectsmethod for meta-analysis of GWASs. RE2C is built upon RE2, butwith two modifications that improve its power: (1) accounting opti-mally for correlations, and (2) focusing on heterogeneous effectsconditioned on the application of FE. C in RE2C refers to both cor-relations and conditioning.2.3.1 Optimizing for the meta-analysis of correlated datasetsWe extended the RE2 model to include correlations between statis-tics. Let x be the length n vector denoting the observed effect sizes.Then, we could build a modelx ¼ l~e þ uþ ewhere u   Nð0; s2IÞ are random effects reflecting between-study heterogeneity and e   Nð0;RÞ are random errors. GivenIncreasing the power of meta-analysis for heterogeneous effects i381Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018the correlation of statistics C, which can be approximatedanalytically using the Lin-Sullivan approach (Lin andSullivan, 2009), we have R ¼ diag sð Þ  C   diagðsÞ where s is thevector of the standard errors. Then, the variance-covariancematrix of x isH ¼ Var xð Þ ¼ Rþ s2I :The likelihood functions under the null and alternative hypothesesbecomeL0 ¼ ð2pÞ n2   jRj 12   exp  12x0R 1x  L1 ¼ ð2pÞ n2   Hj j 12   exp  12x  l~eð Þ0H 1 x  l~eð Þ  ;To build a likelihood ratio test, we must find the maximum likeli-hood estimation (MLE) of the parameters l and s2. Previously, forindependent statistics, RE2 utilized an iterative procedure suggestedby Hardy and Thompson (Hardy and Thompson, 1996). However,their method only considers independent statistics. Therefore, wedeveloped an optimization procedure that can be applied efficientlyfor both independent and correlated statistics. We chose to use thetechnique developed for the restricted maximum likelihood (REML)framework. The key idea of our optimization procedure is to trans-form the two-dimensional search into a one-dimension search usingthe technique that was developed by Patterson and Thompson(Patterson and Thompson, 1971). A similar technique has been usedpreviously to correct for population stratifications (Kang et al.,2008).We decomposed the observations using a direct sum, whereone of the decomposed observations is the observation for theREML function after integrating out the fixed effects. That is, wedecomposed x into two matrix-vector multiplications of S and Qsuch as:x ¼ Sx Qx;where S is a transformation matrix of rank n  1 and Q is a trans-formation matrix of rank 1. The specific forms of S and Q for ourpurpose are described below. The properties of the direct sum meanthat the log-likelihood function of the mixed model l1ð Þ can bedecomposed into two log-likelihood functions of independent obser-vations as follows:l1 ¼ l01 þ l001 :The projection matrix S is an idempotent and symmetrical matrixthat integrates out the fixed effects (mean) of the observation x. Inour problem, matrix S is:S ¼ I  ~e ~e0  ~e    1~e 0h i¼1 00 1. . .00...0 0. .. .... . . 1266666664377777775  1n1 11 1. . .11...1 1. .. .... . . 1266666664377777775:Here, ~e is a vector of ones of size n. The matrix S satisfiesE Sxð Þ ¼ 0, i.e. S  ~e ¼~0. Then, matrix Q becomes:Q ¼~e 0H 1 :Matrix Q satisfies the conditions covðSx;QxÞ ¼ 0 and SHQ0 ¼ 0.Next, we considered the full log-likelihood l1 with the parameters ofinterest s2 and l as followsl1 l; s2jx   ¼  12nlog 2pð Þ þ log Hj j þ x  l~eð Þ0H 1 x  l~eð Þ  S is an orthogonal projection matrix; therefore, S is in the form ofAA0, where A is an n ðn  1Þ matrix with orthonormal columns,such that A0A ¼ I. To reduce the complexity of the restricted likeli-hood function l01 for Sx, Harville (1974) suggested the use of the re-stricted likelihood function for A0x, where the MLE for the twolikelihood functions are the same. As Harville showed, the restrictedlikelihood can be shown as:l01 s2jA0x   ¼   12n  1ð Þlog 2pð Þ þ log A0HA þ A0xð Þ0 A0HAð Þ 1 A0xð Þh i¼  12nlog 2pð Þ þ log Hj j þ x  bl~eð Þ0H 1 x  bl~eð Þ  þ12log 2pð Þ þ log ~e0~e   log ~e 0H 1~e   ;where bl ¼~e 0H 1x   ~e 0H 1~e    1. Let the orthogonal matrix, B, be theeigenvectors of the matrix A0HA such that B0A0HAB is diagonal. LetP ¼ AB. The matrix P then has the following properties: (i) P0P ¼ I,(ii) PP0 ¼ S, (iii) SP ¼ P and (iv) P0S ¼ P0. Using the spectral decom-position framework, the symmetric matrix SHS can be shown as:SHS ¼ PdiagðnSRS þ s2~1ÞP0 (4)where nSRS is the eigenvalues of the matrix SRS, where at least onevalue is zero, and the n ðn  1) matrix P has the eigenvectors asso-ciated with nSRS as the columns. We use ~1 to refer to a vector ofones of size n  1. Note that nSHS is equal to nSRS þ s2~1. Using theproperties of the matrix P and S, we haveP0HP ¼ P0SHSP ¼ diagðnSRS þ s2~1Þ:Here, we considered the full (not restricted) likelihood functionwhose l is substituted with bl.l1 s2jx   ¼  12nlog 2pð Þ þ log Hj j þ x  bl~eð Þ0H 1 x  bl~eð Þ  :For our problem of finding the MLE, this modified function is suffi-cient, because it satisfies that l ¼ bl at the MLE. Note that althoughwe focused on the full likelihood function to build a likelihoodratio test, the same optimization procedure below can be appliedto the restricted likelihood function. Following Equation (4), wecould define the generalized inverse of the matrix SHS, SHSð Þg,which isSHSð Þg ¼ PdiagðnSRS þ s2~1Þ 1P0:Next, we could transform x  bl~eð Þ0H 1 x  bl~eð Þ into a simpler ex-pression as follows:x  bl~eð Þ0H 1 x  bl~eð Þ ¼ x0S SHSð ÞgS0x¼ x0PdiagðnSRS þ s2~1Þ 1P0x;Thus, the likelihood becomesl1 s2jx   ¼  12nlog 2pð Þ þXni¼1log nR;i þ s2    þXn 1i¼1g2inSRS;i þ s2  " #;i382 C.H.Lee et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018where the scalar value nR;i is the ith eigenvalue of the matrix R, andgi is the ith component of the vector P0x. Now, the transformationhas reduced the number of parameters to one (s2). Thus, we can usea simple Newton-Raphson procedure to estimate the unknown par-ameter, s2. The first and the second derivatives of the transformedlog-likelihood functions are:dl1ds2¼  12Xni¼11nR;i þ s2    þ 12Xn 1i¼1g2inSRS;i þ s2   2 !;d2l1d s2ð Þ2¼ 12Xni¼11nR;i þ s2   2  12Xn 1i¼1g2inSRS;i þ s2   3 !:In summary, using this optimization procedure, the parameterestimation needs only the application of the Newton-Raphsonmethod to a single parameter, which is very efficient. Thus, we havea high chance of obtaining the global optimum using a grid searchas the starting point for the Newton-Raphson procedure. After wefind the MLE, we can build a likelihood ratio test statistic,logRj jRþbs2Iþ x0R 1x  x ~eblð Þ0 Rþbs2I    1x ~eblð Þ264375 5ð Þwhich follows a 50:50 mixture of v21 and v22 asymptotically.2.3.2 Focusing on heterogeneous effectsWe then modified the test procedure of RE2 to focus on heteroge-neous effects. In most meta-analyses of GWASs, detecting variantswith homogeneous effects is of primary interest. For this reason, it isoften essential to apply FE before applying RE2, while accounting forthe increased multiple testing burden. We surveyed the literature thatcited and used RE2; at least in all the> 50 papers that we examined,the studies used RE2 coupled with FE. Thus, considering this uniquesituation of meta-analysis of GWASs, where the prior application ofFE is mandatory, we can improve the power of RE2 by focusing onthe heterogeneous effects that would not be identified by FE.Specifically, we designed a statistic as follows,SRE2C ¼SRE2 if pRE2  pFE0 if pRE2 > pFE(In short, this statistic can become significant only if the RE2 P-value is more significant than the FE P-value. Although the statisticlooks simple, calculating the P-value of this statistic is non-trivial.Obviously, unlike RE2, this statistic does not follow a knownasymptotic distribution. One possible way is to use a resampling ap-proach that samples null z-scores repeatedly. However, P-valuestypically observed in GWASs are extremely small < 5  10 8   . Toestimate such a small P-value using resampling, a large number ofsamplings are required. Thus, in GWASs where millions of markersare analyzed, resampling can be very slow.To approximate the P-value of the new method efficiently, weused the following strategy. Recall that the RE2 statistic is a likeli-hood ratio statistic that measures the difference between the twolikelihoods: L0 in Equation (2) and L1 in Equation (3). We intro-duced an intermediate likelihood function,Lint ¼Yi1ffiffiffiffiffiffiffiffiffiffi2pVip exp   Xi   lð Þ22Vi !which is similar to L1, but with a restriction of s2 ¼ 0: Then, theRE2 statistic can be decomposed into the sum of the differencebetween L0 and Lint and the difference between Lint and L1, as fol-lows (Han and Eskin, 2011):SRE2 ¼ lnsup L1 s2; ljXi;Vi    sup L0 ØjXi;Við Þf g   2¼ ln sup Lint ljXi;Við Þf gsup L0 ØjXi;Við Þf g   2þ lnsup L1 s2; ljXi;Vi    sup Lint ljXi;Við Þf g   2¼XX2iVi X Xi  XFEð Þ2Vi( )þXlogViVi þ s2  þX Xi  XFEð Þ2Vi X Xi   blð Þ2Vi þ s2( )¼ SFE þ SHetwhere Ø indicates an empty set. The first statistic, SFE, is equal tothe square of the FE statistic (Z2FE). The second statistic, SHet, testsfor nonzero between-study variance, similar to the Cochran’s Q test.The two statistics are independent under the null hypothesis (Selfand Liang, 1987). Asymptotically, SFE follows v21, and SHet follows a50:50 mixture of 0 and v21. However, the conditions for them to fol-low their asymptotic distributions are different. Under the assump-tion that the effect size (Xi) follows a normal distribution due to alarge sample in each study, which is the case in GWASs, SFE followsv21 regardless of the number of studies (N). However, even under thenormality assumption, SHet follows a 50:50 mixture of 0 and v21 onlyif N is large. N is small in typical meta-analysis of GWAS; therefore,the true distribution of SHet can deviate greatly from the asymptoticdistribution. For our method, we approximated and tabulated thedistribution of SHet empirically for every possible N.In the previous section, we extended the RE2 model to accountfor correlations between statistics. Equation (5) can also be decom-posed into two parts,SFE ¼ x0R 1x  x ~eXLSð Þ0R 1 x ~eXLSð Þ  ;SHet ¼"logRj jRþbs2Iþ x ~eXLSð Þ0R 1 x ~eXLSð Þ  x ~eblð Þ0 Rþbs2I    1x ~eblð Þ#:where XLS is the Lin-Sullivan estimator of l, which issup Lint ljx;Rð Þf g. SFE is equivalent to the square of the z-score ofthe Lin-Sullivan method in this situation.Now that the RE2 statistic can be decomposed into SFE and SHetwhose null distributions are known, given an observed RE2 statistic,its P-value can be interpreted as an integral over a region in the two-dimensional space. Specifically, in Figure 1, the RE2 P-value is thevolume of the region excluding the bottom left triangle (i.e. regionAþ B). However, in RE2C, we only consider the region wherepRE2  pFE. Thus, for each SFE, we can search for SHet that wouldsatisfy pRE2  pFE, ork N; SFE þ SHetð Þ   ½0:5   P v21 > SFE þ SHet   þ 0:5   P v22 > SFE þ SHet      P v21 > SFE   :Let this lower boundary of SHet that satisfies pRE2  pFE beSHet:lowðSFE;NÞ. This boundary is plotted as a dashed line inFigure 1. Then, given an observed RE2C statistic dSRE2C , we calcu-lated the P-value as follows. We divided the range of SFE into KIncreasing the power of meta-analysis for heterogeneous effects i383Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018small bins (e.g. 1000 bins in [0,50]), denoted as xi (i ¼ 1; . . . ; K).The approximated P-value ispRE2C  XKi¼1P SHet >maxð dSRE2C  xi; SHet:lowðxi;NÞÞ    v21ðxiÞ  Dx;where Dx is the width of the bins. That is, we calculated the prob-ability that SHet would be large enough to satisfy SRE2C   dSRE2C forevery bin of SFE, and integrated them together. We took the max-imum function because if SHet is smaller than SHet:lowðxi;NÞ, thenSRE2C ¼0 by definition. Thus, we calculated the volume of region Ain Figure 1. As a result, it always satisfies the equation:pRE2C < pRE2as long as pRE2  pFE, because we have removed region B in Figure1. This shows that the RE2C P-value can never be less significantthan the RE2 P-value when those methods are used coupled withFE, for the variants with pRE2  pFE. Note that the calculation ofthe P-value is efficient because we have pre-calculated SHet:lowðx;NÞfor every x and N and the cumulative density function of SHet forevery N. Thus, the computational complexity is only O(K).Moreover, the complexity is not dependent on how small theP-value is, unlike in the resampling approaches.3 Results3.1 SimulationsWe evaluated the performance of RE2C using simulations. Weassumed seven studies, each of which comprised 2; 000 individuals,half of which were controls and half were cases. We assumed a SNPwith a minor allele frequency (MAF) of 0.1, following the Hardy-Weinberg equilibrium.3.1.1 False positive rateWe assumed the null hypothesis of no association and evaluated thefalse positive rate of RE2C. We repeated the null simulations 109times and estimated the false positive rate as the proportion of the re-peats whose P-value was  a, where a 2 f5  10 2;5  10 4; . . . ;5  10 8g. Table 1 shows that the false positive rates of RE2C werewell calibrated. We then assumed that the statistics were correlated,with a correlation coefficient q¼0.4. The false positive rates for thecorrelated statistics were also controlled (Table 1). There was a slightconservative tendency, which was possibly caused by the errors in ourapproximation of P-values using bins. However, the discrepancieswere very small.3.1.2 Power for independent statisticsWe compared the powers of FE, RE2 and RE2C. We generated10 000 sets for meta-analysis, where we again assumed seven studieswith sample size equal to 2000 and a MAF of 0.1. In our simula-tions, we considered the practical situations that FE was alreadyapplied before the application of RE2 or RE2C. Thus, we consideredthe combined use of RE2 (or RE2C) with FE where multiple testswere accounted. Specifically, the power of FE was the proportion ofthe sets whose P-value exceeded the genome-wide thresholdpGWAS ¼ 5  10 8. The power of RE2 (or RE2C) was the propor-tion of the sets whose FE or RE2 (RE2C) P-value exceededpGWAS=2 ¼ 2:5  10 8. To model the effect size heterogeneity in oursimulations, we assumed four different effect size distributions. Letl be a specific, assumed target log odds ratio. The four distributionswere as follows, in order of increasing amount of heterogeneity.First, we assumed a unimodal distribution that was a normal distri-bution with mean l and standard error l, truncated to [0, 2l].Second, we assumed a uniform distribution spanning ½0; 2l . Third,we assumed a bimodal distribution that followed N(0, l2) truncatedto [0, l] with one half probability, and N(2l, l2) truncated to [l,2l] with another half probability. These three distributions all hadmean l. Finally, we assumed a distribution representing opposite ef-fects, which followed N(-1.2l, l2) with one-half probability andN(1.2l, l2) with another half probability. Although opposite effectsbetween studies can be rare in genetic studies of the same disease,they can occur in cross-disease meta-analyses or cross-tissue eQTLanalyses. Once we assumed one of the distributions above, we ran-domly sampled bk, the log odds ratio in study k 2 1; . . . ;Kf g, fromthe distribution. We then sampled the minor allele counts in controland case samples assuming the control and case MAF, respectively.The control MAF was assumed to be the same as the populationMAF (0.1), assuming a very small prevalence, and the case MAFwas MAFcases ¼ ebk  MAFcontrol=ð ebk   1    MAFcontrol þ 1Þ. Foreffective comparisons of power, we adjusted l for each distributionsuch that the power of the most powerful method was approxi-mately 70%.Figure 2 shows the power comparison results. The powers ofRE2 and RE2C are shown as stacked bars. We assumed a prior ap-plication of FE to random effect methods; therefore, we applied adifferent color scheme to the proportion of datasets determined assignificant by FE (light grey) and the proportion of datasets wherethe random effect methods newly identified as significant (darkgrey). Note that the height of light grey bar is slightly shrunk inRE2/RE2C compared in FE, because the significance level was ad-justed to one-half. As the heterogeneity increases, the combined useof the random effect methods with FE gave increasingly higherpowers than compared with using FE alone, as expected. Under alltested scenarios of effect size distributions, RE2C was the mostpowerful. RE2C increased power of RE2 by 1.55, 1.85, 2.07 and2.98% for unimodal, uniform, bimodal and opposite effects, re-spectively. Although the increase in the absolute amount of powerwas modest, the increase in relative power gain compared with FEwas non-negligible. For example, in the unimodal distribution, thepower gain of RE2C from FE was 1.71%, which was more than 10times greater than that of RE2 (0.16%).Fig. 1. Two-dimensional representation of SFE and SHet . Given the observedstatistic bSRE2C , pRE2C is the probability in area A, while pRE2 is the probabilityin areas A and BTable 1. False positive rates of RE2Ca 5.0 10 2 5.0 10 4 5.0 10 6 5.0 10 8Independent input 4.8 10 2 4.8 10 4 4.7 10 6 5.5 10 8Correlated input (q¼ 0.4) 4.7 10 2 4.6 10 4 4.5 10 6 4.0 10 8i384 C.H.Lee et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018Increasing HeterogeneityNormalµ 2µEffect SizeUniformµ 2µEffect SizeBimodalµ 2µEffect SizeOpposite1.2µ 1.2µEffect SizeFE RE2 RE2CPower(%)020406080FE RE2 RE2CPower(%)020406080FE RE2 RE2CPower(%)020406080FE RE2 RE2CPower(%)020406080Fig. 2. Power of FE, RE2 and our new RE2C method for the meta-analysis of independent statistics. Assuming the statistics are independent, we simulated variouseffect size distributions with differing amounts of heterogeneity. We considered the scenario that RE2 or RE2C is additionally applied to FE while accounting formultiple testing. The power of RE2 and RE2C are shown as two-color stacked bars, where we colored the proportion identified by FE as significant in light greyand the proportion that RE2/RE2C additionally identified as significant in dark greyCorrelation  = 0.1Correlation  = 0.4Increasing HeterogeneityNormalµ 2µEffect SizeUniformµ 2µEffect SizeBimodalµ 2µEffect SizeOpposite1.2µ 1.2µEffect SizeLS DR2 RE2CPower(%)020406080LS DR2 RE2CPower(%)020406080LS DR2 RE2CPower(%)020406080LS DR2 RE2CPower(%)020406080LS DR2 RE2CPower(%)020406080LS DR2 RE2CPower(%)020406080LS DR2 RE2CPower(%)020406080LS DR2 RE2CPower(%)020406080Fig. 3. Power of Lin-Sullivan (LS), Decoupling-RE2 (DR2) and our new RE2C method for meta-analyzing correlated statistics. Assuming statistics are correlatedwith correlation coefficient q, we simulated various effect size distributions with differing amount of heterogeneity. We considered the scenario that DR2 or RE2Cis additionally applied to LS while accounting for multiple testing. DR2 and RE2C power is shown as two-color stacked bars, where we colored the proportion thatLS was significant in light grey and the proportion that DR2/RE2C additionally identified as significant in dark greyIncreasing the power of meta-analysis for heterogeneous effects i385Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 20183.1.3 Power for correlated statisticsUsing a similar simulation scheme, we evaluated the power of RE2Cunder the situation that the statistics were correlated. After wesampled the effect sizes of the studies, we generated the observed effectsizes assuming that they were correlated with correlation coefficient q.We assumed q¼0.1 and q¼0.4, and calculated the power for eachsetting. The value q¼0.4 was derived from assuming a cross-diseaseanalysis with 2000 cases and 3000 shared controls (Wellcome TrustCase Control Consortium, 2007). The competing approaches in thissimulation were the Lin-Sullivan (LS) and the Decoupling-RE2 (DR2)methods. As described in the Methods, the Lin-Sullivan method is anextended FE method to account for correlations. Decoupling-RE2refers to the application of the transformed data by decoupling ap-proach, which became independent, to RE2. Figure 3 shows thatRE2C outperformed the other methods greatly in all scenarios of effectsize distributions and correlations. For example, for the uniform distri-bution where q¼0.4, RE2C achieved 71% power while the power ofLin-Sullivan method and Decoupling-RE2 were only 23.8% and21.4% respectively. Surprisingly, Decoupling-RE2 performed poorlyeven for large heterogeneity when the correlations were high (q¼0.4).This demonstrates that although the application of the decoupled datato RE2 is possible, it may not provide optimal power.3.2 Applications to real dataWe wanted to evaluate the utility of RE2C for real data. To thisend, we used the cross-disease analysis data of Moskvina et al.(Moskvina et al., 2013) who performed a meta-analysis of associationresults for the Alzheimer’s disease (AD) and the Parkinson’s disease(PD). Moskvina et al. examined the meta-analysis P-values of 10 lociknown to be associated with AD and 18 loci known to be associatedwith PD. The two diseases shared some controls Nshared ¼ 5571ð Þ;therefore, there were correlations between the statistics of the two dis-eases. To account for these correlations, Moskvina et al. used the Lin-Sullivan method. However, the same variant may have differing ef-fects on the two diseases. Therefore, random effect methods mighthelp in association tests. We obtained the reported effect sizes (OR)and P-values for these 28 loci from the table shown in their manu-script. We then calculated the standard errors from the OR andP-values, and used them for meta-analysis. We removed three lociwhose OR was 1.00 (because the paper reported only two digitsbelow zero), and applied RE2C to the remaining 25 loci.Table 2 gives the details of the collected data and the meta-analysis results. Out of 25 loci, LS was the most significant in 13loci. In all the remaining 12 loci, RE2C was the most significant.Note that for the 13 loci where LS was the most significant, RE2CP-values were completely non-significant (pRE2C ¼ 1). This is be-cause RE2C was designed to be used with FE (LS), but focusing onlyon loci with heterogeneity. We also show the results of an RE2C im-plementation with optimization for correlated statistics but withoutthe technique for focusing on heterogeneous effects (denoted asRE2C*), which shows that focusing on heterogeneous effects im-proved P-values at these 12 loci. Overall, these results showed thatTable 2. Cross-disease meta-analysis results of the Alzheimer’s disease and Parkinson’s disease based on the reported data from Moskvinaet al.MethodsParkinson Disease Alzheimer Disease LS DR2 RE2C* RE2CChr Base Pairs SNP OR P OR P P P P PAlzheimer Disease1 207 819 492 1-207819492 0.61 0.062 0.50 0.058 0.016 0.019 0.02389 12 127 892 810 rs6733839 1.07 0.0098 1.23 5.2E-5 0.00029 0.00033 0.00017 3.0E-56 47 327 031 rs9367271 1.11 0.0014 1.06 0.339 0.0017 0.0020 0.00279 17 143 106 884 rs7806047 0.87 0.001 0.89 0.151 0.0007 0.0008 0.00118 18 27 466 181 rs1532277 0.99 0.709 0.81 1.8E-6 0.024 0.0011 8.11E-05 1.5E-511 60 045 900 rs7949816 0.95 0.073 0.82 0.00075 0.0084 0.010 0.00589 0.001211 85 677 094 11-85677094 1.20 0.0055 1.26 0.057 0.0019 0.002 0.00314 119 01 032 228 rs56059558 0.86 0.0023 0.84 0.05 0.0008 0.0009 0.001302 119 45 392 254 rs6857 0.95 0.154 5.55 4.4E-92 0.0002 2.9E-53 3.24E-94 1.6E-9519 51 724 326 rs200656 1.06 0.089 1.06 0.23713 0.055 0.06 0.07461 1Parkinson Disease1 155 135 036 rs35749011 1.43 6.1E-5 1.02 0.938 0.00012 0.00014 0.00022 12 135 592 245 rs6758044 1.12 1.2E-5 0.96 0.383 0.0005 0.0003 7.99E-05 1.5E-52 169 119 178 rs13392079 1.14 1.1E-6 0.95 0.296 0.0001 3.2E-5 6.07E-06 1.0E-63 161 114 968 rs336549 0.90 9.4E-6 1.05 0.275 0.0004 0.0002 4.68E-05 8.5E-63 182 760 073 rs10513789 1.11 0.0007 1.01 0.921 0.001 0.0012 0.00164 14 15 737 882 rs4698413 1.15 4.4E-9 0.98 0.651 5.6E-7 2.2E-7 5.38E-08 8.2E-94 77 146 751 rs56275416 1.15 2.0E-6 1.01 0.84 3.8E-5 4.1E-5 2.80E-05 4.9E-64 90 646 886 rs356165 0.76 1.2E-28 1.04 0.38 9.4E-21 8.0E-25 3.81E-28 3.2E-296 32 440 158 rs7453703 1.10 0.0006 1.20 0.00021 1.4E-5 1.7E-5 2.68E-05 18 16 718 969 rs587738 1.10 0.00015 1.02 0.616 0.0008 0.0009 0.00117 18 89 647 688 8-89647688 1.63 1.9E-5 1.50 0.078 1.2E-5 1.4E-5 2.26E-05 112 40 582 993 rs2263418 1.24 1.5E-8 0.93 0.354 1.4E-6 5.6E-7 9.45E-08 1.5E-812 123 110 365 rs6489158 0.91 0.00018 0.93 0.119 0.0001 0.0002 0.00023 116 31 103 796 rs2359612 1.12 3.3E-6 1.08 0.073 2.8E-6 3.4E-6 5.55E-06 117 43 804 317 rs9897399 0.75 1.5E-19 0.92 0.107 1.4E-16 4.6E-17 8.19E-18 8.3E-19We compared the results of the Lin-Sullivan method (LS), Decoupling-RE2 (DR2) and RE2C. RE2C* refers to an RE2C implementation with optimization forcorrelated statistics but without the technique for focusing on heterogeneous effects. The most significant P-value among all methods is in bold-face.i386 C.H.Lee et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018if RE2C is used in combination with LS, a high association testpower to detect both loci with and without heterogeneity is ob-tained. Interestingly, RE2C found two loci (rs4698413 andrs2263418) as genome-wide significant < pGWAS ¼ 2:5  10 8   that were not identified by LS alone.We also performed additional real data analyses where statisticswere uncorrelated, to demonstrate the performance of RE2C forcombining independent datasets. The results are shown inSupplementary Materials (Supplementary Table S1).3.3 EfficiencyWe evaluated the efficiency of the methods (Table 3). To this end,we measured the running time of methods for the meta-analysis ofdiffering numbers of studies (from 2 to 100). We timed how long ittook to analyze 1 000 000 SNPs. We used the software R to run FEand RE2C, and Java to run RE2. RE2C was highly efficient. Theestimated time to analyze a million SNPs in a meta-analysis combin-ing 100 studies was 0.07 hours for RE2 and 0.44 hours for RE2C.Our results imply that RE2C is suitable for future large-scale meta-analyses, where the number of datasets to be combined is expectedto grow.4 ConclusionWe proposed a new random effects model meta-analysis methodRE2C, which has an improved power for the detection of heteroge-neous effects between studies. We optimized the statistic for meta-analyzing correlated statistics, and modified the statistics to only focuson heterogeneous effects. We expect that our method will be appliedto a wide range of study designs in the future, such as cross-disease orcross-population studies, to help identify new associations.FundingThis work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government (MSIP) [grant number2016R1C1B2013126].Conflict of Interest: none declared.ReferencesBhattacharjee,S. et al. (2012) A subset-based approach improves power andinterpretation for the combined analysis of genetic association studies ofheterogeneous traits. Am. J. Hum. Genet., 90, 821–835.Chimusa,E.R. et al. (2014) Genome-wide association study of ancestry-specific TB risk in the South African Coloured population. Hum. Mol.Genet., 23, 796–809.Cochran,W.G. (1954) The Combination of Estimates from DifferentExperiments. Biometrics, 10, 101–129.de Bakker,P.I.W. et al. (2008) Practical aspects of imputation-driven meta-analysis of genome-wide association studies. Hum. Mol. Genet., 17,R122–R128.DerSimonian,R. and Laird,N. (2015) Meta-analysis in clinical trials revisited.Contemp. Clin. Trials, 45, 139–145.DerSimonian,R. and Laird,N. (1986) Meta-analysis in clinical trials.Controlled Clin. Trials, 7, 177–188.Dichgans,M. et al. (2014) Shared genetic susceptibility to ischemic stroke andcoronary artery disease: a genome-wide analysis of common variants.Stroke, 45, 24–36.Evangelou,E. and Ioannidis,J.P.A. (2013) Meta-analysis methods forgenome-wide association studies and beyond. Nat. Rev. Genet., 14,379–389.Fleiss,J. (1993) The statistical basis of meta-analysis. Stat. Methods Med. Res.,2, 121–145.Greene,William H. (2012) Econometric Analysis (7th ed.). Pearson Education.pp. 549–642. ISBN 9780131395381.Han,B. and Eskin,E. (2011) Random-effects model aimed at discovering asso-ciations in meta-analysis of genome-wide association studies. Am. J. Hum.Genet., 88, 586–598.Han,B. et al. (2016) A general framework for meta-analyzing dependent stud-ies with overlapping subjects in association mapping. Hum. Mol. Genet.,25, 1857–1866.Hardy,R.J. and Thompson,S.G. (1996) A likelihood approach to meta-analysis with random effects. Statist. Med., 15, 619–629.Harville,D.A. (1974) Bayesian inference for variance components using onlyerror contrasts. Biometrika., 61, 383–385.Hibar,D.P. et al. (2012) Genome-wide association identifies genetic variantsassociated with lentiform nucleus volume in N¼ 1345 young and elderlysubjects. Brain Imaging Behav., 7, 102–115.Kang,E.Y. et al. (2014) Meta-Analysis Identifies Gene-by-EnvironmentInteractions as Demonstrated in a Study of 4,965 Mice. PLoS Genet, 10,e1004022–16.Kang,H.M. et al. (2008) Efficient control of population structure in model or-ganism association mapping. Genetics, 178, 1709–1723.Kar,S.P. et al. (2016) Genome-wide meta-analyses of breast, ovarian, andprostate cancer association studies identify multiple new susceptibilityloci shared by at least two cancer types. Cancer Discov., 6,1052–1067.Keller,M.F. et al. (2014) Trans-ethnic meta-analysis of white blood cell pheno-types. Hum. Mol. Genet., 23, 6944–6960.Kiryluk,K. et al. (2012) Geographic Differences in Genetic Susceptibility to IgANephropathy: GWAS Replication Study and Geospatial Risk Analysis. PLoSGenet., 8, e1002765–16.Lee,C.H. et al. (2016) Comparison of Two Meta-Analysis Methods: Inverse-Variance-Weighted Average and Weighted Sum of Z-Scores. GenomicsInform., 14, 173–180.Lee,J.H. et al. (2014) Genetic susceptibility for chronic bronchitis in chronicobstructive pulmonary disease. Respir. Res., 15, 1–12.Lin,D.-Y. and Sullivan,P.F. (2009) Meta-analysis of genome-wide associationstudies with overlapping subjects. Am. J. Hum. Genet., 85, 862–872.Liu,J.Z. et al. (2015) Association analyses identify 38 susceptibility loci for in-flammatory bowel disease and highlight shared genetic risk across popula-tions. Nat. Genet., 47, 979–986.Manolio,T.A. (2013) Bringing genome-wide association findings into clinicaluse. Nat. Publishing Group, 14, 549–558.Manolio,T.A. (2010) Genomewide association studies and assessment of therisk of disease. N. Engl. J. Med., 363, 166–176.Mantel,N. and Haenszel,W. (1959) Statistical aspects of the analysis ofdata from retrospective studies of disease. JNCI J. Natl. Cancer Inst., 22,719–748.Moskvina,V. et al. (2013) Analysis of genome-wide association studies ofAlzheimer disease and of parkinson disease to determine if these 2 diseasesshare a common genetic risk. JAMANeurol., 70, 1268–1276.Patterson,H.D. and Thompson,R. (1971) recovery of inter-block informationwhen block sizes are unequal. Biometrika, 58, 545–554.Perry,J.R.B. et al. (2012) Stratifying Type 2 Diabetes Cases by BMI IdentifiesGenetic Risk Variants in LAMA1 and Enrichment for Risk Variants in LeanCompared to Obese Cases. PLoS Genet., 8, e1002741–14.Table 3. Efficiency of RE2C2 studies 10 studies 25 studies 100 studiesFE (R) 25s 52s 93s 297sRE2 (Java) 36s 51s 85s 260sRE2C (R) 23s 51s 118s 1615s (0.44h)Increasing the power of meta-analysis for heterogeneous effects i387Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018Petersen,G.M. et al. (2010) A genome-wide association study identifies pan-creatic cancer susceptibility loci on chromosomes 13q22.1, 1q32.1 and5p15.33. Nat. Genet., 42, 224–230.Sapkota,Y. et al. (2014) Association between endometriosis and the interleu-kin 1A (IL1A) locus. Hum. Reprod., 30, 239–248.Self,S.G. and Liang,K.Y. (1987) Asymptotic properties of maximum likeli-hood estimators and likelihood ratio tests under nonstandard conditions.J. Am. Stat. Assoc., 82, 605–610.Stein,J.L. et al. (2012) Identification of common variants associatedwith human hippocampal and intracranial volumes. Nat. Genet., 44,552–563.Sul,J.H. et al. (2013) Effectively identifying eQTLs from multiple tissues bycombining mixed model and meta-analytic approaches. PLoS Genet., 9,e1003491–e1003413.Wellcome Trust Case Control Consortium. (2007) Genome-wide associationstudy of 14,000 cases of seven common diseases and 3,000 shared controls.Nature, 447, 661–678.Welter,D. et al. (2014) The NHGRI GWAS Catalog, a curated resource ofSNP-trait associations. Nucleic Acids Res., 42, D1001–D1006.Western,B. and Bloome,D. (2009) Variance Function Regressions forStudying Inequality. Sociological Methodology., 39, 293–326.Ye,C.J. et al. (2014) Intersection of population variation and autoimmunitygenetics in human T cell activation. Science., 345, 1–9.Zaykin,D.V. (2011) Optimally weighted Z-test is a powerful methodfor combining probabilities in meta analysis. J. Evol. Biol., 24,1836–1841.Zeggini,E. and Ioannidis,J.P. (2009) Meta-analysis in genome-wide associ-ation studies. Pharmacogenomics, 10, 191–201.i388 C.H.Lee et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i379/3953957by gueston 07 January 2018
5028881975002	PMID28881975	5028881975	https://watermark.silverchair.com/btx241.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881975.main.pdf	Bioinformatics, 33, 2017, i333–i340doi: 10.1093/bioinformatics/btx241ISMB/ECCB 2017Molecular signatures that can be transferredacross different omics platformsM. Altenbuchinger1, P. Schwarzfischer2, T. Rehberg1, J. Reinders2,Ch. W. Kohler1, W. Gronwald2, J. Richter3, M. Szczepanowski3,N. Masque-Soler3, W. Klapper3, P. J. Oefner2 and R. Spang1,*´1Statistical Bioinformatics and 2Institute of Functional Genomics, University of Regensburg, Regensburg, Germanyand 3Department of Pathology, Hematopathology Section and Lymph Node Registry, University HospitalSchleswig-Holstein, Campus Kiel/Christian-Albrecht University, 24105 Kiel, Germany*To whom correspondence should be addressed.AbstractMotivation: Molecular signatures for treatment recommendations are well researched. Still it ischallenging to apply them to data generated by different protocols or technical platforms.Results: We analyzed paired data for the same tumors (Burkitt lymphoma, diffuse large B-celllymphoma) and features that had been generated by different experimental protocols and analytical platforms including the nanoString nCounter and Affymetrix Gene Chip transcriptomics as wellas the SWATH and SRM proteomics platforms. A statistical model that assumes independent sample and feature effects accounted for 69–94% of technical variability. We analyzed how variability ispropagated through linear signatures possibly affecting predictions and treatment recommendations. Linear signatures with feature weights adding to zero were substantially more robust thanunbalanced signatures. They yielded consistent predictions across data from different platforms,both for transcriptomics and proteomics data. Similarly stable were their predictions across datafrom fresh frozen and matching formalin-ﬁxed parafﬁn-embedded human tumor tissue.Availability and Implementation: The R-package ‘zeroSum’ can be downloaded at https://github.com/rehbergT/zeroSum. Complete data and R codes necessary to reproduce all our results can bereceived from the authors upon request.Contact: rainer.spang@ur.de1 IntroductionToday, molecular data describing blood, urine, stool or tissue specimens is high-content data. Machine learning methods extract biomarker signatures from molecular data that can be used for therapyrecommendations. Among the best established methods are penalized linear regression models such as the LASSO (Tibshirani, 1996)and the elastic net (Zou and Hastie, 2005). A more recent method iszero-sum regression (Altenbuchinger et al., 2017; Lin et al., 2014).These algorithms select features and endow them with weights forming predictive linear signatures.Data sharing is critical to advance precision medicine. Molecularhigh-content data of patient specimens together with matching diagnostic, histologic and clinical data across many studies are made easily accessible, comparable and jointly analyzable (Quackenbush,2014). Projects like DECIPHER (Firth et al., 2009), the NCIGenomic Data Commons (NCI Center for Cancer Genomics (CCG),2016), or the Australian Genomics Health Alliance are on theforefront of buildung such digital medicine resources. But data ambiguity and data dissonance still present major obstacles in the sharing of data (Grossman et al., 2016; Quackenbush, 2014).Current data resources are not harmonized. Protocols for retrieval of biological specimens, extraction and measurement ofmolecules of interest, and data processing may vary considerablyamong datasets. Differences in data generation leave traces in thedatasets rendering their joint analysis difficult. Cross platform analysis is particularly essential in case of signatures developed onomics high-content platforms to be later applied to targeted platforms that generate data only for the selected features. Similarly,signatures developed for fresh frozen material need to be transferred to formalin-fixed paraffin-embedded (FFPE) material, whichis more readily available (Masque-Soler et al., 2013; Scott et al.,´2014).To identify requirements for data harmonization, we need a better understanding of inter-technical variability: the systematicCV The Author 2017. Published by Oxford University Press.i333This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i333/3953956by gueston 07 January 2018i334M.Altenbuchinger et al.discrepancies in data generated by different protocols. Moreover,we need to better understand how these dissonances propagatethrough subsequent analysis steps. From the precision medicine perspective, there are two types of data problems: those that can affecttreatment decisions and those that cannot. Vice versa, there are twotypes of predictive signatures, those that are sensitive to specific datadissonances and those that are not.Here we study and model systematic discrepancies in transcriptome and proteome data generated by various protocols and platforms. Based on the model we study which properties of linearsignatures enhance or reduce the effect that data dissonance has ontreatment recommendations and give advice on choosing proper regression models.2 Results2.1 Modelling inter-technical variabilityDij ¼ zij À xij ¼ hi þ xj þ rij ;(b)(c)−1 0123mappedmapped−3Affy. vs. RNA−Seq.orig.SWATH vs. SRMorig.Affy. vs. nCounterorig.(a)−3−10123−2−10123Fig. 2. Most of the inter-technical variability can be explained by our independent effects model. Figures (a) to (c) show box plots of the differences between data generated by different technologies. The plots on top show theoriginal non-adjusted but individually normalized data, while those belowcompare adjusted datasets. 69%-94% of inter-technical variability could beexplained by our model(1)where rij is the residue of the model. Tukey’s median polish algorithm (Tukey, 1977) estimates hi and xj. The plots in column (c) ofFigure 1 show~zij ¼ zij À hi À xj :Fig. 1. Comparison and adjustment of omics data of the same samples proﬁled with different technologies and protocols. The ﬁrst two columns contraststate of the art normalized datasets. Row (1) shows paired gene expressiondata of the same non-Hodgkin lymphomas using the Affymetrix GeneChip (a)and NanoString nCounter (b) technology. Row (2) shows paired protein expression data acquired by SWATH (a) and SRM (b), for a subset of the nonHodgkin lymphomas. And Row (3) shows paired expression levels of activated T cells for microarray (a) and RNA-Seq data (b). Column (c) shows heatmaps for the datasets (b) adjusted to match the datasets (a) using our model.Columns always correspond to molecular features (mRNA or protein) androws to samplesmappedLet x and z be two datasets covering the same features for the samepatients but generated with different protocols. Both xij and zij arematrices with i ¼ 1, . . ., N denoting samples and j ¼ 1, . . ., p denoting features. We further assume that both data matrices are normalized using a state of the art protocol and are log-transformed.Data generated by different technical platforms for the samesample is quantitatively and qualitatively different even after normalization. In Figure 1 row (1), the heatmaps (a) and (b) contrastAffymetrix gene expression data of fresh frozen material of 40non-Hodgkin lymphomas from (Klapper et al., 2012) to matchingnCounter data of FFPE material (Masque-Soler et al., 2013). The´plots in row (2) show proteomics data for 23 of the lymphomasusing (a) sequential window acquisition of all theoretical fragmention spectra (SWATH) (Gillet et al., 2012) and (b) targeted selectedreaction monitoring (SRM) (Faktor et al., 2016). For measurementdetails we refer to the methods section. Finally, the third rowshows (a) microarray mRNA and (b) RNA-Seq profiles from Zhaoet al. (2014) for the 12 most variable genes in 12 activated T cellsamples. For details on data preprocessing, see the methodssection.To model the discrepancies we assume that they result from twoindependent biases: (i) sample effects hi that systematically affect allfeatures of a sample i in the same way. (ii) feature effects xj that systematically affect feature j in all samples in the same way. We modeltechnical variability D using these two effects by:(2)2.2 Propagation of inter-technical variabilityHere we analyze how technical variability that can be modeled by(2) propagates in linear signatures of the formXyi ¼ b0 þbj xij ;(3)j2CThis is the data of technology (b) adjusted to the systematic sampleand feature effects. The adjusted data of the technology (b) is visually closer to the data of technology (a), and also quantitatively:Figure 2 shows for each of the three paired datasets box plots of thedifferences between the two original datasets (top) and between theoriginal first versus the adjusted second dataset (bottom). The twoindependent biases accounted for 69%, 79% and 94% of the intertechnology variability D2 , respectively.ijBy minimizing Dij in Equation (1) we adjust data from differenttechnologies. However, this is not our primary aim here. Instead westrive for signatures that can be used on non-harmonized data directly. The model will guide us to these signatures.where the bj are feature weights, and yi is a response variable likethe response of patient i to a certain treatment. C contains all indicesof non-zero regression weights. Assume that the signature featuresare covered by both datasets x and z but that the signature was onlytrained on x. What happens if we apply the signature unchanged todataset z from a different platform?2.2.1 An instructive simulationWe used Affymetrix GeneChip data from 281 diffuse large B-celllymphomas (DLBCL) (Hummel et al., 2006; Klapper et al., 2008;Salaverria et al., 2011). 122 DLBCL are of the ABC and 159 of theDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i333/3953956by gueston 07 January 2018correlation Affy. vs. nCounterAUC (Affy.)(b)0.5 0.6 0.7 0.8 0.9 1.0(a)i335024680.5 0.6 0.7 0.8 0.9 1.0Cross-platform signatures100| sum of regr. weights |246810| sum of regr. weights |Fig. 3. Comparing classiﬁcations across technologies: Plot (a) shows the absolute sum of regression weights for 1000 signatures trained on re-sampleddata from technology 1 (Affymetrix) plotted against their classiﬁcation performances (area under the receiver operating characteristic curve (AUC)) onindependent data of the same technology. All signatures perform excellentindependent of their strongly varying weights. The y-axis of Figure (b) showsthe correlation (agreement) of classiﬁcation scores for data of technology 1(Affymetrix) and 2 (nanoString). Predictions from signatures with balancedweights (x-axis near zero) agree well across technologies, while unbalancedsignatures produce conﬂicting predictions on the second technologyGCB subtype. We first restricted the dataset to 47 genes, which werealso covered by nanoString nCounter data of 40 DLBCL (Masque´Soler et al., 2013). Next, we divided the data for which we only hadAffymetrix data into a training set of 51 ABC and 87 GCB and avalidation set of 71 ABC and 72 GCB. Using the LASSO logisticregression algorithm implemented in the R package glmnet(Friedman et al., 2010), we trained 1000 linear GCB/ABC classifierseach on a different random subset of 40 genes and evaluated themon the validation cohort. While the gene weights varied stronglyacross signatures, all signatures reached almost perfect performance(Fig. 3a).We next applied these signatures unchanged to 40 DLBCL forwhich nanoString data were available and which were not part ofthe training set. For signatures with regression weights that sum upclose to zero the linear predictive scoresXbj xijj2Cfor the Affymetrix data and the nanoString data showed correlationaround 0.9, while for signatures with unbalanced weights this wasreduced to 0.75 on average and fell for some signatures below 0.6(Fig. 3b). Balanced signatures worked equally well on data fromboth technologies while unbalanced signatures did not.This observation can be explained by model (1). Plugging equation (1) into equation (3), assuming that the residues rij are small,yieldsXXyi ¼ b0 þbj xij ¼ b0 þbj ðzij À hi À xj Þ :(4)j2Cj2CIf the regression weights bj add up to zero this simplifies toX~y i ¼ b0 þbj zij ;(5)j2CP~where b0 ¼ b0 À j2C bj xj . Hence if the weights bj sum up to zero,the sample effects hi cancel, while the feature effects xj absorb into~the intercept b0 . Thus, the same model can be applied to x and z. Inthis case hi and xj which account for the majority of technologyrelated discrepancies in the data do not affect the predictions exceptmaybe for a constant shift across all samples. The same argumentalso holds for generalized and penalized linear models like theLASSO logistic regression used above.2.2.2 Zero-sum regression reduces cross platform adjustments to thecalibration of a single parameterThe LASSO can yield signatures with a small sum of regressionweights but it does not guarantee it (Fig. 3). However, balancedweights can be enforced. Zero-sum regression is an instance of constrained LASSO regression (Tibshirani, 1996). It was originally developed for compositional data only (Lin et al., 2014), but itsspectrum of applications is broader (Altenbuchinger et al., 2017).Here we argue that zero-sum regression is a method of choice forcross technology data analysis. The method adopts the penalizedLASSO log-likelihood but additionally enforces the sum of the regression weights to zero:1b bÞðb 0 ; b ¼ argminb0 ; b f2Nþkjjbjj1 g ;NXyi À b0 ÀpXbj xijj¼1i¼1pXsubject to bj ¼ 0 :!2(6)j¼1Above we showed in three examples that platform differencescan be mostly explained by two independent effects: a sample effecthi and an independent feature effect xj. Zero-sum signatures yieldthe same prediction for shifted data xij þ hi and non-shifted data xij.Moreover, unlike the standard LASSO, zero-sum learns the samesignature if applied to xij or xij þ hi (Altenbuchinger et al., 2017),and, up to an arbitrary offset b0, also on xij þ hi þ xj . If we use theLASSO, we must adjust all regression weights when moving fromdata of one technology to the next. For data where our model explains 100% of the inter-technology variability, zero-sum signatureswill only need an adjustment of the off-set b0. Below we will showthat on real data, where the model explains only some 80% of theinter-technology variability, zero-sum signatures, nevertheless, yieldconsistent predictions across datasets.2.3 Simulation studiesHere we further substantiate the benefits of zero-sum signatures incross technology data analysis in simulation studies. We simulatedpaired data representing two technologies linked by Equation (1),and quantitatively study how the simulated inter-technology variability propagates from feature data to predictions.Omics data can be either continuous intensity or discrete countdata. NanoString nCounter, RNA-seq, and many other quantitativenext-generation sequencing based methods yield discrete data. Wethus simulated counts from a negative binomial distribution,NBðlj ; /j Þ, where lj is the mean count of feature j and /j its dispersion, which is directly related to its variance via varðXj Þ ¼ lj þ /j l2 .jTo obtain realistic values for both lj and /j we estimated 1000 pairsðlj ; /j Þ by a maximum likelihood estimate using the 1000 mostabundant genes from the RNA-seq data (doi:10.1371/journal.pone.0078644.s008) of Zhao et al. (2014).For each simulation run, we randomly drew 100 meandispersion pairs and simulated counts from the corresponding negative binomial distributions. In total, we simulated 150 samples, ofwhich 50 served as training set and 100 as test set. Taking logarithms yielded simulated screening data matrices. For the two weightvectors b shown in Table 1 we calculated response variables towhich we added the random Gaussian errors  i $ Nðl ¼ 0; r ¼ 1Þresulting in a vector of responses (yi). Experimentally, no calibrateddata is available. Thus, we normalized the raw count data, X, by itssample-wise means. After taking the logarithms, we ended up withthe normalized predictor data x ¼ (xij), where i ¼ 1, . . ., 150 andj ¼ 1, . . ., 100.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i333/3953956by gueston 07 January 2018M.Altenbuchinger et al.2.3.1 Only zero-sum signatures apply on both technology 1 and 2 dataWe first describe the performance of signatures, for which only theoffset b0 was adjusted. For these signatures we obtained predictionsbby 1 on the first dataset and y 2 on the second. Figure 4b and e showsr1 À r2 ¼ corðb1 ; yÞ À corðb2 ; yÞ calculated for the 100 test samplesyyfor signature A and B, respectively. As expected, the performance ofzero-sum signatures applied to z(s) is not compromised at all whilethose of LASSO and f-OLS models frequently is. For signature Athis effect was less pronounced than for signature B. LASSO estimates coefficients close to those used for data generation. In A thesecoefficients correctly summed to a small number, thus LASSO signatures approximate zero-sum signatures. In contrast f-OLS predictions were off target. In B the LASSO correctly estimatedpredominantly positive coefficients yielding signatures far awayfrom a zero-sum. In simulation A LASSO signatures lost precisionon the second dataset, in simulation B they broke down. f-OLS signatures performed poorly in both simulations.2.3.2 Unchanged zero-sum signatures were more reliable thanretrained signaturesThe most common approach to moving signatures from one technology to the next, is to keep the selected features from the first study0.0z.−sumLASSOf−OLSz.−sumLASSOf−OLSr1 − r2 (retrained)f−OLSz.−sumLASSO0.20.0−0.6 −0.4 −0.2r1 − r2 (retrained)0.40.6f−OLSz.−sumLASSO0.4z.−sum−0.6 −0.4 −0.20.20.0r1 − r2−0.6 −0.4 −0.2(f)0.6z.−sum0.6r10.40.20.20.40.60.41.00.80.0(e)0.8(d)0.0We trained linear models on (y, x) using (i) zero-sum regression,(ii) the standard LASSO and (iii) and ordinary least square regression(OLS) combined with feature filtering. The latter method, first screensfor the top k features with highest absolute correlation to y and fits astandard linear model on the selected features using OLS. k is calibrated in cross validation on the training data and so is the tuning parameter k of both LASSO and zero-sum regression. While by definitionthe sum of coefficients is zero for all zero-sum signatures, it rangesfrom À10.7 to 11.1 (À0.1 to 21.1) in the LASSO signatures and fromÀ16.5 to 10.7 (À12.6 to 18.9) in the f-OLS signatures for simulationscenario A (B), respectively, showing that the three methods producedifferent signatures. Figure 4a and d shows violin plots of the correlations r1 ¼ corðb1 ; yÞ between observed and predicted responses calybculated on the test cohort, where y 1 are the predictions, for scenarioA and B, respectively. For both zero-sum and LASSO signatures weobserve that the median correlation over all simulation runs, shownas a grey dot, is roughly the same indicating that the zero-sum constraint is not compromising predictive performance on the screeningdata. This is not surprising for signature A where the data generatingcoefficients b sum up to zero. But also for signature B with only positive coefficients the predictive performance of zero-sum regressionwas not compromised. f-OLS combined with feature filtering performed significantly worse in scenario A.Next we simulated a matching second dataset representing a second technological platform. Technological platforms typically donot cover the exact set of features. Nevertheless, we here assumethat at least all signature features are covered. We thus perturbed xby xij ! xij þ hi þ xj , cut out the features selected by a signature sand normalized this data by the same strategy as x, but now on thesignature features s only. This yielded a data matrix z(s).0.2In scenario A the weights are balanced, while in B they are all nonnegative.0.2000.0......r1 − r200(c)−0.6 −0.4 −0.2À1 À2 À3 À4 À512345f−OLS55f−OLS44LASSO33LASSO22b9 b10 b11 . . . b1000.611b8r1ABb7(b)0.4Simulation b1 b2 b3 b4 b5 b6(a)1.0Table 1. Data generating weights for the two simulation scenarios0.6i336Fig. 4. Simulation results: Correlations between predicted and true responsesfor simulation scenario A (top) and B (bottom) summarized in Table 1.Models were trained using zero-sum regression (z.-sum), LASSO and OLSwith feature ﬁltering (abbreviated as f-OLS). Plots (a) and (d) show correlations between true and predicted responses for simulated technology 1data, r1 ¼ corðb 1 ; y Þ, for scenario A and B, respectively. Zero-sum regressionycan compete with the standard LASSO and out competes f-OLS on consistentdata from the same technology. Plots (b) and (e) show correlation differencesfor simulated technology 1 and technology 2 data. The signatures weretrained on simulated technology 1 data and applied unchanged to simulatedtechnology 2 data. For the simulation with balanced weights (top) both zerosum and LASSO show good agreement across datasets, while for unbalanced weights (bottom) the LASSO and f-OLS show systematic reducedagreement across datasets. Plots (c) and (f) also show correlation differencesfor simulated technology 1 and technology 2 data, but this time the signatures were retrained on simulated technology 2 data. Retraining did not improve the agreement of predictions across technologiesand retrain all feature weights on a training dataset generated withthe second technology. In this simulation study, we compared retraining to learning a zero-sum signature and leaving it unchanged.Thus, using the simulated data we predicted the response y by (i)using the original signatures and adjusting only the offset b0 and (ii)by retraining all coefficients b0 and b 2 s on (y,z(s)).bbAgain we had test data predictions y 1 and y 2 . Figure 4c and fcompare the performances of zero-sum, the LASSO and f-OLS forsimulations A and B, respectively. All three methods did not profitfrom retraining. Unchanged zero-sum signatures guaranteed equalperformance on targeted data, while a retrained signature was a lottery that yielded very good performance but even more frequently astrongly reduced performance. In summary, the unchanged zerosum signatures appeared to be the safest choice.2.4 Classifications using different proteomics platformsDLBCL are a heterogeneous group of lymphomas comprising distinct molecular subtypes: the activated B-cell like (ABC) and the germinal center B-cell like (GCB) lymphomas (Alizadeh et al., 2000;Rosenwald et al., 2002). Differential diagnosis becomes increasinglyimportant as drugs are under investigation that appear to be effective for only one of the subtypes (Wilson et al., 2015).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i333/3953956by gueston 07 January 2018Cross-platform signaturesWe frame ABC/GCB diagnosis as a regression problem. TheABC and GCB subtypes are themselves heterogeneous groups. Theyare not sharply separated. Instead, there is a continuous spectrum onhow ‘GCB-like’ a DLBCL can be. We have lymphomas that are either clear GCBs or ABCs, but many are in between these prototypiccases. Such lymphomas are labelled ‘unclassified’ (Rosenwald et al.,2002). Masque-Soler et al. (2013) have accounted for the continu´ous transition between GCB and ABC by using a GCB score insteadof just the three classes GCB, unclassified and ABC. The score ispositive for GCB, negative for ABC and near zero for unclassifiedlymphomas. We follow this strategy. In more statistical words, weframe this diagnostic problem as a regression problem and not as aclassification problem. The gold standard for DLBCL subtyping is agene expression signature generated by the Affymetrix GeneChiptechnology applied to fresh frozen material. On FFPE material, thediagnosis can be performed using either the nanoString nCountertranscriptomics platform (Masque-Soler et al., 2013; Scott et al.,´2014) or a shotgun proteomics approach employing a stable-isotopetagged reference proteome (Deeb et al., 2012). To complement thesemethods, we here attempted diagnosis with low cost proteomicsdata from (a) the SWATH and (b) the SRM platform, again usingFFPE material.The ABC/GCB subtype is a property of a lymphoma. Its diagnosis must not depend on the technology used nor should it depend onwhether the tissue was frozen or FFPE. Here, we aimed for a singleset of regression coefficients that can be used with both SWATHand SRM data.2.4.1 Data and resultsThe data comprised 23 DLBCL of which 12 were GCBs, 7 wereABCs and the remaining 4 were unclassified. FFPE biopsy specimensof this cohort were subjected to SWATH proteomics, which yieldedexpression levels for 235 proteins that were supported by at least 6 detected peptides in all samples. We trained zero-sum signatures usingthe SWATH proteomics data to predict the gold standard ABC/GCBscores yi using zero-sum regression. This was done in a leave-one-outcross validation across platforms. Figure 5a shows predicted scores(SWATH proteomics) plotted against gold standard scores(Affymetrix transcriptomics). The scores correlate well (r ¼ 0.93) andGCB samples (blue circles) are clearly separated from ABC samples(red triangles). The dashed lines are classification boundaries forABC, unclassified and GCB, derived from the gold-standard ABC/GCB scores. The heatmap below the plot contrasts the correspondingclassifications showing an excellent agreement between the proteomics predictions and the gold standard classifications.Next we applied this signature unchanged to SRM data generated for FFPE material from the same lymphomas. As expected bytheory, the resulting ABC/GCB scores correlated well with both thegold standard (r ¼ 0.88) and the SWATH based scores (r ¼ 0.95)(Fig. 5b and c). As an alternative strategy to zero-sum signatures wetested re-training a signature on the SRM data, which ended in notably more discrepancies to the gold standard than the zero-sum signature (Fig. 5d).To ensure comparability of results we kept the regularizationparameter k fixed at 0.5 in all results from Figure 5a thru 5d. Figure5e and f illustrate how the results depend on k. The circles correspond to scenario (a), the crosses to (b), the triangles to (c) and thediamonds to (d). Interestingly, the SWATH signature remains remarkably predictive between k ¼ 0.25 and k ¼ 2, also on the SRMdata. Furthermore, SWATH and SRM predictions are highly concordant for all values of k.i337(a)(b)(c)(d)(e)(f)Fig. 5. DLBCL subtyping using different technological platforms and differentbiopsy conservation protocols. Plot (a) shows the ABC/GCB gold-standardscores (Affymetrix gene expression) versus zero-sum scores predicted in aleave-one-out cross validation on SWATH proteomics data. The scores fromboth technologies agree well. The dashed lines are classiﬁcation boundariesfor ABC, unclassiﬁed and GCB, derived from the gold-standard scores. Thecolor bars below the plot contrast the resulting classiﬁcations showing an excellent agreement between the proteomics predictions and the gold standardclassiﬁcations. Similarly, Plot (b), shows gold-standard scores versus scorespredicted on SRM data. Here, the original SWATH signature was applied onthe SRM data directly, where only the offset b0 was retrained. The SWATHsignature carried over well to SRM data. Plot (c) shows SWATH versus SRMpredictions with excellent agreement. Plot (d) shows scores predicted onSRM versus the gold standard scores, where this time the signature wascompletely retrained on SRM data. The retrained signature was inferior to theSWATH trained zero-sum signature in (b). All signatures were trained for thepenalizing parameter k ¼ 0.5. In all four ﬁgures, (a–d), GCBs are indicated inred (triangles), ABCs in blue (circles) and unclassiﬁed cases in green(crosses). The dependence of correlations and mean squared errors of Figure(a) to (d) on k is shown in Figure (e) and (f). Comparison (a) corresponds tothe blue circles, (b) to the red crosses, (c) to the green triangles and (d) to thepurple diamondsIn summary, zero-sum proteomics signatures accurately reproduced the transcriptomics based gold standard ABC/GCB classification. Zero-sum signatures did not break down when switching fromSWATH to SRM based proteomics. Moreover, when applied toDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i333/3953956by gueston 07 January 2018i338M.Altenbuchinger et al.SRM data, unchanged zero-sum SWATH signatures were morefaithful to the classification than a retrained signature that was specifically adapted to SRM data.nCounter and the tissue preservation protocol from freezing toFFPE, and again the transferred signature out-competed retraining.3 Discussion2.5 Classifications using different transcriptomicsplatforms and different tissue preservation protocolsMost proteomics platforms work well for both fresh frozen andFFPE material. The situation is different for transcriptomics, asRNA degradation in FFPE material will affect quantification.We used gene expression data of molecular Burkitt lymphomas(mBL) and DLBCL (non-mBL) that were assessed on fresh frozenmaterial using Affymetrix Gene Chips (Hummel et al., 2006).Furthermore, FFPE data of a separate set of lymphomas was available from (Masque-Soler et al., 2013) using the nanoString´nCounter platform. The differential diagnosis between mBL andnon-mBL can be challenging using standard histopathological assessment (Hummel et al., 2006), but is nevertheless important because the entities are generally treated differently (Dave et al.,2006). As for the ABC/GCB sub-typing we use a continuous mBLscore, with the label intermediate between mBL and non-mBL cases,and frame the diagnostic challenge as a regression problem.We applied zero-sum regression to the training cohort fromHummel et al. (2006), consisting of 23 mBL, 26 intermediate casesand 62 DLBCL, available on Affymetrix Gene Chips. Two sampleswere removed from the training cohort, because they were also partof the data from (Masque-Soler et al., 2013). The predictor variables´were restricted to features that were also covered by the nCounterplatform. The signature was then applied to a validation set of 9mBL, 8 intermediate and 23 DLBCL, for which nCounter data wasavailable (Masque-Soler et al., 2013). All regression weights were´used as they were, except the offset b0, which was readjusted in aleave-one-out cross validation.The results are summarized in Figure 6. The nCounter datareproduced the gold standard scores well in line with (Masque-Soler´et al., 2013). The zero-sum signatures did not break down whenswitching the platform from Affymetrix Gene Chips to nanoString(a)(b)Fig. 6. Differential diagnosis of mBL and DLBCL. Figure (a) shows mBL scorespredicted on FFPE data (nanoString) versus the gold standard scores fromfresh frozen material (Affymetrix). The signature was trained by zero-sum regression on GeneChip data and was directly applied to the FFPE data, whereonly the offset b0 was readjusted in cross validation. The color bars below theplot contrast the resulting classiﬁcations showing an excellent agreement between the FFPE predictions and the gold standard classiﬁcations. In Figure (b)the FFPE nCounter scores were obtained by a leave-one-out cross validation,where the signature was retrained on the nCounter data. Retraining did notyield any advantages over the original zero-sum signature. In both ﬁgures,DLBCLs are indicated in red (triangles), mBLs in blue (circles) and intermediate cases in green (crosses)This is the first systematic analysis of how data dissonance causedby varying experimental protocols propagates in downstream analysis from signature learning to predictions and possible treatmentrecommendations. We have observed that data dissonance can bemostly modeled by independent sample and feature effects. As aconsequence we showed that in zero-sum signatures the sample effects fully vanish while feature effects can be absorbed in a singleparameter that can be easily adjusted. For these signatures data dissonance appears under control. This is in contrast to signatures withpredominantly positive or negative regression weights, where datadissonance strongly compromises predictions.Independent sample and feature effects together accounted for69 to 94% of data dissonance. This still leaves considerable disagreements. Clearly, non-linear effects cannot be compensated norcan systematic difference in noise, like for lowly expressed genes incontinuous microarray versus discrete RNA-Seq and nCounter data.One remedy might be to learn signatures and select features not onlyon a single platform but on joint dissonant data. In such a case lowlyexpressed features and features that display non-linear discrepanciesacross platforms might not be selected simply because other featuresout-compete them.To date, the most frequently used strategy to transfer signaturesacross platforms was a two step procedure: first learn a sparse signature on high-content data. Then transfer the features to a second targeted analytical platform. On the second platform keep the featuresbut discard the weights of this signature. New platform adjustedweights can be learned in a training phase on the data of the newplatform (Masque-Soler et al., 2013; Scott et al., 2014, 2013; Sha´et al., 2015). Surprisingly, in our studies this strategy was inferior tosimply keeping the weights of a zero-sum regression signaturelearned on high-content data. While this observation might not holdup for all data types, we nevertheless believe that zero-sum signatures are a method of choice when working with large but diversedata collections from digital medicine initiatives. With data fromnumerous labs with different underlying experimental protocols itmight not be practical to readjust the parameters of prediction algorithms for all of them.Advancing a culture of data sharing and harmonized data generation is key to treatment decisions that build on all scientific evidenceavailable at any point of time. Maybe our results make data harmonization a little less tedious. We observed that with the right algorithmstreatment recommendations remained stable even if the data were notyet perfectly harmonized, supporting the prospect that data sharingand integration can improve patient care immediately.4 Materials and methods4.1 Data preprocessingThe Affymetrix GeneChip data of Figure 1.1a, Sections 2.2 and 2.5was preprocessed as in Hummel et al. (2006). Corresponding GCB/ABC diagnosis scores, which served as responses throughoutSections 2.4 and 2.5, were provided by the Molecular Mechanismsin Malignant Lymphoma (MMML) consortium.NanoString nCounter data from (Masque-Soler et al., 2013)´were preprocessed by, first, scaling sample-wise to an equal numberof endogenous gene counts (to the total average over the raw countsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i333/3953956by gueston 07 January 2018Cross-platform signaturesof endogenous genes), second, a pseudocount of 1 was added and, finally, the data was transformed by the natural logarithm. Here, thenatural logarithm is necessary to ensure that the data is comparableto the GeneChip data.The protein expression data shown in Figure 1.2a, 1.2b, and usedin Section 2.4, were first normalized sample-wise by their total intensity, then scaled by a factor of 1000 and finally log2 transformed.The data shown in Figure 1.3a and 1.3b were taken from (Zhaoet al., 2014). RMA normalized microarray data for 12 samples weredownloaded from doi:10.1371/journal.pone.0078644.s006. Probesets were annotated with gene names using the annotation filedoi:10.1371/journal.pone.0078644.s004 and redundant genes weresummarized by mean averaging. Corresponding RPKM normalizedRNA-seq data was downloaded from doi:10.1371/journal.pone.0078644.s009. After summing up RPKM-normalized countsof redundant genes, a pseudocount of 1 was added and the data waslog2 transformed. Finally, both datasets were restricted to commongenes, as provided in doi:10.1371/journal.pone.0078644.s005.For all comparisons in Figure 1, the datasets were additionallybrought to the same scale by subtracting the mean over all featuresand samples available on both platforms.4.2 SWATH/SRM signature transfer: computational andmeasurement details4.2.1 Protein profiling using SWATHTM acquisitionTwo 10 mm sections of each FFPE-specimen were extracted according to Ostasiewicz et al. (2010) and the proteins were subjected totryptic digestion using the GASP-protocol (Fischer and Kessler,2015). An aliquot of 1 mL of the digest was used for SWATHmeasurements using a 3 h binary gradient and variable SWATHwindows (Reinders et al., 2016; Simburger et al., 2016; Zhang¨et al., 2015). Targeted data extraction was conducted with theSWATH Acquisition MicroApp 2.0 within the PeakView 2.2 software (Sciex, Darmstadt, Germany).4.2.2 Targeted protein profiling using SRMThe two most intensive, proteotypic peptide signals for each of the respective proteins were used for scheduled SRM measurements using10 min retention time windows on a 73 min binary gradient with anaccumulation time of 120 ms per precursor (Limm et al., 2016).Quantification of the signals was done with the Skyline software (version 3.6) (MacLean et al., 2010) using at least 4 transitions.4.2.3 Model training and cross validation across platformsIn each cross-validation step, we performed the following steps onthe training data. First, we fix k to a specific value. Here, we focusedon the interval k ¼ 0.25 to 4. This restriction was necessary, becauseeach feature selected on SWATH needed to be remeasured on SRM.For this reason, smaller k values, i.e. less sparse signatures could notbe studied using SRM, due to drastically increasing experimental effort. Next, we trained a signature consisting of features j 2 C withcoefficients unequal zero. Of these selected proteins, we removedthose that acquired coefficients jbj j < 0:5, leaving a protein set C0(this cutoff was included to enforce additional sparseness of models). Then, a zero-sum model was retrained on the proteins C0 , yielding the final signature. The selected proteins were measured in atargeted SRM measurement (this measurement was done once andcovered all proteins selected in all cross-validations). The offset b0was adjusted on the SRM training data, and finally we predicted ascore for the left-out test sample, measured on SRM.i339AcknowledgementThis work was supported by the e:Med initiative of the German FederalMinistry of Education and Research - BMBF grant 031A428A and by theGerman Research Foundation - DFG (grant SP 938/3-1). We thank Dr.Helena U. Zacharias for useful discussions and careful proofreading of themanuscript.Conﬂict of Interest: none declared.ReferencesAlizadeh,A.A. et al. (2000) Distinct types of diffuse large B-cell lymphomaidentiﬁed by gene expression proﬁling. Nature, 403, 503–511.Altenbuchinger,M. et al. (2017) Reference point insensitive molecular dataanalysis. Bioinformatics, 33, 219–226.Dave,S.S. et al. (2006) Molecular diagnosis of Burkitt’s lymphoma. N. Engl. J.Med., 354, 2431–2442.Deeb,S.J. et al. (2012) Super-SILAC allows classiﬁcation of diffuse large B-celllymphoma subtypes by their protein expression proﬁles. Mol. Cell.Proteomics, 11, 77–89.Faktor,J. et al. (2016) Comparison of targeted proteomics approaches for detecting and quantifying proteins derived from human cancer tissues.Proteomics, 17, S.1600323.Firth,H.V. et al. (2009) DECIPHER: database of chromosomal imbalance andphenotype in humans using ensembl resources. Am. J. Hum. Genet., 84, 524–533.Fischer,R. and Kessler,B.M. (2015) Gel-aided sample preparation (GASP) – asimpliﬁed method for gel-assisted proteomic sample generation from proteinextracts and intact cells. Proteomics, 15, 1224–1229.Friedman,J. et al. (2010) Regularization paths for generalized linear modelsvia coordinate descent. J. Stat. Softw., 33, 1.Gillet,L.C. et al. (2012) Targeted data extraction of the MS/MS spectra generated by data-independent acquisition: a new concept for consistent and accurate proteome analysis. Mol. Cell. Proteomics, 11, 016717–01O111.Grossman,R.L. et al. (2016) Toward a shared vision for cancer genomic data.N. Engl. J. Med., 375, 1109–1112.Hummel,M. et al. (2006) A biologic deﬁnition of Burkitt’s lymphoma fromtranscriptional and genomic proﬁling. N. Engl. J. Med., 354, 2419–2430.Klapper,W. et al. (2008) Molecular proﬁling of pediatric mature B-cell lymphomatreated in population-based prospective clinical trials. Blood, 112, 1374–1381.Klapper,W. et al. (2012) Patient age at diagnosis is associated with the molecularcharacteristics of diffuse large B-cell lymphoma. Blood, 119, 1882–1887.Limm,K. et al. (2016) Characterization of the methylthioadenosine phosphorylase polymorphism rs7023954-incidence and effects on enzymatic functionin malignant melanoma. PloS One, 11, e0160348.Lin,W. et al. (2014) Variable selection in regression with compositional covariates. Biometrika, 101, 785–797.MacLean,B. et al. (2010) Skyline: an open source document editor for creatingand analyzing targeted proteomics experiments. Bioinformatics, 26, 966–968.Masque-Soler,N. et al. (2013) Molecular classiﬁcation of mature aggressive´B-cell lymphoma using digital multiplexed gene expression on formalinﬁxed parafﬁn-embedded biopsy specimens. Blood, 122, 1985–1986.NCI Center for Cancer Genomics (CCG) (2016) NCI’s Genomic DataCommons (GDC). https://gdc.cancer.gov.Ostasiewicz,P. et al. (2010) Proteome, phosphoproteome, and N-glycoproteome are quantitatively preserved in formalin-ﬁxed parafﬁn-embedded tissue and analyzable by high-resolution mass spectrometry. J. Proteome Res.,9, 3688–3700.Quackenbush,J. (2014) Learning to share. Sci. Am., 311, S22.Reinders,Y. et al. (2016) Testing suitability of cell cultures for SILACexperiments using SWATH-mass spectrometry. Proteomics Syst. Biol.Methods Protoc., 1394, 101–108.Rosenwald,A. et al. (2002) The use of molecular proﬁling to predict survivalafter chemotherapy for diffuse large-B-cell lymphoma. N. Engl. J. Med.,346, 1937–1947.Salaverria,I. et al. (2011) Translocations activating IRF4 identify a subtype ofgerminal center-derived B-cell lymphoma affecting predominantly childrenand young adults. Blood, 118, 139–147.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i333/3953956by gueston 07 January 2018i340Scott,D.W. et al. (2013) Gene expression–based model using formalin-ﬁxedparafﬁn-embedded biopsies predicts overall survival in advanced-stage classical hodgkin lymphoma. J. Clin. Oncol., 31, 692–700.Scott,D.W. et al. (2014) Determining cell-of-origin subtypes of diffuse largeB-cell lymphoma using gene expression in formalin-ﬁxed parafﬁn-embeddedtissue. Blood, 123, 1214–1217.Sha,C. et al. (2015) Transferring genomics to the clinic: distinguishing Burkittand diffuse large B cell lymphomas. Genome Med., 7, 1.Simburger,J.M. et al. (2016) Optimizing the SWATH-MS-workﬂow for label¨free proteomics. J. Proteomics, 145, 137–140.Tibshirani,R. (1996) Regression shrinkage and selection via the lasso. J. R.Stat. Soc. Ser. B (Methodological), 58, 267–288.M.Altenbuchinger et al.Tukey,J.W. (1977) Exploratory Data Analysis. Addison-Wesley, Reading,MA.Wilson,W.H. et al. (2015) Targeting B cell receptor signaling with ibrutinib indiffuse large B cell lymphoma. Nat. Med., 21, 922–926.Zhang,Y. et al. (2015) The use of variable Q1 isolation windows improvesselectivity in LC–SWATH–MS acquisition. J. Proteome Res., 14,4359–4371.Zhao,S. et al. (2014) Comparison of RNA-Seq and microarray in transcriptome proﬁling of activated T cells. PloS One, 9, e78644.Zou,H. and Hastie,T. (2005) Regularization and variable selection via theelastic net. J. R. Stat. Soc. Ser. B (Stat. Methodol.), 67, 301–320.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i333/3953956by gueston 07 January 2018
5028881974002	PMID28881974	5028881974	https://watermark.silverchair.com/btx239.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881974.main.pdf	Bioinformatics, 33, 2017, i23–i29doi: 10.1093/bioinformatics/btx239ISMB/ECCB 2017Large-scale structure prediction by improvedcontact predictions and model qualityassessmentMirco Michel1, David Menendez Hurtado1, Karolis Uziela1 and´1,Arne Elofsson *1Science for Life Laboratory and Department of Biochemistry and Biophysics, Stockholm University, Box 1031,17121 Solna, Sweden*To whom correspondence should be addressedAbstractMotivation: Accurate contact predictions can be used for predicting the structure of proteins. Untilrecently these methods were limited to very big protein families, decreasing their utility. However,recent progress by combining direct coupling analysis with machine learning methods has made itpossible to predict accurate contact maps for smaller families. To what extent these predictionscan be used to produce accurate models of the families is not known.Results: We present the PconsFold2 pipeline that uses contact predictions from PconsC3, theCONFOLD folding algorithm and model quality estimations to predict the structure of a protein. Weshow that the model quality estimation signiﬁcantly increases the number of models that reliablycan be identiﬁed. Finally, we apply PconsFold2 to 6379 Pfam families of unknown structure andﬁnd that PconsFold2 can, with an estimated 90% speciﬁcity, predict the structure of up to 558 Pfamfamilies of unknown structure. Out of these, 415 have not been reported before.Availability and Implementation: Datasets as well as models of all the 558 Pfam families are available at http://c3.pcons.net/. All programs used here are freely available.Contact: arne@bioinfo.se1 IntroductionA few years ago maximum entropy methods revolutionized the accuracy of contact predictions in proteins (Weigt et al., 2009; Burgerand van Nimwegen, 2010; Aurell, 2016). This enabled the prediction of accurate protein models using no information from homologous protein structures (Marks et al., 2011; Morcos et al., 2011). Ithas been shown that accurate protein structures can be obtained forsoluble proteins (Marks et al., 2011), membrane proteins (Nugentand Jones, 2012; Hopf et al., 2012; Hayat et al., 2015) and even disordered proteins (Toth-Petroczy et al., 2016). These methods havealso been used to predict interactions between proteins (Weigt et al.,2009; Ovchinnikov et al., 2014; Hopf et al., 2014).Until recently such methods have been limited to very large protein families (Kamisetty et al., 2013; Skwark et al., 2014). However,by the inclusion of additional information and improved machinelearning methods it is now often possible to obtain accurate contactmaps for families as small as a few hundred effective sequences(Michel et al., 2017; Jones et al., 2015; Wang et al., 2017).Pfam contains today approximately 16 000 protein families thatvary in size between a few tens to hundreds of thousands effectivesequences. About half (46%) of these protein families contain norepresentative structure, i.e. there is more than 7500 protein familieswithout a structure. The families with structure are on averagelarger than the ones without, median size 680 versus 134 effectivesequences, i.e. most of the families without a structure are too smallfor maximum entropy contact prediction but might be within reachfor methods that combine DCA and advanced machine learning.Now, we ask the question how many of these roughly 7500 protein families without a structure can be modeled reliably by usingstate of the art contact prediction methods. To the best of our knowledge the largest effort to model protein families was performed bythe Baker group who modeled structures for 614 families by including a very large set of sequences from meta-genomics (Ovchinnikovet al., 2017). However, their approach for contact prediction wasbased on a maximum entropy method (Antala et al., 2015) and notthe newer methods using machine learning.The PconsFold2 pipeline is described in Figure 1. Given an inputsequence PconsFold2 generates four alignments. These alignmentsare then used by PconsC3 (Michel et al., 2017) to predict four different contact maps. The 2.5L (L ¼ length of sequence) top rankedCV The Author 2017. Published by Oxford University Press.i23This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i23/3953955by gueston 07 January 2018i24M.Michel et al.Fig. 1. PconsFold2 workﬂow. Given an input sequence four alignments are created using HHblits and Jackhmmer at two different E-value thresholds of 1 and10–4. Based on these alignments PconsC3 generates four contact maps. The 2.5L (L ¼ length of sequence) top ranked contacts are then used by CONFOLD to generate 50 models for each alignment, resulting in 200 models for each query sequence. These models are ﬁnally ranked by a model quality assessment programcontacts are then used to fold the protein. In contrast to PconsFold(Michel et al., 2014), PconsFold2 uses CONFOLD (Adhikari et al.,2015), i.e. the NMR protocol of CNS (Brunger, 2007) and notROSETTA (Leaver-Fay et al., 2011) to generate 50 models for eachcontact map. This makes the pipeline much faster but possiblyslightly less accurate.The final step in the pipeline is the model quality assessment. Inaddition to existing model quality assessment methods, we introduce PcombC. PcombC is a linear combination of three separate assessment scores: Pcons (Lundstrom et al., 2001), ProQ3D (Uziela¨et al., 2017) and the agreement between predicted and observed contacts in the model.Finally, we apply PconsFold2 to 6379 Pfam families without aknown structure. Using the cutoffs for a false positive rate of 10%,we do identify 558 protein families where the first ranked model isestimated to be correct by one of the three model quality estimators.Out of these 558 Pfam families 415 are not present in the earlierstudies by the Baker group (Ovchinnikov et al., 2017, 2015).2 Methods2.1 DatasetsThere are 16 295 protein domain families in Pfam 29.0. Out of these7733 domains have a known structure with a HHsearch (Soding,¨2005) hit in PDB with an E-value of <10– 3 that covers at least 75%of it’s representative sequence. For each Pfam family, we assign a representative sequence. We refer to the length and properties of a Pfamfamily by the representative sequence. The representative sequence ofa Pfam domain with known structure is set to be the protein sequenceranked first by HHsearch against the PDB database bundled withHHsuite (Meier and Soding, 2015) (date: September 7, 2016)¨The test dataset was generated from 626 Pfam domains thatwere randomly selected from 6925 domains with known structurethat are longer than 50 residues.From the remaining Pfam domains, we excluded all Pfam domains that can be found in the pdbmap file from Pfam release 29.0and those shorter than 50 residues. This results in a set of 7537Pfam domains with unknown structure. For each of these sequence,we define the highest ranked sequence in the HHblits (Remmertet al., 2012) alignment against uniref20 (date: February 26, 2016) tobe the reference sequence of the family.2.2 AlignmentsThe input to direct coupling analysis (DCA)-based contact prediction methods is a multiple sequence alignment. These alignmentswere generated using both HHblits and Jackhmmer (Eddy, 2011),each at E-value thresholds of 1 and 10–4. HHblits was run againstthe uniprot20 database from HHsuite (date: February 26, 2016).The parameter -all has been used and -maxfilt and -realign_maxwere set to 999999 as in (Michel et al., 2017). Jackhmmer searcheswere performed against Uniprot90 (Magrane and Consortium,2011) (April 13, 2016) and were run for five iterations with both-E and -incE set to the respective E-value cutoffs. The searches werestarted from the Pfam representative sequence, i.e. the Pfam alignments were ignored. Alignment (family) size is measured in effectivesequences (Meff) as defined in (Ekeberg et al., 2013).2.3 Contact predictionPconsC3 is used to predict contacts between pairs of amino acids inthe Pfam reference sequences. To overcome the limit of DCA methods requiring large alignments, PconsC3 combines the results ofsuch methods with contacts predicted by a machine-learning basedmethod (Michel et al., 2017). It then uses a similar pattern recognition approach as PconsC2 (Skwark et al., 2014) to iteratively increase the quality of the predicted contact map. PconsC3 was run asdescribed earlier (Michel et al., 2017). However, PconsFold2 usesall four alignments as inputs predicting one contact map for eachalignment. Contact map quality is measured in positive predictivevalue (PPV) over the same number of top-ranked contacts that wereused during folding [2.5 Á sequence length (L)]. It should be notedthat most earlier papers report the PPV for L or even fewer contacts.The average contact score for a contact map refers to the meanPconsC3 score of these 2.5 L contacts.2.4 Model generationContacts predicted by PconsC3 are then applied as distance restraints between the Cb-atoms (Ca in the case of glycine) during protein structure prediction. We use CONFOLD (Adhikari et al., 2015)for this task. Secondary structure predictions from PSIPRED (Jones,1999) are also used as inputs. When folding a protein usingCONFOLD a fixed number of contacts are used. Here, contacts aresorted by their PconsC3 score and a threshold is set on the numberDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i23/3953955by gueston 07 January 2018Large-scale structure prediction and model quality assessment(a)0.8All fouri25(b)r = 0.3r = 0.410.8HHblits E00.6Jackhmmer E−40.6Jackhmmer E0TM−scorePPV to the native structureHHblits E−40.40.40.20.20.0101102103104105101effective sequences102103104105effective sequencesFig. 2. Family size in effective sequences of the benchmark dataset against (a) PconsFold2 model quality in TM-score of the top-ranked models and (b) againstcontact map quality in PPV. Lines show moving averages for using the HHblits alignment at E-value 10–4 (cyan), HHblits at E-value 1 (blue), Jackhmmer at E-value10–4 (orange), Jackhmmer at E-value 1 (red) and all four alignments combined (black). Circles show individual results when using all four alignments combinedof top-ranked contacts to use. This threshold is based on the lengthof the sequence as an input to CONFOLD, which folds the proteinusing CNS (Brunger, 2007). It is to be noted that, we only run thefirst stage of CONFOLD and omit the refining second stage in orderto keep runtime low. For each alignment, we generate 50 models resulting in a pool of 200 models per Pfam family.Table 1. Performance for different alignment methodsPPV to nativeAllHHblits 10–4HHblits 100Jackhmmer 10–4Jackhmmer 100TM-score (top)TM-score (max)0.350.340.340.350.330.420.400.400.410.400.490.450.450.460.442.5 Model rankingThe CONFOLD pipeline generates several models that by default areranked by the CNS contact energy (NOE). This is the sum of all violations of all contact restraints used to generate a model. In order tomake this score comparable between targets, we normalize it by thelength of the input protein sequence and refer to it as CNS-contact.In addition to using the CNS contact energy, we also usedProQ3D (Uziela et al., 2017) and Pcons (Lundstrom et al., 2001).¨Further, we developed PcombC a linear combination between thescores of ProQ3D, Pcons and PPV, similar to what we used inCASP4 (Wallner et al., 2003) and CASP5 (Wallner and Elofsson,2005a). Coefficients have been determined using a grid-search on a10 Â 10 Â 10 grid with values ranging from 0 to 1 and a step size of0.1, optimizing the area under the ROC-curve for determiningwhether a model is correct or not (TM-score threshold of 0.5). Inorder for the score to remain within the same scale as the inputscores, the coefficients have been normalized to:SPcombC ¼0:30:61:0Á SPcons þÁ SProQ3D þÁ PPV1:91:91:92.6 EvaluationModel quality is measured in template modeling score (TM-score) scores(Zhang and Skolnick, 2004). For the ROC-analysis, we set a TM-scorethreshold of 0.5 to distinguish between correct and incorrect models.2.7 RuntimeThe running time of the folding step was measured on a single coreof an Intel Xeon E5-2690 v4 processor. For the test dataset, it takesaround 30 s on average to generate one model with a minimum of 4sper model for the shortest family (50 residues) and 245s for the longest (524 residues).3 Results3.1 Utilization of predicted contactsFirst, we set out to find the best way to generate models using contacts predicted from PconsC3 (Michel et al., 2017) and theCONFOLD (Adhikari et al., 2015) folding algorithm. Preliminarydata indicated that using a threshold for the number of contacts utilized during folding of 2.5 times the length of the sequence is closeto optimal. Using this threshold, we then investigated the effect ofthe number of generated models on the quality of the best and topranked model. Here, the CNS-contact score from CONFOLD isused to rank the models. Increasing the number of generated modelsfrom the default 20 to 50 does not increase the quality of topranked models (average TM-score 0.40). However, the average TMscore of the best among all generated models increases from 0.43 to0.45. Increasing the number of models further (up to 200) only generates a marginal improvement of the best TM-score to 0.46. Wethus decided that generating 50 models for a given contact map is agood tradeoff between model qualities and running time.It has previously been observed that the quality of predicted contacts depends on the underlying alignment (Skwark et al., 2013). Wetherefore tried to identify the optimal alignment method and E-valuecutoff. In addition, we investigated whether model quality can beimproved by using a set of alignments with varying methods and Evalue thresholds instead of a single fixed alignment. In Figure 2, itcan be seen that the performance is similar for all four alignmentmethods used both regarding the agreement with the contact mapand the average TM-score of the generated models, see Table 1.However, the quality of both the top-ranked (0.42) and best possiblemodel (0.49) is improved slightly when using a combination of allalignments. Therefore, we decided to use this for the pipeline.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i23/3953955by gueston 07 January 2018i26M.Michel et al.3.2 Overall performanceTable 2. Fraction of correct models and contact maps>0.5 PPV (%)>0.5 TM-score (%)02235Meff33451<100100–1000>1000(a)0.8(b)r = 0.63All fourFigure 2 shows the performance of the pipeline of the top-rankedmodel against the family size, measured in effective sequences of theunderlying alignment. Here, models were ranked by their CNScontact score as described above. Generally, both contact predictionaccuracy and model accuracy clearly depends on family size withr = 0.450.8HHblits E−4HHblits E0Jackhmmer E−40.60.6TM−scoreTM−scoreJackhmmer E00.40.40.20.20.00.20.40.60.20.40.60.8average score of top 2.5L contactsPPV to the native structureFig. 3. TM-score of the models using CONFOLD and CNS-contact ranking against (a) PPV of the underlying 2.5L contacts to the native structure and (b) the average score of these contacts(a)(b)r = 0.790.8r = 0.670.8<TM> = 0.43<TM> = 0.42TM−scoreTM−score0.60.40.20.60.40.20.20.30.40.50.60.70.20.40.6(c)(d)0.8r = 0.73r = − 0.560.8<TM> = 0.42<TM> = 0.420.60.6TM−scoreTM−score0.8ProQ3D scorePcombC score0.40.20.40.20.20.40.6Pcons score5102050100200CNS−contact scoreFig. 4. Model quality in TM-score on the benchmark dataset of the top-ranked models against (a) PcombC score (scoring function of the PconsFold2 pipeline), (b)ProQ3D score, (c) Pcons score and (d) and (d) CNS contact energy normalized by sequence length. Pearson correlations r and average TM-scores (<TM >) are shown,black lines represent moving averages with a window of 60 proteins. For CNS-contact, the Pearson correlation has been calculated on log10(CNS-contact)Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i23/3953955by gueston 07 January 2018Large-scale structure prediction and model quality assessmenti27correlations of 0.30 for PPV and of 0.41 for TM-score. However,what might be most notable is that there is a large variation betweendifferent proteins. Some protein families with as few as 100 effectivesequences both have good contact predictions and good final models, while some proteins with 105 are no better than completely random models. This indicates that we need to use other evaluationprocedures to identify the correct models.Table 2 further reveals that for families smaller than 100 effective sequences, structure prediction has a success rate (percentage ofmodels that are above 0.5 TM-score) of only 3%. As soon as thefamily becomes larger than 100 effective sequences the number ofcorrect models increases rapidly. The success rate is 34% for families between 100 and 1000 effective sequences and 52% for largefamilies with more than 1000 effective members.model using deep learning (Uziela et al., 2017). Both of these methods have been shown to perform among the best methods in CASP.It is also possible to evaluate the quality of a model by comparing how well it satisfies the predicted contacts. This can be done byeither just counting the number of fulfilled contacts (PPV) or usingthe CNS-NOE energy. Both these methods perform very similar interms of this evaluation (data not shown). We thus only reportCNS-contact in all further analysis.We have also developed a combined model quality estimator,PcombC, using all three methods. PcombC is a linear combination between the scores of these three methods, similar to what we used inCASP4 (Wallner et al., 2003) and CASP5 (Wallner and Elofsson,2005a). PcombC has been optimized for discriminating between accurate and inaccurate models. This has the advantage of being able to interpret the predicted score in terms of absolute model quality, enablingstatements about the confidence of a predicted model being correct.Figure 4 shows how the scores of different QA tools predict TMscore. As before, for each family in the test set we ranked all models byeach QA score and selected the top ranked model. Pearson correlation (r)is highest for PcombC, followed by Pcons and ProQ3D, while the CNScontact energy correlates slightly worse. However, the average TM-scoreof the top-ranked model is almost identical for all four methods.3.3 All contact maps do not generate good modelsNext, we compared the quality of the models (TM-score) with theaccuracy of the predicted contacts (PPV), Figure 3a. Clearly, there isa strong Pearson correlation (r ¼ 0.63). There is also a weaker correlation (r ¼ 0.45) between the strength of the PconsC3 contact prediction and the quality of the model, Figure 3f.From Figure 3, it is clear that there exist a number of bad modelsfor large families with good contacts and also good models for smallfamilies with weakly predicted contacts. This indicates that it is notsufficient to only use the contact map or number of sequences in thealignment for identification of the successfully predicted proteinmodels. Instead it might be better to evaluate the models directly.3.4 Model quality assessmentTo estimate the quality of a protein model, model quality assessmentmethods can be used. Model quality estimators can be divided inconsensus and single model methods. Here, we have used the Pconsconsensus method to predict a quality score for each model based ona comparison of all models against each other (consensus method;Wallner and Elofsson, 2005b). For single model quality estimation,we have used ProQ3D a method that assesses the quality of a single3.5 ROC-curveIn order to estimate model quality when there is no known structureavailable it needs to be predicted as accurately as possible. The goalis not only to select the best models from a set of predictions butalso to predict how much these models can be trusted. Figure 5shows the false positive rate (FPR) for different quality assessment(QA) tools when classifying predictions into correct (TM-score! 0.5) or incorrect (TM-score < 0.5) models.Although the overall number of correct top-ranked models doesnot change much (number of true positives at FPR ¼ 1.0), there aredifferences in the ability of the different scores to classify the models.Table 3. ROC analysis when classifying whether a model is correct(TM-score ! 0.5) or not# Models at FPR300MethodPcombCProQ3DPconsCNS−contactPconsC3 scoreMeff250TP200PcombCPconsProQ3DCNS-contactPconsC3 scoreMeffbest possible1500.010.11.087 (29%)37 (12%)42 (14%)46 (15%)3 (1%)1 (0.3%)305152 (50%)136 (45%)114 (37%)106 (35%)64 (21%)40 (13%)305221 (72%)227 (74%)210 (69%)214 (70%)214 (70%)214 (70%)30510050Table 4. Number of Pfam families with unknown structure that canbe modeled at 1% and 10% FPR of which the overlap with theBaker studies are given by number in parenthesis00.010.101 − specificity (=FPR)1.00Fig. 5. ROC-like plot for different ways of evaluating and ranking the modelsin the benchmark dataset. While the x-axis shows true positive rate on alogarithmic scale, the y-axis shows the number of proteins with TMscore !0.5. The horizontal line indicates the best possible outcome, i.e. thenumber of families with TM-score !0.5 when ranking the models by TMscore0.01ProQ3DPcombCPconsCNS-contactUnionAllDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i23/3953955by gueston 07 January 20180.136 (10)42 (21)18 (9)62 (13)114 (35)6379 (558)225 (75)179 (74)218 (91)232 (38)558 (143)6379 (558)i28M.Michel et al.Table 5. Properties of Pfam families that can be modeled accurately at FPR 0.1<PconsC3 score>HelixSheetCoil<Meff ><Length >0.41**0.45**0.320.33*0.340.360.18*0.21**0.150.50.46**0.50427*1075**300104**126**187UnionPfamPDBNoPDBTransmembrane<TM-score >0.11**0.04**0.250.560.53Note: Statistical signiﬁcant differences from a students t-test at P-values 0.01 and 10–5 are marked with * and **, respectively, for all columns except the last.It can be seen that all methods evaluating the model quality are significantly better than just using the number of effective sequences orthe PconsC3 scores, see Figure 5.Table 3 also shows that the combined method, PcombC, clearlyis better at identifying methods than any of the individual methods.At a FPR of 10% PcombC identifies 152 models compared with106–136 for the individual methods. At the more strict 1% FPRPcombC identifies 29% of the models compared with 12–15% forCNS-contact and ProQ3D and Pcons.The results shown here indicate that a combination of QA methods along with contact map agreement provides a significant improvement in detecting correct models over the best single methods.It can be used to reliably predict model accuracy while at the sametime being more sensitive than previous methods.4 DiscussionWe then applied PconsFold2 as well as all quality assessment toolsto the set of Pfam domains without known structure. This resultedin predicted structures for 6379 Pfam families (85% of all Pfam domains with unknown structure) 4. Based on the results from theROC analysis on the test dataset, we set the score cutoffs at FPR0.01 and 0.1. We then use these cutoffs to estimate how many Pfamfamilies of unknown structure can be predicted accurately (TMscore !0.5) at a given FPR. The union is defined as the nonoverlapping number of families for which any quality assessmentmethod predicts a model to be accurate.At 0.01 FPR, or 99% specificity, models for a total of 114 Pfamfamilies are predicted to be accurate, Table 4. This number increasesalmost four fold to 558 families, when allowing for a FPR of 0.1.More than 74% of these families do not overlap with other largescale structure prediction studies (Ovchinnikov et al., 2017, 2015).This indicates that our approach of using improved contact prediction combined with model quality assessment is complementary tousing larger sequence databases and a more extensive foldingprocedure.In Table 5, it can be seen that the average length of the successfully predicted models is shorter than for the average length of allmodels. The PconsC3 scores are also stronger, as expected.However the number of effective sequences and other properties aresurprisingly not that different.Of the 558 models predicted at a FPR of 10% 143 have alsobeen predicted by the Baker group. The comparison of these modelsis not absolutely trivial as different protein sequences have beenused. However, in 74% of the cases our models have a TM-alignscore of 0.50 or higher indicating that they represent the same fold.In some cases, where the models differs it appears as our models aremirror images of the Baker models.Figure 6 shows a side-by-side comparison of PconsFold2 models(a) and (c) with those predicted by the Baker group (b) and (b) fortwo exemplary Pfam domains. PF02660 in Figure 6a and b is aGlycerol-3-phosphate acyltransferase transmembrane proteinFig. 6. Comparison for two example Pfam families (a) PconsFold2 model forPF02660, (b) model by the Baker group for the same family, (c) PconsFold2model for PF03808 and (d) model by the Baker group for the same familydomain family. The TM-score between the two models is 0.79. Itcan be seen that PconsFold2 misses helical regions in the termini(red and blue ends of the model). The models for the Glycosyl transferase WecB/TagA/CpsF family of PF03808 in Figure 6c and d havean agreement of 0.75 TM-score. Again in the PconsFold2 model thesecondary structural elements are not as defined as in the Rosettamodel by the Baker group. It is to be noted though that thePconsFold2 models are taken directly from the output ofCONFOLD stage 1 and have not been further refined.5 ConclusionIn this study, we first present a novel protein folding pipeline,PconsFold2 that combines contact prediction, structure generationand model quality estimations. We show that the model quality estimation is an important step in the pipeline as the number of modelsthat reliably can be predicted increases significantly when it isincluded. We also use this pipeline to predict the structure of 6379Pfam families of unknown structure. At a FPR of 10% we find 558Pfam families. The structure of 74% of these has not been reported before in any prediction study. Further, these models are obtained without the use of meta-genomic data, and the number of accurate modelsmight therefore increase significantly if such sequences were included.AcknowledgementWe thank Bjorn Wallner, Christian Blau and the entire CASP community for¨valuable discussions.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i23/3953955by gueston 07 January 2018Large-scale structure prediction and model quality assessmentFundingThis work has been supported by the grants from the Swedish ResearchCouncil (VR-NT 2012-5046) and Swedish e-Science Research Center.Computational resources were provided by the Swedish NationalInfrastructure for Computing (SNIC) at NSC, HPC2N and Uppmax.ReferencesAdhikari,B. et al. (2015) Confold: residue–residue contact-guided ab initioprotein folding. Proteins, 83, 1436–1449.Antala,S. et al. (2015) Computation and functional studies provide a modelfor the structure of the zinc transporter hZIP4. J. Biol. Chem., 290,17796–17805.Aurell,E. (2016) The maximum entropy fallacy redux?. PLoS Comput. Biol.,12, e1004777.Brunger,A. (2007) Version 1.2 of the crystallography and NMR system. Nat.Protoc., 2, 2728–2733.Burger,L., and van Nimwegen,E. (2010) Disentangling direct from indirect coevolution of residues in protein alignments. PLoS Comput. Biol., 6,e1000633.Eddy,S.R. (2011) Accelerated proﬁle HMM searches. PLoS Comput. Biol., 7,e1002195.Ekeberg,M. et al. (2013) Improved contact prediction in proteins: using pseudolikelihoods to infer Potts models. Phys. Rev. E, 87, 012707.Hayat,S. et al. (2015) All-atom 3D structure prediction of transmembrane b-barrel proteins from sequences. Proc. Natl. Acad. Sci. USA, 112, 5413–5418.Hopf,T. et al. (2014) Sequence co-evolution gives 3d contacts and structuresof protein complexes. Elife, 3, 03430.Hopf,T.A. et al. (2012) Three-dimensional structures of membrane proteinsfrom genomic sequencing. Cell, 149, 1607–1621.Jones,D.T. (1999) Protein secondary structure prediction based on positionspeciﬁc scoring matrices1. J. Mol. Biol., 292, 195–202.Jones,D.T. et al. (2015) MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins.Bioinformatics, 31(7), 999–1006.Kamisetty,H. et al. (2013) Assessing the utility of coevolution-based residue–residue contact predictions in a sequence- and structure-rich era. Proc. Natl.Acad. Sci. USA, 110, 15674–15679.Leaver-Fay,A. et al. (2011) ROSETTA3: an object-oriented software suite forthe simulation and design of macromolecules. Methods Enzymol., 487,545–574.Lundstrom,J. et al. (2001) Pcons: a neural-network-based consensus predictor¨that improves fold recognition. Protein Sci., 10, 2354–2362.Magrane,M., and Consortium,U. (2011) UniProt Knowledgebase: a hub ofintegrated protein data. Database, 2011, bar009. doi: 10.1093/database/bar009.Marks,D.S. et al. (2011) Protein 3D structure computed from evolutionary sequence variation. PloS One, 6, e28766.i29Meier,A., and Soding,J. (2015) Automatic prediction of protein 3d structures¨by probabilistic multi-template homology modeling. PLoS Comput. Biol.,11, e1004343.Michel,M. et al. (2014) Pconsfold: improved contact predictions improve protein models. Bioinformatics, 30, i482–i488.Michel,M. et al. (2017). Large-scale structure prediction by improved contactpredictions and model quality assessment, https://doi.org/10.1101/128231.http://biorxiv.org/content/early/2017/04/18/128231.Morcos,F. et al. (2011) Direct-coupling analysis of residue coevolution captures native contacts across many protein families. Proc. Natl. Acad. Sci.USA, 108, 1293–1301.Nugent,T., and Jones,D.T. (2012) Accurate de novo structure predictionof large transmembrane protein domains using fragment-assembly andcorrelated mutation analysis. Proc. Natl. Acad. Sci. USA, 109,1540–1547.Ovchinnikov,S. et al. (2014) Robust and accurate prediction of residueresidue interactions across protein interfaces using evolutionary information. Elife, 3, e02030.Ovchinnikov,S. et al. (2015) Large-scale determination of previously unsolvedprotein structures using evolutionary information. Elife, 4, e09248.Ovchinnikov,S. et al. (2017) Protein structure determination using metagenome sequence data. Science, 355, 294–298.Remmert,M. et al. (2012) HHblits: lightning-fast iterative protein sequencesearching by HMM-HMM alignment. Nat. Methods, 9, 173–175.Skwark,M.J. et al. (2013) PconsC: combination of direct information methodsand alignments improves contact prediction. Bioinformatics, 29,1815–1816.Skwark,M.J. et al. (2014) Improved contact predictions using the recognitionof protein like contact patterns. PLoS Comput. Biol., 10,Soding,J. (2005) Protein homology detection by HMM–HMM comparison.¨Bioinformatics, 21, 951–960.Toth-Petroczy,A. et al. (2016) Structured states of disordered proteins fromgenomic sequences. Cell, 167, 158–170.Uziela,K. et al. (2017) ProQ3D: improved model quality assessments usingdeep learning. Bioinformatics, doi: 10.1093/bioinformatics/btw819.Wallner,B., and Elofsson,A. (2005a) Pcons5: combining consensus, structuralevaluation and fold recognition scores. Bioinformatics, 21, 4248–4254.Wallner,B., and Elofsson,A. (2005b) Pcons5: combining consensus, structuralevaluation and fold recognition scores. Bioinformatics (Oxford, England),21, 4248–4254.Wallner,B. et al. (2003) Automatic consensus-based fold recognition usingpcons, proq, and pmodeller. Proteins, 53 Suppl 6, 534–541.Wang,S. et al. (2017) Accurate de novo prediction of protein contact map byultra-deep learning model. PLOS Comput. Biol., 13, e1005324.Weigt,M. et al. (2009) Identiﬁcation of direct residue contacts in proteinprotein interaction by message passing. Proc. Natl. Acad. Sci. USA, 106,67–72.Zhang,Y., and Skolnick,J. (2004) Scoring function for automated assessmentof protein structure template quality. Proteins, 57, 702–710.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i23/3953955by gueston 07 January 2018
5028881973002	PMID28881973	5028881973	https://watermark.silverchair.com/btx238.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881973.main.pdf	BIOSSES: a semantic sentence similarityestimation system for the biomedical domainGizem So gancıo glu1,2,*, Hakime €Oztürk1 and Arzucan €Ozgür1,*1Department of Computer Engineering, Bogazici University, Istanbul 34342, Turkey and 2R&D and Special ProjectsDepartment, Yapı Kredi Technology, Istanbul, Turkey*To whom correspondence should be addressed.AbstractMotivation: The amount of information available in textual format is rapidly increasing in thebiomedical domain. Therefore, natural language processing (NLP) applications are becoming in-creasingly important to facilitate the retrieval and analysis of these data. Computing the semanticsimilarity between sentences is an important component in many NLP tasks including text retrievaland summarization. A number of approaches have been proposed for semantic sentence similarityestimation for generic English. However, our experiments showed that such approaches do noteffectively cover biomedical knowledge and produce poor results for biomedical text.Methods: We propose several approaches for sentence-level semantic similarity computation inthe biomedical domain, including string similarity measures and measures based on thedistributed vector representations of sentences learned in an unsupervised manner from a largebiomedical corpus. In addition, ontology-based approaches are presented that utilize general anddomain-specific ontologies. Finally, a supervised regression based model is developed that effect-ively combines the different similarity computation metrics. A benchmark data set consisting of100 sentence pairs from the biomedical literature is manually annotated by five human experts andused for evaluating the proposed methods.Results: The experiments showed that the supervised semantic sentence similarity computationapproach obtained the best performance (0.836 correlation with gold standard human annotations)and improved over the state-of-the-art domain-independent systems up to 42.6% in terms of thePearson correlation metric.Availability and implementation: A web-based system for biomedical semantic sentence similaritycomputation, the source code, and the annotated benchmark data set are available at: http://tabilab.cmpe.boun.edu.tr/BIOSSES/.Contact: gizemsogancioglu@gmail.com or arzucan.ozgur@boun.edu.tr1 IntroductionSemantic text similarity estimation is a research problem that aims tocalculate the similarities among texts based on their meanings and se-mantic content, rather than their shallow or syntactic representation.The measures on semantic text similarity have undertaken a crucialrole in many natural language processing (NLP) applications such asmachine translation (Finch et al., 2005), automatic summarization(Wang et al., 2008), and question answering (Jeon et al., 2005).Several approaches for semantic sentence similarity computationhave been proposed for generic English. These approaches are ingeneral based on computing word-level similarities and combiningthese to obtain sentence-level similarity scores. Corpus-based meas-ures such as Latent Semantic Indexing (LSA), knowledge-basedmeasures that utilize general-domain ontologies including WordNet(Miller, 1995), and string-based measures such as edit distance havebeen effectively used for word-level similarity computation (Li et al.,2006; Liu et al., 2015; Mihalcea et al., 2006). The SemEvalSemantic Textual Similarity (STS) task series, which is being con-ducted annually since 2012 has also boosted research in this area(Agirre et al., 2012, 2013, 2014, 2016; Agirrea et al., 2015).Manually annotated and test datasets provided by STS enabled thedevelopment and comparison of different approaches for semantictext similarity estimation. Supervised machine learning methodsthat integrate different features such as WordNet and corpus-basedfeatures, syntactic features, and features based on the distributeddense vector representation of words were shown to be effective forVC The Author 2017. Published by Oxford University Press. i49This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i49–i58doi: 10.1093/bioinformatics/btx238ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018semantic text similarity computation (Han et al., 2013;  Sari c et al.,2012; Sultan et al., 2015).Publicly available tools such as ADW (Align, Disambiguate andWalk) (Pilehvar et al., 2013; Pilehvar and Navigli, 2015) andSEMILAR (Semantic Similarity Toolkit) (Rus et al., 2013) for gen-eric domain sentence semantic similarity computation have alsobeen developed. ADW is a knowledge-based system that uses theTopic-sensitive PageRank algorithm (Haveliwala, 2002) over agraph generated using WordNet to model the similarity between lin-guistic items of different granularity such as words, sentences, anddocuments (Pilehvar et al., 2013; Pilehvar and Navigli, 2015). ADWwas evaluated on SemEval 2012 data set and was shown to outper-form the top three ranked systems (Pilehvar et al., 2013). SEMILARis a toolkit that implements several measures based on WordNet orLSA (Rus et al., 2013). Different algorithms such as the optimalmatching and the quadratic assignment problem algorithm areapplied for assessing the similarity of sentence pairs by using the cal-culated word-level similarities (Rus et al., 2013). The general do-main state-of-the-art systems ADW and SEMILAR are considered asbaseline models in our study.Assessing the similarity between two sentences is an importantproblem in the biomedical domain as well, due to the huge amountof information available in textual format, which renders effectiveretrieval, extraction and summarization of information vital. Theexcessive use of domain specific-language along with the rich varietyof expressions and inadequate training corpora make measuring sen-tence similarity in the biomedical domain a difficult task. Therefore,semantic text similarity measures to be used in biomedical NLPstudies call for domain-specific approaches including the use of bio-medical domain-specific corpora or biomedical knowledge sources.As an example, consider the following two sentences taken from(Wang et al., 2014) and (Fu et al., 2013), respectively.• S1: This form of necrosis, also termed necroptosis, requires theactivity of receptor-interacting protein kinase 1 and its relatedkinase 3.• S2: Moreover, other reports have also shown that necroptosiscould be induced via modulating RIP1 and RIP3.The example sentences S1 and S2 are on the same topic and are simi-lar to each other. The ‘receptor-interacting protein kinase 1’ in S1 isthe same concept as ‘RIP1’ in S2; likewise ‘kinase 3’ and ‘RIP3’ referto the same biomedical term. Domain-independent semantic textsimilarity measures developed for generic English can neither recog-nize these concepts nor give high weight to them while estimatingthe similarity between the sentences.These examples illustrate that new approaches that can handleboth biomedical and domain independent words are needed for sen-tence similarity computation in the biomedical domain. Garla andBrandt (2012) compared knowledge-based (ontology-based) anddistributional (corpus-based) similarity measures and observed thatknowledge-based measures are more effective for semantic similaritycomputation in the biomedical domain. Most previous work on se-mantic similarity in the biomedical domain focused on computingontology-based similarity between terms (Aouicha and Taieb, 2016;Harispe et al., 2014; Mabotuwana et al., 2013; Pedersen et al.,2007; Pesquita et al., 2009; S aNchez and Batet, 2011). Several stud-ies showed that the use of biomedical ontologies to measure seman-tic similarity provided valuable information for a number of tasksperformed in this domain such as similarity computation betweengene products (Lord et al., 2003), scoring protein–protein inter-actions (Jain and Bader, 2010) as well as disambiguation of biomed-ical terms (McInnes and Pedersen, 2013). To the best of ourknowledge, there is neither a manually annotated benchmark dataset, nor a comprehensive study on sentence-level semantic similaritycomputation in the biomedical domain. Although sentence-level se-mantic similarity computation has recently been used as a compo-nent in a text-mining system for evidence-based medicine(Hassanzadeh et al., 2015) and for biomedical question answering(Papagiannopoulou et al., 2016), these studies used general domainsemantic similarity computation methods and did not perform anydomain-specific adaptation.In this study, we show that general domain state-of-the-art sen-tence similarity computation systems fail to effectively model sen-tence similarity in the biomedical domain. We propose newapproaches specifically adapted for the biomedical domain thatcan be categorized into four areas: string similarity measures,ontology based measures, a distributional vector model and asupervised method combining these different measures. Besides ageneral domain ontology, namely WordNet (Miller, 1995), we alsoexploit a biomedical ontology, UMLS (Unified Medical LanguageSystem) (Bodenreider, 2004). The distributional vector representa-tions of sentences are learned using a large biomedical corpus offull text articles. In addition, we present a manually annotatedbenchmark data set for biomedical sentence similarity estimation,which can be used for training and evaluation in future studies inthis area.2 System and methods2.1 BIOSSES datasetSince there are no suitable datasets that comprise sentence pairsfrom the biomedical domain, we created a benchmark dataset forbiomedical sentence similarity estimation. The dataset comprises100 sentence pairs, in which each sentence was selected from theTAC (Text Analysis Conference) Biomedical Summarization TrackTraining Dataset containing articles from the biomedical domain.TAC dataset consists of 20 articles (reference articles) and citing art-icles that vary from 12 to 20 for each of the reference articles. We se-lected the BIOSSES sentence pairs from citing sentences, i.e.sentences that have a citation to a reference article, instead of choos-ing random sentence pairs, majority of which would be unrelated.Our motivation to use the TAC data set was that both semanticallyrelated and irrelevant sentence pairs occur in the annotation files.Some of the citing sentences cite the same reference articles becauseof similar reasons such as referring to a recent study on protein–pro-tein interactions. Sentences citing the same reference article for asimilar reason, in general have some degree of semantic similarity.On the other hand, there are also some citing sentences that cite ref-erence article that are written about different topics or researchfields (e.g. one refers to a study on microbiology, the other mentionsresearch on embryology). Such citing sentences are expected to havelower or no semantic similarity. Therefore, it was possible to obtainsentence pairs with different similarity degrees by using this ap-proach over the TAC dataset.The sentence pairs were evaluated by five different human ex-perts that judged their similarity and gave scores ranging from 0 (norelation) to 4 (equivalent). The score range was described based onthe guidelines of SemEval 2012 Task 6 on STS (Agirre et al., 2012).Besides the annotation instructions, example sentences from the bio-medical literature were provided to the annotators for each of thesimilarity degrees. These example sentence pairs that are scored be-tween 0 and 4 are shown in Table 1.Table 2 shows the Pearson correlation of the scores of eachannotator with respect to the average scores of the remaining fouri50 G.So gancıo glu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018annotators. It is observed that there is strong association among thescores of the annotators. The lowest correlations are 0.902, whichcan be considered as an upper bound for an algorithmic measureevaluated on this dataset.The distribution of the scores by each of the annotators is illus-trated in Figure 1. The distribution suggests that there are enough in-stances for each of the similarity degrees in our dataset.The BIOSSES dataset of sentence pairs and the annotators’scores are publicly available at http://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html.2.2 String similarity measuresWe evaluated the character- and term-based string similarityapproaches briefly described in the following subsections using theannotated dataset. Simple pre-processing steps consisting of removalof the punctuation marks (Dot, Comma, Colon, Exclamation Mark,Semicolon, Slash Mark, Dash, Question Mark) and stop-words(http://www.ranks.nl/stopwords) were applied to the sentence pairsbefore applying the similarity algorithms. The implementations ofthe string similarity methods in the SimMetrics Library (https://github.com/Simmetrics/simmetrics) were used.2.2.1 Qgram similarityQgram similarity (Ukkonen, 1992) is typically used in approximatestring matching by ‘sliding’ a window of length q over the charactersof a string to create ‘q’ length grams for matching. A match is thenrated as the number of q-gram matches within the second stringover the possible q-grams obtained from the first string.2.2.2 Block distanceBlock distance (Krause, 1987), also known as Manhattan Distance,computes the distance between two points by summing the differ-ences of their corresponding components. The Equation for blockdistance between a point A ¼ (A1,A2,. . .,An) and a point B ¼(B1,B2,. . .,Bn) in n-dimensional space is:BDðA;BÞ ¼Xni¼1jAi   Bij (1)In our case, Ai refers to the count of term i in sentence A and Birefers to the count of term i in sentence B.2.2.3 Jaccard similarityJaccard similarity (Jaccard, 1908) measures the similarity betweentwo sets and is computed as the number of common terms over thenumber of unique terms in both sets (Equation 2). In our case, set Aconsists of the unique words in the first sentence and set B consistsof the unique words of the second sentence.similarity ¼ JACðA;BÞ ¼ jA \ BjjA [ Bj (2)2.2.4 Overlap coefficientOverlap coefficient (Lawlor, 1980) is a similarity measure that dif-fers from Jaccard similarity with being divided by the size of thesmaller sized of the two sets (Equation 3).similarity ¼ OverlapðA;BÞ ¼ jA \ BjjMinðjAj; jBjÞj (3)2.2.5 Levenshtein distanceLevenshtein distance (Levenshtein, 1966) is a simple edit distance,which consists of the operations for transforming one of the givenTable 2. Correlation scores among annotatorsCorrelation rAnnotator A 0.952Annotator B 0.958Annotator C 0.917Annotator D 0.902Annotator E 0.941Table 1. Example annotationsSentence 1 Sentence 2 Comment ScoreHere we show that both C/EBPa and NFI-Abind the region responsible for miR-223upregulation upon RA treatment.Isoleucine could not interact with ligandfragment 44, which contains aminogroup.The two sentences are on different topics. 0Membrane proteins are proteins that inter-act with biological membranes.Previous studies have demonstrated thatmembrane proteins are implicated inmany diseases because they are positionedat the apex of signaling pathways thatregulate cellular processes.The two sentences are not equivalent, butare on the same topic.1This article discusses the current data onusing anti-HER2 therapies to treat CNSmetastasis as well as the newer anti-HER2agents.Breast cancers with HER2 amplificationhave a higher risk of CNS metastasis andpoorer prognosis.The two sentences are not equivalent, butshare some details.2We were able to confirm that the cancer tis-sues had reduced expression of miR-126and miR-424, and increased expression ofmiR-15b, miR-16, miR-146a, miR-155and miR-223.A recent study showed that the expression ofmiR-126 and miR-424 had reduced bythe cancer tissues.The two sentences are roughly equivalent,but some important information differs/missing.3Hydrolysis of b-lactam antibiotics by b-lac-tamases is the most common mechanismof resistance for this class of antibacterialagents in clinically important Gram-nega-tive bacteria.In Gram-negative organisms, the most com-mon b-lactam resistance mechanism in-volves b-lactamase-mediated hydrolysisresulting in subsequent inactivation of theantibiotic.The two sentences are completely or mostlyequivalent, as they mean the same thing.4BIOSSES i51Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018strings to the other, where an operation is defined as an insertion, de-letion, substitution or copying of a character. The distance is definedas the minimum number of the required operations to change onestring into another. The Levenshtein distance and block distance val-ues are converted into similarity values by subtracting from 1.2.3 Distributional vector model2.3.1 Paragraph vector modelThe word2vec model (Mikolov et al., 2013), which constructs distrib-uted representations of words, has been widely adopted to many re-cent NLP tasks including the biomedical domain (Aydin et al., 2017;Chiu et al., 2016; Moen and Ananiadou, 2013; Muneeb et al., 2015).In this model, a large amount of unlabeled text data is used in trainingto represent words in a new low-dimensional space as real-valued vec-tors. The model’s ability of considering the word context allows us toeasily relate word vectors in a semantic way (e.g. similar words havesimilar vectors). Word2vec is an unsupervised neural network basedlearning model based on two approaches, namely Skip-Gram andContinuous-Bag-of-Words (CBOWs). In the CBOW approach, thewords are predicted based on their surrounding words ignoring theword order, whereas in Skip-Gram, a word is used to predict its sur-rounding words while considering how distant they are in the text.Paragraph vector is presented following the word2vec model asa way to describe sentences (Le and Mikolov, 2014). The paragraphvector method was utilized to capture semantic information fromthe texts. The difference of this model from the word2vec model isthat the paragraphs are also mapped to distributed vector represen-tations and used to predict the next word in the given context to-gether with the distributed vector representations of the words inthe paragraph. We trained a paragraph vector model by using a sub-set of the Open Access Subset of PubMed Central (http://www.ncbi.nlm.nih.gov/pmc/) dataset, which comprises  4G text data of  37Karticles. The size of the output sentence vectors was set to 100 andthe Skip-Gram approach was employed.2.4 Ontology-based similarityOntologies are widely used for measuring semantic similarity be-tween concepts/terms, since their representation links terms seman-tically. Due to the fact that a sentence consists of a set of words, wecan utilize ontology-based word-level similarity measures to com-pute semantic similarity scores between sentences. To make our pro-posed algorithms clearer, we first briefly introduce the WordNet(Section 2.4.1) and the UMLS ontologies (Section 2.4.2), then de-scribe the ontology-based word-level similarity algorithms (Section2.4.3). Finally, we present our proposed approaches (Section 2.4.4),which exploit the word-level algorithms described in Section 2.4.3to obtain sentence-level similarity scores.2.4.1 WordNetWordNet (Miller, 1995) is a large English lexical thesaurus that hasbeen widely used for computing semantic similarity by using themeasures described in Section 2.4.3. According to the structure ofWordNet, each word consists of a form ‘f’ which is a string and asense ‘s’ represented by a set of synonyms that have that meaning.Words in WordNet are categorized according to their syntactic cate-gories such as verb, noun, adjective, and adverb. Since the samewords can be interpreted as having different part-of-speech (POS)tags according to the contexts they occur in, this syntactic categor-ization allows to save the same word with each possible POS tagsseparately in a taxonomy. In addition, words and word senses areconnected to each other with various types of relationships. Thetypes of relationships most commonly used for measuring semanticsimilarity are listed below:• Synonymy is the basic relation type in WordNet, since sets ofsynonyms (synsets) are used to represent word senses.• Hyponymy and hypernymy represent the hierarchical relationsbetween a word and its sub-name and super-name, respectively.• Antonymy represents the relation between a name and its oppos-ite-name.2.4.2 UMLSUMLS (Bodenreider, 2004) is a comprehensive thesaurus consistingof >1.7 million biomedical concepts. It comprises of the vocabularysources on specialized topics such as MeSH consisting of medicalsubject headings, OMIM containing genetic knowledge bases, andSnomedCT which consists of the concepts belonging to clinical repo-sitories. Since UMLS consists of various terminology sources, someconcepts can overlap. In other words, the same concept can belongto different sources. To be able to use multiple sources as a single re-source in the UMLS Metathesaurus, concept unique identifiers areassigned to the concepts.2.4.3 Word-level similarity methodsThe rich semantic information carried by ontologies enables thecomputation of semantic similarity scores among concepts. In thissubsection, we briefly describe the ontology based path-based andinformation content (IC)-based similarity metrics that are employedFig. 1. Distribution of the similarity scores in the dataseti52 G.So gancıo glu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018in our proposed sentence-level similarity computation method. Path-based approaches utilize the structure of the taxonomy, whereas IC-based approaches use extra information that is learned from corpusstatistics.The Path algorithm (Rada et al., 1989) measures the semanticsimilarity of two concepts by calculating the shortest path betweenthem in taxonomy. The intuition behind the algorithm is that theshorter the path between concepts in a hierarchy the more similarthey are.SimPathðc1; c2Þ ¼ ð2   depthmaxÞ   lenðc1; c2Þ (4)In Equation 4, the len function computes the shortest path between con-cepts c1–c2, and depthmax refers to the maximum depth of the tax-onomy. For example, given the sample taxonomy provided in Figure 2,the semantic distance between the terms ‘protein’ and ‘beta-lactams’ iscomputed as:SimPathðprotein; beta-lactamsÞ ¼ ð2   5Þ   4 ¼ 6 (5)The shortest path between c1 and c2 counts all nodes betweenthem—including themselves. Since the maximum depth of the tax-onomy is constant, this measure does not take into consideration thespecificity of the concepts. According to the definition, len(c1,c2) isequal to 4 and depthmax is 5.Similarly, the Leacock and Chodorow (LCH) measure (Leacockand Chodorow, 1998) takes the maximum depth of the taxonomyinto account and the similarity is determined as:SimLCHðc1; c2Þ ¼  loglenðc1; c2Þ2   depthmax(6)Unlike the Path and LCH measures, Wu and Palmer (WP) (Wu andPalmer, 1994) measure accounts for the specificity of the concepts,due to the concept depth feature. WP similarity between concepts c1and c2 is measured as twice the depth of the lowest common sub-sumer of the given concepts over the sum of the depths of c1 and c2.SimWPðc1; c2Þ ¼2   depthðLCSðc1; c2ÞÞdepthðc1Þ þ depthðc2Þ(7)The following example based on the sample taxonomy in Figure 2 il-lustrates the effect of concept depth using the WP and the Pathmetrics.SimWPðcephem; ampicillinÞ ¼ ð2   3Þ=ð4 þ 5Þ ¼ 0:66 (8)SimWPðantibiotic; enzymeÞ ¼ ð2   1Þ=ð2 þ 3Þ ¼ 0:40 (9)SimPathðcephem; ampicillinÞ ¼ 10   4 ¼ 6 (10)SimPathðantibiotic; enzymeÞ ¼ 10   4 ¼ 6 (11)Although the Path algorithm gives the same semantic similarity scorefor the two pairs, which have different specificity, WP estimates thatcephem and ampicillin are more similar than antibiotic and enzyme.The result of the WP metric is reasonable for this example, since thepath between deeper concepts causes less semantic distance.Both the concept depth feature and the frequency of the conceptin a corpus give an idea about the specificity of the concept. Withthe motivation of these facts, IC is used for measuring the semanticsimilarity between concepts. IC of a concept is defined as the nega-tive log likelihood of encountering concept c in a given corpus.ICðcÞ ¼  log ðpðcÞÞ (12)The probability of encountering concept c is given as,pðcÞ ¼ freqðcÞ=N (13)In Equation (13), N denotes the total number of words in the corpusused, while freq(c) is the number of occurrences of concept c in thecorpus.The Resnik (Resnik, 1995) similarity measure is determined asthe IC of the lowest common subsumer of concepts c1 and c2.SimResnikðc1; c2Þ ¼ ICðLCSðc1; c2ÞÞ (14)The Lin (Lin, 1998) similarity between concepts c1 and c2 is calcu-lated as twice the IC of the lowest common subsumer of the con-cepts over the sum of ICs of c1 and c2.SimLinðc1; c2Þ ¼2   ICðLCSðc1; c2ÞÞICðc1Þ þ ICðc2Þ(15)Jiang and Conrath (JCN) (Jiang and Conrath, 1997) measures the se-mantic similarity between concepts c1 and c2 as in Equation (16),which uses the ICs of the concepts and their lowest common subsumer.SimJCNðc1; c2Þ ¼1ICðc1Þ þ ICðc2Þ   2   ICðLCSðc1; c2ÞÞ(16)2.4.4 Sentence-level ontology-based methodsIn this section, we introduce our sentence-level ontology-basedmethods namely WordNet-based Similarity Method (WBSM),UMLS-based Similarity Method (UBSM) and combined ontologymethod (COM). The general design of these approaches is shown inFig. 3. Sentence-level similarity moduleFig. 2. Hierarchical relationships among a small subset of proteins andantibioticsBIOSSES i53Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018Figure 3. There are two main tasks in the general flow; calculationof word-level similarities (Section 2.4.3), adapting word-level simi-larities to obtain sentence-level score (sentence-level similaritymethod). Although the proposed three methods use the same algo-rithms for these tasks, they differ from each other by using differentontologies for word-level similarity calculation.Inspired by the study of Li et al. (2006), we developed asentence-level similarity method, which is an algorithm to adaptword-level similarities to sentence-level. The algorithm is explainedbelow using a walk-through example.A walk-through example• S1: Necroptosis requires the activity of RIP1 and RIP3.• S2: Necroptosis could be induced via modulating RIP1 andRIP3.Given two sentences S1 and S2, dictionary D is constructed,which consists of the union of the unique words from the two sen-tence. D for the example sentences S1 and S2 is:D: {Necroptosis, requires, the, activity, of, RIP1, and, RIP3,could, be, induced, via, modulating}D is used to build the semantic vectors D1 and D2 for S1 and S2,respectively, which have the same dimension as the dictionary. Forinstance, in order to build a semantic vector for S1, each word in thedictionary is compared with every word in S1 and the highest simi-larity score is assigned for the corresponding dimension index in thesemantic vector. As shown in Figure 4, D is obtained by using all dis-tinct words in S1 and S2. For determining the score of the 10th di-mension of the semantic vector D1, the ontology-based word-levelsimilarity scores between each word in S1 and the 10th dimension ofD are computed. Since the highest score is 0.33 among all similarityscores, the score of the 10th index of D1 is set as 0.33. This processis repeated for the remaining indexes of the semantic vector D1.Then, the same algorithm is applied to create the semantic vectorD2. Finally, the cosine similarity between D1 and D2 gives the se-mantic similarity score between the two sentences S1 and S2.WBSM. WBSM takes two sentences to be compared as inputsand returns the semantic similarity score by exploiting WordNet.We used the WS4J library (https://github.com/Sciss/ws4j) for calcu-lating the similarities between words by utilizing the WordNetontology. The algorithms described in Section 2.4.3 were evaluatedfor WBSM. These measures were calculated using the Is-A relationsin the WordNet ontology. Then, the sentence-level similaritymethod was used to combine word-level similarity scores to sen-tence-level.UBSM. Differently from WBSM, UBSM uses METAMAP(Aronson, 2001), which is a tool for extracting medical conceptsfrom text rather than assuming each word as a concept. This ap-proach is more reliable, since concepts can consist of more than oneword. The METAMAP tool is run on both sentences S1 and S2 anda dictionary is constructed from the unique mapped concepts/phrases in the two sentences. Therefore, the word-level similaritymethod utilizing UMLS takes concepts mapped by METAMAP asinputs. The rest of the methodology for constructing the sentence-level vectors is the same as WBSM.Umls:Similarity (McInnes et al., 2009) web interface was used tocalculate the similarity of the concepts, which were mapped byMETAMAP. The scope of Umls:Similarity is limited to the OMIM(Online Mendelian Inheritance in Man) and MeSH (Medical SubjectHeadings) ontologies, which are subsets of the UMLS ontology.Parent/Child (PAR/CHD) relationship was used as the relationshipparameter in the UMLS:Similarity web interface. The algorithmsdescribed in Section 2.4.3 were evaluated for UBSM.COM. The major motivation behind the COM was to benefitfrom both biomedical domain and general domain ontologies, sincesentences in biomedical text consist of both general terms andbiomedical-specific terms. To utilize the knowledge from bothUMLS and WordNet ontologies, we propose a new approach in thissection. Our method performs combination of different approacheson sentence-level. As shown in Figure 5, the sentence-level COMtakes the similarity scores of WBSM and UBSM for a sentence pair,then combines these scores by using Equation 17, where k representsthe weight parameter. When k is set to 0.5, equal weight is given tothe similarity scores obtained from the WordNet and UMLS ontolo-gies. When k is set to a value >0.5, higher weight is given to theFig. 4. Illustration of the proposed sentence-level ontology-based similarity algorithm which constructs semantic vectors of sentencesFig. 5. Sentence-level COMi54 G.So gancıo glu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018similarity score obtained from WordNet, and when it is set to avalue smaller than 0.5, higher weight is given to the similarity scoreobtained from UMLS.combinedscore ¼ ScoreWordNet:k þ ScoreUMLS:ð1   kÞ (17)If a word does not occur in either of the ontologies (UMLSand/or WordNet), the similarity score between the word and anyother word with respect to the corresponding ontology is consideredto be 0.2.5 Supervised combination of similarity measuresWe combined our unsupervised semantic similarity measures withina supervised method. We used the similarity scores computed by theunsupervised COM, Paragraph Vector and Qgram similarity as fea-tures in a supervised regression model. Linear Regression imple-mented in the Weka library (Hall et al., 2009) was used as thesupervised model. A linear regression model can be expressed as inEquation (18) (Alpaydin, 2014; Buckley and James, 1979; Rafteryet al., 1997),y ¼Xkj¼1 bjxj þ b0 (18)where y is the dependent variable, each xj is an input variable, and kequals to the number of predictors (input variables). bjs correspondto the parameters of the linear regression model, which are esti-mated from the training data. Therefore, in our supervised similaritymodel, the predicted sentence similarity score (y) is calculatedthrough the similarity scores (xj) that were obtained by the unsuper-vised methods.The supervised system exploiting the results of the unsupervisedsimilarity computation methods is illustrated in Figure 6. The pre-pro-cessed sentences are given to each unsupervised system as inputs. Then,the output score of each system, which is the semantic similarity scorefor the given pair, is used as a feature in our supervised system.3 Experimental resultsThe proposed sentence-level semantic similarity estimation algo-rithms are evaluated using the manually annotated dataset describedin Section 2.1. For each sentence pair in the dataset, the mean of thescores assigned by the five human annotators was taken as the goldstandard. The Pearson correlation (Pearson, 1895) between the goldstandard scores and the scores estimated by the algorithms was usedas the evaluation metric. The strength of correlation can be assessedby the general guideline proposed by Evans (1996) as follows:• very strong: 0.80–1.00• strong: 0.60–0.79• moderate: 0.40–0.59• weak: 0.20–0.39• very weak: 0.00–0.19Since there is no previous study on sentence semantic similaritycomputation developed specifically for the biomedical domain, weconsidered the domain-independent state-of-the-art approachesADW (Pilehvar et al., 2013; Pilehvar and Navigli, 2015) andSEMILAR (Rus et al., 2013) introduced in Section 1 as our baselinemodels. According to the results shown in Table 3, both ADW andSEMILAR obtain moderate correlation based on Evans’ definition(Evans, 1996). The poor results of these generic-domain similarityestimation systems demonstrate the need for new approaches forthis domain-specific research field.We evaluated several string similarity measures on our dataset.We experimented with performing preprocessing as described inSection 2.2 and without performing preprocessing for all string-based methods as well as for the other evaluated methods. Pre-pro-cessing improved the performances of all methods. Therefore, inTable 3 we report the results when preprocessing was performed.Our experiments showed that the application of preprocessing meth-ods contributed more to the performance of the string similaritymeasures compared with the other methods. The range of increasein Pearson correlation varies between 10 and 31% for the stringsimilarity measures. This result is expected, as string-basedapproaches are highly sensitive to small changes, since they do nottake into consideration the semantic information of text.Fig. 6. Supervised combination of similarity measuresTable 3 Experimental results of the presented approachesMethods Pearson correlationDomain-independent systemsADW 0.586SEMILAR 0.419String similarity measuresQgram 0.754Jaccard 0.710Block 0.752Levenshtein 0.592Overlap coefficient 0.695Word Embeddings based SimilarityParagraph Vector 0.787Ontology-based similarityWBSM-Path 0.644WBSM-Resnik 0.234WBSM-Lin 0.495WBSM-WP 0.354WBSM-JCN 0.623WBSM-LCH 0.287UBSM-Path 0.651UBSM-Resnik 0.473UBSM-Lin 0.645UBSM-WP 0.576UBSM-JCN 0.624UBSM-LCH 0.333COM ([k ¼ 0.5]) 0.710Supervised semantic similarity systemLinear regression 0.836BIOSSES i55Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018Paragraph vector is an unsupervised approach, which we usedwith a large unlabeled corpus of biomedical text to learn semanticinformation. The strong correlation result obtained by theParagraph Vector method shows that it is a promising method forrepresenting sentences as vectors while capturing semantics.For both WBSM and UBSM, using the path algorithm as theword-level similarity approach yielded the best performance withPearson correlation scores of 0.644 and 0.651, respectively.Therefore, for the combined ontology approach, we used the pathalgorithm both for computing the WordNet- and the UMLS-basedscores. Then, the weighted sum of the similarity scores obtainedfrom the WordNet- and UMLS-based methods was assigned as thefinal similarity score. The best combination was achieved when theweight parameter lambda was set to 0.5 (k ¼ 0.5) in Equation (17).The comparison between the COM and the methods that use a sin-gle ontology show that the efficient unification of the available bio-medical information coming from a biomedical ontology withgeneral domain information increased the overall performance. Theresults of the combined ontology approach justify our hypothesis,which was based on exploiting both general-domain and domain-specific ontologies for domain-specific text. The significant increasein the correlation performance of the combined model, comparedwith the individual correlation scores, indicate that the combinationis useful.The evaluation of the supervised model was performed usingstratified 10-fold cross-validation over all the sentence pairs, due tothe small size of the dataset. The final result for the supervised seman-tic similarity system was obtained by averaging the individual correl-ation results of each fold. As the learning model, Linear Regressionimplemented in the Weka library (Hall et al., 2009) was employed.The experimental results indicate that the supervised combin-ation of the similarity scores computed by the different methods out-performs the individual performance of each unsupervised method.This shows that these unsupervised system scores complement eachother. Although each unsupervised method obtained strong associ-ation with the gold standard, combination of these approaches by asupervised algorithm led to very strong correlation. The SupervisedSemantic Similarity System exploiting the scores of the unsupervisedsystems as features produced the best correlation of 83.6% amongthe others.4 DiscussionIn this study, we presented and compared several approaches tomeasure semantic sentence similarity in the biomedical domain. Wedemonstrated the need for adapted or new approaches for domain-specific semantic sentence-level similarity, since our results showedthat state-of-the-art domain-independent semantic similarity meas-ures are inadequate when applied to biomedical text. Another im-portant contribution of this research is that we provide a strongbaseline as well as a hand-crafted benchmark dataset for furtherstudies due to attempting the first methods in this unexplored re-search area of biomedical sentence-level semantic similaritycomputation.Thanks to the ontologies that enable the computation of seman-tic distances between concepts, ontology-based measures have beenused in our semantic similarity computation study. Since the sen-tences in our dataset are selected from biomedical articles, we uti-lized WordNet as the general domain ontology and UMLS as thebiomedical domain-specific ontology. The evaluations indicated thatthe COM, which utilizes both the WordNet and UMLS ontologies,accomplished better results on estimating the similarity among bio-medical sentences compared with the methods where a single ontol-ogy was utilized. This outcome is reasonable, since sentences in thebiomedical domain comprise both biomedical and general concepts.Thus, the knowledge extracted from both WordNet and UMLScomplements each other and contributes to the overall performanceof the system.Besides UMLS, there are various biomedical ontologies special-ized on different subtopics in the biomedical domain such as theChEBI ontology focusing on chemical entities (Degtyarenko et al.,2008), the Interaction Network Ontology specializing in the domainof molecular interactions ( €Ozgür et al., 2016), and the HumanPhenotype Ontology providing controlled vocabulary for pheno-typic features related to human diseases (Köhler et al., 2017).Integrating the semantic similarity scores computed by using differ-ent biomedical ontologies might contribute to the performance ofthe COM. As future work, we aim to make use of the knowledge ob-tained from different biomedical ontologies, in order to enhance oursystem to respond to a wider range of concepts and relationships.Our results revealed that the unsupervised Paragraph Vector ap-proach based on a biomedical corpus to learn the distributional vec-tor representations of sentences is a promising method forbiomedical semantic similarity computation.Finally, we presented a supervised semantic similarity estimationsystem based on a linear regression model, which exploits high-levelfeatures. The high-level features consist of the similarity scores ofthe best performing unsupervised systems, namely Qgram,Paragraph Vector and the COM. Combining the unsupervised meth-ods with the help of a supervised learning model increased the over-all performance of the system. Experiments showed that usingdifferent approaches to estimate the similarity contributes to theoverall performance of the system.The manually annotated dataset and the developed semanticsimilarity estimation systems are publicly available. We believe thatour biomedical-domain specific semantic sentence-level similaritymeasures can be used in various applications of biomedical NLPsuch as automatic summarization, question answering, text categor-ization and text retrieval.The upper bound in this study can be considered as the perform-ance of a typical human, which is 90.2% according to the correl-ations between the human annotators. Although our bestperforming system achieved high correlation with human annota-tions (83.6%), there is still room for improvement for biomedicaldomain-specific semantic sentence similarity estimation.AcknowledgementsWe would like to thank Ecem So gancıo glu, Jesus Lago Garcia, Kübra Erenand Onur Yanar for annotations. We also thank Bridget T. McInnes for herkind help about the UMLS interface. We respectfully acknowledge theTUBITAK (BIDEB 2210-A and 2211-E) scholarship programmes (to G.S. andH.O.) and the BAGEP Award of the Science Academy (to A.O.).Conflict of Interest: none declared.ReferencesAgirre,E. et al. (2014). Semeval-2014 task 10: Multilingual semantic textualsimilarity. In Proceedings of the 8th International Workshop on SemanticEvaluation (SemEval 2014), Dublin, Ireland, pp. 81–91.Agirre,E. et al. (2016). Semeval-2016 task 1: semantic textual similarity,monolingual and cross-lingual evaluation. Proceedings of SemEval, SanDiego, CA, pp. 497–511.i56 G.So gancıo glu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018Agirre,E. et al. (2013). sem 2013 shared task: Semantic textual similarity,including a pilot on typed-similarity. In *SEM 2013: The Second JointConference on Lexical and Computational Semantics. Association forComputational Linguistics, Atlanta, Georgia.Agirre,E. et al. (2012). Semeval-2012 task 6: A pilot on semantic textual simi-larity. In Proceedings of the First Joint Conference on Lexical andComputational Semantics-Volume 1: Proceedings of the Main Conferenceand the Shared Task, and Volume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pp. 385–393. Association forComputational Linguistics, Montreal, Canada.Agirrea,E. et al. (2015). Semeval-2015 task 2: Semantic textual similarity, eng-lish, spanish and pilot on interpretability. In Proceedings of the 9th interna-tional Workshop on Semantic Evaluation (SemEval 2015), Denver,Colorado, pp. 252–263.Alpaydin,E. (2014). Introduction toMachine Learning. MIT press, Cambridge.Aouicha,M.B. and Taieb,M.A.H. (2016) Computing semantic similarity be-tween biomedical concepts using new information content approach.J. Biomed. Informatics, 59, 258–275.Aronson,A.R. (2001). Effective mapping of biomedical text to the umls meta-thesaurus: the metamap program. In Proceedings of the AMIA Symposium,p. 17. American Medical Informatics Association, Washington, DC.Aydin,F. et al. (2017) Automatic query generation using word embeddings forretrieving passages describing experimental methods. Database, doi:10.1093/database/baw166.Bodenreider,O. (2004) The unified medical language system (umls): integrat-ing biomedical terminology. Nucleic Acids Res., 32(Suppl. 1), D267–D270.Buckley,J. and James,I. (1979) Linear regression with censored data.Biometrika, 429–436.Chiu,B. et al. (2016). How to train good word embeddings for biomedical nlp.ACL 2016, Vol. 66, pp. 166.Degtyarenko,K. et al. (2008) Chebi: a database and ontology for chemicalentities of biological interest. Nucleic Acids Res., 36(Suppl. 1), D344–D350.Evans,J.D. (1996). Straightforward Statistics for the Behavioral Sciences.Brooks/Cole, Pacific Grove, CA, USA.Finch,A. et al. (2005). Using machine translation evaluation techniques to de-termine sentence-level semantic equivalence. In Proceedings of the ThirdInternational Workshop on Paraphrasing (IWP2005), pp 17–24.Fu,Z. et al. (2013) The anti-tumor effect of shikonin on osteosarcoma byinducing rip1 and rip3 dependent necroptosis. BMC Cancer, 13, 1.Garla,V.N., and Brandt,C. (2012) Semantic similarity in the biomedical domain:an evaluation across knowledge sources. BMCBioinformatics, 13, 261.Hall,M. et al. (2009) The weka data mining software: an update. ACMSIGKDD Expl. Newslett., 11, 10–18.Han,L. et al. (2013). Umbc ebiquity-core: Semantic textual similarity systems.In Proceedings of the Second Joint Conference on Lexical andComputational Semantics, Vol. 1, pp. 44–52. Association forComputational Linguistics, Atlanta, Georgia.Harispe,S. et al. (2014) A framework for unifying ontology-based semanticsimilarity measures: A study in the biomedical domain. J. Biomed. Inform.,48, 38–53.Hassanzadeh,H. et al. (2015) A supervised approach to quantifying sentencesimilarity: with application to evidence based medicine. PloS One, 10,e0129392.Haveliwala,T.H. (2002). Topic-sensitive pagerank. In Proceedings of the 11thInternational Conference on World Wide Web, pp. 517–526. ACM,Honolulu, Hawaii, USA.Jaccard,P. (1908) Nouvelles recherches sur la distribution florale. Bull. Soc.Vaudoise Sci. Nat., 44, 223–270.Jain,S., and Bader,G.D. (2010) An improved method for scoring protein-protein interactions using semantic similarity within the gene ontology.BMC Bioinformatics, 11, 562.Jeon,J. et al. (2005). Finding similar questions in large question and answerarchives. In Proceedings of the 14th ACM International Conference onInformation and Knowledge Management, Bremen, Germany, pp. 84–90.ACM.Jiang,J.J. and Conrath,D.W. (1997). Semantic similarity based on corpus stat-istics and lexical taxonomy, In Proceedings of the 10th InternationalConference on Research on Computational Linguistics, page 19–33.Köhler,S. et al. (2017) The human phenotype ontology in 2017. Nucleic AcidsRes., 45, D865.Krause,E.F. (1987). Taxicab geometry. Dover Publications, New York.Lawlor,L.R. (1980) Overlap, similarity, and competition coefficients.Ecology, 61, 245–251.Le,Q.V. and Mikolov,T. (2014). In Proceedings of the 31st InternationalConference onMachine Learning, PMLR, 32, 1188–1196.Leacock,C. and Chodorow,M. (1998) Combining local context and wordnetsimilarity for word sense identification. In WordNet, MIT Press, Cambridge,pp. 265–283.Levenshtein,V.I. (1966). Binary codes capable of correcting deletions, inser-tions, and reversals. In Soviet Physics Doklady, Vol. 10, pp. 707–710.Li,Y. et al. (2006) Sentence similarity based on semantic nets and corpus statis-tics. IEEE Trans. Knowl. Data Eng. 18, 1138–1150.Lin,D. (1998). An information-theoretic definition of similarity. In ICML, Vol. 98,pp. 296–304. Morgan Kaufmann Publishers Inc, Madison, Wisconsin, USA.Liu,Y. et al. (2015). Computing semantic text similarity using rich features.29th Pacific Asia Conference on Language, Information and Computation,Shanghai, China, Vol. 1, pp. 44–52.Lord,P.W. et al. (2003) Investigating semantic similarity measures across thegene ontology: the relationship between sequence and annotation.Bioinformatics, 19, 1275–1283.Mabotuwana,T. et al. (2013) An ontology-based similarity measure for bio-medical data–application to radiology reports. J. Biomed. Inform., 46,857–868.McInnes,B.T., and Pedersen,T. (2013) Evaluating measures of semantic simi-larity and relatedness to disambiguate terms in biomedical text. J. Biomed.Inform., 46, 1116–1124.McInnes,B.T. et al. (2009). Umls-interface and umls-similarity: open sourcesoftware for measuring paths and semantic similarity. In AMIA AnnualSymposium Proceedings, Vol. 2009, p. 431. American Medical InformaticsAssociation, San Francisco, CA.Mihalcea,R. et al. (2006). Corpus-based and knowledge-based measures oftext semantic similarity. In AAAI, Vol. 6, pp. 775–780.Mikolov,T. et al. (2013). Distributed representations of words and phrasesand their compositionality. In Advances in Neural Information ProcessingSystems, Vol. 26, pp. 3111–3119.Miller,G.A. (1995) Wordnet: a lexical database for english. Commun. ACM,38, 39–41.Moen,S. and Ananiadou,T.S.S. (2013). Distributional semantics resources forbiomedical text processing. In Proceedings of the 5th International Symposiumon Languages in Biology andMedicine, Tokyo, Japan, pp. 39–43.Muneeb,T. et al. (2015). Evaluating distributed word representations for capturingsemantics of biomedical concepts. In Proceedings of the 2015 Workshop onBiomedical Natural Language Processing (BioNLP 2015), Association forComputational Linguistics, Beijing, China, p. 158.€Ozgür,A. et al. (2016) The interaction network ontology-supported modelingand mining of complex interactions represented with multiple keywords inbiomedical literature. BioData Mining, 9, 41.Papagiannopoulou,E. et al. (2016). Large-scale semantic indexing and ques-tion answering in biomedicine. In Proceedings of the 4th BioASQWorkshop, ACL 2016, Berlin, Germany, pp. 50–54.Pearson,K. (1895) Note on regression and inheritance in the case of two par-ents. Proc. R Soc. Lond., 58, 240–242.Pedersen,T. et al. (2007) Measures of semantic similarity and relatedness inthe biomedical domain. J. Biomed. Inform., 40, 288–299.Pesquita,C. et al. (2009) Semantic similarity in biomedical ontologies. PLoSComput. Biol., 5, e1000443.Pilehvar,M.T. et al. (2013). Align, disambiguate and walk: A unified approachfor measuring semantic similarity. In ACL (1), Sofia, Bulgaria, pp.1341–1351.Pilehvar,M.T., and Navigli,R. (2015). An open-source framework for multi-level semantic similarity measurement. In Proceedings of NAACL-HLT,Denver, Colorado, Association for Computational Linguistics, pp. 76–80.Rada,R. et al. (1989) Development and application of a metric on semanticnets. IEEE Trans. Syst. Man Cybernet., 19, 17–30.Raftery,A.E. et al. (1997) Bayesian model averaging for linear regression mod-els. J. Am. Stat. Assoc., 92, 179–191.BIOSSES i57Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018Resnik,P. (1995). Using information content to evaluate semantic similarity ina taxonomy. In Proceedings of the 14th International Joint Conference onArtificial Intelligence, Vol. 1, IJCAI’95, pp. 448–453.Rus,V. et al. (2013). Semilar: The semantic similarity toolkit. In ACL(Conference System Demonstrations), pp. 163–168. Citeseer, Sofia,Bulgaria.S aNchez,D. and Batet,M. (2011) Semantic similarity estimation in the bio-medical domain: An ontology-based information-theoretic perspective.J. Biomed. Inform., 44, 749–759. Sari c,F. et al. (2012). Takelab: Systems for measuring semantic text similarity. InProceedings of the First Joint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the Main Conference and the Shared Task,and Volume 2: Proceedings of the Sixth International Workshop on SemanticEvaluation, pp. 441–448. Association for Computational Linguistics,Stroudsburg, PA, USA.Sultan,M.A. et al. (2015). Dls@ cu: sentence similarity from word alignmentand semantic vector composition. In SemEval-2015, p. 148.Ukkonen,E. (1992) Approximate string-matching with q-grams and maximalmatches. Theor. Comput. Sci., 92, 191–211.Wang,D. et al. (2008). Multi-document summarization via sentence-level seman-tic analysis and symmetric matrix factorization. In Proceedings of the 31st an-nual International ACM SIGIR Conference on Research and Development inInformation Retrieval, pp. 307–314. ACM, New York, NY, USA.Wang,H. et al. (2014) Mixed lineage kinase domain-like protein mlkl causesnecrotic membrane disruption upon phosphorylation by rip3. Mol. Cell, 54,133–146.Wu,Z., and Palmer,M. (1994). Verbs semantics and lexical selection. InProceedings of the 32nd annual meeting on Association for ComputationalLinguistics, pp. 133–138. Association for Computational Linguistics,Stroudsburg, PA, USA.i58 G.So gancıo glu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i49/3953954by gueston 07 January 2018
5028881972002	PMID28881972	5028881972	https://watermark.silverchair.com/btx237.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881972.main.pdf	Bioinformatics, 33, 2017, i124–i132doi: 10.1093/bioinformatics/btx237ISMB/ECCB 2017Abundance estimation and differential testingon strain level in metagenomics dataMartina Fischer, Benjamin Strauch and Bernhard Y. Renard*Bioinformatics Unit (MF1), Department for Methods Development and Research Infrastructure, Robert KochInstitute, Berlin 13353, Germany*To whom correspondence should be addressed.AbstractMotivation: Current metagenomics approaches allow analyzing the composition of microbial communities at high resolution. Important changes to the composition are known to even occur onstrain level and to go hand in hand with changes in disease or ecological state. However, speciﬁcchallenges arise for strain level analysis due to highly similar genome sequences present. Only alimited number of tools approach taxa abundance estimation beyond species level and there is astrong need for dedicated tools for strain resolution and differential abundance testing.Methods: We present DiTASiC (Differential Taxa Abundance including Similarity Correction) as anovel approach for quantiﬁcation and differential assessment of individual taxa in metagenomicssamples. We introduce a generalized linear model for the resolution of shared read counts whichcause a signiﬁcant bias on strain level. Further, we capture abundance estimation uncertainties,which play a crucial role in differential abundance analysis. A novel statistical framework is built,which integrates the abundance variance and infers abundance distributions for differential testingsensitive to strain level.Results: As a result, we obtain highly accurate abundance estimates down to sub-strain level andenable ﬁne-grained resolution of strain clusters. We demonstrate the relevance of read ambiguityresolution and integration of abundance uncertainties for differential analysis. Accurate detectionsof even small changes are achieved and false-positives are signiﬁcantly reduced. Superior performance is shown on latest benchmark sets of various complexities and in comparison to existingmethods.Availability and Implementation: DiTASiC code is freely available from https://rki_bioinformatics.gitlab.io/ditasic.Contact: renardB@rki.deSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionRapid advances in next generation sequencing (NGS) technologieshave revolutionized the field of metagenomics (Oulas et al., 2015;Wooley et al., 2010). Metagenomics enables the study of complexcommunities in environmental or human samples by direct analysis ofwhole shotgun metagenomes, without prior need for cultivation.Among others, two major goals in metagenomics profiling studies arepursued. One is to unravel the taxonomic composition of the community in a given sample, the second concerns the abundance change oftaxa between different metagenomes (Neelakanta and Sultana, 2013).Especially, differences occurring on strain level in microbiomescan be of high relevance for disease and health state (Nawy, 2015).Investigations on strain level have been proven to be crucial for theunderstanding of evolutionary processes, adaption, pathogenicity,drug resistance and transmission (Lieberman et al., 2014; Rosenet al., 2015; Shapiro et al., 2012; Snitkin et al., 2011). However, although importance of resolution on strain level is acknowledged,there are still only a limited number of tools focusing on accurateprofiling beyond species level (Sczyrba et al., 2017).Altogether, in this context, three main concepts are relevant:strain identification, abundance estimation and differential abundance assessment. Our objective in this work is to address all thesesteps by specifically focusing on strain level resolution and its arisingchallenges. In particular for differential abundance evaluation onthe strain level, there is a need for novel tools. Here, we use the termstrain level referring to the highest possible resolution available andalways work on the exact genome level.CV The Author 2017. Published by Oxford University Press.i124This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018DiTASiCMany concepts have been pioneered for taxa identificationand quantification, apart from assembly and binning methods, diverse metagenomics profiling tools have specialized on this task(Lindgreen et al., 2016; Sczyrba et al., 2017). In practice, these concern the assignment of the sequenced reads to taxa and corresponding inference of taxa abundance. Read assignment can be conductedeither by the full alignment of reads to genome sequences or by usingpseudo-alignment approaches (Wood and Salzberg, 2014). The latter is sufficient for many metagenomics quantification studies due tothe fact that only the assignment of reads is required and not exactalignments. Another variant is to rely on marker genes instead ofcomplete genome sequences (Segata et al., 2012; Scholz et al., 2016;Luo et al., 2015); however, a general drawback is the requirementof high-sequencing coverage contrasting typical metagenomics scenarios of many low abundant taxa (Li, 2015). One of the first andpopular reference-based tools for read assignment in metagenomicswas MEGAN (Huson et al., 2007), which assigns the reads to thelowest common ancestor in the taxonomic tree at which a uniquealignment is achieved. However, this approach limits MEGAN tothe identification and quantification of only higher taxonomic levels. A main characteristic on strain level is the presence of highlysimilar reference sequences, causing many reads to match to multiple genomes equally. A further common practice is to assign multiply mapped reads heuristically to reference genomes according touniquely mapped read proportions (Liu et al., 2017; Nayfach et al.,2016). Yet, this can easily result in biased abundance estimates dueto reference sequence similarities as observed e.g. by Liu et al.,2017. GRAMMy (Xia et al., 2011) and GASiC (Lindner andRenard, 2013) were the first tools to include reference genome similarities in a model for the resolution of ambiguously mapped reads.Since being based on read alignments, these methods can encountercomputational limits in large sample sizes. A new era evolved by utilizing fast k-mer approaches, significantly accelerating read assignments, with Kraken being a popular representative (Wood andSalzberg, 2014), but showing reduced resolution power on strainlevel (Schaeffer et al., 2017). As a consequence, the importance ofcombining fast mapping approaches with methods for read ambiguity resolution was recognized. This was likewise applied in the fieldof RNA-Seq, resulting in the development of kallisto (Bray et al.,2016), which promises to also support metagenomics abundanceanalysis (Schaeffer et al., 2017). kallisto consists of two parts, a newfast pseudo-aligner based on k-mer hashing and an expectation–maximization (EM) algorithm on equivalence classes, which carriesout the statistical resolution of read ambiguities.In this work, we present DiTASiC (Differential Taxa Abundanceincluding Similarity Correction) which relies on pseudo-alignmentsfor mapping and is built on a novel generalized linear model (GLM)framework for read ambiguity resolution. Hereby, we significantlyimprove on our previous development in this field, GASiC. Our newmodel framework is developed to adapt more precisely to the characteristics of absolute mapping count data observed for taxa.Moreover, our method improves on existing pure abundance profiling strategies by including additional error terms in the model andcapturing abundance estimation uncertainties.The integration of variance of abundance estimates plays a crucial role for the differential abundance analysis. This variance reflects the uncertainty in the resolution of read mapping ambiguitiesin the presence of similar reference sequences. Hence, it is of particular importance on strain level to integrate this variance to enable accurate detections of differential or non-differential abundance of ataxon in co-existence of similar strains, most notably in the case ofsmaller changes.i125Most approaches developed for identification of differentialabundance in the field of comparative metagenomics focus exclusively on experimental sources of variance, namely on sample variance relevant within technical and biological replicates. A largevariety of tools is available (Jonsson et al., 2016); amongst others,software packages implementing diverse parametric and nonparametric statistical standard tests (Karlsson et al., 2013; Parkset al., 2014; Parks and Beiko, 2010; Segata et al., 2011; White et al.,2009). Another group comprises zero-inflated models either combined with Gaussian mixture distribution (Paulson et al., 2013), lognormal distribution (Sohn et al., 2015), or beta-regression (Penget al., 2015), concentrating on the potential sparsity in count data.Further, popular methods from RNA-Seq analysis such as edgeR(Robinson et al., 2010), DESeq2 (Love et al., 2014) and voom (Lawet al., 2014) are also commonly applied in comparative metagenomics. Without doubt, the integration of experimental variance is ofhigh necessity when comparing groups of samples. However, here,we want to emphasize and raise awareness for variance in abundance estimates and its impact on differential abundance analysis onstrain level.Further it should be noted that many methods treat the differential assessment of taxa and genes equivalently. However, assumptions such as the majority of features will show non-differentialabundance, which has widely been proven for gene expression, arenot necessarily valid for taxa abundance in a sample. Antibioticstreatment and other life influential factors have shown rapidchanges of microbial compositions in human samples (David et al.,2014) and similar scenarios are found in ecological environments(Gibbons and Gilbert, 2015). Thus, commonly used assumptionscannot be easily transferred to composition change.In summary, we present DiTASiC, which addresses abundanceestimation as well as differential abundance of taxa specificallyfocusing on strain level. A new GLM framework is proposed forresolution of read mapping ambiguities and allows inference ofhighly accurate taxa abundance estimates. Second, a statisticalframework, which integrates abundance estimate uncertainties, isbuilt for differential abundance testing. Here, no prior assumptionson overall composition change are required. A resulting list of testedtaxa is reported with estimated abundances, fold-changes and P-values to infer significance. The performance of DiTASiC is evaluatedon different metagenomics data sets from four different data sourcesand in comparison to existing tools.2 Materials and methodsDiTASiC is designed as a comprehensive approach for abundanceestimation and differential abundance assessment of individual taxa.Thereby, the main focus is on distinguishing on the strain level withhighly similar sequences and its corresponding challenges. The stepsof the DiTASiC workflow are illustrated in Figure 1, it consists ofthree main parts: mapping, abundance estimation and differentialabundance assessment.In the first two parts we built on some of the core ideas of ourpreviously published tool GASiC (Lindner and Renard, 2013), whilestrongly improving on abundance quantification and introducingnew methodology to address the critical aspects of variance of abundance estimates and differential abundance.In a metagenomics sample measured by NGS technologies weface millions to billions of reads which are derived from diversetaxa. DiTASiC relies on a pre-filtering of species by fast profilingtools such as Kraken (Wood and Salzberg, 2014), CLARKDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018i126(Ounit et al., 2015), Kaiju (Menzel et al., 2016), or by using Mash(Ondov et al., 2016), a genome distance calculator, to reduce thenumber of potential reference genomes and keep the main focus onspecies expected in the data. Here, we specifically aim at revealingthe picture on the highest available strain levels. In the first mappingstep, all reads are assigned to the given references as a first attemptto decipher their potential origin. The number of hits per referencegenome is counted. We refer to it as mapping abundance of a taxon.In the next step of abundance estimation, a new generalized linearmodel (GLM) is introduced for the resolution of shared read counts,which are crucial on strain level. As a result, more accurate abundance estimates are obtained for the different strains along withstandard errors for abundance uncertainty. In the last section, thefocus is on the comparison of whole metagenomics samples and theassessment of differential abundance of taxa. Thereby, we concentrate on a method to integrate the variance of abundance estimates.Abundances are transformed into distributions, divergence of distributions is used to infer differential events and corresponding P-values are calculated.The details of the three DiTASiC parts are explained in the following sections. The following notation is applied: different metagenomics samples are denoted as D ¼ {Dk, k ¼ 1,. . ., K}, eachcontaining N ¼ {Nk, k¼ 1,. . ., K} total input reads. A set of taxaS ¼ {Si, i¼ 1,. . ., M} with known reference sequences is considered.Thereby, Si is synonymously used for both the taxa itself as well asits exact reference genome. Mapping and abundance estimation areaddressed for each data set separately, while the last step of differential abundance estimation is defined on a pair of samples from D.2.1 MappingTo identify their origin, the assignment of reads is conducted by acompetitive mapping approach, which means all selected referencegenome sequences S are simultaneously offered to all reads of a sample D   D for mapping. Particularly on strain level, reference sequences exhibit high sequence similarities, thus some reads areexpected to match to different genome sequences equally well.M.Fischer et al.These reads are defined as shared reads and we account for all theirmultiple hits. However, the exact matching position in a referencegenome Si is not of importance and several position hits of one readon the same reference Si are counted as one. For the mapping itself,a pseudo-alignment approach provided as part of the kallisto implementation (Bray et al., 2016) is applied. As no exact alignments arerequired for our purpose, a pseudo-aligner is sufficient and proves tobe much faster and accurate using a fast kmer-based approach.Here, we gain significant improvements over our previously published tool GASiC, which relied on individual reference alignmentsby Bowtie 2 (Langmead and Salzberg, 2012).Altogether, we extract and count the number of read hits each reference genome receives and refer to it as mapping abundance ci oftaxon Si. In case the data set D consists of mainly dissimilar referencesand is dominated by clearly unique mappings, the observed mappingabundances ci may already closely reflect the underlying true abundances of the taxa. However, if many similar references are present,which is a common scenario on strain level, a large bias is present dueto multiple hits of shared reads. The sum of the mapping abundancesof all taxa then drastically exceeds the number of input reads.2.2 Abundance estimationFollowing the idea introduced in GASiC, we rely on a simulationbased representation of reference genome similarities to resolve theeffect of shared reads. A similarity matrix is constructed, which encodes the proportion of reads which are expected to be sharedamong all pairwise combinations of reference sequences considered.Reads are simulated using Mason (Holtgrewe, 2010) based on eachreference sequence, and are subsequently mapped to all referencesfollowing the same competitive mapping setup as applied to thereads of D in the step before. The key element is to imitate sequencing, read, and mapping characteristics as good as possible toreproduce the source of ambiguities. Parameters such as read lengthand mismatch probability are crucial for the simulation ofreads, and are inferred from the raw reads of D. The square matrixA ¼ (aij), i,j ¼ 1,. . .,M, is computed column-wise for eachFig. 1. Workﬂow of DiTASiC. It consists of three main parts: (i) mapping, (ii) taxa abundance estimation and (iii) differential abundance assessment. (i) We rely onprior pre-ﬁltering of species by external proﬁling tools such as Kraken or Mash. Reads are mapped to the given reference genome sequences and the number ofmatching reads per reference are counted (mapping abundance). A similarity matrix reﬂecting the genome similarities is constructed. (ii) Subsequently, a GLM isbuilt for resolution of read count ambiguities, resulting in corrected abundance estimates along with standard errors. (iii) For the comparison of metagenomes,abundances are formulated as distributions and their divergence reﬂects differential events. A ﬁnal list of tested taxa with fold change and adjusted P-values isreportedDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018DiTASiCi127reference, with aij referring to the count of reads simulated fromreference j which map to reference i. Next, the matrix is normalizedcolumn-wise by the read count ajj, the number of simulated readswhich are assigned back to their reference of origin. Thus, the matrixA ¼ (aij/ajj), i,j ¼ 1,. . .,M, holds values between zero and one.Replacing the classic linear model of GASiC, we formulate anew GLM with the vector of absolute mapping abundances c andsimilarity matrix A to correct for the shared read biases. Aiming torecover the true, but unknown, abundances r of the taxa:c¼AÁrþewith A ¼ (aij), i,j¼ 1,. . .,M, c ¼ (c1, c2, . . ., cM)T, r ¼ (r1, r2, . . ., rM)Twith non-negativity constraint r ! 0, and error term E.The observed mapping count ci of taxon i corresponds to asummed mixture of the underlying true abundance ri of taxon i anda proportion of shared read counts rj due to the other references:ci ¼ ri þMXaij Á rj þ ei ; with taxon i and taxa j ¼ f1 . . . Mg 6¼ ii6¼jThe GLM is defined by an identity link function as a linear relation ofcomponents holds to explain the observed mapping counts. However,in this setting of discrete counts the error E is defined to follow aPoisson distribution. We expect and observed no overdispersion in theabundance estimates within a sample after ambiguity correction bythe model (Supplementary Material). This is in contrast to measurements of replicate samples, which may display overdispersion and motivate a negative-binomial assumption (Anders and Huber, 2010).The GLM is internally solved by an ‘iteratively reweighted leastsquares’ to find the maximum likelihood estimates referring to the‘true’ abundance estimates ri for each taxon i. Along with the abundance estimates, standard errors are computed which report the rangeof accuracy and reliability of the abundance estimates. Further, P-values are given for each taxa estimate as a measure of significance.In case of high uncertainty about the presence of a crucialamount of taxa within the selected set of references, the applicationof an implemented filtering is possible. Thereby, P-values above aset threshold, commonly a value of 0.05, and estimates below a minimum number of assigned reads are used as indicators for falsepositive estimates. The filtering step helps to numerically stabilizethe equation system in case of many absent taxa and a reoptimization step is subsequently conducted.2.3 Differential abundanceIn this section, the focus is on comparing metagenomics samples.The objective is to identify which taxa significantly change theirabundance from one metagenome sample to another as well aswhich hold a constant abundance. For the differential abundance assessment of similar strains the integration of the variance of theirabundance estimates is crucial. Hence, in place of directly comparing abundance point estimates of taxa between samples, we makeuse of the estimates as well as their standard errors.First, the comparison of different samples requires accountingfor potentially different numbers of total input reads N. The numberof input reads has a significant impact on the computed abundancesr and standard error estimates. A linear dependence is clearly noticeable (see Supplementary Fig. S1) and is in agreement with theoreticalderivations of the GLM framework. The abundance count estimater scales linear with the number of reads whereas the standard errorscales quadratic. This means the accuracy of abundance estimatesimproves with increased number of input reads as expected.Altogether, a normalization factor is required and a factor of Nx/Nyis correspondingly applied to samples Dx and Dy to achieve a comparable base between samples.In the next step, we integrate abundance estimates and corresponding standard errors to infer an abundance distribution for eachtaxon in each sample. Here, it is assumed that the unknown trueabundance count of a taxon underlies a Poisson distribution. Thepotential bias due to falsely assigned reads to taxa, after correctionfor read ambiguities by the GLM model, is not expected to exceedthe variance of a Poisson distribution. But, an analytical approach isnot feasible here, as the exact distribution is described in practice bya mixture of Poisson distributions. However, an empirical approach can be pursued, which is realized by a two-step samplingprocess: In the first step, we define intervals with abundance estimates ri 6 their standard errors as boundaries for each listed taxon.We use a scale unit of one standard error, as this reflects the uncertainty interval which is expected to contain the abundance estimate.Subsequently, potential abundance point estimates are uniformlysampled from this interval. Concurrently each of these sampled valuesrefers to a k value of a Poisson distribution. In the second step, foreach taxon and each potential k of it, 500 values in a default setup aredrawn from the corresponding defined Poisson distribution with parameter k. This creates one empirical distribution based on a specific kfor the taxon. Pooling all empirical distributions, created by all the different k which are assigned to the taxon, results in an overall empiricaldistribution comprising 50 000 Poisson draws by default setup. Werefer to it as empirical abundance distribution of a taxon.In order to assess whether taxa show differential abundance between two samples, their abundance distributions need to be compared. As we rely on empirical distributions here, no analytical formof standard differential testing is applicable. Yet, we can transfer theassessment of differential abundance to the question to which extentthe corresponding abundance distributions overlap. Clearly separated distributions refer to a significant abundance change, while anincreasing overlap points to smaller or no significant difference.Measuring the separation of the distributions is implemented by randomly drawing pairs of values from either distribution. The difference within each pair is computed and yields an overall distributionof differences as a result. Thereby, the location of the zero valuerelated to the distribution of differences is meaningful. A zero valuemoving towards the center of the distribution reflects a higher previous overlap and corresponds to a less significant abundance change.An empirical P-value is correspondingly inferred by determining thequantile of the zero value within the distribution.In case a taxon is only detected within one sample, while absent inthe other, the single abundance distribution of the taxon is tested againsta user-defined threshold corresponding to a minimum read count. Thelatter test yields the significance of taxa presence in this one sample.Generally, P-values are calculated individually for all taxa considered in the samples of comparison, either to assess differentialabundance of taxa present in both samples or to infer new appearance of taxa in only one sample. Thus, P-values need to be adjustedfor multiplicity, which is performed by the method of BenjaminiHochberg (Benjamini and Hochberg, 1995). A final report is provided listing all taxa tested for differential abundance along withnormalized abundance estimates for each sample, log2 fold changes,and adjusted P-values.2.4 ImplementationDiTASiC is implemented in Python3 and R (version ! 3.3.1), and isavailable from https://rki_bioinformatics.gitlab.io/ditasic. Further, aDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018i128M.Fischer et al.linked webpage and user manual provides easy guidance throughthe three main commands. DiTASiC is based on a flexible designand allows the integration of mapping algorithms and read simulators of choice. Our implementation uses the current state of the artpseudo-alignment algorithm provided within the kallisto framework(Bray et al., 2016), which can be individually called by the commandkallisto pseudo. As a prerequisite, an overall index is built on selected reference sequences. Using the generated tsv and ec file formats, we extract the mapping counts of the contigs and merge themaccording to genomes. This allows circumventing the use of largeSAM files. Further, read simulators need to be optimally adaptedto capture the read characteristics. Here, the Mason simulator(Holtgrewe, 2010) serves as default.3 Experimental setupWe tested DiTASiC and existing approaches on a variety of data setsfrom four different sources (Table 1), challenging the tools by number of taxa, total number of input reads, read characteristics, abundance complexity and degree of reference similarities.A comprehensive simulation setup is established to enable abundance estimation as well as differential evaluation on an exact groundtruth at which taxa proportions are known. In total, we consider 11different simulation sets characterized by many strain clusters(Supplementary Fig. S2-3), and distinguish between three groups:Group (1) serves to evaluate the abundance performance with different proportions of absent taxa, group (2) defined by all 35 taxa ensures an unbiased differential abundance evaluation in pairwisecomparisons and group (3) focuses on the resolution of large andhighly similar strain clusters as well as on the impact of missingstrains. Further, we relied on the Illumina based FAMeS data set ofPignatelli and Moya (2011), evolved from the original set byMavromatis et al. (2007), which covers low (LC), medium (MC) andhigh complexity (HC) metagenomics profiles (Supplementary Fig.S4). Additionally, we tested the popular Illumina 100 data sample(Mende et al., 2012), which serves as benchmark set in the latest relevant studies (Lu et al., 2017; Schaeffer et al., 2017). Last, we usedtwo benchmark data sets of medium complexity from a current comparative metagenomics challenge, CAMI (Sczyrba et al., 2017). Wefurther extended the CAMI sets by simulated spike-in data, adding 30new strains of genera already present in the original set and 20 millionreads per sample, to create an additional ground truth for differentialassessment. Further details on the data sets and parameter settings arefound in the Supplementary Material. In all presented data sets,ground truth of relative abundances of taxa is available. Comparingthe samples, a ground truth to classify differentially or nondifferentially abundant taxa is given for the simulation and CAMIstudy, while fold-change accuracy can be assessed in all data sources.4 ResultsIn the following sections, we demonstrate the performance ofDiTASiC on the presented data sets in comparison to existing tools.We separately investigate three aspects: (i) abundance estimation,(ii) absent and missing taxa and (iii) differential abundance.Evaluations focus on the accuracy of estimates of relative taxa abundance as well as fold change, and on sensitivity and specificity concerning detection of differentially abundant taxa.4.1 Abundance estimationIn this first part, we address the quantification of taxa in a givenmetagenomics sample, aiming for the highest taxonomic level. Wehighlight the strength of our proposed GLM model for the resolution of shared read counts and subsequent inference of correctedabundance estimates for taxa considered.We compare to our previously published tool GASiC (Lindnerand Renard, 2013), which relies on individual reference alignments and a non-negative LASSO modelling approach for abundance estimation, and present significant improvements. Furtherwe test against the most recently published tool for RNA-Seq analysis, kallisto (Bray et al., 2016; Schaeffer et al., 2017), which hasalso been shown to perform superior to other existing tools in theapplication to metagenomics. We also evaluate on the same benchmark data to allow further comparison of tools (seeSupplementary Material). Although we compare against the fullversion of kallisto, it is important to note, that we use and integrate the pseudo-aligner of kallisto for mapping purpose, but notkallisto’s quantification and modelling framework. Yet our mainfocus in this work is the modeling and resolution of arising readambiguities due to highly similar genome sequences considered.Hence, the comparison of DiTASiC to kallisto in this section refersto a comparison of our GLM model to the statistical EM framework of kallisto.All tools are applied to each sample individually, in total we consider and evaluate 17 different samples from four data sources.The output of all three tools are absolute read counts assigned toeach taxa in the data set considered. Normalization is applied bydividing all absolute taxa counts by the total number of input readsof the corresponding sample. We receive an estimation of a quantitative taxa composition of a sample as a result.All data sets described here provide a ground truth of taxa abundance proportions, enabling us to assess the difference between truthand estimate. As an error measure we apply the SSE (Sum ofSquared Errors) to evaluate the accuracy of the given estimates, theSSE also penalizes abundance estimates obtained for absent taxa.The resulting error measures of abundance estimation byDiTASiC, GASiC and kallisto, according to all different data sets arereported in Table 2. Overall, DiTASiC strongly reduces the error onTable 1. Characteristics of the four data sources: CAMI, FAMeS, Illumina 100 data (i100) and the simulation setups (Sim (1), (2), (3))SourceSamplesReferencesGeneraSpeciesReads (M)Length (bp)Abundance rangeCAMISet 1-2FAMeSLC, MC, HCSim (1)Set 1-3Sim (2)Set 4-9Sim (3)Set 10-11i100225128199$1501000.0009–8%12281108$1.01102–20%3512220.75a1001–30%3512220.75a1000.1–15%5512260.75a1000.1–2%100638553.3750.8–2.2%Note: Each reference set is deﬁned by the union of references of the underlying samples. All read proﬁles follow Illumina characteristics (areads are simulatedby Mason).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018DiTASiCall data sets compared with GASiC by several orders of magnitude.Further, DiTASiC shows either comparable and in many cases improved performance to kallisto. Generally, reported error values aredependent on data size and prevailing genome similarities. However,the presented values refer to a remarkably high accuracy of abundance estimates overall. Smallest divergences of estimates from theground truth are found for the FAMeS data sets (Supplementary Fig.S5). This is expected due to less pronounced reference similaritieswithin the data and moderate median abundance proportions, meaning less challenge for the resolution models. The CAMI data do posea much greater challenge, considering 255 taxa for quantificationwith several strain clusters and some extremely small relative abundance values. Yet, highly accurate taxa estimates, apart from fewsmall outliers, are obtained by DiTASiC; notably also for very lowrelative abundances below 0.01% (see also Supplementary Fig. S6).CAMI data were not analyzed with GASiC due to computationallimitations. The commonly used i100 data set is characterized byshorter reads derived from different bacterial strain clusters. DiTASiCachieves an improved accuracy in comparison to kallisto, and also tofurther tools when compared with the values reported in a recentbenchmark study of different abundance profiling tools on the i100set (Schaeffer et al., 2017) (see Supplementary Material andSupplementary Fig. S7). The simulation data serves as a challengewith a high number of similar strains and a smaller number of readsavailable for assignment. In comparison, samples in CAMI hold 150times more reads with only seven times more taxa. The results showthat DiTASiC performs superior in all sets of simulation group (2),where all taxa are present, while errors are proportionally higher insets of group (1), where taxa are absent. Group (1) is primarilydefined by the absence of distant strains or entire strain clusters; theEM algorithm of kallisto proves to be slightly more accurate in thesescenarios. However, sets in simulation group (3) are characterized bythe absence of strains from highly similar clusters and by the presenceof very large clusters of high sequence similarities. Here, DiTASiCdemonstrates to be more powerful (Supplementary Fig. S8). Notably,we observe an increased error of abundance estimates in kallisto predominantly for highly similar strain sequences. In contrast, DiTASiCreveals its particular strength in the resolution of these strain clusters,it demonstrates to precisely distinguish abundances down to substrains with sequence similarities above 95%. Different examples arefound for the CAMI, i100 and simulation data, considering diverseEscherichia coli cluster, Corynebacterium and Staphylococcus aureuscluster (Supplementary Fig. S9). Here, an accurate cluster resolution isobtained by DiTASiC, and common errors such as abundance interchange or equalization of similar sub-strains are avoided.Supplementary Figure S10 visualizes the taxa abundance estimatesof the different tools in comparison to the observed mapping abundances, exemplary for three simulation sets of different complexity. Itclearly demonstrates how the mapping abundance, biased due to readambiguities, mainly overestimates the ground truth and further assignsabundance counts to absent taxa. GASiC shows some significant overand underestimations, while the accuracy of DiTASiC and kallisto isconsistently high. Further, a study of two replicate sets, defined by readsets simulated with the same abundance profile, proves robustness andprecise reproducibility of results by DiTASiC as well as kallisto, withsignificant improvement over GASiC (Supplementary Fig. S11).i129Table 2. Accuracy of taxa abundance estimates by DiTASiC, kallistoand GASiCDiTASiCkallistoGASiCCAMISet 1Set 26.98 e-025.36 e-021.05 e-015.69 e-02n.a.n.a.i100i1008.23 e-065.62 e-059.32 e-04FAMeSLCMCHC6.87 e-063.07 e-088.34 e-081.73 e-081.70 e-082.79 e-083.18 e-044.17 e-047.79 e-05Simulation group (1)Set 1Set 2Set 38.38 e-079.33 e-074.37 e-077.61 e-079.61 e-072.59 e-076.92 e-031.13 e-029.73 e-03Simulation group (2)Set 4Set 5Set 6Set 7Set 8Set 92.54 e-061.85 e-062.67 e-063.41 e-064.93 e-064.15 e-064.09 e-055.94 e-053.46 e-052.84 e-042.99 e-045.37 e-056.10 e-038.54 e-032.22 e-036.55 e-032.27 e-031.63 e-03Simulation group (3)Set 10Set 113.94 e-063.39 e-055.43 e-055.07 e-041.84 e-027.29 e-03Note: Accuracy is deﬁned by the SSE between estimates and availableground truth. A signiﬁcant error reduction is shown for DiTASiC comparedwith GASiC and a comparable performance is observed for kallisto (highestaccuracy is depicted in bold print). GASiC was not run on CAMI data due tocomputational limitations.more references than taxa actually present in the data and an inclusion of all potentially abundant strains is advised.Hence, in the simulation groups (1) and (3) and the FAMeS data,which hold different proportions of absent taxa, we tested the detection performance of DiTASiC. The internal filtering is conducted toinfer potential false-positive taxa in the given sets. In the simulationgroup (1) the abundant taxa proportions of 28, 40 and 45%, respectively, are exactly detected with neither false-positive nor falsenegative calls. In the FAMeS data, proportion of absent taxa basedon the reference set corresponds to 8, 9 and 8% in the three samples.DiTASiC achieves sensitivity and specificity of 100% for the MCand HC data. In the LC set, a false-negative is caused by missing oneabundant taxon, resulting in a decreased sensitivity of 99.1%.(Supplementary Table S1). In simulation group (3), set 10 serves tostudy the impact of absent strains from highly similar clusters andindicates un-biased abundance estimation of strains of the affectedclusters by DiTASiC (refer to Supplementary Fig. S7). In anotherstudy, reads derived from 55 taxa are contrasted to a reduced reference set of 35 taxa to investigate the impact of missing taxa in a selected reference set. First, we observe that 11% of the reads are notaligned; second, it is shown that abundance estimates of some taxaare overestimated by DiTASiC. However, a closer look reveals thatit concerns closely related strains which show an increased abundance due to missing strains within their cluster. The results proposethat no overall abundance bias is caused (Supplementary Fig. S12).4.3 Differential taxa abundance4.2 Absent and missing taxaWe recommend prior pre-filtering of references to focus on referencegenomes of species expected in the data. Still, frequently we considerHere, we evaluate pairwise comparisons of metagenomics samples, aiming to reveal the change of taxa compositions at the highest taxonomiclevel. We demonstrate how the entire process of read ambiguity resolution and incorporating the uncertainty of abundance estimates has acrucial impact on differential assessment on strain level. As a result, aDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018i130M.Fischer et al.similarity of 98% with another E.coli sub-strain DH10B. The knownrelative abundance decrease by 1% is very small and hereby falls inthe abundance variance range, while an increase by 3% for sub-strainDH10B could be detected as well as differences below 1% for theother E.coli strains in the cluster. In general contrast are the resultsobtained for STAMP, showing a strong tendency of identifying nondifferential taxa as differentially expressed, causing high numbers offalse-positives. As abundance estimates underlie some variation, additionally biased due to read ambiguities, these results confirm how theinclusion of standard errors is crucial to identify taxa with consistentabundances. The FDR of STAMP ranges from 12 to 63% and theoverall accuracy from 46 to 86%.more accurate detection of differential events is achieved, particularly incase of small changes. False-positives are significantly reduced.In order to evaluate independent of technical and biological variance factors, we do not consider replicate samples and comparisonshere. This way we can test our specifically addressed differentialmethod and prove the validity and impact of the abundance variancewithout bias. We compare our approach to STAMP (Parks andBeiko, 2010; Parks et al., 2014), which is available for pairwisecomparisons to exemplary demonstrate the importance of the issuesof read ambiguities and abundance estimation uncertainties. Themapping abundances of the taxa serve as input for STAMP. STAMPis a software package providing several statistical tests for differential taxonomic and functional assessment and a user-friendly graphical interface. The recommended option of a G-test with Yatescontinuity correction followed by a Benjamini-Hochberg adjustmentis selected.A similar situation is observed for the CAMI spike-in data.DiTASiC correctly detects all 15 differential and 15 non-differentialtaxa. However, all 30 taxa are found to be differentially abundant bySTAMP, resulting in an accuracy of only 50%. Considering the entireCAMI data set, fold changes, spanning from 0.0009 to 1024, are proven to be highly accurate for DiTASiC with an SSE 19 times smallercompared with the STAMP output. Further, the assigned P-values byDiTASiC clearly separate the spiked-in non-differential and differentialtaxa (Supplementary Fig. S13). All other taxa of the data set, holdingfold change values greater than zero, also receive very small P-valuesstating differential abundance, but cannot be further confirmed.Different metagenome comparisons are conducted within thepresented data sources. Evaluations focus on correct detections ofdifferentially abundant taxa and on accuracy of taxa fold changes.For the simulation data and the CAMI spike-in data, ground truthis available for specific classification into differential and nondifferential taxa, results are described by measures of sensitivity, specificity and accuracy as combined measure of correct detections. For theFAMeS and the original CAMI data, no classification is provided, here,the accuracy of fold changes is evaluated by using the SSE instead.Different pairwise comparisons of the simulation data cover various scenarios of non-differential and differential events. A P-valuecutoff of 0.05, adjusted for multiplicity, is used to define differentiallyabundant taxa. Evaluation results for the simulation data are presented in Table 3. For all scenarios, DiTASiC reports no false-positivehits, holding a false discovery rate (FDR) of zero and a resulting specificity of one. Further, in five out of eight comparisons also a sensitivityof 100% is achieved. In the other three cases, the detection of oneknown differentially abundant taxon fails resulting in one falsenegative detection and corresponding sensitivities of 97%. Here, itconcerns the differential detection of the sub-strain E.coli K12MG1655, which holds accurate abundance estimates but fairly largestandard errors, arising due to uncertainties because of high sequencePairwise metagenome comparisons within the FAMeS data alsoexhibit high fold change accuracies, as consequence to the formerhighly accurate abundance estimates. Corresponding SSE values aretwo magnitudes smaller compared with the ones computed bySTAMP (see Supplementary Table S2).5 DiscussionOur work demonstrates the challenges concerning strain level resolution in metagenomics data and the need for dedicated methodsfor quantification and differential abundance testing. DiTASiCaddresses these challenges and provides novel approaches.The inference of taxa abundances by directly counting mappedreads is not suitable on strain level. Although read mappers have significantly improved in speed and mapping accuracy, they cannotTable 3. Evaluation of differential taxa abundance by DiTASiC and STAMP based on sample comparisons within the simulation data andthe CAMI data setData sourceSamplescomparedNo. of nondifferentialeventsNo. ofdifferentialeventsFalse positives(FPs) and Falsenegatives (FNs)FDRSensitivity j SpeciﬁcityDiTASiC STAMP DiTASiC STAMP DiTASiCFP FN FP FNCAMIspike-in dataSimulation group (2):Pairwise samplecomparisonsof differentsimulation sets(numberedfrom 4 to 9)SamplesS1 versus S2set 4 versus set 5set 5 versus set 9set 5 versus set 6set 6 versus set 7set 7 versus set 8set 6 versus set 8set 4 versus set 7set 4 versus set 815150015000.501j135281817106550717182529303000000000001000110121816745500200000n.a.0000000n.a.0.630.510.470.220.120.140.14n.a. j 11j10.94 j 11j11j11j10.97 j 10.97 j 1AccuracySTAMP DiTASiC STAMP1 j 0.5n.a j 11 j 0.70.89 j 0.51 j 0.511 j 0.591 j 0.61 j 0.51 j 0.510.5110.971110.970.9710.660.430.540.80.890.860.86Note: A P-value cutoff of 0.05 is used to deﬁne differentially abundant taxa. In most scenarios, DiTASiC achieves exact detections, holding a FDR of zero andaccuracy above 97% overall. A reduced accuracy performance by STAMP, using mapping abundances, conﬁrms the signiﬁcant impact of read ambiguities andabundance estimate uncertainties. In case of no differential events, FDR and sensitivity cannot be computed (n.a.).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018DiTASiCresolve shared read assignments and thereby cannot directly outputcorrect abundances. Our results show the bias introduced by thepseudo-aligner of kallisto (without its well working EM-based quantification framework): the abundances of most taxa are overestimated and many actually absent taxa are assigned positiveabundances. This effect is due to shared read counts, caused byhighly similar reference sequences of strains in a metagenomics sample. DiTASiC is based on a new GLM framework, adapted to characteristics of taxa data for the resolution of shared read counts. As aresult, it provides highly accurate abundance estimates for taxa indifferent metagenomics samples. Thereby, DiTASiC proves excellentperformance independent of abundance profile complexities andalso shows reduced errors in comparison to existing tools on a recent benchmark study on the i100 data (Schaeffer et al., 2017). It enables accuracy in a large range of relative abundances from 0.001 to30% present in the various data sets. Further, while generally theread coverage in a metagenomics sample is a critical factor for abundance estimation, the degree of reference similarities of present taxameans a greater challenge. Thus, on the FAMeS data set with 122taxa, but many dissimilar species, all tools achieve overall higherabundance accuracy compared with the simulation sets with only 35taxa holding almost the same number of input reads, but differentchallenging strain clusters. However, the GLM model of DiTASiCproves specific strength in highly accurate abundance resolutionwithin strain clusters, as is shown for various examples in the i100,CAMI and simulation studies. In particular, it demonstrates to precisely distinguish abundances down to sub-strains which share sequence similarities above 95%. Whereas this is more challenging forkallisto, which was similarly reported in a benchmark study byMcLoughlin (2016). An important point is that the similarity matrixused in DiTASiC is not necessarily symmetric. Hence, the simulatedproportion of reads shared from reference i with reference j can differ from the proportion reference j shares with reference i. We observe these dissimilarities in the matrix e.g. for the E.coli clustersand hypothesize that this may explain the good performance ofDiTASiC, as it allows capturing sub-strain sequences, which may beshorter, but highly similar to other longer strain sequences.The framework of DiTASiC is also robust with increasingsequencing error, as the internal matrix simulations account forthe error profiles found in the raw reads. However, as a consequence, misaligned reads in addition to shared reads will causeabundance bias, which poses another resolution challenge. Further,missing or unknown taxa in reference sets may introduce quantification bias. However, one of our studies indicates that closelyrelated strains compensate for missing ones and not affected straincluster remain stable. Overall, DiTASiC shows certain robustnesson imperfect reference sets with either missing or false-positivetaxa included. Nevertheless, explicitly accounting for non-mappedreads and their missed abundance proportion could be included infuture work.All in all, the accuracy of the abundance estimation has an immediate impact on the accuracy that can be achieved in the differentialabundance analysis of the taxa. This is clearly observable in thecomparisons of the FAMeS data sets, which result in highly accuratefold change estimates in consequence of the accurate abundance estimates that were obtained.However, for differential abundance testing, in order to distinguish differentially and non-differentially abundant taxa, the uncertainty of the abundance estimates plays a crucial role. Especially onstrain level, this variance reflects uncertainties in the underlying readambiguity resolution in the presence of highly similar reference sequences. DiTASiC introduces a new statistical framework, whichi131integrates the abundance variance and forms abundance distributions for differential testing sensitive to strain level.Generally in comparative metagenomics, it is difficult to predicthow a community of taxa in a sample will change, as there is a varietyof influential factors involved. A study by (David et al., 2014), demonstrates how human actions can cause next-day abundance change inthe microbiome. Hence, putting assumptions on data for compositionchange is complex. Further, although taxa abundance data and gene expression data share discrete count data characteristics, assumptionscommonly made for gene expression for differential analysis cannot beeasily transferred. One of the most common assumptions is that the majority of features will not be differentially changed. This is reasonablefor genes in a cell as no global change of expression of all genes is biologically expected. In metagenomics studies though, antibiotics treatment has shown to cause rapid change of microbial compositions inhuman samples (Dethlefsen and Relman, 2011). Further, gene expression data in RNA-Seq studies are often characterized by overdispersionand correspondingly modelled by negative binomial distributions.Different popular RNA-Seq tools as well as standard statistical testsare frequently applied to metagenomics gene data for differential analysis, however, have been shown to not capture the data well in all cases(Jonsson et al., 2016). Similar problems are observed when consideringdifferential taxa abundance. In a study of plaque samples, DESeq andedgeR were also shown to not fit the data properly (Paulson et al.,2013). Hence overall, it is important to distinguish gene and taxa leveland critically assess corresponding assumptions. Furthermore, definingassumptions to capture all diverse structures of metagenomics data mightpose an almost impossible challenge. Here, we propose an independentstatistical framework for differential testing of all individual taxa in theset, without putting any assumptions on overall composition change.We evaluated our approach on diverse scenarios, covering setswith only non-differential events to sets with overall change, andcan indicate overall correct detections. Further, the method is notdependent on the presence of a taxon in both samples of comparison, it also serves as test on taxa emergence or extinction.In contrast, STAMP yields many false-positives, which reflectsthe importance of read ambiguity resolution and integration ofabundance uncertainties for strain level analysis. In cases of extremely similar strain sequences, however, large standard errors forthe estimates can occur, as shown for the two E.coli sub-strains, andcan consequently cause a lower limit for the detection of very smallfold-changes in DiTASiC.Generally, DiTASiC is neither limited to bacteria nor any taxonomic level. Also its concept is applicable to any ambiguity resolution in which the similarities causing the ambiguities can bedescribed. Further, variance of sample replicates pose another crucial variance source, integration could be achieved by not samplingfrom the mixture of Poisson distributions of one experiment, butacross all replicates. DiTASiC is independent of specific databasesor any additional data information, it simply relies on the raw readsand on a (pre-filtered) species reference set in fasta format, the lattercan also contain assemblies or fragmented sequences.6 ConclusionThis contribution focuses on the resolution on strain level in metagenomics data concerning taxa quantification and differential abundance assessment. We point out the challenges arising on strain leveldue to the presence of highly similar reference sequences. We presentDiTASiC, which provides a new GLM framework for the resolutionof shared read counts and introduce a statistical framework, whichDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018i132integrates abundance variances, for differential testing sensitive tostrain level. As a result, highly accurate abundance estimates downto sub-strain level as well as detections of differentially abundanttaxa are obtained. Evaluations are conducted on different data sources and in comparison to existing methods.AcknowledgementsWe thank Kathrin Trappe, Tobias Loka and Vitor Piro (Robert KochInstitute) for critical reading and helpful comments on the article as well asMartin S. Lindner for inspiring discussions.FundingWe acknowledge ﬁnancial support by Deutsche Forschungsgemeinschaft[grant number RE3474/2-1 to B.Y.R.].Conﬂict of Interest: none declared.ReferencesAnders,S. and Huber,W. (2010) Differential expression analysis for sequencecount data. Genome Biol., 11, R106.Benjamini,Y. and Hochberg,Y. (1995) Controlling the false discovery rate: apractical and powerful approach to multiple testing. J. R. Stat. Soc. Ser B(Methodological), 57, 289–300.Bray,N.L. et al. (2016) Near-optimal probabilistic RNA-seq quantiﬁcation.Nat. Biotechnol., 34, 525–527.David,L.A. et al. (2014) Host lifestyle affects human microbiota on daily timescales. Genome Biol., 15, R89.Dethlefsen,L. and Relman,D.A. (2011) Incomplete recovery and individualized responses of the human distal gut microbiota to repeated antibiotic perturbation. Proc. Natl. Acad. Sci. USA, 108(Suppl 1), 4554–4561.Gibbons,S.M. and Gilbert,J.A. (2015) Microbial diversity–exploration of natural ecosystems and microbiomes. Curr. Opin. Genet. Dev., 35, 66–72.Holtgrewe,M. (2010) Mason – a read simulator for second generationsequencing data. Tech. Rep. FU Berl.Huson,D.H. et al. (2007) MEGAN analysis of metagenomic data. GenomeRes., 17, 377–386.Jonsson,V. et al. (2016) Statistical evaluation of methods for identiﬁcationof differentially abundant genes in comparative metagenomics. BMCGenomics, 17, 78.Karlsson,F.H. et al. (2013) Gut metagenome in European women with normal, impaired and diabetic glucose control. Nature, 498, 99–103.Langmead,B., and Salzberg,S.L. (2012) Fast gapped-read alignment withBowtie 2. Nat. Methods, 9, 357–359.Law,C.W. et al. (2014) voom: Precision weights unlock linear model analysistools for RNA-seq read counts. Genome Biol., 15, R29.Li,H. (2015) Microbiome, metagenomics, and high-dimensional compositional data analysis. Annu. Rev. Stat. Appl., 2, 73–94.Lieberman,T.D. et al. (2014) Genetic variation of a bacterial pathogen withinindividuals with cystic ﬁbrosis provides a record of selective pressures. Nat.Gene., 46, 82–87.Lindgreen,S. et al. (2016) An evaluation of the accuracy and speed of metagenome analysis tools. Sci. Rep., 6, 19233.Lindner,M.S., and Renard,B.Y. (2013) Metagenomic abundance estimationand diagnostic testing on species level. Nucleic Acids Res., 41, e10.Liu,Y. et al. (2017) AFS: identiﬁcation and quantiﬁcation of species composition by metagenomic sequencing. Bioinformatics, 33, 1396–1398.Love,M.I. et al. (2014) Moderated estimation of fold change and dispersionfor RNA-seq data with DESeq2. Genome Biol., 15, 550.Lu,J. et al. (2017) Bracken: estimating species abundance in metagenomicsdata. PeerJ Comput. Sci., 3, e104.Luo,C. et al. (2015) ConStrains identiﬁes microbial strains in metagenomicdatasets. Nat. Biotechnol., 33, 1045–1052.M.Fischer et al.Mavromatis,K. et al. (2007) Use of simulated data sets to evaluate the ﬁdelityof metagenomic processing methods. Nat. Methods, 4, 495–500.McLoughlin,K. (2016) Technical report: benchmarking for quasispecies abundance inference with conﬁdence intervals from metagenomic sequence data.Tech. Rep., LLNL-TR-681108Mende,D.R. et al. (2012) Assessment of metagenomic assembly using simulated next generation sequencing data. Plos One, 7, e31386.Menzel,P. et al. (2016) Fast and sensitive taxonomic classiﬁcation for metagenomics with Kaiju. Nat. Commun., 7, 11257.Nawy,T. (2015) MICROBIOLOGY: the strain in metagenomics. Nat.Methods, 12, 1005.Nayfach,S. et al. (2016) An integrated metagenomics pipeline for strain proﬁling reveals novel patterns of bacterial transmission and biogeography.Genome Res., 26, 1612–1625.Neelakanta,G., and Sultana,H. (2013) The use of metagenomic approaches toanalyze changes in microbial communities. Microbiol. Insights, 6, 37–48.Ondov,B.D. et al. (2016) Mash: fast genome and metagenome distance estimation using MinHash. Genome Biol., 17, 132.Oulas,A. et al. (2015) Metagenomics: tools and insights for analyzingnext-generation sequencing data derived from biodiversity studies.Bioinformatics Biol. Insights, 9, 75–88.Ounit,R. et al. (2015) CLARK: fast and accurate classiﬁcation of metagenomicand genomic sequences using discriminative k-mers. BMC Genomics, 16,236.Pignatelli,M., and Moya,A. (2011) Evaluating the ﬁdelity of de novo shortread metagenomic assembly using simulated data. PloS One, 6, e19984.Parks,D.H. et al. (2014) STAMP: statistical analysis of taxonomic and functional proﬁles. Bioinformatics, 30, 3123–3124.Parks,D.H. and Beiko,R.G. (2010) Identifying biologically relevant differencesbetween metagenomic communities. Bioinformatics, 26, 715–721.Paulson,J.N. et al. (2013) Differential abundance analysis for microbialmarker-gene surveys. Nat. Methods, 10, 1200–1202.Peng,X. et al. (2015) Zero-inﬂated beta regression for differential abundanceanalysis with metagenomics data. J. Comput. Biol., 23, 102–110.Robinson,M.D. et al. (2010) edgeR: a Bioconductor package for differentialexpression analysis of digital gene expression data. Bioinformatics, 26,139–140.Rosen,M.J. et al. (2015) Microbial diversity. Fine-scale diversity and extensiverecombination in a quasisexual bacterial population occupying a broadniche. Science, 348, 1019–1023.Schaeffer,L. et al. (2017) Pseudoalignment for metagenomic read assignment.Bioinformatics, DOI: 10.1093/bioinformatics/btx106.Scholz,M. et al. (2016) Strain-level microbial epidemiology and populationgenomics from shotgun metagenomics. Nat. Methods, 13, 435–438.Sczyrba,A. et al. (2017) Critical Assessment of Metagenome Interpretation – abenchmark of computational metagenomics software. bioRxiv, 99127.Segata,N. et al. (2011) Metagenomic biomarker discovery and explanation.Genome Biol., 12, R60.Segata,N. et al. (2012) Metagenomic microbial community proﬁling usingunique clade-speciﬁc marker genes. Nat. Methods, 9, 811–814.Shapiro,B.J. et al. (2012) Population genomics of early events in the ecologicaldifferentiation of bacteria. Science, 336, 48–51.Snitkin,E.S. et al. (2011) Genome-wide recombination drives diversiﬁcation ofepidemic strains of Acinetobacter baumannii. Proc. Natl. Acad. Sci. USA,108, 13758–13763.Sohn,M.B. et al. (2015) A robust approach for identifying differentially abundant features in metagenomic samples. Bioinformatics, 31, 2269–2275.White,J.R. et al. (2009) Statistical methods for detecting differentially abundant features in clinical metagenomic samples. PLoS Comput. Biol., 5,e1000352.Wood,D.E., and Salzberg,S.L. (2014) Kraken: ultrafast metagenomic sequenceclassiﬁcation using exact alignments. Genome Biol., 15, R46.Wooley,J.C. et al. (2010) A primer on metagenomics. PLoS Comput. Biol., 6,e1000667.Xia,L.C. et al. (2011) Accurate genome relative abundance estimation basedon shotgun metagenomic reads. PloS One, 6, e27992.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i124/3953953by gueston 07 January 2018
5028881971002	PMID28881971	5028881971	https://watermark.silverchair.com/btx236.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881971.main.pdf	Bioinformatics, 33, 2017, i118–i123doi: 10.1093/bioinformatics/btx236ISMB/ECCB 2017Modelling haplotypes with respect to referencecohort variation graphsYohei Rosen, Jordan Eizenga and Benedict Paten*Baskin School of Engineering, UC Santa Cruz Genomics Institute, University of California Santa Cruz, Santa Cruz,CA 95064, USA*To whom correspondence should be addressed.AbstractMotivation: Current statistical models of haplotypes are limited to panels of haplotypes whosegenetic variation can be represented by arrays of values at linearly ordered bi- or multiallelic loci.These methods cannot model structural variants or variants that nest or overlap.Results: A variation graph is a mathematical structure that can encode arbitrarily complex geneticvariation. We present the ﬁrst haplotype model that operates on a variation graph-embeddedpopulation reference cohort. We describe an algorithm to calculate the likelihood that a haplotypearose from this cohort through recombinations and demonstrate time complexity linear in haplotype length and sublinear in population size. We furthermore demonstrate a method of rapidlycalculating likelihoods for related haplotypes. We describe mathematical extensions to allow modelling of mutations. This work is an important incremental step for clinical genomics and geneticepidemiology since it is the ﬁrst haplotype model which can represent all sorts of variation in thepopulation.Availability and Implementation: Available on GitHub at https://github.com/yoheirosen/vg.Contact: benedict@soe.ucsc.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 BackgroundStatistical modelling of individual haplotypes within populationdistributions of genetic variation dates back to the Kingman (1982)n-coalescent. In general, the coalescent and other models describehaplotypes as generated from some structured state space via recombination and mutation events.Although coalescent models are powerful generative tools, theircomputational complexity is unsuited to inference on chromosomelength haplotypes. Therefore, the dominant haplotype likelihoodmodel used for statistical inference is the Li and Stephens (2003)model (LS) and its various modifications. LS closely approximatesthe more exact coalescent models but admits implementations withrapid runtime.Orthogonal to statistical models, another important frontier ingenomics is the development of the variation graph, as described inPaten et al. (2014). This is a structure which encodes the wide varietyof variation found in the population, including many types of variation which cannot be represented by conventional models. Variationgraphs are a natural structure to represent reference cohorts of haplotypes since they encode haplotypes in a canonical manner: as node sequences embedded in the graph (see Novak et al., 2016).Dilthey et al. (2015) demonstrate the benefit of incorporating agraph representation of population information into a model for genome inference. However, their model does not account for haplotypephasing. In this paper, we present the first statistical model for haplotype modelling with respect to graph-embedded populations.We also describe an efficient algorithm for calculating haplotypelikelihoods with respect to large reference panels. The algorithmmakes significant use of the graph positional Burrows-Wheeler transform (gPBWT) index of haplotypes described by Novak et al. (2016).2 Materials and methods2.1 Encoding the full set of human variationHaplotypes in the Kingman (1982) n-coalescent and Li and Stephens(2003) models are represented as sequences of values at linearlyordered, non-overlapping binary loci. Some authors model multiallelic loci (for example, single base positions taking on values of A, C,T, G or gap) as in Lunter (2016), but all assume that the entirety ofgenetic variation can be expressed by values at linearly ordered loci.However, many types of genetic variation cannot be represented in this manner. Copy number variations, inversions orCV The Author 2017. Published by Oxford University Press.i118This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i118/3953952by gueston 07 January 2018Modelling haplotypes with respect to reference cohort variation graphstranspositions of sequence create cyclic paths which cannot be totally ordered. Large population cohorts such as the 1000 GenomesProject Consortium et al. (2015) project data contain simple insertions, deletions and substitution at a sufficient density that thesevariants sometimes overlap or nest into structures not representableby linearly ordered sites. Two examples of this phenomenon from1000 Genomes data [Phase 3 Variant Call Format file (VCF)] forchromosome 22 are pictured in Figure 1.In order to represent these more challenging types of variation,we use a variation graph. This is a type of sequence graph—a mathematical graph in which nodes represent elements of sequence, augmented with 50 and 30 sides, and edges are drawn between sides if theadjacency of sequence is observed in the population cohort (seePaten et al., 2017). Haplotypes are embedded as paths through oriented nodes in the graph. We are able to represent novel recombinations, deletions, copy number variations or other structural eventsby adding paths with new edges to the graph, and novel inserted sequence by paths through new nodes.2.2 Adapting the recombination component of LS tographsThe Li and Stephens (2003) model (LS) can be described by anHMM with a state space consisting of previously observed haplotypes and observations consisting of the haplotypes’ alleles at loci.Recombinations correspond to transitions between states and mutations are modelled within the emission probabilities. Since variationgraphs encode full nucleic acid sequences rather than lists of sites weextend the model to allow recombinations at base-pair resolution rather than just between loci.Let G denote a variation graph. Let S ðGÞ be the set of allpossible finite paths visiting oriented nodes of G. A path h in S ðGÞencodes a potential haplotype. A variation graph posesses anembedded population reference cohort H which is a multisetof haplotypes p 2 S ðGÞ. Given a pair ðG; HÞ, we seek the likelihood PðhjG; H Þ that h arose from haplotypes in H viarecombinations.Recall that every oriented node of G is labelled with a nucleicacid sequence. Therefore, every path h 2 S corresponds to a nucleicacid sequence seqðhÞ formed by concatenation of its node labels.We represent recombinations between haplotypes by assemblingsubsequences of these sequences seqðhÞ for h 2 H. We call ai119concatenation of such subsequences a recombination mosaic. This ispictured in Figure 2.We can assign a likelihood to a mosaic x by analogy withthe recombination model from LS. Assume that nucleotide in xhas precisely one successor in each p 2 H to which it couldrecombine. Then, between each base pair, we assign a probabilitypr of recombining to a given other p 2 H, and therefore a probability ð1 À ðjHj À 1Þpr Þ of not recombining. Write pc forð1 À ðjHj À 1Þpr Þ.By the same argument underlying the LS recombination model,we then we have a probability of a given mosaic having arisen fromðG; HÞ through recombinations:PðxjG; HÞ ¼ pRðxÞ pjxjÀRðxÞrc(1)where jxj is the length of x in base pairs and RðxÞ the number of recombinations in x. We will use this to determine the probability PðhjG; HÞ for a given h 2 S ðGÞ, noting that multiple mosaics x cancorrespond to the same node path h 2 S ðGÞ.Given a haplotype h 2 S ðGÞ, let vðhÞ be the set of all mosaicsinvolving the same path through the graph as h. The law of totalprobability givesXPðhjG; HÞ ¼PðxjG; HÞ(2)x2vðhÞ¼XpjhjÀRðxÞ pRðxÞ ¼ pjhjcrx2vðhÞ c   RðxÞprx2vðhÞ pcX(3)prLet q :¼ pc ; then PðhjG; H Þ is proportional to a qRðxÞ -weighted enumeration of x 2 vðhÞ.We can extend this model by allowing recombination rate pðnÞand effective population size jHjeff ðnÞ to vary across the genome according to node n 2 G in the graph. Varying the effective populationsize allows the model to remain sensible in regions traversed multiple times by cycle-containing haplotypes. In our basic implementation we will assume that pðnÞ is constant and jHjeff ðnÞ ¼ jHj;however varying these parameters does not add to the computational complexity of the model.2.3 A linear-time dynamic programming for likelihoodcalculationPWe wish to calculate the sum x2vðhÞ qRðxÞ efficiently. (See (3) above)We will achieve this by traversing the node sequence h left-to-right,computing the sum for all prefixes of h. Write hb for the prefix of hending with node b.DEFINITION 1. A subinterval s of a haplotype h is a contiguoussubpath of h. Two subintervals s1 ; s2 of haplotypes h1 ; h2 are consistent if s1 ¼ s2 as paths, however we distinguish them as separateobjects.DEFINITION 2. Given a indices a; b of nodes of a haplotype h, Sa isbthe set of subintervals sÃ of p 2 H such that1. there exists a subinterval s of h which begins with a, ends with band is consistent with sÃ2. there exists no such subinterval of p which begins with a À 1,the node before a in h (left-maximality)DEFINITION 3. For a given prefix hb of h and a subinterval sÃ of ahaplotype p 2 H, define the subset vðhÞsÃ   vðhÞ as the set of all mosaics whose rightmost segment arose as a subsequence of sÃ .Fig. 1. Two examples of non-linearly orderable loci in a graph of 1000Genomes variation data for chromosome 22 which form overlapping ornested sitesThe following result is key to being able to efficiently enumeratemosaics:Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i118/3953952by gueston 07 January 2018i120Y.Rosen et al.Fig. 2. The labelled path shows the recombination mosaic x superimposed on the embedded haplotypes H in our 1000 Genomes project chr 22 graph; below, xis mapped onto its nucleic acid sequenceCLAIM 1. If s1 ; s2 2 Sa for some a, then there exists abrecombination-count preserving bijection between vðhb Þs1 andvðhb Þs2 .PROOF. See Supplementary Material.COROLLARY 1. If we defineXRb ðsi Þ :¼qRðxÞx2vðh Þ(4)b sithen Rb ðs1 Þ ¼ Rb ðs2 Þ if s1 ; s2 2 Sa for some a. Call this shared valuebRb ðaÞ.DEFINITION 4. Ab is the set of all nodes a 2 G such that Sa isbnonempty.Using these results, the likelihood Pðhb jG; HÞ of the prefix hbending at index b can be written asXXjhPðhb jG; H Þ ¼ pjhb j s Rb ðsi Þ ¼ pc b j a2A jSa Rb ðaÞ(5)cbibLet b À 1 represent the node preceding b in h; we wish to showthat if we know RbÀ1 ðaÞ for all a 2 AbÀ1 , we can calculate Rb ðaÞ forall a 2 Ab in constant time with respect to jhj. This can be recognized by inspection of the following linear transformation:Rb ðaÞ ¼ qfs ðw; ‘ÞðA þ BÞþ1a6¼b ð1 À qÞðft ð‘ÞRbÀ1 ðaÞþfs ðw; ‘Þ þ ft ð‘ÞAÞw(6)Pwhere w ¼ a jSa j, fs ðw; ‘Þ :¼ ð1 þ ðw À 1ÞqÞ‘À1 , ft ð‘Þ :¼ ð1 À qÞ‘À1 ,band A; B are the jAbÀ1 j-element sumsXA :¼jSa jRbÀ1 ðaÞ;(7)ba2AbÀ1B :¼Xa2AbÀ1Â aÃjSbÀ1 j À jSa j RbÀ1 ðaÞb(8)Proof that (6) computes Rb ðÁÞ from RbÀ1 ðÁÞ is straightforward butlengthy and therefore deferred to the Supplementary Material.If we assume memoization of the polynomials fs ðh; ‘Þ; ft ð‘Þ, andknowledge of w; ‘ and all jSa j’s, then all Rb ðaÞ’s can be calculated tobgether in two shared jAbÀ1 j-element sums (to calculate A and A þ B)followed by a single sum per Rb ðaÞ. Therefore, by computingincreasing prefixes hb of h, we can compute PðhjG; H Þ in timecomplexity which is Oðn Á mÞ in n ¼ jhj, and m ¼ maxb jAb j. Thelatter quantity is bounded by jHj in the worst theoretical case;we will show experimentally that runtime is asymptotically sublinear in jHj.2.4 Using the gPBWT to enumerate equivalence classesin linear timeThe gPBWT index described by Novak et al. (2016) is a succinctdata structure which allows for linear-time subpath search in a variation graph. This is graph analogue of the positional BurrowsWheeler transform by Durbin (2014) which is used in the Lunter(2016) fast implementation of the Viterbi algorithm in the LS model.Like other Burrows-Wheeler transform variants, the gPBWT possesses a subsequence search function which returns intervals in asorted path index.Novak et al. (2016) prove that the gPBWT allows OðnÞ query ofthe number of subintervals from a set of graph-embedded paths containing a sequence of length n. Therefore, for any indices a; b in apath h we can compute the following quantity in Oðb À aÞ time.aDEFINITION 5. Jb :¼ the number of subpaths in H matching h between nodes a and b.aSince we can cache the search interval used to compute Jb fromathe gPBWT, we can also calculate Jb in Oð1Þ time given that weahave already computed JbÀ1 . This is important becauseaaaÀ1CLAIM 2. jSb j ¼ Jb À JbProof.By straightforward manipulation of definitions 2 and 5.And therefore, if we have already calculated fjSa j : a 2 AbÀ1 g,bÀ1then in order to compute fjSa j : a 2 Ab g, we need only perform jbAbÀ1 j Oð1Þ extensions of the gPBWT search intervals used to compute the jSa j’s and one additional Oð1Þ query to compute jSb j.bÀ1bTherefore, we can compute all nonzero values jSa j, for indices abb of h, using jAbÀ1 j þ 1 Oð1Þ gPBWT search interval extensionsfor each node b 2 h. This makes the calculation of all such nonzerojSa j’s calculable in Oðn Á mÞ time overall, where n ¼ jhj andbm ¼ maxb jAb j. This result, combined with the results of Section 2.3,show that we can calculate PðhjG; H Þ in Oðn Á mÞ time, for n ¼ jhjand m ¼ maxb jAb j.2.5 Modelling mutationsWe can assign to two haplotypes h; h0 the probability Pm ðhjh0 Þ thath arose from h0 through a mutation event. As in LS model, we canassume conditional independence properties such thatXPtot ðhjG; H Þ ¼P ðhjh0 ÞPr ðh0 jG; H Þ(9)h0 2seqðGÞ mIt is reasonable to make the simplifying assumption that Pm ðhjh0 Þ ¼ 0unless h0 differs from h exclusively at short, non-overlapping substitutions, indels and cycles since more dramatic mutation events arevanishingly rare. This assumption is implicitly contained in then-coalescent and LS models by their inability to model more complexmutations.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i118/3953952by gueston 07 January 2018Modelling haplotypes with respect to reference cohort variation graphsi121Fig. 3. A sketch of the ﬂow of information in the likelihood calculation algorithm described. Blue arrows a represent the rectangular decomposition, RÁ ðÁÞare preﬁx likelihoodsDetection of all simple sites in the graph traversed by h can beachieved in linear time with respect to the length of h. The numberof such paths remains exponential in the number of simple sites.However, our model allows us to perform branch-and-bound typeapproaches to exploring these paths. This is possible since we cancalculate upper bounds for likelihood from either a prefix, or frominterval censored haplotypes where we do not specify variantswithin encapsulated regions in the middle of the path.Furthermore, it is evident from our algorithm that if two pathsshare the same prefix, then we can reuse the calculation over thisprefix. If two paths share the same suffix, in general we only need torecompute the jSa j values for a small number of nodes. This is dembonstrated in Section 4.2.3 ImplementationWe implemented the algorithms described in C þþ, building on thevariation graph toolkit vg by Garrison (2016). This is found in the‘haplotypes’ branch at https://github.com/yoheirosen/vg. No comparable graph-based haplotype models exist, so we could not provide comparative performance data; absolute performance on asingle machine is presented instead.4 Results4.1 Runtime for individual haplotype queriesWe assessed time complexity of our likelihood algorithm using theimplementation described above. Tests were run on single threads ofan Intel Xeon X7560 running at 2.27 GHz.To assess for time dependence on haplotype length, we measuredruntime for queries against a 5008 haplotype graph of humanchromosome 22 built from the 1000 Genomes Phase 3 VCF onthe hg19 assembly created using vg and 1000 Genomes ProjectConsortium et al. (2015) project data. Starting nodes and haplotypesat these nodes were randomly selected, then walked out to specificlengths. In our graph, 1 million nodes correspond, on average, to16.6 million base pairs. Reported runtimes are for performingboth the rectangular decomposition and likelihood calculation steps(Fig. 4).The observed relationship (see Fig. 4) of runtime to haplotypelength is consistent with OðnÞ time complexity with respect ton ¼ jhj.We also assessed the effect of reference cohort size on runtime.Random subsets of the 1000 Genomes data were made usingvcftools (Danecek et al., 2011) and our graph-building process wasrepeated. Five replicate subset graphs were made per population sizewith the exception of the full population graph of 2504 individuals.We observe (see Fig. 5) an asymptotically sublinear relationshipbetween runtime and reference cohort size.Fig. 4. Runtime (s) versus haplotype length (nodes) for Chr 22 1000 Genomesdata. Line with slope 1:01 and R 2 ¼ 0:972 was ﬁtted to samples with length>50 000 nodes in the log-log plot. This supports a OðnÞ time complexity withrespect to haplotype length4.2 Time needed to compute the rectangulardecomposition of a haplotype formed by arecombination of two previously queried haplotypesThe assessments described above are for computing the likelihood ofa single haplotype in isolation. However, haplotypes are generallysimilar along most of their length. It is straightforward to generaterectangular decompositions for all haplotypes h 2 H in the population reference cohort by a branching process, where rectangular decompositions for shared prefixes are calculated only once. This willcapture all variants observed in the reference cohort.Haplotypes not in the reference cohort can then be generatedthrough recombinations between the h 2 H. If this produces anotherhaplotype also in H, it suffices to recognize this fact. If not, thengiven that h is formed by a recombination of h1 and h2 , then h mustcontain some sequence of nodes c ! j contained in neither h1 norh2 . We only need to recalculate Sa for ajb.bWe have implemented methods to recognize these nodes and perform the necessary gPBWT queries to build the rectangular decomposition for h. The distribution of time taken (in milliseconds) togenerate this new rectangular decomposition for randomly chosenh1 ; h2 and recombination point is shown in Figure 6.Mean time is 141 ms, median time 34 ms, first quartile time12 ms and third quartile time 99 ms. To compute a rectangular decomposition from scratch mean time is 71 160 ms, first quartile time68 690 ms and third quartile time 73 590 ms.This rapid calculation of rectangular decompositions formed by recombinations of already-queried haplotypes is promising for the feasibility of a mutation model or of sampling the likelihoods of large numbersof haplotypes. Similar methods for the likelihood computation usingthis rectangular decomposition are a subject of our current research.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i118/3953952by gueston 07 January 2018i122Y.Rosen et al.Fig. 5. Runtime (s) versus reference cohort size (diploid individuals) forchromosome 22 1000 Genomes data. Line with slope 0:27 and R 2 ¼ 0:888was ﬁtted to samples with population size >300 individuals in the log-logplot. This supports an asymptotically sublinear time complexity with respectto reference cohort sizeFig. 7. Left: density plot of relative log-likelihood of reads not containing variants below 5% prevalence or novel recombinations (black line) versus readscontaining novel recombinations. Right, density plot of relative log-likelihoodof reads not containing variants below 5% prevalence or novel recombinations (black line) versus reads containing variants present at under 5%prevalence and under 1% prevalenceFig. 6. Distribution of times (in milliseconds) required to recompute the rectangular decomposition of a haplotye given that it was formed by recombination of two haplotypes for which rectangular decompositions have beenconstructed. This graph omits 0.6% of observations which are outliers beyond 1 s of timeWe define a read to contain n ‘novel recombinations’ if it is asubsequence of no haplotype in the reference, but it could be madeinto one using a minimum of n recombination events. We define theprevalence of the rarest variant of a read to be the lowest percentageof haplotypes in the index which pass through any node in the read’ssequence.We segregated our set of mapped reads according to these features. We make three qualitative observations, which can beobserved in (Fig. 7). First, the likelihood of a read containing a novelrecombination is lower than one without any novel recombinations.Second, this likelihood decreases as novel recombinations increase.Third, the likelihood of a read decreases with decreasing prevalenceof its rarest variant.A further comparison (Fig. 8) of these same mapped readsagainst reads which were randomly simulated without regard tohaplotype structure shows that the majority of mapped reads fromNA12878 score are assigned higher relative log-likelihoods than themajority of randomly simulated reads.4.3 Qualitative assessment of the likelihood function’sability to reflect rare-in-reference features in reads5 ConclusionsWe used vg to map the 1000 Genomes low coverage read set for individual NA12878 on chromosome 22 against the variation graphdescribed previously. 1 476 977 reads were mapped. Read likelihoods were computed by treating each read as a short haplotype.These likelihoods were normalized to ‘relative log-likelihoods’ bycomputing their log-ratio against the maximum theoretical likelihood of a sequence of the same length. An arbitrary value of 10À9was used for precomb .We have introduced a method of describing a haplotype with respectto the sequence it shares with a variation graph-encoded referencecohort. We have extended this into an efficient algorithm for haplotype likelihood calculation based on the gPBWT described byNovak et al. (2016). We applied this method to a full-chromosomegraph consisting of 5008 haplotypes from the 1000 Genomes dataset to show that this algorithm can efficiently model recombinationwith respect to both long sequences and large reference cohorts.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i118/3953952by gueston 07 January 2018Modelling haplotypes with respect to reference cohort variation graphsi123FundingY.R. is supported by a Howard Hughes Medical Institute Medical ResearchFellowship. This work was also supported by the National Human GenomeResearch Institute of the National Institutes of Health under Award Number[5U54HG007990] and grants from the W.M. Keck foundation and theSimons Foundation. The content is solely the responsibility of the authors anddoes not necessarily represent the ofﬁcial views of the National Institutes ofHealth.Conﬂict of Interest: none declared.ReferencesFig. 8. Density plot of relative log-likelihood of mapped reads versus randomly generated simulated haplotypesThis is an important proof of concept for translating haplotypemodelling to the breadth of genetic variant types and structures representable on variation graphs.Our basic algorithm does not directly model mutation, howeverwe describe an extension which does. Making this extension computationally tractable will depend on being able to very rapidly compute likelihoods of sets of similar haplotypes. We demonstrate thatour algorithm can be modified to compute rectangular decompositions for haplotypes related by a recombination event inmillisecond-range times. We have also devised mathematical methods for recomputing likelihoods of similar haplotypes which takeadvantage of analogous redundancy properties; however, they haveyet to be implemented and tested. However, we anticipate that wewill be able to compute likelihoods of large sets of related haplotypes on a time scale which makes modelling mutation feasible.AcknowledgementsWe thank Wolfgang Beyer for his variation graph visualizations, on whichFigures 1 and 2 are based.1000 Genomes Project Consortium. et al. (2015) A global reference for humangenetic variation. Nature, 526, 68–74.Danecek,P. et al. (2011) The variant call format and VCFtools. Bioinformatics,27, 2156–2158.Dilthey,A. et al. (2015) Improved genome inference in the MHC using a population reference graph. Nat. Genet., 47, 682–688.Durbin,R. (2014) Efﬁcient haplotype matching and storage using the positional Burrows–Wheeler transform (PBWT). Bioinformatics, 30,1266–1272.Garrison,E. (2016). vg: the variation graph toolkit. https://github.com/vgteam/vg/blob/80e823f5d241796f10b7af6284e0d3d3d464c18f/doc/paper/main.tex(20 March 2017, date last accessed).Kingman,J.F. (1982) On the genealogy of large populations. J. Appl. Prob.,19(A), 27–43.Li,N. and Stephens,M. (2003) Modeling linkage disequilibrium and identifying recombination hotspots using single-nucleotide polymorphism data.Genetics, 165, 2213–2233.Lunter,G. (2016). Fast haplotype matching in very large cohorts using the Liand Stephens model. bioRxiv doi:10.1101/048280.Novak,A.M. et al. (2016). A graph extension of the positional BurrowsWheeler transform and its applications. In International Workshop onAlgorithms in Bioinformatics, pp. 246–256, Springer, Aarhus, Denmark.Paten,B. et al. (2014). Mapping to a reference genome structure. arXiv preprint arXiv:1404.5010.Paten,B. et al. (2017). Superbubbles, ultrabubbles and cacti. Proceedings ofRECOMB 2017, Hong Kong, doi:10.1101/101493.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i118/3953952by gueston 07 January 2018
5028881970002	PMID28881970	5028881970	https://watermark.silverchair.com/btx235.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881970.main.pdf	Improving the performance of minimizers andwinnowing schemesGuillaume Marçais1,*, David Pellow2, Daniel Bork1, Yaron Orenstein3,Ron Shamir2 and Carl Kingsford1,*1Computational Biology Department, Carnegie Mellon University, Pittsburgh, PA, USA, 2Blavatnik School ofComputer Science, Tel-Aviv University, Tel-Aviv, Israel and 3Computer Science and Artificial IntelligenceLaboratory, MIT, Cambridge, MA, USA*To whom correspondence should be addressed.AbstractMotivation: The minimizers scheme is a method for selecting k-mers from sequences. It is used inmany bioinformatics software tools to bin comparable sequences or to sample a sequence in a de-terministic fashion at approximately regular intervals, in order to reduce memory consumptionand processing time. Although very useful, the minimizers selection procedure has undesirable be-haviors (e.g. too many k-mers are selected when processing certain sequences). Some of theseproblems were already known to the authors of the minimizers technique, and the natural lexico-graphic ordering of k-mers used by minimizers was recognized as their origin. Many software toolsusing minimizers employ ad hoc variations of the lexicographic order to alleviate those issues.Results: We provide an in-depth analysis of the effect of k-mer ordering on the performance of theminimizers technique. By using small universal hitting sets (a recently defined concept), we showhow to significantly improve the performance of minimizers and avoid some of its worse behav-iors. Based on these results, we encourage bioinformatics software developers to use an orderingbased on a universal hitting set or, if not possible, a randomized ordering, rather than the lexico-graphic order. This analysis also settles negatively a conjecture (by Schleimer et al.) on the ex-pected density of minimizers in a random sequence.Availability and Implementation: The software used for this analysis is available on GitHub: https://github.com/gmarcais/minimizers.git.Contact: gmarcais@cs.cmu.edu or carlk@cs.cmu.edu1 IntroductionThe winnowing scheme was introduced by Schleimer et al. (2003) tofingerprint documents for plagiarism detection. Independently, theminimizers algorithm was introduced by Roberts et al. (2004b) tocompute overlaps between sequencing reads. Even though these algo-rithms were designed for different purposes, they are essentially identi-cal. The original minimizers scheme compares nucleotide k-mers usingthe lexicographic ordering, while the winnowing scheme hashes thek-grams of letters in a document, effectively randomizing the order.In the bioinformatics field, minimizers have since been used inmany different settings, such as binning input sequences (Deorowiczet al., 2015; Li and Yan, 2015; Roberts et al., 2004a,b), generatingsparse data structures (Grabowski and Raniszewski, 2015; Ye et al.,2012), and sequence classification (Wood and Salzberg, 2014). Allthese applications share the need for a small signature, or finger-print, in order to recognize longer exact matches between sequences.The winnowing scheme is defined as follows: given an input se-quence S and parameters k and w, select the smallest k-mer (accord-ing to a predefined ordering) in each window of w consecutivek-mers in S (such a window contains wþ k  1 bases). In this paper,we will interchangeably use the terms ‘minimizers scheme’ or ‘win-nowing scheme’. Either term does not imply a particular ordering aswe study the effect of various orderings on these schemes. The min-imizers (or fingerprints), are, depending on the application, eitherthe set of positions in S of the selected k-mers, or the set of selectedk-mers itself. The terms fingerprint and minimizer are interchange-able in the remainder of this paper.These schemes are designed to select a set of k-mers that is assparse as possible while satisfying the following two properties.First, the sequence is approximately uniformly sampled; that is, thedistance between two selected k-mers is always less than w. Second,if two sequences share an exact subsequence of length at leastVC The Author 2017. Published by Oxford University Press. i110This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i110–i117doi: 10.1093/bioinformatics/btx235ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i110/3953951by gueston 07 January 2018kþw  1, then those two sequences have at least one minimizer incommon.Many of the tools mentioned above do not use lexicographicordering of the k-mers, as originally proposed in Roberts et al.(2004b). It was recognized early on that homo-polymer runs, par-ticularly repeated As, can cause a lexicographic minimizer algorithmto select many consecutive k-mers as minimizers in genomics appli-cations, leading to a high minimizer density. Because A is the small-est letter, if the 5-mer AAAAC is selected as a minimizer in awindow, it is likely that the shifted 5-mer AAACx (for any base x)will also be selected as a minimizer in a subsequent window, andAACxy in the following window, and so on.Another problem with the lexicographic ordering is that any oneof the 4k possible k-mers can theoretically be chosen as a minimizerin some window. In particular, when k is short, many or even allpossible k-mers could appear as a minimizer. If the minimizers areused for binning sequences, then potentially a very large number ofbins will be necessary.Schleimer et al. defined the density of a k-mer selection schemefor a given sequence S as the fraction of the k-mers that are selected.Formally, let d A; S; kð Þ denote the density of procedure A andA(S, k) as the set of selected k-mers positions by A on sequence S,thend A; S; kð Þ ¼ jA S;kð ÞjjSj   kþ 1 : (1)The expected density of procedure A, d(A, k), is calculated by takingthe expectation over all possible sequences S, with every base chosenindependently with equal probability.Schleimer et al. show that the winnowing scheme with a randomordering has expected density of 2= wþ 1ð Þ. In practice, the minim-izers scheme with lexicographic ordering has density greater than2= wþ 1ð Þ. In addition, they define a local scheme as a procedurethat only has access to the sequence within a given window when se-lecting a k-mer. In other words, the k-mer selected from a windowcan be expressed as a function of only the identity and relativeordering of the bases within the window itself, and does not dependon the content of any other window or on the position of the win-dow within the overall sequence. They prove that the expected dens-ity of a local scheme has a lower bound of 1:5= wþ 1ð Þ, but theyconjecture that this bound cannot be achieved in practice, and thatthe true lower bound is in fact 2= wþ 1ð Þ. This conjecture wouldtherefore imply that the winnowing scheme with random order is anoptimal local scheme.In this paper, we investigate the effect of different orderings ofthe k-mers on the density of the winnowing scheme. Here we onlyconsider local schemes, and exclude global schemes, such as thecounting-based orderings used in Chikhi et al. (2015, 2016).By using universal k-mer hitting sets, as defined in Orenstein et al.(2016a), we show how to create orderings that lead to densitiessmaller than with the lexicographic or randomized ordering. Thesmall universal hitting sets created by the DOCKS algorithm de-veloped in Orenstein et al. (2016a) achieve densities below1:8= wþ 1ð Þ. As a consequence, the above conjecture of Schleimeret al. does not hold and densities below 2= wþ 1ð Þ are achievablewith a local scheme. Using some properties of the universal k-mer hit-ting sets, we also show that, surprisingly, the winnowing scheme withrandom ordering can itself have a density slightly below 2= wþ 1ð Þ.We also explain why the original minimizers procedure, using the lex-icographic order, performs worse than random ordering.Finally, we look at the potential effect of using universal hittingset orderings on bioinformatics applications. In the case of DNAsequence binning, such as performed by the k-mer counters KMC2(Deorowicz et al., 2015) and MSPKmerCounter (Li and Yan, 2015),we compare the distribution of the number and sizes of bins createdby different orderings proposed in the literature. The universal hit-ting sets ordering perform better than the other ordering in coupleof ways. First, the number of bins created has a known bound, un-like other orderings that can create as many bins as there are k-mers(4k). In practice, this bound is also much lower than the actual num-ber of bins created by other orderings. Second, the sizes of the binsare more uniform than with other orderings.Based on this analysis of the performance of the minimizers algo-rithm, we advise bioinformatics tool authors to use the winnowingscheme with an appropriate universal hitting set in their applicationif possible, and a random ordering otherwise, in lieu of the defaultlexicographic ordering.2 ApproachWe will first give an overview of the results in this paper. Formalproofs for the results mentioned in this section are presented in sub-sequent sections.In the original papers on minimizers and the winnowing schemes,the density is computed by considering any window of wþ1 consecu-tive k-mers. They make use of the following hypothesis:HYPOTHESIS 1. Every k-mer in a wþ 1ð Þ-long window has an equalprobability of 1= wþ 1ð Þ of being the smallest k-mer.Although not strictly true in practice, this hypothesis is reason-able and reflects reality accurately when using a randomized order-ing. We define the density factor df of a k-mer selection scheme A asthe density times wþ 1ð Þ, i.e.df A;kð Þ ¼ d A;kð Þ   wþ 1ð Þ: (2)Assuming that Hypothesis 1 holds, we can rephrase the theorems ofSchleimer et al. and Roberts et al. as:THEOREM 1 (Roberts et al., 2004b; Schleimer et al., 2003). UnderHypothesis 1, the density factor of the minimizers is df ¼ 2.A universal k-mer hitting set for given k and w is a set Uk;w ofk-mers such that every window of w consecutive k-mers must containan element from Uk;w. In Orenstein et al. (2016a), we introduced uni-versal hitting sets, and provided an efficient algorithm for construct-ing a compact universal set. These universal hitting sets do not needto contain all possible k-mers, and typically contain only about 1=k ofall the k-mers. Here, we connect the ideas of universal k-mer hittingsets with minimizers by creating an ordering where the elements ofUk;w are the smallest elements. Because every window contains anelement of Uk;w and these elements are smaller than other k-mers,only elements of Uk;w can be selected as minimizers. Consequently,k-mers that are not in Uk;w have probability 0 of being selected asminimizers and Hypothesis 1 does not hold anymore.We now consider a refined hypothesis.HYPOTHESIS 2. If a wþ 1ð Þ-long window contains j k-mers from auniversal hitting set, each of these k-mers has an equal probability of1=j of being the smallest k-mer.Motivated by the proof of Theorem 3 (see below), we define:DEFINITION 2. The sparsity of a universal hitting set Uk;w; SP Uk;w   ,is the proportion of wþ 1ð Þ-long windows containing only one k-mer from Uk;w.Assuming that Hypothesis 2 holds, we obtain a new estimationof the density factor.Improving the performance of minimizers and winnowing schemes i111Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i110/3953951by gueston 07 January 2018THEOREM 3. Under Hypothesis 2, the density factor for the minim-izers scheme with universal hitting sets Uk;w isdf ¼ 2 1   SP Uk;w      : (3)Note that when using the scheme we propose below, Hypothesis 2provides no practical constraint on the use of minimizers in applica-tions. Since any window will contain an element of Uk;w by definition,any application assuming Hypothesis 1 can be made to work underHypothesis 2 through the use of the appropriate k-mer ordering.The expected density of the minimizers scheme for a given order-ing is defined by an expectation over the set of all random se-quences. So the density of minimizers computed for a givensequence might differ from the expected density. On the other hand,we will show how the expected density can be computed exactly bycomputing the density of minimizers on a particular sequence.LEMMA 4. The expected density of a minimizers scheme with param-eters k and w is equal to the density of minimizers on any de Bruijnsequence of order kþw.We also show that Theorem 3 provides a good approximation tothe density for minimizers with universal hitting sets and exhibitcounterexamples to the Schleimer et al. conjecture.THEOREM 5. The density factor of a local scheme can be less than 2.The original intent of the minimizers is to select k-mers from asequence as uniformly and sparsely as possible. Theorem 5 showsthat schemes based on universal hitting set get closer to that goal.Using these schemes can greatly improve the performance of bio-informatics applications (See Section 4).3 Proofs and mathematical details3.1 Density with random orderingFirst, we succinctly reproduce the proof from Schleimer et al. (2003)and Roberts et al. (2004b) of Theorem 1.THEOREM 1. (Roberts et al., 2004b; Schleimer et al., 2003). UnderHypothesis 1, the density factor of the minimizers is df ¼ 2.PROOF: Define a charged window Wi of w consecutive k-mers start-ing at position i in a sequence S as a window such that the smallestk-mer is different in Wi than in Wi 1. That is, as we sweep throughthe sequence from left to right, we charge the left-most window inwhich a given fingerprint is first selected. The number of fingerprintsselected by the winnowing scheme is equal to the number of win-dows that are charged. Define the random variable Xi to be 1 if Wiis charged and 0 if not. Then the expected density can be written asd ¼ EPi Xi   =n ¼Pi E Xi½  =n, where n ¼ jSj   kþ 1 is the numberof k-mers in the sequence.Consider the larger window W0i of wþ1 k-mers starting at pos-ition i – 1 (Wi   W0i), and the smallest k-mer m in W0i (see Fig. 1). Ifm starts at position i – 1, then Wi must be charged, as the previousfingerprint is now outside of the window Wi. If m starts at positioniþw  1, then Wi must be charged, as the new smallest k-mer justarrived in Wi. In all other cases, Wi and Wi 1 selected the same fin-gerprint and Wi is not charged. Assuming that the minimum m hasequal probability to be in any of the positions i  1; iþw  1½  , thenE Xi½   ¼ P Xi ¼ 1½   ¼ 2= wþ 1ð Þ. So d ¼ 2= wþ 1ð Þ and df¼2. h3.2 Computing the densityEven though the density is defined as an expectation over all pos-sible random sequences where the bases are uniformly andindependently chosen, for small values of k and w, it is possible inpractice to compute the exact value of the expected density.A de Bruijn sequence (de Bruijn, 1946) of order k on the alpha-bet R is a cyclic sequence that contains every possible k-mer as a sub-string exactly once and has length rk, where r is the number ofsymbols in the alphabet.LEMMA 4. The expected density of a minimizers scheme with param-eters k and w is equal to the density of minimizers on any de Bruijnsequence of order kþw.PROOF: As seen in the proof of Section 3.1, whether a window Wi ischarged depends only on the sequence of the wþ 1ð Þ-long windowW0i. Under the assumption that the input sequence is random, eachwþ 1ð Þ-long window has a probability of 1=rwþk. Hence, thedensity is the number of wþ 1ð Þ-long windows W0 that cause thewindow W to be charged, divided by rwþk. By computing the dens-ity on increasingly long random input sequences, the density wouldconverge to the expected density as the proportion of the wþ 1ð Þ-windows converge to 1=rwþk.A de Bruijn sequence of order kþw contains every one of therwþk possible wþ 1ð Þ-windows of k-mers exactly once (since it con-tains every wþ kð Þ-long sequence exactly once). In other words, thisde Bruijn sequence has all the wþ 1ð Þ-windows in exactly thedesired proportion. Therefore, we can compute the expected densityexactly by computing the density on the de Bruijn sequence of orderwþk. h3.3 Universal hitting set orderingThe de Bruijn graph Gk on a fixed alphabet R of size r is a directedgraph whose vertices are all the k-mers and a directed edge (u, v) rep-resents a kþ 1ð Þ-mer with u as its prefix and v as its suffix. A univer-sal hitting set for the parameters k and w is a set of k-mers, Uk;w, sothat any string of length wþ k  1 over R contains a substring fromUk;w. Equivalently, every walk of length w over the nodes in the deBruijn graph Gk contains at least one k-mer from Uk;w. In otherwords, it is a set of nodes that covers (has non-empty intersectionwith) all the length-w walks. (Here and throughout, the length of awalk is the number of vertices along it.) The set V Gkð Þ of all thenodes of Gk is trivially a universal hitting set, showing that the con-cept is well defined. In Orenstein et al. (2016a), a heuristic is given togenerate smaller universal hitting sets. The existence of such sets withcertain desired properties is further discussed in Section 3.5.Given a universal hitting set Uk;w, we define an ordering < Uk;won the k-mers as follows: u< Uk;wv if u 2 Uk;w and v 62 Uk;w; other-wise, u< Uk;wv if u is less than v in lexicographical order. In otherwords, the ordering makes the elements of the universal hitting setFig. 1. Windows Wi, starting at position i, and window W 0i starting at positioni – 1. There are 3 different qualitative cases for the start position of the small-est k-mer m: i – 1 (left dot), i þ w   1 (right dot) or in the range ½i ; i þ w   2 i112 G.Marçais et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i110/3953951by gueston 07 January 2018the smallest elements, and uses lexicographic order within the uni-versal hitting set and its complement.The most important property of this ordering is that, in eachwindow of w consecutive k-mers, the smallest element must be anelement of Uk;w.3.4 Density with universal hitting setsWith the ordering defined by a universal hitting set, we use the moreappropriate Hypothesis 2 to prove the following theorem.THEOREM 3. Under Hypothesis 2, the density factor for the minim-izers scheme with universal hitting sets Uk;w isdf ¼ 2 1   SP Uk;w      : (3)PROOF: We adapt the proof of Section 3.1 to the ordering < Uk;w . Withthis ordering, only the elements of Uk;w are selected as fingerprints, andthe number of such elements in the wþ 1ð Þ-long window varies be-tween 1 and wþ1. We can compute the probability that a window ischarged depending on how many elements from Uk;w it contains. Let’sdefine the event Yi;j that the window Wi is charged, given that there arej elements of Uk;w in the longer window W0i that contains it. Then:Yi;j ¼ Xi ¼ 1 jjUk;w \W0ij ¼ jn o(4)P Xi ¼ 1½   ¼Xwþ1j¼1P Yi;j   P jUk;w \W 0ij ¼ j   : (5)We can further condition the event Yi;j on whether the k-mers mi 1and miþw 1, respectively at positions i – 1 and iþw  1, are in theuniversal hitting set. For 2   j < w:P Yi;j   ¼w  1j  1  wþ 1j  P Yi;jjmi 1 2 Uk;w;miþw 1 62 Uk;w   (6)þw  1j  1  wþ 1j  P Yi;j jmi 1 62 Uk;w;miþw 1 2 Uk;w   (7)þw  1j  2  wþ 1j  P Yi;j jmi 1 2 Uk;w;miþw 1 2 Uk;w   (8)þw  1j  wþ 1j  P Yi;j jmi 1 62 Uk;w;miþw 1 62 Uk;w   : (9)The factor w  1j  1   = wþ 1j   in (6) expresses the number of waysto place the j – 1 k-mers of Uk;w in the w – 1 positions available(from i to iþw  2), given that mi 1 is in Uk;w and miþw 1 is not inUk;w. The other factors are analogous.For 2   j < w, we assume, based on Hypothesis 2, that all ofthe j k-mers from Uk;w \W 0i have equal probability to be the small-est for the order < Uk;w . Therefore the conditional probabilities equal1=j for (6) and (7), 2=j for (8) and 0 for (9). HenceP Yi;j   ¼2w  1j  1  þ w  1j  2    jwþ 1j   ¼2wj  1  jwþ 1j   ¼ 2wþ 1 :For j¼w, term (9) is omitted, as all of but one the k-mers in W 0i arein Uk;w. For j ¼ wþ 1, which corresponds to the case in the proof ofSection 3.1, only term (8) is present, as all the k-mers in W0i are inUk;w. Both these cases lead to the same result as the general case,hence P Yi;j   ¼ 2= wþ 1ð Þ; for 2   j   wþ 1.On the other hand, for j¼1, P Yi;1   ¼ 0. (This distinction motiv-ates our definition of SP Uk;w   .) This corresponds to the case wherethere is only one element from the universal hitting set in the wþ 1ð Þ-long window W0i. In that case, this element cannot be located at pos-ition i – 1 or iþw  1 as that would leave a window of size w with-out any k-mer from Uk;w, which is impossible by construction. Hence,when j¼1, Wi cannot be charged and P Yi;1   has to be 0.Finally, becausePwþ1j¼1 P jUk;w \W0ij ¼ j   ¼ 1, we get from equa-tion (5) thatE Xi½   ¼ P Xi ¼ 1½   ¼2 1   P jUk;w \W0ij ¼ 1      wþ 1 : (10)Under the assumption of random input sequence, the sparsity SP Uk;w   is precisely P jUk;w \W 0ij ¼ 1 and we have the main resultdf ¼ 2 1   SP Uk;w      :hEquation (3) implies that if the universal hitting set Uk;w is suchthat there exist wþ 1ð Þ-long walks in Gk with only one elementfrom Uk;w, then the expected density factor is less than 2.A universal hitting set Uk;w that satisfies this condition, that is,SP Uk;w   > 0, will be called w-sparse. A w-sparse universal hittingset has density factor less than 2. See Section 3.7 for an example ofw-sparse universal hitting set with low density.Similarly to Section 3.2, the sparsity of a universal hitting set Uk;wcan be computed exactly as the proportion of wþ 1ð Þ-long windowswith only one k-mer from Uk;w in the de Bruijn sequence of order kþw.3.5 Existence of a sparse universal hitting setWe propose here a simple construction of w-sparse universal hittingsets to show their existence. This construction is not immediatelyuseful in practice as the universal hitting sets it generates are largeand have small sparsity (even though they are w-sparse). Different,more practical constructions are used when we discuss applicationsof this technique, in Sections 3.7 and 4.1.Let C ¼ c0; . . . ; c‘ 1ð Þ be a simple cycle of length ‘ > 3 in Gk 1,the de Bruijn graph of order k – 1. C is not necessarily an inducedcycle, that is, it may have chords. Because Gk is the line graph ofGk 1, the edges of cycle C induce a simple cycle C0 of the samelength in Gk. That is, using indices modulo ‘, node c0i of Gk repre-sents the edge ci; ciþ1ð Þ of Gk 1 and C0 ¼ c00; . . . ; c0‘ 1   .We claim that the cycle C0 must be chordless. Suppose it is notthe case, and there is an edge ðc0i; c0jÞ where j 6¼ iþ 1. This impliesthat in Gk 1 the head of edge ci; ciþ1ð Þ is equal to the tail ofcj; cjþ1   , that is ciþ1 ¼ cj. Because j 6¼ iþ 1 and the cycle is simple,this is a contradiction.Let Uk;w be all the k-mers, minus the nodes of C0 unless theirindex is a multiple of w. That is Uk;w ¼ V Gkð ÞnC0ð Þ [ fc0igwji.Because C0 is chordless, there is no cycle using only nodes not inUk;w. Also, by construction, there is no path of length w in GknUk;w.Hence Uk;w is a universal hitting set (though not a minimallyImproving the performance of minimizers and winnowing schemes i113Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i110/3953951by gueston 07 January 2018sized one). By construction, for every i that is not a multiple of w,W0i ¼ c0i; c0iþ1; . . . c0iþw   contains only one element from Uk;w, whichimplies that Uk;w is w-sparse.So any cycle of length ‘ in Gk 1 induces a w-sparse hitting set forany w   2 ‘   1ð Þ. In particular, any Hamiltonian cycle of Gk 1 in-duces a chordless cycle of length rk 1 in Gk (these are the longestpossible chordless cycles of Gk). Therefore, for any given k, thereare w-sparse universal hitting sets for all w   2 rk 1   1   . Hence,there are sets for which SP Ukw   is strictly greater than 0, indicatingthat sets with density factor<2 exist.Moreover, for small value of w, given that every cycle of lengthw=2   1 in Gk 1 induces a w-sparse universal hitting set in Gk, thereexists a large number of w-sparse universal hitting sets for the par-ameters k and w.3.6 Universal hitting sets in random orderingsConsider again the winnowing scheme with random ordering, as wedid in Section 3.1. In Section 3.1, it was assumed that all wþ1k-mers in the window W0i can potentially be selected as a fingerprint(Hypothesis 1). That assumption is likely not valid because of theexistence and abundance of universal hitting sets. That is, eventhough the ordering of the k-mers is randomized, not all k-mers canbe selected as minimizers with this ordering.The following gives hints on why this is true. Consider a randomordering, or permutation, of the k-mers. Let h(m) be the index of thek-mer m in this permutation. In other words, m< hm0 ()h mð Þ < h m0ð Þ. More precisely, consider a k-mer bm that has a highindex value in the permutation (h bmð Þ close to rk). If there exists auniversal hitting set Uk;w such that u< h bm for all u 2 Uk;w, then thek-mer bm cannot ever be selected by the winnowing scheme with thatrandom ordering. This holds because every window of size w mustcontain a k-mer from Uk;w, which is less than bm in the ordering. LetU be the set of all universal hitting sets of minimum size and letM ¼ minU2Ufmaxm2Uh mð Þg. Then any k-mer with an index greaterthan M can never be selected.As a test, we ran the minimizers scheme (k¼10, w¼10, binaryalphabet) for 1000 different random orderings on a de Bruijn se-quence of order wþk. For each ordering Ri, we obtain a set of min-imizers Mi, 1   i   1000. Because we used a de Bruijn sequence oforder wþk, the set Mi contains a k-mer from every possible w-win-dow. Hence Mi is a, possibly trivial, universal hitting set for param-eters k and w. Even though it is possible for Mi to be the set of all k-mers, this was never the case in our 1000 random orderings, and thesets Mi contain on average 51% of the k-mers (see Table 1).Furthermore, we computed the sparsity SP Mið Þ of the sets Mi. Inevery case, the sparsity was small (average of 0.07%, see Table 1),but always greater than 0. Therefore, every observed universal hit-ting set Mi is w-sparse. In other words, a random ordering implicitlydefines a universal hitting set, and empirically this set is w-sparse inall randomized orderings we tested.3.7 Density factor of various ordering schemesOrenstein et al. (2016a) introduces the concept of universal hittingsets and provides a heuristic algorithm, called DOCKS (Design OfCompact K-mer Sets), to generate small (although not necessarilyoptimal) universal hitting sets. Table 1 reports the actual density fac-tor, computed as explained in section 3.2, of the winnowing schemeusing three different orderings: randomized, DOCKS universal hit-ting sets and lexicographic. Because the densities are computed on ade Bruijn sequence of order kþw (binary alphabet, k¼10 andw¼10), the density computed is equal to the expected density.In Schleimer et al. (2003), the authors prove a lower bound of1.5, as well as a slight improvement of 1:5 þ 1=2w, on the densityfactor for any local scheme. In addition, they conjecture that 2 is infact the lowest possible density factor. The density factor of 1.737obtained for DOCKS disproves this conjecture, but is not equal tothe lower bound of 1:5 þ 1=2w ¼ 1:55, leaving a gap between theempirical results and the best known lower-bound.The density factor estimation is obtained from the sparsity usingequation 3. For the random and DOCKS orderings, this formulaprovides a good estimation of the density factor. The average spars-ity of the random ordering is very low but not 0.The DOCKS ordering, on the other hand, has a relatively highsparsity, which accounts for the low density factor. Also, the numberof k-mers that are minimizers picked by the random and DOCKSorderings is significantly less than the total number of 10-mers.The situation is very different for the lexicographic ordering,which selected all possible 10-mers and whose sparsity is 0. The esti-mation of its density factor from its sparsity is erroneous. Thismeans that in addition to not being w-sparse, the lexicographicordering does not satisfy Hypothesis 1. This is most likely due to thehomo-polymer runs.4 Application: binning4.1 Binning DNA sequencesWe consider the application of binning subsequences of a largeDNA sequence, for example, as is done in k-mer counters such asKMC2 (Deorowicz et al., 2015) and MSPKmerCounter (Li andYan, 2015). These applications compute the number of occurrencesof each k-mer in a DNA sequence. They are disk-based programs:first the input sequence is split into bins written to files on disk, thenk-mer occurrences are computed in each bin independently. Thenumber and size of bins considerably affects the running time ofthese programs.For example, suppose we count L-mers in the human genome.Typically, for this application, 16   L   30. Following KMC2, weset k¼7 and w ¼ L  kþ 1, and we bin together all the w-long sub-sequences that have the same minimizer for parameters k and w.Then, it is possible to count the L-mers in each bin independentlyand in parallel to obtain the desired counts.Ideally we would like a large number of bins, to allow for goodparallelism, but not too large, to avoid extra overhead of creatingand writing to many small files. Say at least a few hundred bins, butno more than a few thousand. For example, KMC2 uses a heuristicto merge in the same file the smaller bins so as to create at most2000 files. Also, we would like the amount of data in each bin to beTable 1. Statistics on the sparsity and density factor of the universalhitting sets generated by random ordering, the DOCKS universalhitting set and the lexicographic orderingOrdering SPðUk;wÞ jUk;wj=rk df df ;SP% %random 0.07 51 1.999 1.998DOCKS 13.3 21 1.737 1.733lexicographic 0.00 100 2.236 2.000The computation is done with k¼ 10, w¼ 10 on a binary alphabet. Thevalues for the random ordering are averages over 1000 different randomizedorderings. df is the density factor and df ;SP is the density factor estimated giventhe sparsity of the set by equation 3. The difference between these numbers isdue to the imperfect nature of Hypotheses 2.i114 G.Marçais et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i110/3953951by gueston 07 January 2018roughly the same. The lexicographic ordering for example generatesa much larger bin corresponding to the homo-polymer of A.In particular, we would like the size of the largest bin to not be toolarge compared to the average bin size.We next look at different orderings used in bioinformatics appli-cations and their impact on the number of bins generated and thedistribution of the data between the bins. In every test below, thesame algorithm is used to generate the bins, and only the selection ofthe minimizers changes.4.2 Summary of heuristic orderings used in practiceThe following ordering was proposed by the authors of the minim-izers technique in the implementation of the UMD Overlapper(Roberts et al., 2004a). They ‘assign the values 0, 1, 2, 3 to C, A, T,G, respectively, for the odd numbered bases of k-mers and assign 0,1, 2, 3 to G, T, A, C, respectively, for the even numbered bases.’In the k-mer counter KMC2 (Deorowicz et al., 2015), the au-thors use the lexicographic order but ban a subset of k-mers frombeing minimizers. They use as minimizers k-mers that ‘do not startwith AAA, neither start with ACA, neither contain AA anywhere ex-cept at their beginning’. This effectively creates a (non-optimallysized) universal hitting set (provided that the homopolymer AA . . . Ais preserved).In Kraken (Wood and Salzberg, 2014), the authors XOR thek-mer with a random value, before using lexicographic comparison.It is a form of randomization.Minimap (Li, 2016) uses randomization as well, by employing aparticular invertible hashing function.The DOCKS ordering is based on the small universal k-mer hit-ting sets created by the DOCKS heuristic. Creating the DOCKS setSk;w for parameter k and w takes OðjRjkðjSk;wj   jRjk=kÞÞ time andOðjRjkÞ memory. We can build DOCKS sets for k   13 and w  8within five days and requiring at most 12 GB of memory. In prac-tice, these sets are precomputed once for each combination of k andw and then can be used by any algorithm. Precomputed DOCKS setsare available from the DOCKS website (Orenstein et al., 2016b).4.3 Densities using various heuristics including auniversal k-mer-based schemeFigure 2 shows the distribution of the distance between consecutiveminimizers, for k¼7 and w¼11, and Table 2 has statistics on thesedistributions. The distribution is computed for a de Bruijn sequenceof order 18 ¼ kþw, as explained in Section 3.2, and on the full se-quence of the human genome (with all ambiguous bases and Nsremoved). Ideally, we would like the selection of k-mers to be assparse as possible and, therefore, the distribution to be skewed to-ward larger distances.The overall behavior for the different orderings does not changequantitatively between the de Bruijn sequence and the DNA se-quence, showing that in the limit, the expected density computed ona de Bruijn sequence is a good proxy for performance on a biologicalsequence. The distribution of the randomized ordering is close to auniform distribution. All the orderings succeeded in their stated goalof reducing the number of cases where minimizers have a low separ-ation (e.g. k-mers that are consecutive or separated by one base)compared to the lexicographic ordering. For larger separation be-tween minimizers, the distribution for all orderings except DOCKSare very similar to that of the randomized ordering.The distribution for the DOCKS ordering is markedly different,with a mode at 6 (which is wþ 1ð Þ=2) and is generally skewed to-ward larger separation. The percentage of minimizers with low sep-aration is the lowest among all orderings. This provides strongevidence that using a universal k-mer-based ordering can reduce thenumber of minimizers selected.4.4 Distribution of bins created by heuristic anduniversal k-mer based schemesTable 3 reports statistics on the number and sizes of the bins createdusing various orderings. The DOCKS-based ordering creates farfewer bins than the other orderings, and the ratio between the larg-est bin and the average size is smaller. The DOCKS distribution of05101520250 2 4 6 8 10 1205101520250 2 4 6 8 10 12%ofminimizersSeparation between minimizersDe Bruijn sequence%ofminimizersSeparation between minimizersHuman genomeABlexico.KMC2randomDOCKSUMD ovlKrakenMinimapFig. 2. Distribution of the separation between minimizers for k¼7 and w¼ 11on DNA sequences. (A) Results on a de Bruijn sequence of order wþ k.(B) Results computed on the human reference genome (hg19). Each line rep-resents a different minimizer scheme using a different ordering. Note thatprevious heuristic orderings all behave like the randomized orderings (uni-form distribution) except for separation of 1 and 2. The universal k-mer order-ing computed by DOCKS has a noticeably different distribution, with a modeand a higher meanTable 2. Statistics on the distribution of the distances between min-imizers in Figure 2Ordering df distance low sep.mean 6 stdev %dbg lexico. 2.18 5.5(34) 27random 2.00 6.0(32) 18Minimap 2.05 5.9(32) 21KMC2 1.97 6.1(32) 18UMD Ovl 1.91 6.3(30) 14Kraken 1.88 6.4(29) 11DOCKS 1.75 6.9(25) 4.6human lexico. 2.34 5.1(34) 33random 2.02 6.0(32) 19Minimap 2.09 5.8(33) 22KMC2 2.02 5.9(33) 19UMD Ovl 1.97 6.1(31) 17Kraken 1.93 6.2(30) 13DOCKS 1.77 6.7(26) 6.2The table reports the density factor (df), the mean distance between minim-izers (mean6 stdev) and the percentage of selected k-mers that are consecu-tive or separated by one base (low sep.). These were computed on a de Bruijnsequence (dbg) and on the human genome sequence (human).Improving the performance of minimizers and winnowing schemes i115Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i110/3953951by gueston 07 January 2018bin sizes is closer to the stated goals of not having too many bins, orany bins that are too large, and according to the Kullback-Leibler di-vergence, the distribution for DOCKS is closer to uniform than anyof the other orderings. These results indicate that if these toolsadopted a universal k-mer ordering, their runtime and memoryusage performance would be significantly improved.5 DiscussionWe introduced a new analysis of the performance of the winnowingscheme based on the sparsity of an ordering. In addition, we showedhow to construct orderings from small universal hitting sets thatoutperform randomized and previously designed orderings. Whilethese new orderings perform well on uniformly distributed se-quences and actual DNA sequences, many questions of theoreticaland practical interest remain.First, several orderings based on universal hitting sets beat theconjectured lower bound on the density factor of 2, but did notachieve 1:5 þ 1=2w. The question of the actual lower bound istherefore open. Second, every winnowing local scheme consideredhere is based on an ordering of the k-mers. But there exists localschemes that are not based on an ordering. Can the minimum dens-ity always be achieved with an ordering? Third, although the per-formance was similar between the uniformly distributed inputsequence and DNA sequence, can we improve the winnowingscheme by taking into account the actual distribution of the bases inthe input?Fourth, the construction of small universal hitting sets withDOCKS is based on a heuristic and is not guaranteed to be optimal.As it stands, it is also intractable for large k. Can we design an effi-cient algorithm to construct some (or all) of the optimal universalhitting sets? Or, can we design optimal orderings, even thoughthe actual corresponding universal hitting set is never explicitlyconstructed?All of the work in this paper considers a de Bruijn graph. Inmany situations, it makes sense to consider a k-mer and its reversecomplement to be equal. The above schemes can be used by replac-ing every k-mer with its canonical representation (i.e. the smallest ofa k-mer and its reverse complement). But this is likely not optimal.Define the RC de Bruijn graph from a de Bruijn graph where nodesrepresenting reverse complements are identified and parallel edgesare collapsed into a single edge. A fifth question arises in this setting:can an equivalent to universal hitting sets, and orders, be defined soas to create better winnowing schemes when reversed complementedk-mers are identified? How much can we transfer of what we al-ready know on de Bruijn graphs to RC de Bruijn graphs?6 ConclusionWe provided a novel theoretical framework to estimate the densityof minimizer procedures. This framework explains why it is possiblefor w-sparse local winnowing schemes to have lower density factorthan 2. In addition, we provided a practical method to create fromsmall universal hitting sets, such as those generated by DOCKS, w-sparse winnowing schemes that achieve density factors below 2.These two results combined answer negatively a standing conjectureof Schleimer et al.We showed that the lexicographic ordering has the worst behav-ior while the different orderings used in current software tools per-form similarly to randomized orderings, which is slightly better. Weshowed that universal hitting set based schemes can perform sub-stantially better than random orderings in practice. Therefore, forthe development of bioinformatics software using minimizers, wesuggest using an ordering based on small universal hitting sets suchas those created by the DOCKS algorithm, or, if not practical,randomized orderings. We showed that for binning applications,doing so would lead to a significant improvement in computationalefficiency.FundingThis research is funded in part by the Gordon and Betty Moore Foundation’sData-Driven Discovery Initiative through Grant GBMF4554 to C.K., by theUS National Science Foundation (CCF-1256087, CCF-1319998) and by theUS National Institutes of Health (R01HG007104). D.P. was supported inpart by a Ph.D. fellowship from the Edmond J. Safra Center forBioinformatics at Tel-Aviv University. R.S. was supported in part by theIsrael Science Foundation as part of the ISF-NSFC joint program 2015-2018.Conflict of Interest: none declared.ReferencesChikhi,R. et al. (2015) On the representation of De Bruijn graphs. J. Comput.Biol., 22, 336–352.Chikhi,R. et al. (2016) Compacting de Bruijn graphs from sequencing dataquickly and in low memory. Bioinformatics, 32, i201–i208.de Bruijn,N.G. (1946) A combinatorial problem. Proceedings of the Section ofSciences of the Koninklijke Nederlandse Akademie Van Wetenschappen TeAmsterdam, 49, 758–764.Deorowicz,S. et al. (2015) KMC 2: fast and resource-frugal k-mer counting.Bioinformatics, 31, 1569–1576.Grabowski,S. and Raniszewski,M. (2015) Sampling the suffix array with min-imizers. In: Iliopoulos,C. et al. (eds.) String Processing and InformationRetrieval: 22nd International Symposium, SPIRE 2015, London, UK,September 1-4, 2015, Proceedings. Springer International Publishing,Berlin, pp. 287–298.Li,H. (2016) Minimap and miniasm: fast mapping and de novo assembly fornoisy long sequences. Bioinformatics, 32, 2103–2110.Table 3. Statistics on the distribution of the bin sizesOrdering # bins avg size max ratio DKL(mega bases)dbg lexico. 16384 4.19 367 1.37random 12003 5.73 9.32 0.23Minimap 13267 5.18 10.4 0.27KMC2 12370 5.56 166 1.13UMD Ovl 13108 5.24 274 1.37Kraken 12502 5.5 210 1.31DOCKS 4063 16.9 7.44 0.05human lexico. 16285 0.175 96.4 0.91random 11388 0.251 38.3 0.62Minimap 12280 0.233 69.5 0.65KMC2 12287 0.233 74.5 0.77UMD Ovl 11015 0.259 26.1 0.61Kraken 10389 0.275 26.3 0.57DOCKS 4046 0.706 23.5 0.12The table reports the number of bins created (# bins), the average bin size(avg size, in million of bases), the ratio of the largest bin size to the average(max ratio), and the Kullback-Leibler divergence, DKLðBkUÞ, between B, thedistribution of the bin sizes, and U, the uniform distribution (a smaller diver-gence implies that the distribution B is closer to the uniform distribution).These were computed on a de Bruijn sequence (dbg) and the human genomesequence (human).i116 G.Marçais et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i110/3953951by gueston 07 January 2018Li,Y. and Yan,X. (2015) MSPKmerCounter: a fast and memory efficientapproach for K-mer counting. arXiv:1505.06550 [cs, q-bio]. arXiv:1505.06550.Orenstein,Y. et al. (2016a) Compact universal k-mer hitting sets. In: Frith,M.and Pedersen,C.N.S. (eds.) Algorithms in Bioinformatics, number 9838 inLecture Notes in Computer Science. Springer International Publishing,Berlin, pp. 257–268.Orenstein,Y. et al. (2016b) DOCKS website. http://acgt.cs.tau.ac.il/docks/.Roberts,M. et al. (2004a) A preprocessor for shotgun assembly of large gen-omes. J. Comput. Biol., 11, 734–752.Roberts,M. et al. (2004b) Reducing storage requirements for biological se-quence comparison. Bioinformatics, 20, 3363–3369.Schleimer,S. et al. (2003) Winnowing: Local Algorithms for DocumentFingerprinting. In: Proceedings of the 2003 ACM SIGMOD InternationalConference on Management of Data, SIGMOD ’03, New York, NY, USA.ACM, pp. 76–85.Wood,D.E. and Salzberg,S.L. (2014) Kraken: ultrafast metagenomic sequenceclassification using exact alignments. Genome Biol., 15, R46.Ye,C. et al. (2012) Exploiting sparseness in de novo genome assembly. BMCBioinformatics, 13, S1.Improving the performance of minimizers and winnowing schemes i117Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i110/3953951by gueston 07 January 2018
5028881969002	PMID28881969	5028881969	https://watermark.silverchair.com/btx234.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881969.main.pdf	Bioinformatics, 33, 2017, i92–i101doi: 10.1093/bioinformatics/btx234ISMB/ECCB 2017Chromatin accessibility prediction viaconvolutional long short-term memorynetworks with k-mer embeddingXu Min1,2, Wanwen Zeng1,3, Ning Chen1,2, Ting Chen1,2,4,* andRui Jiang1,3,*1MOE Key Laboratory of Bioinformatics and Bioinformatics Division, TNLIST, 2Department of Computer Scienceand Technology, State Key Lab of Intelligent Technology and Systems, 3Department of Automation, TsinghuaUniversity, Beijing 100084, China and 4Program in Computational Biology and Bioinformatics, University ofSouthern California, CA 90089, USA*To whom correspondence should be addressed.AbstractMotivation: Experimental techniques for measuring chromatin accessibility are expensive andtime consuming, appealing for the development of computational approaches to predict openchromatin regions from DNA sequences. Along this direction, existing methods fall into twoclasses: one based on handcrafted k-mer features and the other based on convolutional neural networks. Although both categories have shown good performance in speciﬁc applications thus far,there still lacks a comprehensive framework to integrate useful k-mer co-occurrence informationwith recent advances in deep learning.Results: We ﬁll this gap by addressing the problem of chromatin accessibility prediction with a convolutional Long Short-Term Memory (LSTM) network with k-mer embedding. We ﬁrst split DNA sequences into k-mers and pre-train k-mer embedding vectors based on the co-occurrence matrix ofk-mers by using an unsupervised representation learning approach. We then construct a supervised deep learning architecture comprised of an embedding layer, three convolutional layers anda Bidirectional LSTM (BLSTM) layer for feature learning and classiﬁcation. We demonstrate thatour method gains high-quality ﬁxed-length features from variable-length sequences and consistently outperforms baseline methods. We show that k-mer embedding can effectively enhancemodel performance by exploring different embedding strategies. We also prove the efﬁcacy ofboth the convolution and the BLSTM layers by comparing two variations of the network architecture. We conﬁrm the robustness of our model to hyper-parameters by performing sensitivity analysis. We hope our method can eventually reinforce our understanding of employing deep learningin genomic studies and shed light on research regarding mechanisms of chromatin accessibility.Availability and implementation: The source code can be downloaded from https://github.com/minxueric/ismb2017_lstm.Contact: tingchen@tsinghua.edu.cn or ruijiang@tsinghua.edu.cnSupplementary information: Supplementary materials are available at Bioinformatics online.1 IntroductionIn every human cell, genetic and regulatory information is stored inchromatin, where DNA is tightly packed and wrapped around histone proteins. The chromatin structure affects gene expression, protein expression, biological pathway and eventually complexphenotypes. Concretely, some regions of the genome are accessibleto transcription factors (TFs), RNA polymerases (RNAPs) and othercellular machines involved in gene expression, while others are compactly wrapped, sequestered and unavailable to most cellular machinery. These two kinds of regions on the genome are known asopen regions and closed regions (Wang et al., 2016; Niwa, 2007).Recent high-throughput genome-wide methods invented several biological experiment techniques for measuring the accessibility ofchromatin to cellular machines related to gene expression, such asCV The Author 2017. Published by Oxford University Press.i92This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018Chromatin accessibility prediction with k-mer embeddingDNase-seq, FAIRE-seq and ATAC-seq. For example, DNase-seqtakes advantage of DNase I, a DNA-digestion enzyme, to degradeaccessible chromatin while leaving the closed regions largely intact.This assay allows systematic identification of hundreds of thousandsof DNase I-hypersensitive sites (DHS) per cell type, and this in turnhelps to delineate genomic regulatory compartments (Crawfordet al., 2006; Vierstra et al., 2014). Still, biological experiments areexpensive and time consuming, making large scale assay impracticaland motivating development of computational methods.In recent years, several sequence-based computational methodshave been proposed to identify functional regions, which mainly fallinto two classes. One class is the kmer-based methods, where k-mersare defined as oligomers of length k. For example, kmer-SVM provided a support vector machine (SVM) framework for mammalianenhancers discrimination based on k-mer features (Lee et al., 2011).Shortly after kmer-SVM was proposed, gkm-SVM introduced an alternative feature sets named gapped k-mer features, which presentedrobustness to estimate k-mer frequencies, and consistently improvedperformance of kmer-SVM (Ghandi et al., 2014). The other class isdeep learning-based methods which are mainly established uponconvolutional neural networks (CNNs). Indeed, deep learning algorithms are attractive solutions for such sequence modeling problems.DeepBind (Alipanahi et al., 2015) and DeepSEA (Zhou andTroyanskaya, 2015) successfully applied CNNs to modeling the sequence specificity of protein binding with a performance superior tothe conventional SVM-based methods. Instead of crafting featuresets like k-mers, CNNs can adaptively capture informative sequencefeatures with aid of convolution operations. Zeng et al. (2016) presented us a systematic exploration of CNN architectures for predicting DNA sequence binding, and showed the benefits of adding moreconvolutional kernels in learning higher-order sequence features.Moreover, there also exist many other deep learning-basedapproaches, such as Basset (Kelley et al., 2016) and DeepEnhancer(Min et al., 2016), suggesting us that CNNs have strong power in sequence representation and classification.Although having been successfully used, both the above twoclasses of approaches have their own advantages and disadvantages.On one hand, k-mers are an unbiased, general, complete set of sequence features, which can be defined on arbitrary-length sequences.However, k-mers can merely capture local motif patterns withoutability to learn long-distance dependencies of DNA sequences. Onthe other hand, CNNs can detect sequence motifs automatically andyield superior performance in classification tasks. Despite this, onebiggest disadvantage for CNNs is that they usually require fixedlength sequences as input, which may limit their application.Besides, almost without exception, current deep learning-basedmethods simply transform DNA sequences composed of four basesinto images with four channels corresponding to A, C, G and T,using one-hot encoding. With such representation, these methods actually execute one-dimensional convolution operations on binaryimages with only two possible values for each pixel rather than onreal continuous-valued images in computer vision field (Krizhevskyet al., 2012), which may impose restrictive effects on theirperformance.In fact, it is more natural to regard one DNA sequence as a sentence with four types of characters, namely A, C, G and T, ratherthan an image, and thus related research work in natural languageprocessing has offered valuable experience for DNA sequence modeling. To address the sentence classification task, Kim (2014) trainedCNNs with one layer of convolution on top of word vectors obtained from an unsupervised neural language model. Word vectors,wherein words are projected from a sparse, one-hot encoding onto ai93lower dimensional vector space via language models such as wellknown Skip-gram (Mikolov et al., 2013) and GloVe (Penningtonet al., 2014), are essentially semantic features that encode contextualinformation of words. In this work, the author used word vectorstrained by Mikolov et al. (2013) on a corpus of Google News whichare publicly available and achieved excellent results on multiplebenchmarks, suggesting that the pre-trained vectors are useful feature extractors that can be utilized for various classification tasks. Inaddition, due to their capability for processing arbitrary-length sequences, the recurrent neural network (RNN) is a natural choice forsentence modeling tasks. Especially, RNNs with Long Short-TermMemory (LSTM) units (Hochreiter and Schmidhuber, 1997) havere-emerged as a popular architecture because of their representational power and effectiveness at capturing long-term dependencies.For instance, Tai et al. (2015) successfully generalized the standardLSTM to tree-structured network topologies and showed their superiority for representing sentence meaning.With the above consideration, we address the problem of predicting chromatin accessibility from DNA sequence, by proposingan innovative computational approach, namely convolutional longshort-term memory networks with k-mer embedding, as shown inFigure 1. We overcome the drawbacks of current DNA sequencemodeling approaches in two aspects: (i) we fuse the informativek-mer features into a deep neural network by embedding k-mersinto a low dimension vector space; (ii) we are able to handlevariable-length DNA sequences as input and capture long-distancedependencies thanks to LSTM units. Specifically, we first cut original DNA sequences with varying lengths into k-mers in a slidingwindow fashion. Based on the co-occurrence information of k-merscontained in the resulted corpus, we train an unsupervised GloVemodel to obtain embedding vectors of all k-mers. In our superviseddeep learning architecture, the first embedding layer is designed toturn an original DNA sequence, i.e. a sequence of k-mer indexes,into dense vectors according to the embedding matrix pre-trained byGloVe. The convolutional layers are intended to scan on thesequence of vectors through one-dimensional filtering operations,together with dropout layers and max-pooling layers. The maxpooling layer is followed by a Bidirectional LSTM (BLSTM) layer,which is capable of learning complex high-level grammar-like relationships and handling variable-length input sequences. The lastlayers are fully-connected dense layers of rectified linear units(ReLU) with dropout to avoid overfitting, and a two-way softmaxoutput layer to generate the final classification probabilities.To verify the efficacy of our approach, we carry out chromatinaccessibility prediction experiments on datasets collected from theEncyclopedia of DNA Elements (ENCODE) project (Consortiumet al., 2004). We demonstrate that our framework consistently surpasses other methods on binary classification of DNA sequences.We show that it is beneficial to incorporate the k-mer contextual information into the deep learning framework by learning the lowdimensional real-valued dense embedding vectors. We illustrate thatthe Bidirectional LSTM units are well-suited for DNA sequencemodeling. We expect that our approach could provide insights intogeneral DNA sequence modeling and contribute to understanding ofDNA regulatory mechanisms.2 Materials and methodsIn this section, we describe our framework for performing effectivedeep learning algorithms on DNA sequences data. We begin by constructing the general network architecture that we use. Then weDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018i94X.Min et al.discuss in detail k-mer embedding, which aims to encode the cooccurrence information of k-mers into a low-dimensional vectorspace by unsupervised learning. In addition, we describeBidirectional LSTMs, which we applied in order to capture longrange dependencies and form fixed-length feature representation ofarbitrary-length DNA sequences.2.1 General network architectureGiven a DNA sequence of L0 base pairs (bps), we first split it intok-mers using the sliding window approach. We extract all subsequences of length k with stride s, resulting in a k-mer sequence withlength L ¼ bðL0 À kÞ=sc þ 1, wherein all these k-mers are indexedby positive integers in set C ¼ ½1; 2; . . . ; 4k  . We will investigate howto do feature learning for such sequence data x 2 CL with varyinglength L, i.e. how to learn a feature map g : CL 7!Rd that maps x2 CL into a vector of features h 2 Rd useful for machine learningtasks.Suppose that we have N variable-length DNA sequences, eachwith a binary label representing whether it is a chromatin accessibleregion in a specific cell type. Thus, we have N labeled instancesfxi ; yi gN , where xi 2 CL ; yi 2 f0; 1g. Notice again, length L is varyi¼1ing across samples. Our goal is to learn a function which can beused to assign the label to each instance xi. We use a convolutionallong short-term memory network with k-mer embedding as shownin Figure 1. We can decompose the feature learning functiong : CL 7!Rd into three stages:h ¼ gðxÞ ¼ glstm ðgconv ðgembed ðxÞÞÞ:(1)The embedding stage computes the co-occurrence statistics ofk-mers, and learns to project them into a D-dimensional space RD .The convolution stage scans on the embedding representation of sequences using a set of one-dimensional convolution filters in orderto capture sequence patterns or motifs. The BLSTM stage performsa Bidirectional LSTM network on the input to learn long-termdependencies, and finally yields a fixed-length feature vector in Rd .Eventually, in the supervised training stage, we treat the binaryclassification as a logistic regression on the feature representations.The conditional likelihood of yi given xi and model parameters Hcan be written as:log pðyi jxi ; HÞ ¼ yi log rðbT hi Þ þ ð1 À yi log ð1 À rðbT hi ÞÞÞ;(2)where b 2 Rd is the prediction parameters, hi 2 Rd is the learnedfixed-length feature for xi, and rðzÞ ¼ 1=ð1 þ exp ðÀzÞÞ is the logistic sigmoid function. We train our deep neural network by minimizing the following loss function:‘¼ÀNXlog pðyi jxi ; HÞ:(3)i¼12.2 k-mer embedding with GloVeThis section explains why and how we embed k-mers into a lowdimensional vector space. As mentioned above, traditional kmerbased methods simply calculate the vector of k-mer frequencieswithout utilizing the co-occurrence relationship of k-mers. Thek-mer feature is an analogy of the ‘Bag-of-Words’ feature (Harris,1954) widely used in natural language processing and informationretrieval, which is an orderless document representation.Meanwhile, the co-occurrence matrix contains global statistical information, which may assist us to construct better feature representations. Here, we apply the popular GloVe (Pennington et al., 2014)Fig. 1. A graphical illustration of our computational framework. We ﬁrst splitevery input DNA sequence into k-mers. We use an unsupervised learningmethod, namely GloVe, to learn the embedding vectors of all the k-mersbased on the corpus of k-mer sequences. The embedding layer will embedeach k-mer into a vector space based on the GloVe k-mer vectors, which turnsthe k-mer sequence into a dense real-valued matrix. Then three convolutionlayers with dropout and max-pooling will scan on the matrix using multipleconvolutional ﬁlters to detect spatial motifs. The following BLSTM layer contains two LSTMs run in parallel to capture long-range dependencies on theprevious output and yield a ﬁxed-length feature representation. The ﬁnalfully-connected layer and the softmax layer will serve as a classiﬁer to generate probability predictions to be compared with the true target labels via aloss function. For a more detailed description of data shape in each layer, seeSupplementary Table S1model for k-mer embedding, where GloVe stands for ‘GlobalVectors’ for word representation based on factorizing a matrix ofword co-occurrence statistics. The superiority of GloVe over othermethods for learning vector space representations lies in that it combines the advantages of both global matrix factorization and localcontext window methods.The statistics of k-mer occurrences are the primary source of information available for learning embedding representations. Let usdenote the matrix of kmer-kmer co-occurrence counts by X, whoseentry Xij tabulates the number of times that k-mer j occurs in thecontext window of k-mer i. i, j 2 [1, V] are two k-mer indexes,where V ¼ 4k is the vocabulary size. According to the GloVe model,the cost function to be minimized is,Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018J¼VXi; j ¼ 1Xij 6¼ 0~f ðXij ÞðwT wj þ bi þ bj À log Xij Þ2 ;i ~(4)Chromatin accessibility prediction with k-mer embeddingi95~where w 2 RD are expected k-mer vectors, w 2 RD are separate con~text vectors for auxiliary purpose, and b; b 2 R are biases. The nondecreasing weighting function f can be parameterized as,(ðx=xmax Þa if x < xmaxf ðxÞ ¼;(5)1otherwisewhere xmax is a cutoff value and a controls the fractional power scaling which is usually set to 3/4.As can been seen from Equation (4), the computational complexity depends on the number of nonzero entries in the co-occurrencematrix X. We minimize the cost function in Equation (4) usingAdaGrad (Duchi et al., 2011) to obtain our embedding vector representations w1 ; w2 ; . . . ; wV 2 RD for all k-mers. Given these vectors,we can fulfill the embedding stage of feature learning gembed : CL 7!RDÂL by embedding every k-mer into the vector space RD :gembed ðxÞ ¼ ½wx1 ; wx2 ; . . . ; wxL  ;(6)where x ¼ ½x1 ; x2 ; . . . ; xL   2 CL . Based on the output D Â L matrix,we proceed with the convolution stage, which further extracts spatial features using convolutional layers and max-pooling layers.2.3 Bidirectional LSTMThis section introduces the LSTM unit and explains how aBidirectional LSTM network can produce a fixed-length output regardless of input sequence lengths. RNNs are able to process inputsequences of arbitrary length by means of the recursive applicationof a transition function on a hidden state vector ht 2 Rd . At eachtime step t, the hidden state vector ht is the function of the input vector xt received at time t and its previous hidden state ht–1.Commonly, the RNN transition function is an affine transformationfollowed by a point-wise nonlinearity such as the hyperbolic tangentfunction:ht ¼ tanhðWxt þ UhtÀ1 þ bÞ:ht ¼ ot   tanhðct Þ;(13)where xt denotes the input from the previous layer at the currenttime step, r denotes the logistic sigmoid function and   denoteselement-wise multiplication (Fig. 2).Intuitively, the hidden state vector in an LSTM unit is a gated,partial view of the state of the unit’s internal memory cell. Since thevalue of the gating variables varies for each vector element, themodel can learn the long-range information over multiple timescales. In our application, we only output the hidden state vector ofLSTM at the last time step, which remembers the whole sequence information and keeps a fixed-length representation for variablelength input sequences. Besides, a common-used variant of the basicLSTM is the Bidirectional LSTM, which consists of two LSTMs runin parallel: one on the input sequence and the other on the reverse ofthe input sequence. We concatenate the outputs of two parallelLSTMs to obtain our final feature representation containing boththe forward and backward information of a DNA sequence.3 Results and discussionTo verify our framework, we run a series of classification experiments using datasets collected from the ENCODE project. First, inSection 3.1, we give an introduction to the datasets prepared forclassification tasks and some details about model training procedure. Then in Section 3.2, we evaluate our method and compare itsperformance with gkmSVM and DeepSEA. Next in Section 3.3, weanalyze k-mer embedding by probing into the k-mer statistics andvisualizing the embedding vectors. Additionally in Section 3.4, weprove the effectiveness of k-mer embedding by exploring differentembedding strategies in our network architecture. In Section 3.5 and3.6, we prove the efficacy of both the convolution and BLSTMstages, by proposing two variant deep learning architectures. Finallyin Section 3.7, we perform sensitivity analysis to show the robustness of our model.(7)3.1 Experiment setupUnfortunately, a problem with transition functions of this form isthat components of the gradient vector can grow or degrade exponentially over long sequences during training (Hochreiter, 1998;Bengio et al., 1994). Hence, the LSTM architecture (Hochreiter andSchmidhuber, 1997) is designed to remedy this exploding or vanishing gradients problem in RNNs by introducing a memory cell whichcan choose to retain their memory over arbitrary periods of timeand also forget if necessary. While various LSTM variants have beendescribed, here we present the transition equations in Equations (8)–(13) stating the forward recursions for a single LSTM layer.We define the LSTM unit at each time step t to be a collection ofvectors in Rd : an input gate it, a forget gate ft, an output gate ot, aninput modulation gate gt, a memory cell ct and a hidden state ht. Theentries of the gating vectors it, ft and ot are in [0, 1]. The LSTM transition equations are the following:it ¼ rðW ðiÞ xt þ UðiÞ htÀ1 þ bðiÞ Þ;(8)ft ¼ rðW ðf Þ xt þ Uðf Þ htÀ1 þ bðf Þ Þ;(9)ot ¼ rðW ðoÞ xt þ UðoÞ htÀ1 þ bðoÞ Þ;(10)gt ¼ tanhðW ðgÞ xt þ UðgÞ htÀ1 þ bðgÞ Þ;(11)ct ¼ it   gt þ ft   ctÀ1 ;(12)To benchmark the performance of our deep learning framework, weselected DNase-seq experiments of six typical cell lines, includingGM12878, K562, MCF-7, HeLa-S3, H1-hESC and HepG2.GM12878 is a lymphoblastoid cell line produced from the blood ofFig. 2. Elaborate description of the LSTM unit. i: input gate, f: forget gate, o:output gate, g: input modulation gate, c: memory cell, h: hidden state. Theblue arrowhead refers to ct–1, namely the memory cell at the previous timestep. The notations correspond to Equations (8) - (13) such that W(  ) denotesweights for xt to the output gate, and U(f) denotes weights for ht–1 to the forgetgate, etc. Adapted from Sønderby et al. (2015)Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018i96X.Min et al.a female donor with northern and western European ancestry byEBV transformation. K562 is an immortalized cell line producedfrom a female patient with chronic myelogenous leukemia (CML).MCF-7 is a breast cancer cell line isolated in 1970 from a 69-yearold Caucasian woman. HeLa-S3 is an immortalized cell line thatwas derived from a cervical cancer patient. H1-hESC is a human embryonic stem cell (ESC) line. HepG2 is a cell line derived from amale patient with liver carcinoma.For each cell type, we downloaded raw sequencing data fromwebsite of ENCODE, mapped reads to human reference genome(hg19) using the tool bowtie, and identified chromatin accessible regions (peaks) using the tool HOTSPOT (John et al., 2011). We regarded these variable-length sequences as positive samples, andadditionally generated the negative samples by cropping an equalnumber of sequences randomly from the whole genome with thesame length distribution as the positive samples. In this way, weconstructed six chromatin accessibility datasets as described inTable 1. Each dataset has 244–504k samples whose lengths presenta long-tailed distribution. We then split at random each dataset intostrictly non-overlapping training, validation and test sets with proportion 0.85:0.05:0.10. The training set was used to adjust theweights on the neural network. The validation set was used to avoidoverfitting. The test set was used for testing the final solution inorder to confirm the actual predictive power of the network.For the unsupervised training of k-mer embedding, we generatedthe corpus of k-mer sequences by setting k to 6, and the stride s to 2.Consequently, the k-mer vocabulary size is V ¼ 46 ¼ 4096. We usedthe C implementation of GloVe model published on website https://github.com/stanfordnlp/GloVe, which is multi-thread and ultra-fast,to obtain the k-mer embedding vectors. With regard to hyperparameters of GloVe, we set the window size to 15 in computing cooccurrence matrix, the vector size, i.e. the embedding dimension to100, the cutoff value xmax to 30 000 and the maximum number of iterations to 300.We implemented our supervised deep neural network by Keras(Chollet, 2015) which is a deep learning library for Theano andTensorflow. We chose Theano as backend of Keras, while theTensorflow backend also generated very close results through ourtesting. The high-performance NVIDIA Tesla K80 GPU was usedfor model training. During training process, we applied theRMSprop algorithm (Tieleman and Hinton, 2012) for the stochasticoptimization of the objective loss function, with the initial learningrate set to 0.001, and batch size set to 3000. We also applied theearly stopping strategy with the maximum number of iterations setto 60, and it would stop training after 5 epochs of unimproved losson the validation set.Table 1. Description of six cell type-speciﬁc datasets for chromatinaccessibility prediction3.2 Model evaluationTo begin with, we reported accuracy and cross-entropy loss ontraining, validation and test sets for our method on six differentdatasets as described in Table 2. The performance on the test set isfairly close to that on the training set, indicating that our methodavoided overfitting due to the usage of validation set and early stopstrategy. Among the six datasets, we achieved the best prediction onthe MCF-7 dataset with 0.8411 accuracy on its test set. With regardto the model efficiency, our model took around 26-41 trainingepochs until convergence. The training time for each epoch was between 350 and 699s which was roughly proportional to the size ofdataset, while the whole training period consumed about 2.5–7.9h.Next, we compared the performance of our proposed methodand several baseline methods, including the gapped k-mer SVM(gkmSVM) (Ghandi et al., 2014), DeepSEA (Zhou andTroyanskaya, 2015). For gkmSVM, we used the source code published on website http://www.beerlab.org/gkmsvm/. For DeepSEA,we implemented it ourselves using Keras. We slightly modified thenetwork structure of DeepSEA to make it suitable for our task.Besides, to directly prove the effectiveness of k-mer embedding, wealso propose a new variant network named ‘one hot’, which ismainly comprised of an embedding layer (embedding A, C, G, Tusing one-hot encoding), three convolutional layers, and a BLSTMlayer. For evaluation purpose, we computed two often-used measures, the area under the receiver operating characteristic curve(auROC) and the area under the precision-recall curve (auPRC), onthe test set.We reported the classification performance measured in auROCand auPRC on the six datasets in Table 3. The results of twoTable 2. Training details for our method on each dataset, includingaccuracy and cross-entropy loss on training, validation and testsets, number of epochs, average training time for each epoch andtotal training timeGM12878Train lossVal lossTest lossTrain accVal acctest acc# epochsTimeTotalK5620.41940.43460.43520.80800.79400.794734350 s3.3 h0.41610.43970.43420.81050.79620.795935559 s5.4 hGM12878K562MCF-7HeLa-S3H1-hESCHepG2CodeENCSR000EMTENCSR000EPCENCSR000EPHENCSR000ENOENCSR000EMUENCSR000ENPSizel_max l_min l_mean l_median24469241862450381626426426686828314811481133071204111557779514425363636363636610.95675.21471.86615.85430.26652.73381423361420320406Note: Code denotes the corresponding code in ENCODE project, size denotes the number of sequences the dataset contains, and l_max, l_min,l_mean and l_median denote the maximum, minimum, mean and medianvalue of sequence lengths in bps, respectively. Note that in our datasets weremoved regions shorter than 36 bps, so l_min is always 36 bps.0.33770.36340.35950.85620.83970.841141699 s7.9 h0.38180.39760.40120.83330.82030.818036374 s3.7 h0.38000.38130.37480.83020.82230.825226357 s2.5 h0.43360.45140.44400.80300.78410.787133392 s3.5 hTable 3. Classiﬁcation performance for three different methods inchromatin accessibility prediction experimentsGM12878 K562Cell typeMCF-7 HeLa-S3 H1-hESC HepG2(a) auROCgkmSVMDeepSEAOne hotour method(b) auPRCgkmSVMDeepSEAOne hotour methodMCF-7 HeLa-S3 H1-hESC HepG20.85280.87880.87110.88300.82030.86290.86340.88090.89670.92000.90450.92120.86480.89030.89090.90160.89830.88270.90810.90970.83590.86090.85100.87220.84420.87580.86790.87740.80810.85510.85670.87320.88600.91460.89970.91560.86270.88880.89000.89920.88230.87050.89600.89680.81230.85080.84180.8630Note: The top table records auROC values while the bottom one recordsauPRC values. Best results are shown in bold.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018Chromatin accessibility prediction with k-mer embeddingmeasures are consistent with each other. As we can see, with thehelp of k-mer embedding, our method consistently surpasses theother three baseline methods. On average, our method shows anauROC score 3.9% higher than gkmSVM, and 1.4% higher thanDeepSEA. On the MCF-7 dataset, our method yields the best performance with auROC of 0.9212 and auPRC of 0.9156. On theK562 dataset, our method obtains the most significant improvementcompared to gkmSVM, with auROC increasing by 7.4% andauPRC increasing by 8.1%. Besides, our method always outperforms the one hot method on six datasets, with 0.013 higherauROC score and 0.012 higher auPRC score on average. This observation enlightens us that the straightforward one-hot encoding isperhaps not the optimal strategy for representation of DNA sequences. In contrast, k-mer embedding integrates the contextual information of k-mers and accordingly improves the featurerepresentation. In addition, gkmSVM shows worse performancethan DeepSEA except for the H1-hESC dataset, convincing us thatdeep learning, which can automatically learn feature representation,is more powerful than SVMs with handcrafted k-mer features. Tomake our results more solid, we additionally carry out 10-fold crossvalidation experiments on the six datasets, showing that our modelsignificantly outperforms DeepSEA (Supplementary Tables S2–S7).We also run our model several times with different random seeds,showing stability of our model (Supplementary Table S8).It is also noteworthy that our method gained superiority ofrunning-time owing to GPU usage. For the H1-hESC dataset, ourmethod consumed only 2.5 h as shown in Table 2. MeanwhilegkmSVM allocated with sixteen threads costed 23.8 h before convergence, meaning that our method is nearly 10 times faster thangkmSVM. Thus benefitting by computer hardware, our approachallows researchers to obtain models of high accuracy within a shorttime.For the sake of implementation efficiency, we used the commonused zero-padding and truncating strategy in LSTM networks,which is to pad zeros on the right of short sequences and truncatelong sequences to a maximum length, and then adopt the standardbatch gradient descent (Wilson and Martinez, 2003). Given thelength distributions shown in Figure 3, we set the maximum lengthto 2000 bps in our experiments. To explore the effect of this hyperparameter, we changed the maximum length in range of 2000,1500, 1000 and 500 bps, and re-trained our model on theGM12878 dataset. We reported the our model performance withi97different maximum length in Table 4. We find that when we decrease the maximum length, the auROC and auPRC scores will decrease slightly, except for a fixed length of 500 bps that leads tosignificant drop in performance, since some input sequences aretruncated leading to information loss. We also find that small maximum length will decrease training time remarkably, because the reduction of input dimension directly decreases computationalcomplexity.3.3 Visualization of k-mer embeddingIn order to make our model more interpretable, we proceed with athorough investigation about k-mer embedding in this subsection.The primary specialty discriminating our method from other stateof-the-art deep learning methods in genomic analysis is that we utilize the k-mer embedding vectors trained by GloVe, an unsupervisedlearning algorithm, as the representation of DNA sequences.Training is based on aggregated global kmer-kmer co-occurrencestatistics from a corpus of k-mer sequences, and the resulting representations showcase linear substructures of the k-mer vector space,which benefit subsequent classification tasks.Take the MCF-7 dataset, for example, we first split its positivesamples, i.e. chromatin accessible regions, into k-mer sequences.Thus, taking each k-mer as a word and each k-mer sequence as asentence, we have a corpus with vocabulary of size V ¼ 4096. Welist the top five most frequent k-mers with frequency appended,(aaaaaa, 106157), (tttttt, 104409), (gggagg, 50694), (tgtgtg, 50605)and (cctccc, 50593). The least frequent k-mer is cgtacg occurringonly 607 times in total. The overall distribution of k-mer frequenciesis depicted in Figure 4a with k-mers ranked by frequency. The symmetric co-occurrence matrix in Figure 4b is normalized via a logarithm function log10 to make patterns more visible. Entries withvalue <1.0 exist because we applied a distance-weighted function incalculating co-occurrence matrix in practice. There are also 25 823zero entries displayed in white color which are often found in theright bottom corner.Intuitively, we can see the co-occurrence matrix contains complex patterns and wealthy information of k-mer dependencies onDNA sequences. The 100-dimensional embedding vectors of k-merslearned by GloVe are exhibited in Figure 4c. Statistically, the embedding matrix has a mean value of 0.0016, a maximum value of1.9883 and a minimum value of –1.9419. We find the weights havea recognizable distribution, i.e. Gaussian distribution with P-value1.7e-16. Thereby information is relatively decentralized on each dimension which is a good property for feature representation. Tovisualize the learned k-mer vectors, we show the dimension reduction results using two techniques, Principal Component Analysis(PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE)(Maaten and Hinton, 2008) in Figure 4d. We put all k-mers as colored circle points in the plot, and we use a continuous colormap torepresent their frequency order. Interestingly, although the twoTable 4. Our model performance on the GM12878 dataset with different maximum length of input sequencesLength (bps)auPRCTime (s/epoch)# epochsTotal (h)200015001000500Fig. 3. Violin plot for length distribution of DNA sequences in six chromatinaccessibility datasets. The width of each violin indicates the dataset size, andthe middle red line shows the mean values of sequence lengthsauROC0.88300.87930.87470.85280.87740.87450.86820.844435024916571343237303.32.21.70.6Note: The auROC scores, auPRC scores, average training time for eachepoch, number of epochs and total training time are shown.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018i98(a)X.Min et al.(b)Table 5. k-mer pairs are divided into six groups according to theiredit distanceEdit distance# pairsCosine distanceMean(c)12345636 864355 4941 602 3783 272 9942 560 482558 348Std Dev0.52720.81570.94341.00061.04701.08220.14660.16950.14140.14850.13200.1352Note: For each group, we list the number of pairs it contains, and the meanvalue and the standard deviation of their cosine distances between k-mervectors.(d)Fig. 4. Visualization of k-mer statistics and k-mer embedding vectors in MCF-7dataset. The top left ﬁgure shows the frequency of k-mers. The top right ﬁgure illustrates the co-occurrence matrix of k-mers with a log-normalized colormap. The middle ﬁgure demonstrates the k-mer vectors produced by GloVe.In the bottom ﬁgure, we visualize all the k-mers by projecting their vectorsinto a plane using PCA and t-SNEmethods generate different results, they both declare that k-merswith close frequencies tend to be also adjacent in the embedding vector space.To further interpret k-mer vectors, we explore how the cosinedistance between k-mer vectors is related to the edit distance between k-mers themselves. There are totally 4096 Â 4095/2 ¼ 8386560 pairs of k-mers, and we compute the pairwise cosinedistance of k-mer vectors and the pairwise edit distance of k-mers.Since there are only six characters in one k-mer, the edit distance hasonly six possible values, i.e. (1, 2, 3, 4, 5, 6). Thus, we split all thepairs into six groups according to the edit distance. We try to lookat each group to see the distribution of their corresponding k-mervector distances. We summarize the statistics in Table 5 and visualize the distribution of vector distances in Figure 5. In general, wefind that the cosine distance between k-mer vectors is monotonicallyincreasing with the edit distance between k-mers. This phenomenondeclares the rationality of our k-mer embedding in that the more unlike the two k-mers are, the more faraway they are in the embeddingspace. For more details, see Supplementary Fig S1, Tables S9 andS10. Besides, we also attempt to investigate the association betweenthe enriched k-mers and the location of these k-mers in chromatinaccessible regions, and further the relationship between the enrichedk-mers and the DNase-seq signal strength, in Supplementary Fig. S2and S3. Moreover, we try to find the most specific k-mers corresponding to each cell line, in Supplementary Tables S11 and S12.Fig. 5. k-mer edit distance versus k-mer vector cosine distance. Each violin describes the distribution of the k-mer vector cosine distances in each group ofk-mer pairs, where the intermediate line represents the mean value while thelines on two ends represent extreme values3.4 Efficacy of k-mer embeddingIn our method, the embedding layer is fed with sequences of integers, i.e. k-mer indexes, and then map them to vectors found atthe corresponding index in the GloVe embedding matrix. The pretrained k-mer vectors provide a decent initialization and they willbe further fine-tuned during training process. To prove the efficacy of our k-mer embedding, we here put forward two other different embedding strategies. One is to keep the embedding layerfixed during training. The other is that we instead initialize ourembedding layer from scratch and learn its weights throughtraining.We demonstrate auROC scores for the above three embeddingstrategies on the six datasets in Figure 6. The average auROCscores for ‘-init -train’, ‘-init -notrain’ and ‘-noinit’ strategies are0.8948, 0.8756 and 0.8726, respectively. We observe that our original strategy brings the best performance as expected. In fact, thecombination of unsupervised pre-training and supervised finetuning has been proved to be successful in deep learning (Hintonand Salakhutdinov, 2006; Bengio et al., 2007). Moreover, themodel reaches a relatively high accuracy just using k-mer embedding vectors without any fine-tuning, consistently outperformingthe strategy not using pre-trained vectors. This tells us that, in general, the pre-trained k-mer embedding vectors definitely buy ussomething of substantial value, and do help improve the model accuracy. For more details, see Supplementary Table S13.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018Chromatin accessibility prediction with k-mer embeddingi99(a)(b)(c)Fig. 6. Model performance for different embedding strategies. ‘-init -train’means that we initialize the embedding layer using the GloVe k-mer vectors,which is adopted in our proposed model. ‘-init -notrain’ means that we initialize the embedding layer using the GloVe k-mer vectors, but prevent theweights from being updated during training. ‘-noinit’ means that we randomly initialize the embedding layer using uniform distribution and allow theneural network to adjust the embedding weights in supervised learningTable 6. auROC scores and running time of two variant deep learning architectures and our original modelGM12878K562 MCF-7 HeLa-S3 H1-hESC HepG2(a) auROC scoresfull0.8830 0.8809 0.9212no conv 0.8746 0.8677 0.9008no lstm0.8746 0.8741 0.9156(b) running time for each epochfull350s559s699sno conv693s1173s 1399sno lstm331s577s690s0.90160.88780.89540.90970.90490.9087374s739s368s357s755s350s D0.87220.8608 –0.01200.8672 –0.0050392s797s384s465.8s–11.8sNote: ‘Full’ represents the original architecture comprised of three stages,including a embedding stage, a convolution stage and a BLSTM stage; ‘noconv’ represents the variant architecture removing the convolution stage; ‘nolstm’ represents the variant architecture removing the BLSTM layer, which is substituted by a ﬂatten layer. D computes the average difference of auROCscores and running time between the two variants and the full model.3.5 Efficacy of convolutionIn our experiment, the convolution stage contains three convolutional layers with 100, 100 and 80 one-dimensional filtering kernelsof length 10, 8 and 8, each followed by a max-pooling layer withpooling length 4, 2 and 2, respectively. In this subsection, we provethe efficacy of the convolution stage by proposing a variant deeparchitecture getting rid of the convolutional layers and max-poolinglayers from the full model. We directly use the output of the embedding layer as the input of the BLSTM layer instead.We report the auROC scores and running time of this variantmodel and compare them against the original full model in Table 6.We discover that removing convolutional layers can obviously injurethe auROC score, leading to a 0.0120 decrease on average. Thisproves the effectiveness of convolutional operations in detecting spatial motifs. With regard to the running time, we find the variantarchitecture takes almost twice the time to train the network thanthe full architecture. The reason lies in that the max-pooling layerswhich down-sample the input representation, play an important rolein reducing dimensionality and allowing huge decrease inFig. 7. Sensitivity analysis of hyper-parameters k, the embedding dimensionD, and the splitting stride s, performed on the GM12878 dataset. The auROCscores on the test set are reportedcomputation complexity. In view of the above two aspects, we addin the convolution stage in our deep learning framework. For moredetails, see Supplementary Tables S14 and S15.3.6 Efficacy of BLSTMBLSTM is critical to the processing of variable-length input sequences, and also effective in capturing long-term dependencies. Inour method, we have a BLSTM layer with dimension d set to 80. Toconfirm the efficacy of the BLSTM stage, we construct another variant of our deep learning architecture by retraining the embeddinglayer and convolutional layers and removing the BLSTM layer. Wedirectly use the flattened output of the convolutional layers insteadas our final feature vector for classification.The auROC scores and the running time for each epoch of thisvariant architecture and our original full architecture are recordedin Table 6. As expected, the full model always performs better thanthis variant not using BLSTM, with a 0.0055 improvement inauROC score on average. The running time for the two architecturesis quite close to each other, meaning that BLSTM does not increasethe computation amount significantly. Note that only the convolutional layers with max-pooling are incapable of dealing with variable length sequences, unless we adopt the aforementioned zeropadding strategy to truncate each sample into the same shape, or weuse global pooling (He et al., 2014) instead of max-pooling whichextract fixed-dimensional features for temporal data while lose lotsof information unfortunately. The results in Table 6a are generatedusing the former strategy, while the latter one will surely demonstrate even worse performance. Therefore, the BLSTM stage is indispensable in our deep learning architecture for its ability to cope withvariable-length sequences and to capture the rich long-term dependencies. For more details, see Supplementary Tables S14–S16.3.7 Sensitivity analysisTo finish our discussion, we carry out sensitivity analysis to checkthe robustness of our model. We focus on the following three hyperparameters: the k-mer length k, the embedding dimension D, andthe splitting stride s. Without loss of generality, we use theGM12878 dataset for sensitivity analysis experiments.According to Figure 7, our model is insensitive to the choice of kand D. Too large k will bring explosive growth of the vocabulary,Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018i100X.Min et al.while too small k may give us too little useful information on thesmall co-occurrence matrix. We tried three appropriate values of kfrom 5 to 7, retrained the k-mer embedding vectors, and obtainedextremely close performance in classification. Similarly, we alsotried four different values of the embedding dimension D, including50, 100, 150 and 200. A larger D results in more weight parametersto learn in the embedding layer, which will increase the model complexity. Despite this, our model demonstrates quite stable performance, reflecting its ability to avoid overfitting.The splitting stride s will affect the number of k-mers transformed from one DNA sequence byL ¼ bðL0 À kÞ=sc þ 1:design a more elegant embedding algorithm for vector representation of variable-length sequences. Last but not the least, large-scaleexperiments of using our model to analyze various kinds of genomics and epigenomics sequencing data is highly encouraged.Eventually, we hope our deep learning method can allow us achieveexcellent performance in DNA sequence analysis and help boost ourunderstanding of chromatin accessibility mechanism.AcknowledgementWe thank the anonymous reviewers for their helpful suggestions.(14)We find that the auROC score decreases from 0.8830 to 0.8143when we increase the stride s from 2 to 6. A larger s will decreasethe corpus size as Equation (14) said, making part of co-occurrenceinformation lost, and thereby possibly damaging the embedding representation. We do not use the smallest s ¼ 1, since it may cause atoo large k-mer corpus and too large overlap between adjacentk-mers, also perhaps harming the embedding algorithm.Considering the above facts, we here recommend a proper splittingstride s ¼ 2 to make full use of k-mer co-occurrence information.4 ConclusionIn this paper, we propose a convolutional long short-term memoryneural network with the pre-trained k-mer embedding vectors topredict chromatin accessible regions from mere sequence information. Our major contributions can be summarized as bellow. First ofall, we innovatively introduce an effective embedding representationof input DNA sequences using the unsupervised learning algorithmGloVe in the deep learning framework. Instead of using one-hotencoding, we use the k-mer embedding vectors which have absorbedthe statistical information of k-mer co-occurrence relationship andare conducive to following classification tasks, for feature representation. Secondly, we are capable of handling variable-length sequences as input by exploiting the BLSTM network. One bigobstacle that hinders the application of CNNs in DNA sequencemodeling is the variation in sequence lengths. We utilize the BLSTMnetworks, not only to make our model appropriate for variablelength input sequences, but also to capture complex long-termdependencies on them. Moreover, we prove our model producesstate-of-the-art performance in sequence classification tasks, compared to other baseline methods. We visualize the embedding vectors obtained by GloVe and demonstrate the effectiveness of k-merembedding. We provide an in-depth understanding of our deeplearning architecture, by showing the efficacy of both the convolution and BLSTM stages in feature learning, and showing the robustness of our model to hyper-parameters.Certainly, our work can possibly be further improved in severalaspects. First, the attention mechanism has been successfully introduced to LSTM and improved the performance of neural machinetranslation (NMT) (Luong et al., 2015). Attention mechanism canselectively focus on parts of sentences during training, and maybethis can be used in our DNA modeling to help detect and visualizethe important motifs on sequences. Second, we learn k-mer vectorsto embed a DNA sequence into a sequence of dense vectors which isstill in variable length, and then extract the fixed-length featuresthrough BLSTM. In fact, there exist methods to directly learn distributed representations of sentences and documents, such asParagraph Vector (Le and Mikolov, 2014), which motivate us toFundingThis research was partially supported by the National Natural ScienceFoundation of China (Nos. 61573207, 61175002, 71101010, 61673241,61561146396).Conﬂict of Interest: none declared.ReferencesAlipanahi,B. et al. (2015) Predicting the sequence speciﬁcities of DNA-andRNA-binding proteins by deep learning. Nat. Biotechnol., 33(8), 831–838.Bengio,Y. et al. (1994) Learning long-term dependencies with gradient descentis difﬁcult. IEEE Trans. Neural Netw., 5, 157–166.Bengio,Y. et al. (2007). Greedy layer-wise training of deep networks. In:Advances in Neural Information Processing Systems (NIPS), 19, p.153–160.Chollet,F. (2015). Keras. https://github.com/fchollet/keras.Consortium,E.P. et al. (2004) The encode (encyclopedia of DNA elements)project. Science, 306, 636–640.Crawford,G.E. et al. (2006) Genome-wide mapping of dnase hypersensitivesites using massively parallel signature sequencing (mpss). Genome Res., 16,123–131.Duchi,J. et al. (2011) Adaptive subgradient methods for online learning andstochastic optimization. J. Mach. Learn. Res., 12, (Jul), 2121–2159.Ghandi,M. et al. (2014) Enhanced regulatory sequence prediction usinggapped k-mer features. PLoS Comput. Biol., 10, e1003711.Harris,Z.S. (1954) Distributional structure. Word, 10, 146–162.He,K. et al. (2014). Spatial pyramid pooling in deep convolutional networksfor visual recognition. IEEE transactions on pattern analysis and machineintelligence (TPAMI), 37, p.1904–1916.Hinton,G.E., and Salakhutdinov,R.R. (2006) Reducing the dimensionality ofdata with neural networks. Science, 313, 504–507.Hochreiter,S. (1998) The vanishing gradient problem during learning recurrent neural nets and problem solutions. Int. J. Uncertain. Fuzz. KnowledgeBased Syst., 6, 107–116.Hochreiter,S., and Schmidhuber,J. (1997) Long short-term memory. NeuralComput., 9, 1735–1780.John,S. et al. (2011) Chromatin accessibility pre-determines glucocorticoid receptor binding patterns. Nature Genet., 43, 264–268.Kelley,D.R. et al. (2016) Basset: learning the regulatory code of the accessiblegenome with deep convolutional neural networks. Genome Res., 26(7),990–999.Kim,Y. (2014). Convolutional neural networks for sentence classiﬁcation. In:Conference on Empirical Methods on Natural Language Processing(EMNLP), Association for Computational Linguistics (ACL), pp.1746–1751.Krizhevsky,A. et al. (2012). Imagenet classiﬁcation with deep convolutionalneural networks. In: Platt, J.C. et al. (eds) Advances in Neural InformationProcessing Systems, NIPS. Curran Associates, NY 12571. pp.1097–1105.Le,Q.V., and Mikolov,T. (2014). Distributed representations of sentences anddocuments. In: ICML, Vol. 14, p.1188–1196.Lee,D. et al. (2011) Discriminative prediction of mammalian enhancers fromdna sequence. Genome Res., 21, 2167–2180.Luong,M.-T. et al. (2015). Effective approaches to attention-based neuralmachine translation. In: Conference on Empirical Methods on NaturalDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018Chromatin accessibility prediction with k-mer embeddingLanguage Processing (EMNLP), Association for Computational Linguistics(ACL), pp.1412–1421.Maaten,L. v d., and Hinton,G. (2008) Visualizing data using t-SNE. J. Mach.Learn. Res., 9, (Nov), 2579–2605.Mikolov,T. et al. (2013). Distributed representations of words and phrases and theircompositionality. In: Burges, C.J.C. et al. (eds) Advances in Neural InformationProcessing Systems, NIPS. Curran Associates, NY 12571. pp. 3111–3119.Min,X. et al. (2016). DeepEnhancer predicting enhancers by convolutionalneural networks. In: IEEE International Conference on Bioinformatics andBiomedicine, IEEE, pp. 637–644.Niwa,H. (2007) Open conformation chromatin and pluripotency. GenesDev., 21, 2671–2676.Pennington,J. et al. (2014). GloVe: global vectors for word representation. In:EMNLP, volume 14, p.1532–43.Sønderby,S.K. et al. (2015). Convolutional LSTM networks for subcellularlocalization of proteins. In: International Conference on Algorithms forComputational Biology, p.68–80. Springer International Publishing.i101Tai,K.S. et al. (2015). Improved semantic representations from tree-structuredlong short-term memory networks. In: Annual Meeting of the Associationfor Computational Linguistics, p.1556.Tieleman,T., and Hinton,G. (2012). Lecture 6.5 - rmsprop, COURSERA:Neural networks for machine learning, 4.Vierstra,J. et al. (2014) Coupling transcription factor occupancy to nucleosome architecture with DNase-ﬂash. Nat. Methods, 11, 66–72.Wang,Y. et al. (2016) Modeling the causal regulatory network by integratingchromatin accessibility and transcriptome data. Natl. Sci. Rev., 3(2),240–251.Wilson,D.R., and Martinez,T.R. (2003) The general inefﬁciency of batchtraining for gradient descent learning. Neural Netw., 16, 1429–1451.Zeng,H. et al. (2016) Convolutional neural network architectures for predicting DNA-protein binding. Bioinformatics, 32, i121–i127.Zhou,J., and Troyanskaya,O.G. (2015) Predicting effects of noncodingvariants with deep learning-based sequence model. Nat. Methods, 12,931–934.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i92/3953949by gueston 07 January 2018
5028881968002	PMID28881968	5028881968	https://watermark.silverchair.com/btx233.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881968.main.pdf	Bioinformatics, 33, 2017, i30–i36doi: 10.1093/bioinformatics/btx233ISMB/ECCB 2017SnapDock—template-based docking byGeometric HashingMichael Estrin* and Haim J. Wolfson*Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel*To whom correspondence should be addressed.AbstractMotivation: A highly efﬁcient template-based protein–protein docking algorithm, nicknamedSnapDock, is presented. It employs a Geometric Hashing-based structural alignment scheme toalign the target proteins to the interfaces of non-redundant protein–protein interface libraries.Docking of a pair of proteins utilizing the 22 600 interface PIFACE library is performed in < 2 min onthe average. A ﬂexible version of the algorithm allowing hinge motion in one of the proteins is presented as well.Results: To evaluate the performance of the algorithm a blind re-modelling of 3547 PDB complexes, which have been uploaded after the PIFACE publication has been performed with successratio of about 35%. Interestingly, a similar experiment with the template free PatchDock docking algorithm yielded a success rate of about 23% with roughly 1/3 of the solutions different from thoseof SnapDock. Consequently, the combination of the two methods gave a 42% success ratio.Availability and implementation: A web server of the application is under development.Contact: michaelestrin@gmail.com or wolfson@tau.ac.il1 IntroductionPrediction of protein–protein interactions and the structures of theresulting complexes is a key task in Computational StructuralBiology. Although experimentally determined structures of complexes are rapidly accumulating, they are far from being able tocover the complete interactome (Stumpf et al., 2008). To bridge thisgap a myriad of docking algorithms have been developed (Andrusieret al., 2008; Bonvin, 2006; Halperin et al., 2002; Huang, 2014;Smith and Sternberg, 2002). The CAPRI community wide experiment (Janin, 2005; Janin et al., 2015) has significantly contributedto the development of docking algorithms and to an improvedunderstanding of the computational challenges involved.Over the years two major paradigms of protein–protein dockinghave emerged. The first one is the, so called, ab initio or templatefree docking, where the task is to model the structure of the protein–protein complex given the experimental (or modelled) structures ofthe individual docking partners without having prior knowledge ofthe interface structure. In the last decade with the improved coverage of experimentally determined protein–protein interfaces(Cukuroglu et al., 2014; Gao and Skolnick, 2010; Kundrotas et al.,2012) template-based docking (TBD) methods have emerged, wherea database of structural interfaces is scanned and the candidatedocking proteins are aligned to both sides of the interface either bystructural alignment or by threading the candidate chains on theinterface structure (Muratcioglu et al., 2015; Szilagyi and Zhang,2014). The major advantages of TBD are higher computationalspeed and more reliable interface modelling, which is based on mimicking of experimentally derived interfaces.One of the first TBD methods, which is based on an underlyinginterface database was PRISM (Ogmen et al., 2005). In its recentversion (Tuncbag et al., 2011), PRISM is based on two algorithms,which have been developed in our Lab, MultiProt (Shatsky et al.,2004) and FiberDock (Mashiach et al., 2010). For each interface inthe template database of PRISM MultiProt is used to structurallyalign the target proteins to both sides of the interface. The resultingmodelled interfaces, which passed successfully structural and hotspot correspondence filtering, are submitted to flexible interface refinement by FiberDock, which allows both side chain and limitedbackbone flexibility of the resulting interface and computes a binding energy score for the interface. The candidate modelled interfacesare ranked by the FiberDock energy score. PRISM has shown significantly faster performance than ab initio docking methods, especially, in large-scale docking.In this study, we introduce SnapDock, a new TBD algorithm,which adheres to the general PRISM scheme of structural alignmentfollowed by flexible interface refinement and scoring of the candidate interfaces. The structural alignment step of SnapDock is performed by a Geometric Hashing type procedure (Lamdan andWolfson, 1988; Nussinov and Wolfson, 1991). Its advantages are2-fold. First, it is significantly faster than MultiProt and second, itsCV The Author 2017. Published by Oxford University Press.i30This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i30/3953948by gueston 07 January 2018SnapDockstructural alignment, which is based on pairs of separated Ca atoms,is truly sequence order independent, while for the MultiProt alignment to be reliable it requires a short consecutive matching fragment. We also present a Flexible Template-Based Docking scheme,which allows hinge-based flexibility in one of the molecules.SnapDock was validated on the ZLAB 4.0 Docking Benchmark(Hwang et al., 2010) using the Dockground (Douguet et al., 2006)and PIFACE (Cukuroglu et al., 2014) template libraries. To mimic“real-life” large-scale docking, we applied SnapDock with thePIFACE library to model the interfaces of all the (non-redundant)complexes (some of them multimeric), which have been added to thePDB from February 26, 2014 and till September 19, 2016 (PIFACEwas published in January 2014). SnapDock received results with at˚most 5 A ligand RMSD deviation in one of the interfaces for 1284out of 3547 complexes (34.83%), while running the ab initio docking PatchDock algorithm (Duhovny et al., 2002) resulted in a success rate of 23.28%. Notably, about 1/3 of the PatchDock “hits”were different from the ones of SnapDock, resulting in a combinedsuccess ratio of 41.89%. These results agree with the findings ofVreven et al. (2014) regarding the partial complementarity of template-based and template-free docking methodologies.2 Materials and methodsThe SnapDock TBD algorithm is a multi-stage protocol for protein–protein docking, using available protein–protein interface templateinformation in its modelling procedure. The stages of the algorithminclude the processing of input template libraries, a novel structuralalignment procedure [inspired by the Geometric Hashing (Lamdanand Wolfson, 1988) approach originated in computer vision applications] and the use of previously developed software for interfacerefinement and protein flexibility detection. The structural alignment procedure is similar in its mathematical principles to the geometric docking method that we previously developed for PatchDock(Duhovny et al., 2002).The SnapDock algorithm is divided into two major stages:1. Preprocessing stage—In this stage, a template library of protein–protein interactions is preprocessed in order to extract critical structural features from the interacting interfaces and their surroundings.The extracted feature set is stored as a ﬁle on a persistent storage—named the table ﬁle, later to be used by the query stage.2. Query stage—In this stage, given an input query of two proteinmolecules, the algorithm predicts their binding complex wherethe interface induced by their binding conformation is structurally similar to an interface in the template library. The algorithmuses the features extracted in the preprocessing stage to structurally align the interfaces and to create a superimposed bindingcomplex model.2.1 Template librariesThe quality of the results that the algorithm produces is highly dependent on the selection of the underlying template library. On theone hand, the library must be diverse enough to cover all the different interfaces currently represented in the Protein Data Bank. Onthe other hand, it must be restricted enough to filter out erroneous,low-quality and biologically irrelevant interfaces. We selected andtested two different state of the art libraries which tackled the taskby two slightly different approaches.1. The DOCKGROUND library (Anishchenko et al., 2015) fortemplate docking is a manually curated set of interfaces that wasi31computationally reﬁned and optimized for high quality. The library consists of 5936 non-redundant protein-protein interfaces.2. The PIFACE library (Cukuroglu et al., 2014) was created byclustering all the available protein–protein interfaces in theProtein Data Bank. The approach that was used is constructionof a protein interface similarity network graph and ﬁnding communities (sub-graphs with dense inter-connectivity) in the network. The library (published in January 2014) consists of22 604 non-redundant protein–protein interfaces.Both libraries were tested and the results produced by the SnapDockalgorithm were evaluated.2.2 Feature extractionA base is defined as a set of features extracted from the structure ofa protein molecule, which is both sufficient for unambiguous definition of a 3D Euclidean reference frame and also enables the definition of a structural signature that is invariant to rigid 3D motion(rotation and translation). In SnapDock, we use both the coordinates of Ca/Cb atoms and the Secondary Structure Elements to definesuch bases. Specifically, for each two residues A, B on a given protein backbone we defined a base if those residues meet the followingcriteria:1. The residues A and B are not located on the same SecondaryStructure Element of the protein.2. The Euclidean distance between the Ca coordinates of the resi˚˚dues is at least 4 A and at most 13 A.   !The base is defined by the two Ca Cb vectors of the residues, whilethe 3D motion invariant signature is defined by the 4-tuple (d, a, b,x) as follows (see Fig. 1):1. The Euclidean distance d between the Ca coordinates.2. The angles a, b formed between the line segment connecting the   !Ca atoms and the line segments Ca Cb for each of the residues.3. The torsion angle x between the plane induced by the Ca coordinates and the Cb of the ﬁrst residue and the plane induced bythe Ca coordinates and the Cb of the second residue.The signature is the index/key used to insert the bases into a hashtable in order to find matching bases. Two bases are consideredmatching, if their signature parameters are close enough, up to thefollowing thresholds:˚1. Euclidean distance difference: DðdÞ < 1:5 A2. Angles difference: DðaÞ < 0:4c DðbÞ < 0:4c and DðxÞ < 0:5c3. Sum of angle differences: DðaÞ þ DðbÞ þ DðxÞ < 0:9c2.3 Preprocessing stageDuring the preprocessing stage, the bases that represent the modelinformation of a protein–protein interface template library are extracted. We filter the residues and only consider the ones that are˚located in the vicinity of the binding site (up to 12 A). For each template, the bases are stored in a 4D-quantized hash table, where the4D key is the structural signature of the base. The bin size, for eachdimension, is the corresponding matching threshold for theFig. 1. A base formed by two residues and their Ca and Cb atomsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i30/3953948by gueston 07 January 2018i32signature, as described in Section 2.2. The collection of hash tablesis serialized to a file and stored in a persistent storage.2.4 Docking protocol—query stage2.4.1 MatchingWe begin with extracting bases from the docked molecules andmatching them to the pre-computed template bases loaded from thetable file. The bases are matched quickly by performing a signaturelook-up in the hash table. Each match defines a rigid transformation(alignment) between one of the docked molecules and the corresponding template interface (Fig. 2).2.4.2 Clustering and votingEach matching pair of bases uniquely defines a rigid 3D transformation (pose), but provides little evidence about the alignment of theentire binding site. The entire collection of matched pairs is used toassess which poses represent a high-confidence structural alignment.This is done by using the Pose Clustering (Stockman, 1987) votingtechnique. The rational is that a good structural alignment will havea high number of locally matched base pairs that yield the same (orvery close) pose.The generated poses are stored in a “voting” hash table using thesix-dimensional transformation parameters (for rotation and translation), such that each bin in the table represents a voxel in this 6Dspace. The number of poses for each bin is counted and only binswith a count exceeding the “above-expected” threshold (typically10–15 matches) are passed to the next stage. Later, RMSD clustering of the poses is applied to eliminate the redundancy of almostsimilar results and to reduce the number of possible solutions.In order to determine the above-mentioned threshold we modelthe pose-voting process as an occupancy problem of placing m balls(poses) into n bins. Thus, in a random placement procedure theprobability of having at least k balls in any given bin can be boundedby:!   m1 kPr½X ! k nkM.Estrin and H.J.WolfsonConsequently, for a given number of matching features we set thethreshold k to be such that the probability of passing the thresholdin a random voting procedure is relatively small. Specifically, in thedescribed experiments we set the voting threshold to satisfyPr½X ! k 2À10 .Each pose is an alignment of one of the docked proteins to oneside of the template interface. The ligand–receptor complex pose isobtained by simultaneously superimposing both docked proteins onboth sides of the interface. As each side might have few alignmentcandidates, an exhaustive all-to-all enumeration of all possiblesuperimpositions is done (the number of poses passing the voting isusually small). The generated complexes are passed for filtering inthe next step.2.4.3 Filtering and refinementSince the poses are computed based on local structural alignment inthe vicinity of the template binding site, they may produce potentialcomplexes that are physically impossible with unacceptable stericclashes. To filter out the undesired results, we test for clashes using adistance transform grid, similar to the one applied in the PatchDockalgorithm (Duhovny et al., 2002). Using the grid we can calculatefor each transformed surface point of the ligand its distance fromthe surface of the receptor. If the distance is above a predefined˚penetration threshold (5 A), the result is rejected. For the results thatare not rejected the PatchDock geometric complementarity score iscalculated. This score favours shape complementarity and penalizesthe remaining steric clashes.At this point, we have a set of roughly rigidly docked complexes.We use the FiberDock flexible refinement algorithm (Mashiachet al., 2010) to considerably improve the docking accuracy of theassociated proteins and bring them to a near-native orientation. Therefinement algorithm accounts for both limited backbone and sidechain flexibility. The results are reranked using the FiberDock calculated energy score.2.5 Parallel computationOne way to boost the performance of the query phase is to parallelize the computation. Fortunately, the work to be done isEmbarrassingly Parallel. The bases for each template can bematched with the docked molecules concurrently. The SnapDockimplementation can spawn N worker threads (configured by theuser) where the templates are distributed between the threads usinga job queue. Once a thread is done working with a given template, itpulls the next template to process from the head of the queue.2.6 Flexible TBDThe basic docking protocol can be extended to accommodate additional scenarios, including protein flexibility. Below we describe anautomated docking protocol to account for large scale motions ofthe proteins backbone. It operates similar to the FlexDock algorithm(Schneidman-Duhovny et al., 2005) (Figs. 3 and 4).Fig. 2. Top level ﬂow chart of the docking protocol of SnapDock1. Partition into rigid parts—By applying the HingeProt method(Emekli et al., 2008) on the given input protein, we detect therigid parts and the hinge regions connecting them. The methodemploys Elastic Network Model to efﬁciently calculate the partition of the protein.2. Independent docking of the rigid parts—The matching, clustering and voting procedures are executed for each of the parts,producing a list of viable partial docking solutions for each partindependently.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i30/3953948by gueston 07 January 2018SnapDocki33Table 1. SnapDock success rate for ﬁnding a near native (ligand˚RSMD <5 A) for the Zlab Benchmark 4.0 test cases, for each one ofthe used template librariesDOCKGROUNDX rigid-bodyMediumHardPIFACE(48/109) 44.03%(5/25) 20%(4/20) 20%(83/109) 73.39%(11/25) 44%(4/20) 20%Fig. 3. Illustration of the layered solution graph. Nodes in each layer representpartial solutions of a given rigid part. The thick arrows represent assembliesof consistent solutions for all the rigid partsFig. 5. The docking solution of 2CGA:B (pink) with 1HPT:A (purple) using thetemplate 1TBQ H:R (gray)Fig. 6. The docking solution of 1Y20:A (pink) with 2A5S:A (purple) using thetemplate 3KG2 A:D (gray)Fig. 4. Top level ﬂow chart of the ﬂexible docking protocol of SnapDock, usingthe building blocks of the rigid SnapDock protocol3. Assembly of a consistent solution—Using a connected layeredgraph, where each layer contains nodes that represent all thepossible solutions for one of the rigid parts. An edge is connecting two nodes if and only if the spatial distance between theN- and C-termini endpoints of the two consecutive rigid parts is˚below a given threshold (<5 A). A path that travels through allthe layers, connecting all the rigid parts, is a valid solution to thegeneral docking problem. Afterwards, the solutions undergo ﬁltering, scoring and reﬁnement as previously described.3 Results and discussion3.1 Docking validationTo evaluate the performance of our algorithm we used the 154 testcases available in the ZLab benchmark 4.0 (Hwang et al., 2010).The test cases are classified into three difficulty classes: rigid-body,medium and hard, based on the degree of the conformationalchanges the molecules undergo upon association. Each test case includes three structures. The structure of the two unbound moleculesto be docked as well as the structure of their co-crystallized complex, so we can evaluate our predicted complex by a ligand–RMSDmetric to the native bound complex counterpart.Table 1 includes the success rates for the total dataset, for eachone of the difficulty categories. We define a near-native (successful)˚result with Ca ligand RMSD cutoff of <5 A that appears among thetop 10 solutions that the algorithm produces.We will discuss in detail few representative solutions we encountered while running the benchmark. The test case involving dockingof Bovine Chymotrypsinogen (2CGA:B) (Wang et al., 1985) andBovine Plasma Retinol-binding Protein (1HPT:A) (Hecht et al.,1992) was solved using the template interface of the double domainKazal inhibitor rhodniin in complex with thrombin (1TBQ H:R)(van de Locht et al., 1995), illustrated in Figure 5. The templateinterface includes a double domain binding to two different bindingsites. SnapDock aligned the docked molecules on one of the domains. The sequence similarities between the docked molecules andthe template are 47 and 52%, respectively.The test case involving docking NR1 Ligand Binding Core(1Y20:A) (Inanobe et al., 2005) and NR2A Ligand Binding Core(2A5S:A) (Furukawa et al., 2005) was solved using the templateinterface between two Glutamate receptor membrane proteins(3KG2 A:D) (Sobolevsky et al., 2009), illustrated in Figure 6. Thedocking solution in this case is a partial structural alignment of thetemplate interface. The sequence similarities between the dockedmolecules and the template are 59 and 20%, respectively.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i30/3953948by gueston 07 January 2018i34M.Estrin and H.J.WolfsonFig. 7. The docking solution of 1RGH:B (pink) with 1A19:A (purple) using thetemplate 1B27 A:D (gray)Fig. 8. The docking solution of 2AYN:A (purple) with 2FCN:A (pink) using thetemplate 3MHS A:D (gray)Fig. 9. The ﬂexible docking solution of 1G0Y:R (pink) with 1ILR:1 (purple)using the template 1IRA X:Y (gray)The test case involving docking of Ribonuclease Hydrolase(1RGH:B) (Sevcik et al., 1996) and Barstar Ribonuclease Inhibitor(1A19:B) (Ratnaparkhi et al., 1998) was solved using the templateof a variation of the Ribonuclease–Barstar complex (1B27 A:D)(Vaughan et al., 1999), illustrated in Figure 7. The Ribonuclease ofthe docked molecule is taken from the Bacillus amyloliquefaciens organism while in the template it was taken from the Streptomycesaureofaciens organism, having only 45% sequence similarity.The test case involving docking of Ubiquitin carboxyl-terminalhydrolase 14 (2AYN:A) (Hu et al., 2005) and Ubiquitin (2FCN:A)(Bang et al., 2006) was solved using the template of the complex ofUbiquitin carboxyl-terminal hydrolase 8 and Ubiquitin (3MHS A:D)(Samara et al., 2010), illustrated in Figure 8. The sequence similaritybetween the two different Ubiquitin carboxyl-terminal hydrolase(14 versus 8) is 44%.The flexible SnapDock version was applied to the test caseinvolving the docking of Interleukin-1 Receptor Type I (1G0Y:R)(Vigers et al., 2000) with Interleukin-1 Receptor Antagonist Protein(1ILR:1) (Schreuder et al., 1995). The test case was solved using thetemplate of their crystallized complex (1IRA X:Y) (Schreuder et al.,1997), illustrated in Figure 9. HingeProt detected that theInterleukin Receptor has two rigid parts, the rigid parts were dockedindependently and the partial solutions were assembled to a consistent overall solution.Table 2. Average running time and consumed physical disk spacemeasured while running the benchmark in Section 3.13.2 Runtime performanceThe major advantage of the SnapDock algorithm (and all methodsusing Geometric Hashing) is the rapid performance, enabling to testtwo potential docking candidates against the entire known collection of protein–protein interfaces in mere minutes. The algorithmwas evaluated on a desktop workstation with two quad-core Inteli7-2600 3.4 GHz CPUs (8 cores total) and 16 GB DDR3 SDRAMmemory. The algorithm was implemented as a multi-threadedDOCKGROUNDNumber of interfacesAverage run-time (s)Table size (MB)PIFACE593641.19731.9322604109.861630.32programme fully utilizing all the available computational cores ofthe machine.Table 2 displays the average measured running time of theSnapDock algorithm working on the Zlab Benchmark 4.0, as wellas the physical disk space consumed by the pre-computed featuretables.To compare the speed of the algorithm to the PRISM method wehave run the same docking experiment with MultiProt (Shatskyet al., 2004) as the structural alignment method. We have used thesame input molecules and the same extracted binding sites from thetemplate library (as the alignment run time is dependent on the sizeof molecules). We measure the total run time of running the benchmark and divide it by the number of CPU cores used and by thenumber of template interfaces to get the average per-template running time. The results are shown in Table 3, where the structuralalignment module of SnapDock is shown to be more than 10 timesfaster compared with MultiProt.3.3 Interaction predictionOne major application of docking software is blind prediction ofprotein–protein interactions. It would be insightful to analyse howthe SnapDock algorithm can benefit in “real life” scenarios. Wetested the algorithm’s ability to predict the protein–protein interactions in a set of independent biological data. The collection ofDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i30/3953948by gueston 07 January 2018SnapDocki35Table 3. Average running time per-template measured while running the benchmark in Section 3.1SnapDockAverage run-time (s)MultiProt0.03880.4068We also present the success rate by independently combining thetwo methods. The 10 top solutions here are simply the top 5 solutions of SnapDock and the top 5 solutions of PatchDock. The comparison between the success rates is shown in Table 6, for predictingprotein-protein interfaces and in Table 7, for predicting at least asingle protein-protein interface in each PDB entry.Table 4. Success rate of ﬁnding a successful result for a giveninterfaceSuccess rate˚Under <10 A˚Under <5 A(2557/8964) 28.53%(2315/8964) 25.83%Table 5. Success rate of ﬁnding at least one successful interface ina PDB entrySuccess rate˚Under <10 A˚Under <5 A(1412/3686) 38.31%(1284/3686) 34.83%Table 6. Comparison of the success rate per method of ﬁnding asuccessful result for a given interfacePatchDockSnapDockCombined˚Under <10 A 17.75% (1591/8964) 28.53% (2557/8964) 33.58% (3010/8964)˚Under <5 A12.73% (1141/8964) 25.83% (2315/8964) 29.12% (2610/8964)Table 7. Comparison of the success rate per method of ﬁnding asuccessful result of at least one interface in a PDB entryPatchDockSnapDock4 ConclusionsWe have presented a new TBD algorithm, SnapDock, which utilizesan interface template library to perform Geometric Hashing-basedstructural alignment of the putative docking partners on all the library templates, discards solutions with severe steric clashes and, finally, refines the surviving modelled interfaces by allowing sidechain and limited backbone flexibility of the interacting proteinswhile ranking them by global energy. The method is highly efficientdue to the preprocessing of the template library, which converts itinto a Geometric Hashing table. Thus, docking with theDockground template library, which includes almost 6000 templatestook on the average 41 s per protein pair from the ZLAB version 4.0benchmark, while docking with the 22 600 template PIFACE librarytook on the average 110 s per protein pair. We have also demonstrated a flexible docking version of the algorithm which performedwell on several test cases. A large scale blind docking experiment,which aimed to model all the interfaces in the PDB, which have beenuploaded after the completion of the PIFACE template library(structures uploaded in the period February 2014–September 2016),yielded a 35% success rate for SnapDock, 23% success rate forPatchDock with a combined (non-overlapping) success rate of 42%.Future work will include introduction of additional protein shapeflexibility options in the docking scheme.Combined˚Under <10 A 31.15% (1148/3686) 38.31% (1412/3686) 48.02% (1770/3686)˚Under <5 A23.28% (858/3686) 34.83% (1284/3686) 41.89% (1544/3686)“future” experimentally determined structures from the PDB, whichwas not available at the creation time of the template library, wasselected as a qualifying data set. A re-docking (bound docking) trialwas run to roughly evaluate how the algorithm could a priori predictthe experimental results.The PIFACE library was chosen for this experiment. The template library was published in January 2014. Thus, we extracted allprotein–protein interfaces in the PDB with a deposition date laterFebruary 26, 2014, up to September 19, 2016, when our experimentwas conducted. Redundant interacting pairs were filtered out with athreshold of up to 50% sequence similarity. We remained with 8964non-redundant protein–protein interfaces from 3686 different PDBentries. A docking experiment was run to evaluate whether our algorithm was able to predict those interactions. The success rate wasmeasured as before—a solution with a given ligand-RMSD cutoffappearing in the top 10 ranked results of the algorithm. The successrates are shown in Table 4, for predicting protein-protein interfacesand in Table 5, for predicting at least a single protein-protein interface in each PDB entry.Finally, we evaluate the performance of the algorithm by comparing it to the performance of template-free docking methods. Thesame dataset was used again for a docking experiment with thePatchDock ab initio method. The success rate was selected as before,considering the top 10 ranked solutions of the algorithm.FundingThis research was supported by the Israel Science Foundation(Grant No. 1112/12), the I-CORE program of the Budgeting andPlanning Committee and the Israel Science Foundation (Center No.1775/12), by a grant from the Ministry of Science, Technology &Space, Israel & the Ministry of Foreign Affairs and InternationalCooperation General Directorate for Political Affairs & SecurityItalian Republic, and by the Hermann Minkowski MinervaGeometry Center.Conflict of Interest: none declared.ReferencesAndrusier,N. et al. (2008) Principles of ﬂexible protein–protein docking.Proteins: Struct. Funct. Bioinform., 73, 271–289.Anishchenko,I. et al. (2015) Structural templates for comparative proteindocking. Proteins: Struct. Funct.Bioinform., 83, 1563–1570.Bang,D. et al. (2006) Dissecting the energetics of protein a-helix c-cap termination through chemical protein synthesis. Nat. Chem. Biol., 2, 139–143.Bonvin,A.M. (2006) Flexible protein–protein docking. Curr. Opin. Struct.Biol., 16, 194–200.Cukuroglu,E. et al. (2014) Non-redundant unique interface structures as templates for modeling protein interactions. PLoS One, 9, e86738.Douguet,D. et al. (2006) Dockground resource for studying protein–proteininterfaces. Bioinformatics, 22, 2612–2618.Duhovny,D. et al. (2002). Efﬁcient unbound docking of rigid molecules. InInternational Workshop on Algorithms in Bioinformatics, pp. 185–200.Springer.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i30/3953948by gueston 07 January 2018i36Emekli,U. et al. (2008) Hingeprot: automated prediction of hinges in proteinstructures. Proteins: Struct. Funct.Bioinform., 70, 1219–1227.Furukawa,H. et al. (2005) Subunit arrangement and function in nmda receptors. Nature, 438, 185–192.Gao,M., and Skolnick,J. (2010) Structural space of protein–protein interfacesis degenerate, close to complete, and highly connected. Proc. Natl Acad.Sci., 107, 22517–22522.Halperin,I. et al. (2002) Principles of docking: an overview of search algorithms and a guide to scoring functions. Proteins: Struct. Funct. Bioinform.,47, 409–443.Hecht,H. et al. (1992) Three-dimensional structure of a recombinant variantof human pancreatic secretory trypsin inhibitor (kazal type). J. Mol. Biol.,225, 1095–1103.Hu,M. et al. (2005) Structure and mechanisms of the proteasome-associateddeubiquitinating enzyme usp14. EMBO J., 24, 3747–3756.Huang,S.-Y. (2014) Search strategies and evaluation in protein–protein docking: principles, advances and challenges. Drug Disc. Today, 19, 1081–1096.Hwang,H. et al. (2010) Protein–protein docking benchmark version 4.0.Proteins: Struct. Funct. Bioinform., 78, 3111–3114.Inanobe,A. et al. (2005) Mechanism of partial agonist action at the nr1 subunitof NMDA receptors. Neuron, 47, 71–84.Janin,J. (2005) Assessing predictions of protein–protein interaction: the capriexperiment. Protein Sci., 14, 278–283.Janin,J. et al. (2015) Assessing structural predictions of protein–protein recognition: the capri experiment. Rev. Comput. Chem., 28, 137–173.Kundrotas,P.J. et al. (2012) Templates are available to model nearly all complexesof structurally characterized proteins. Proc. Natl Acad. Sci., 109, 9438–9441.Lamdan,Y., and Wolfson,H.J. (1988). Geometric hashing: a general and efﬁcientmodel-based recognition scheme. In Proceedings of the 2nd InternationalConference on Computer Vision, IEEE Computer Society, pp. 238–249.Mashiach,E. et al. (2010) Fiberdock: ﬂexible induced-ﬁt backbone reﬁnementin molecular docking. Proteins: Struct. Funct. Bioinform., 78, 1503–1519.Muratcioglu,S. et al. (2015) Advances in template-based protein docking byutilizing interfaces towards completing structural interactome. Curr. Opin.Struct. Biol., 35, 87–92.Nussinov,R., and Wolfson,H.J. (1991) Efﬁcient detection of threedimensional structural motifs in biological macromolecules by computer vision techniques. Proc. Natl Acad. Sci., 88, 10495–10499.Ogmen,U. et al. (2005) Prism: protein interactions by structural matching.Nucleic Acids Res., 33, W331–W336.Ratnaparkhi,G.S. et al. (1998) Discrepancies between the NMR and x-raystructures of uncomplexed barstar: analysis suggests that packing densitiesM.Estrin and H.J.Wolfsonof protein structures determined by NMR are unreliable. Biochemistry, 37,6958–6966.Samara,N.L. et al. (2010) Structural insights into the assembly and function ofthe saga deubiquitinating module. Science, 328, 1025–1029.Schneidman-Duhovny,D. et al. (2005) Geometry-based ﬂexible and symmetricprotein docking. Proteins: Struct. Funct. Bioinform., 60, 224–231.Schreuder,H. et al. (1997) A new cytokine-receptor binding mode revealed bythe crystal structure of the il-1 receptor with an antagonist. Nature, 386,194.Schreuder,H.A. et al. (1995) Reﬁned crystal structure of the interleukin-1 receptor antagonist. Eur. J. Biochem., 227, 838–847.Sevcik,J. et al. (1996) Ribonuclease from streptomyces aureofaciens at atomicresolution. Acta Crystallographica Section D: Biol. Crystallogr., 52,327–344.Shatsky,M. et al. (2004) A method for simultaneous alignment of multipleprotein structures. Proteins: Struct. Funct. Bioinform., 56, 143–156.Smith,G.R., and Sternberg,M.J. (2002) Prediction of protein–protein interactions by docking methods. Curr. Opin. Struct. Biol., 12, 28–35.Sobolevsky,A.I. et al. (2009) X-ray structure, symmetry and mechanism of anampa-subtype glutamate receptor. Nature, 462, 745–756.Stockman,G. (1987) Object recognition and localization via pose clustering.Comput. Vis. Graph. Image Process., 40, 361–387.Stumpf,M.P. et al. (2008) Estimating the size of the human interactome. Proc.Natl Acad. Sci., 105, 6959–6964.Szilagyi,A., and Zhang,Y. (2014) Template-based structure modeling of protein–protein interactions. Curr. Opin. Struct. Biol., 24, 10–23.Tuncbag,N. et al. (2011) Predicting protein-protein interactions on a proteome scale by matching evolutionary and structural similarities at interfacesusing prism. Nat. Protoc., 6, 1341–1354.van de Locht,A. et al. (1995) Two heads are better than one: crystal structureof the insect derived double domain kazal inhibitor rhodniin in complexwith thrombin. EMBO J., 14, 5149.Vaughan,C.K. et al. (1999) Structural response to mutation at a proteinprotein interface. J. Mol. Biol., 286, 1487–1506.Vigers,G.P. et al. (2000) X-ray crystal structure of a small antagonist peptide bound to interleukin-1 receptor type 1. J. Biol. Chem., 275,36927–36933.Vreven,T. et al. (2014) Evaluating template-based and template-free protein–protein complex structure prediction. Brief. Bioinform., 15, 169–176.Wang,D. et al. (1985) Bovine chymotrypsinogen a: X-ray crystal structure˚analysis and reﬁnement. Of a new crystal form at 1.8 A resolution. J. Mol.Biol., 185, 595–624.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i30/3953948by gueston 07 January 2018
5028881967002	PMID28881967	5028881967	https://watermark.silverchair.com/btx232.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881967.main.pdf	Bioinformatics, 33, 2017, i102–i109doi: 10.1093/bioinformatics/btx232ISMB/ECCB 2017HopLand: single-cell pseudotime recovery usingcontinuous Hopfield network-based modelingof Waddington’s epigenetic landscapeJing Guo1,2 and Jie Zheng1,3,4,*1Biomedical Informatics Laboratory, School of Computer Science and Engineering, Nanyang TechnologicalUniversity, Singapore 639798, Singapore, 2Bioinformatics Institute, Agency for Science, Technology, and Research(A*STAR), Singapore 138671, Singapore, 3Genome Institute of Singapore, Agency for Science, Technology, andResearch (A*STAR), Singapore 138672, Singapore and 4Complexity Institute, Nanyang Technological University,Singapore 637723, Singapore*To whom correspondence should be addressed.AbstractMotivation: The interpretation of transcriptional dynamics in single-cell data, especially pseudotimeestimation, could help understand the transition of gene expression proﬁles. The recovery of pseudotime increases the temporal resolution of single-cell transcriptional data, but is challenging due to thehigh variability in gene expression between individual cells. Here, we introduce HopLand, a pseudotime recovery method using continuous Hopﬁeld network to map cells to a Waddington’s epigeneticlandscape. It reveals from the single-cell data the combinatorial regulatory interactions among genesthat control the dynamic progression through successive cell states.Results: We applied HopLand to different types of single-cell transcriptomic data. It achieved highaccuracies of pseudotime prediction compared with existing methods. Moreover, a kinetic modelcan be extracted from each dataset. Through the analysis of such a model, we identiﬁed key genesand regulatory interactions driving the transition of cell states. Therefore, our method has the potential to generate fundamental insights into cell fate regulation.Availability and implementation: The MATLAB implementation of HopLand is available at https://github.com/NetLand-NTU/HopLand.Contact: zhengjie@ntu.edu.sg1 IntroductionThe traditional time-series gene expression data analyses of a largepopulation of cells, e.g. microarray data, overlook the high variability among individual cells. However, the heterogeneity among singlecells contributes to the transcriptional dynamics of a temporal process such as cell differentiation. From the bulk data, it is difficult toseparate cells from different developmental stages or identify raresub-populations of cells. On the contrary, high-throughput singlecell technologies are new and promising to give insights into the heterogeneous distribution and dynamics of individual cells (Buganimet al., 2012).The ‘pseudotime’ is a quantitative measure of progress through abiological process along which cells are arranged based on their expression profiles. The recovery of pseudotime is made possible bytaking advantage of single-cell technologies which provide unprecedented access to the underlying processes and intrinsic functional relationships among cells, and thereby reveals the mechanisms ofcomplex biological systems. For example, using the estimated pseudotimes of single cells from cell differentiation in embryonic development, crucial regulators can be identified by comparing theexpression profiles around the branching time points. The recoveryof pseudotime can also facilitate cancer studies, such as revealing theprogression from normal tissues to malignant lesions.The intrinsic signals of cell-to-cell variability in the extractedgene expression profiles are often corrupted with technical noises(Stegle et al., 2015), such as distortion caused by overdispersion,outliers and dropout events, which makes the interpretation of biological meaning highly challenging. Although several methods havebeen developed to recover pseudotimes from single-cell data, there isstill room for improvement in the analysis. In these methods, individual cells are projected onto the constructed trajectories or landscape estimated from the transcriptional data. The pseudotime of acell in the differentiation process is measured by the distance fromits projected position on the time line to the given starting point,CV The Author 2017. Published by Oxford University Press.i102This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i102/3953947by gueston 07 January 2018HopLand: single-cell pseudotime recoverybased on the assumption that cells with similar expression profilesshould be gathered together.Given the prior knowledge of marker genes, the Wanderlustmethod (Bendall et al., 2014) uses a graph-based trajectory detectionalgorithm that maps cells onto a 1D developmental trajectoryassuming that there is no branch. The pseudotime of a cell is definedby its coordinate on the path. However, Wanderlust fails to reportthe divergent time points when there are branching processes, and itrelies on the prior knowledge of marker genes. Wishbone (Settyet al., 2016) overcomes the defects of Wanderlust by aligning singlecells into bifurcating branches. It identifies the bifurcation pointsand recovers the pseudo-temporal ordering of cells. SCUBA (Marcoet al., 2014) uses the temporal information to perform bifurcationanalysis of single-cell data to recover the cell lineages. In applications where the time information is not available, it fits a smoothcurve passing through the reduced data using the principal curveanalysis. The pseudotime of each cell is determined by its mappedposition along the principal curve.Several other methods, e.g. diffusion map (Haghverdi et al.,2015), Monocle (Trapnell et al., 2014) and Topslam (Zwiessele andLawrence, 2016), which do not require the prior knowledge ofmarker genes or temporal information, are capable of simulatingdifferentiation processes with multiple lineages. The differentiationpath or landscape is visualized by mapping from the highdimensional space of single-cell gene expression profiles to a lowerdimensional space using linear or non-linear dimensionality reduction techniques, such as diffusion map (Coifman et al., 2005), independent component analysis (ICA) (Hyv€rinen and Oja, 2000) andaBayesian Gaussian process latent variable model (Bayesian GPLVM) (Lawrence, 2003; Titsias and Lawrence, 2010).The diffusion map for single-cell analysis uses diffusion distancesto simulate cell differentiation and order cells along the differentiation path while preserving the non-linear structure of data.Monocle builds a minimum spanning tree (MST) to connect cellsand the longest path in the MST serves as the main pseudotime axis.One pitfall of Monocle is the use of ICA, a linear dimensionality reduction method, which may not be able to accurately capture thenonlinearity in the biological system. Topslam estimates the pseudotime by mapping the individual cells to the surface of aWaddington’s epigenetic landscape (Waddington, 1957) using theprobabilistic dimensionality reduction technique of Bayesian GPLVM. Following the topography of the probabilistic landscape, thelocations of cells reflect their degrees of maturity during thedifferentiation.Although the above state-of-the-art methods show promisingperformance for pseudotime estimation, there are a few concerns.For example, most of the current methods project the highdimensional data into two or three latent components, and the distances in the latent space are interpreted as biological cell-to-cellvariability. This assumption might cause misleading results as thedimensionality reduction methods could be sensitive to noise in geneexpression data. Instead of using the distance in the 2D latent space,Topslam has used the topography of the landscape to refine the distance, but the definition of the landscape therein lacks biologicalmeaning. Furthermore, although some of the existing data-drivenmethods could reveal the dynamics of a specific process, they areconfined to the identification of key regulators without the involvement of the system dynamics driven by molecular interactions, e.g.reactions among transcription factors, genes and epigeneticmodifiers.To address these issues, we propose HopLand, a method forpseudotime recovery from single-cell gene expression data byi103mapping cells to the Waddington’s epigenetic landscape. By inferring the gene–gene interactions from single-cell transcriptional data,we construct a kinetic model, using the continuous Hopfield network (CHN) which is a type of recurrent neural network proposedby John Hopfield in 1984 (Hopfield, 1984). Waddington’s epigenetic landscape can be seen as a non-linear map which visualizes thebranching process driven by the interactions among genes in thecells. The performance of HopLand running on single-cell qPCRand RNA-seq datasets was superior to most of the existing methodsin most cases. Moreover, a list of key regulators and interactionswere identified. This method can be applied broadly to understanding various cellular processes, including embryonic development,stem cell reprogramming and cancer cell proliferation.2 Materials and methodsWe adopted the concept of Waddington’s epigenetic landscape toanalyze and visualize the dynamics of the biological processes froma global point of view. The virtual individual cells modeled based onthe single-cell gene expression data are to be placed on the surfaceregions corresponding to their developmental stages. To plot such alandscape, we constructed a kinetic model from transcriptionaldata, using the CHN to describe the transcriptional regulation. Foreach target gene, the model associates its change rate with the adaptation of the neurons. Then, the pseudotime can be estimated by calculating the geodesic distance between every two cells in thelandscape. Based on the above framework, the HopLand algorithmis designed as follows:Step 1. Normalize the gene expression data and select differentially expressed genes (by filtering out genes with low variances).Step 2. Construct the kinetic model by neural network inferencefrom data.Step 3. Construct the Waddington’s epigenetic landscape basedon the kinetic model.Step 4. Calculate geodesic distances to estimate the pseudotimesof the input single cells.Algorithm 1 illustrates the steps of HopLand which are furtherelaborated in the following subsections.Algorithm 1. HopLand algorithmINPUT: Single-cell gene expression data D 2 RSÂN where S isthe number of cells and N is the number of genes, and temporal information cellStages (which is not compulsory)OUTPUT: Kinetic model of Waddington’s epigenetic landscape landModel, and pseudotimes of cells PT1: if cellStages is available then2: Set startPoints as the earliest samples in cellStages;3: randomInitials ¼ generateRandomInitialStates(startPoints);4: realTraj ¼ generateTrajectory(D, cellStages);5: gmmModels ¼ ﬁtMixtureGaussian(D);6: h ¼ ParameterOptimization(D, cellStages, randomInitials,realTraj, gmmModels); // Algorithm 27: else8: h ¼ initializeParam(D);9: end if10: landModel ¼ LandscapeConstruct(D, h); // Algorithm 311: PT ¼ PseudotimeRecovery(landModel); // Algorithm 412: return landModel, PT.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i102/3953947by gueston 07 January 2018i104J.Guo and J.Zheng2.1 Data-driven kinetic modeling2.1.1 Network formulationCHN consists of a set of N interconnected neurons which updatetheir activation values synchronously or asynchronously. Comparedwith the original two-state HN proposed by Hopfield himself in1982 (Hopfield, 1982), CHN uses continuous variables and predictscontinuous responses. The discrete Hopfield network has been usedto study biological systems with each neuron representing a gene(Lang et al., 2014; Maetschke and Ragan, 2014; Taherian Fardet al., 2016). Gene expression values tend to have continuous input–output relations which cannot be fully characterized by the simplified discrete states of neurons in the two-state HN. Thus we adoptedthe framework of CHN to model the system dynamics with eachneuron corresponding to an individual gene whose adaptation indicates the change of gene expression value.In the framework of CHN, the gene expression of a cell is characterized by the outputs of the neurons V ¼ fVi ; i ¼ 1; 2; . . . ; Ng,where N is the number of genes. The inputs to each neuron comefrom two sources, i.e. the background noise and signals from otherneurons. The time evolution of the system is represented by ordinarydifferential equations (ODEs). The change rate of neuron i is modeled bydVi¼ CidtNXWij Uj À di Vi þ Ii ;(1)j¼1Uj ¼ gj ðVj Þ;(2)where Wij is an entry of the weight matrix of CHN representing theinterconnection weight coefficient from neuron j to neuron i, and Ciis an amplifier on the synaptic connections. The external input Iirepresents a combination of propagation delays, regulations byother genes not in our model, and noise in transcriptional regulation. di denotes the degradation rate of gene i. The activation function gj ðVj Þ represents the input–output relationship of a nonlinearamplifier with negligible response time. The activation function isrequired to be a monotonically increasing function to make the system stable (Zhang et al., 2014). In our model, a sigmoid activationfunction is used (Equation (3)) which has been used in (Ay andArnosti, 2011; Chen et al., 2005) to describe the regulatory functionof a gene:gj ðVj Þ ¼ 1=ð1 þ eÀðVj Àlj Þ=rj Þ:(3)In Equation (3), lj and rj are the mean and standard deviation ofthe expression levels of the jth gene in all cells, respectively.2.1.2 Parameter estimationThere are several parameters in the ODE model of kinetics inEquation (1). To infer these parameters h ¼ fdi ; Ii ; Ci ; Wij ; i; j ¼ 1; 2;. . . ; Ng from the data, we propose an optimization method(Algorithm 2), which fits the simulated and observed single-celldata, based on the premise that a realistic model should be able togenerate simulated data consistent with the real data.The consistency between experimental data and simulated data ismeasured in two aspects. First, the gene expression values Di ¼ fDit ;t ¼ t1 ; t2 ; . . . ; tT g where T is the number of time points (or cell stages)in the single-cell data should follow a similar distribution. Normally,it is believed to follow the Gaussian mixture distribution with themean values of components as the representative gene expression values in different lineages (Kalmar et al., 2009; Rais et al., 2013). Thesecond aspect of consistency lies in the change of a single gene, e.g.down-regulated or up-regulated, along the time evolution of cellstates. Thus the objective functions are defined as follows:OBJ1 ¼NXðDFidata À DFisimulate ðhÞÞ;(4)i¼1OBJ2 ¼tm N1 XXrit ðDit À Sit ðhÞÞ2 ;mN t¼t1 i¼1(5)where DFidata and DFisimulate are the density functions for the observedand simulated expression levels of the ith gene, respectively. rit is thestandard deviation of the expression values of gene i at the time point(or cell developmental stage) t. Si ¼ ðSit1 ; Sit2 ; . . . ; Sitm Þ is an averagetrajectory derived from the simulated trajectories with m time points bysimulating the CHN of Equation (1) using the generated initial states.The gradient descent learning algorithm (Baldi, 1995) is used tooptimize the parameters in the CHN. The update of a parametervalue at the kth iteration is defined as  @OBJ2DhðkÞ ¼ Àg;(6)ðkÞ@hhðkþ1Þ ¼ hðkÞ þ DhðkÞ ;(7)where g is the learning rate between 0 and 1, which controls the rate ofparameter adjustment. We also iteratively adapt the learning rate accordingto the Bold Driver technique (Ruder, 2016). The weight matrix is initialized as the Pearson correlation coefficients between samples. To simulatethe dynamic trajectories, we use the Euler’s method (the first-order Runge–Kutta) to solve the ODEs with the initial states generated near the givenstarting points. In each iteration of the gradient descent learning, we calculate the value of the objective function in Equation (4) using the currentparameters. At the end, the optimized parameters are selected with theminimum sum of the two objective functions in Equations (4) and (5).Algorithm 2. Parameter optimizationINPUT: Single-cell gene expression data D, temporal information cellStages, observed trajectories realTraj, coefﬁcient matrix r, and Gaussian mixture models gmmModelsOUTPUT: Optimized parameters h ¼ fdi ; Ii ; Ci ; Wij ; i; j ¼ 1; 2;. . . ; NgInitialization: Set di ¼ 1, Ii ¼ 0, Ci ¼ 1, Wij ¼ corrðDÞ; g ¼ 0:3,maxIts ¼ 2000.1: for k ¼ 1; 2;  . . ; maxIts do. ðhðkÞ2: DhðkÞ ¼ Àg @OBJ2ðkÞ Þ ;@h3: hðkþ1Þ ¼ hðkÞ þ DhðkÞ ;4: if OBJ2 ðhðkÞ Þ > OBJ2 ðhðkþ1Þ Þ then5:g ¼ g Ã 1:2;6: else7:g ¼ g Ã 0:5;8: end if9: end for10: kÃ ¼ argmink ðOBJ1 ðhðkÞ Þ þ OBJ2 ðhðkÞ ÞÞ;Ã11: return h ¼ hðk Þ .2.2 Construction of Waddington’s epigenetic landscape2.2.1 Energy functionUnder certain conditions, the activation values of the units in aCHN undergo a relaxation process such that the network willDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i102/3953947by gueston 07 January 2018HopLand: single-cell pseudotime recoveryi105converge to a stable state in which these activation values will notchange anymore. These conditions include that the weight matrixW ¼ ðWij Þ; where i; j ¼ 1; 2; . . . ; N; has to be symmetric, and the activation functions must be continuous, bounded, and strictly monotonically increasing, such as a sigmoid function (Equation (3)). Thebehavior of a CHN system can be described with an energy function(Equation (8)) which is a Lyapunov function:E¼ÀN NNNXX1XXWij Ui Uj þIi Ui þdi2 i¼1 j¼1i¼1i¼1ð Ui0gÀ1 ðuÞdu:i(8)Based on the essential idea of Waddington’s epigenetic landscape, the differentiation processes follow the paths of losing potential energy determined by the topography of the landscape. Compared with the entrypoint, the valleys possess relatively lower potentials representing statesthat are more stable. Here we use the network energy in Equation (8) toquantify the altitude of the landscape, similar to previous works (Langet al., 2014; Maetschke and Ragan, 2014; Taherian Fard et al., 2016).2.3 Pseudotime estimation. In Waddington’s epigenetic landscape,a single cell with specific gene expression pattern is simplified as a point,hence the time evolution of cell states is defined as the state-transitionmovement on the landscape which is determined by the topography ofthe landscape surface. Thus the geodesic distance between two cells canbe calculated from the coordinates of the cells on the landscape.We used the fast marching algorithm (Sethian, 1999) to perform geodesic extraction on a triangulated mesh generated from the single-celldata. Then, using the extracted geodesic distances as the weights of edgesconnecting the cells, an MST is constructed with the given starting pointas the root. The geodesic distances to the starting point are considered proportional to the pseudotimes, setting the pseudotime of the starting pointto zero. As such, the pseudotime of the ith cell is estimated as the length ofpath from the corresponding tree node to the starting point in the MST.The pseudocode of the pseudotime estimation is shown inAlgorithm 4.Algorithm 4. Pseudotime recovery2.2.2 Visualizing the landscapeTo visualize the cellular dynamics in a landscape whose shape isdetermined by the energy function in Equation (8), we project thehigh-dimensional data to a 2D latent space as the xy-plane in thelandscape. The non-linear dimensionality reduction method, namedGaussian process latent variable model (GP-LVM), is used to generate the mapping between the original space and the latent space(Lawrence, 2003; Wang et al., 2008). GP-LVM is a probabilistic approach to modeling high-dimensional data in a low-dimensional latent space with a probabilistic model. It has been used for computervision, biological data analysis, etc. We have recently used thismethod to process simulated gene expression time-series data in thevisualization of landscape (Guo et al., 2017).Using GP-LVM to reduce the dimensions of the observed data totwo dimensions, we create a mapping from the original space to thelatent space. Then we generate a 2D grid in the reduced space covering all the cells in order to plot a continuous surface. Since the energy is calculated in the high-dimensional space, we project the gridpoints back into their original space. The landscape is plotted on thegrid data. The pseudocode of the landscape construction method isshown in Algorithm 3.INPUT: landModel, StartPointOUTPUT: pseudotimes of cells PT1: manifold ¼ NormalizeðlandModelÞ;2: ½Vertices; Faces  ¼ delaunayTriangulationðmanifoldÞ;3: distMatrix ¼ fast marchingðVertices; FacesÞ;4: T ¼ minimumSpanningTreeðdistMatrix; StartPointÞ;5: PT ¼ calculateDistanceðT; StartPointÞ;6: return PT.3 ResultsIn this section, we evaluate the performance of HopLand by comparing it with 6 state-of-the-art methods, i.e. Monocle, Topslam,Wanderlust, Wishbone, SCUBA and Diffusion map, on 11 testingdatasets including a qPCR dataset GUO2010 (Guo et al., 2010), 5synthetic datasets and 5 scRNA-seq datasets, i.e. DENG2014 usingSmart-seq2 (Deng et al., 2014), YAN2013 (Yan et al., 2013) usingscRNA-seq method demonstrated in (Tang et al., 2009), ES_MEFusing STRT (Islam et al., 2011), LPS (Amit et al., 2009) and HSMM(Trapnell et al., 2014).3.1 Pseudotimes inferred from synthetic dataAlgorithm 3. Landscape constructionINPUT: Single-cell gene expression data D with S samplesand N genes, parameter vector h from Algorithm 2OUTPUT: A landscape model landModel1: Generate mapping X ¼ GPLVMðDÞ, where latent variablesare encoded in matrix X 2 RSÂ2 ;2: Deﬁne a 2D grid Grid ¼ ½minðX1: Þ À e; maxðX1: Þ þ e 0Â½minðX2: Þ À e; maxðX2: Þ þ e , where X1: and X2: are theﬁrst and second components of samples, and e is a smallpositive constant which determines the size of marginsaround the observed data in the latent space;3: Perform inverse dimensionality reductionGridY ¼ GPLVMÀ1 ðGridÞ, where Y 2 RS ÂN , and SGrid is thenumber of points in Grid;4: Calculate the energy according to Equation (8);5: landModel ¼ fX; Grid; energyg;6: return landModel.We tested HopLand on five synthetic datasets generated by simulating the early development of mouse embryos. Each dataset containsa randomly generated differentiation pattern by angled linear splitsin two dimensions (Zwiessele and Lawrence, 2016). We extractedthe pseudotimes of the cells using the HopLand algorithm and compared it with other methods (Table 1). An example of synthetic datais shown in a contour plot (Fig. 1), which contains two divergingevents splitting cells into four lineages. We mapped the cells ontothe landscape surface according to the extracted pseudotimes. Thecells at early developmental stages (dark red dots) are located in thebottom middle region of the landscape with high energy, whilethe four lineages (white or light red dots) rest in valleys. The movement directions of the cells following the shape of the landscape canreflect the irreversible transitions of cell states during the differentiation in the embryonic development.3.2 Single-cell pseudotimes in mouse embryonicdevelopmentThe single-cell dataset of mouse pre-implantation development contains the expression profiles of 438 cells with 48 genes per cellDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i102/3953947by gueston 07 January 2018i106J.Guo and J.ZhengTable 1. Accuracies of pseudotime recovery on 5 synthetic datasets and 6 experimental datasets using different pseudotime recovery methods. Pearson correlation coefﬁcient between the predicted and observed times is used to evaluate the result. We compared HopLand with 6other methods, i.e. Monocle, Wanderlust, Topslam, SCUBA, Wishbone and Diffusion mapMethodSynthetic data(Zwiessele andLawrence, 2016)qPCRscRNA-SeqDatasetHopLandWanderlustMonocleTopslamSCUBAWishboneDiffusion mapSynthetic data 1Synthetic data 2Synthetic data 3Synthetic data 4Synthetic data 5GUO2010 (Guo et al., 2010)DENG2014 (Deng et al., 2014)YAN2013 (Yan et al., 2013)LPS (Amit et al., 2009)HSMM (Trapnell et al., 2014)ES_MEF (Islam et al., 2011)Average of scoresSD of scores0.89970.95780.91590.91110.92610.92300.81980.91290.67120.57160.87120.85270.12160.92240.96270.75270.89880.91000.81210.88790.84260.79020.28100.89190.81380.18740.81580.82020.83220.90950.89880.57960.91770.94210.88990.45600.51660.77990.17560.88720.96930.88400.92360.94880.92970.92690.93800.71170.18900.90350.83740.22560.90690.93960.79470.87720.92050.94010.96550.97760.53070.03970.85180.78850.29900.25340.50890.47040.22490.44980.54760.51150.28760.60640.48500.39520.43100.12560.84410.94200.83650.78490.93900.49490.83950.88930.57830.13860.83430.73830.2432Note: The top three scores in each dataset are in bold.Fig. 1. The contour plot of the constructed Waddington’s epigenetic landscapeusing the third synthetic dataset. The contour lines represent heights in the landscape. The dark areas indicate low energy, and the light regions have high energy.The cells from early stages to late stages are colored from dark red to whitecovering the developmental stage from the 1-cell to 64-cell stages(Guo et al., 2010). Two distinct cell lineages, i.e. trophectoderm(TE) and inner cell mass (ICM), emerge from the 16- to 32-cellstages. During the transitions from the 32- to 64-cell stages, anothertwo cell lineages, i.e. primitive endoderm (PE) and epiblast (EPI),are split from ICM. We applied the HopLand algorithm on thisdataset and recovered the pseudotimes of the cells.Analyzing the expression profiles of individual genes, we foundthat the expression of marker genes follows a mixture of Gaussiandistributions (data not shown). The mean values in different components indicate differential expression in separate lineages. Weinferred the moments of the Gaussian distributions from the data.The cells from the 1-cell stage were used to generate the trainingdata as the initial states for the simulation of Hopfield network inorder to calibrate the model. Then we learned a dynamical model tocapture the kinetics in the cell differentiation process of early mouseembryonic development.After projecting the high-dimensional data into a 2D latentspace using GP-LVM, we calculated the energy values accordingto Equation (8) which are used for the z-axis of the landscape(Fig. 2a). In the contour plot of the landscape (Fig. 2b), two bifurcations are shown corresponding to the cell fate decisionsmade at the 16- to 32-cell stages (cyan dots to light blue dots)and the 32- to 64-cell stages (light blue dots to dark blue dots).Then, we mapped the expression values of marker genes into thelandscape to trace the differentiation process (Fig. 3). The expression profiles of the cells in different branches are separated usingthe marker genes for different cell lineages (e.g. ICM, TE, PE,and EPI). The result shows that cells belonging to the same stageare located together in the landscape and they follow the developmental orders. In the landscape, the change of the network energyshows a decreasing trend along the differentiation process whichconfirms our premise that cell differentiation is a process withdecreasing energy.Using the topography of Waddington’s epigenetic landscapeas a correction, we extracted the pseudotime information. Thecells were connected in an MST (or a forest of multiple trees)which contains the estimated pseudotimes revealing the transitions of cell states (Fig. 4). We identified a small group of cells(in dark red) isolated from others. It contains cells from the1-cell stage due to the sparsity of data during the early mouseembryonic development. We also compared HopLand with othermethods. The accuracies of pseudotime recovery measured bycorrelation coefficient between the predicted and observed pseudotimes are listed in Table 1. For each method, we calculated theaverage and standard deviation of scores in the 11 testing datasets. It shows that HopLand achieves the best performance amongall the methods.The weight matrix in the CHN inferred from the dataset ofGUO2010 (Fig. 5) contains information about the interactions between genes which can help us find the key regulatory relationsand reveal the dynamics of gene expression during cell differentiation. The top 10 significant interactions in the learned weightmatrix of the mouse pre-implantation data are listed in Table 2.Seven of the top 10 gene pairs are confirmed. Some of them havedirect interactions, e.g. transcriptional regulation. Although wehave not yet found evidence for the rest of interactions, somegenes from these interactions, e.g. DPPA1, HAND1, are known tobe involved in mouse embryonic development. For example,HAND1 is a transcription factor expressed in extra-embryonicmesoderm and trophoblast, and DPPA1 is associated with developmental pluripotency.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i102/3953947by gueston 07 January 2018HopLand: single-cell pseudotime recoveryi107Fig. 2. (a) Waddington’s epigenetic landscape recovered using HopLand. (b) The contour plot of the constructed Waddington’s epigenetic landscape. The dotsare colored according to the developmental stages of the represented cellsFig. 3. Mapping gene expression values to Waddington’s epigenetic landscape using (a) FGF4, (b) GATA4, (c) CDX2 and (d) SOX2. The value decreasesfrom dark red to whiteFrom the weight matrix, we also ranked genes by the sum ofweights of incident edges and identified a few essential regulators,e.g. FGF4, OCT4, GATA4 and ESRRB, which have been experimentally tested to be essential for early embryonic development(Guo et al., 2010; Li et al., 2005; Martello et al., 2012; Kehat et al.,2001; Sozen et al., 2014). These key factors play important roles inthe regulation of embryonic development, cell proliferation, and celldifferentiation.3.3 Testing results on single-cell RNA-seq data ofmouse embryonic developmentWe also compared the HopLand algorithm with other methods onmonoallelic mouse pre-implantation embryo RNA-Seq data (Denget al., 2014). This single-cell RNA-seq dataset comprises transcriptome profiles of 317 cells from zygote to blastocyst and two matureFig. 4. The minimum spanning tree constructed from Waddington’s epigenetic landscape. The dots are colored according to the developmental stagesof the cells in the dataset of GUO2010cell types including 11 stages. The constructed landscape is shown inFigure 6. Splitting occurs in both 8- and 16-cell stages. The two developed cell types, fibroblast and adult liver, are separated from theearly embryonic developmental lineages. The blastocyst cells (colored in green, cyan and light blue) are clustered together with lowerenergy than cells of the early lineages. Early blastocyst cells (coloredin green) and middle blastocyst cells (colored in cyan) are mixed together but separated from the late-stage cells (colored in light blue)indicating a closer developmental relations. The mature cells (colored in dark blue) located in a valley have low-energy values.The result of comparing the accuracies of different methods inestimating pseudotimes are shown in Table 1. HopLand is not asgood as most other methods for this dataset, partly due to the dearthof time information from the early blastocyst stage to the late blastocyst stage. In addition, due to the lack of specific temporal information between the late blastocyst samples and mature cells, HopLandDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i102/3953947by gueston 07 January 2018i108J.Guo and J.ZhengTable 2 Top 10 key interactions identiﬁed from the weight matrixranked by the absolute value of the weight in CHNRank12345678910Fig. 5. The weight matrix contains N Â N interactions of CHN learned from themouse embryonic early development dataset. N is the number of genesFig. 6. The contour plot of the landscape constructed from the dataset ofDENG2014. The cells are connected in a minimum spanning treecannot accurately recover the trajectories from the pre-implantationdevelopment to the mature cells. Nevertheless, HopLand can successfully reconstruct the progress from zygote to blastocyst (Fig. 6)achieving a correlation coefficient of 0.91 with real data.4 DiscussionIn this paper, we proposed a novel method, named HopLand, to recover the pseudotimes from single-cell data using CHN-based modeling of Waddington’s epigenetic landscape. The order of cells isdetermined by the geodesic distances in the landscape.Waddington’s epigenetic landscape constructed from the neural network model serves as a stage on which the progression of cell fatedecision is simulated. In addition, our method models the dynamicsof gene regulation using the framework of CHN which generatessimulation results consistent with the observed data. The constructed model has allowed us to make novel, experimentallyGene 1Gene 2References (PMID)GATA4GATA4ATP12AESRRBAQP3AQP3HNF4AGRHL1ESRRBKLF4LCP1GATA4DPPA1ESRRBDPPA1LCP1LCP1HAND1FGF4KLF418555785, 22083510, 16153702, 1499086115987774–16767105, 19136965–18700969, 1988425521852396, 15159395–2620613318264089, 18358816, 19030024, 18555785testable hypotheses about transcriptional mechanisms that controlthe cell fate conversions.Applied to real single-cell gene expression data from differenttypes of biological experiments and compared against other methods, HopLand outperformed most of the other methods in mostcases. In addition, our method could also be used to identify keyregulators and interactions, which is helpful for the understandingof underlying mechanisms.The simulation and analysis results have shown that HopLandhas some advantages, whereas the other methods fail in certain circumstances. First, our method does not rely on any priori knowledgeof key marker genes. Secondly, the non-linear dimensionality reduction method used in HopLand generates a non-linear mappingbetween the landscape and the phenotype space respecting the nonlinear structures of biological systems. Thirdly, HopLand constructsthe landscape based on biological interactions between genes thatallows to simulate real biological processes.HopLand is a pseudotime estimation algorithm using dynamicalsystems modeling. It still needs to address several issues. First, themathematical modeling approach makes use of several types of information, e.g. the physical time points within the single-cell data,which is not required by some other methods. Nevertheless, ourmethod tries to recover the underlying regulatory mechanisms fromthe data using the extracted information. The physical time pointsprovide the consecutive updates during the process that is useful forthe modeling, but for a dataset without temporal information,HopLand can skip the training process and directly predict the pseudotimes based on a landscape constructed using an initial settings ofparameters. Secondly, HopLand is computationally costly comparedwith other methods. The parameter learning process is timeconsuming partially due to the repeated numerical solution ofODEs. Moreover, our method was implemented in MATLAB whichis not suitable for intensive computation. In the future, we will implement our method in C/C þþ to speed it up. Thirdly, our algorithm was proposed under the premise that the single-celltranscriptional data cover the critical stages along a biological process. If that is not the case, however, the predicted model might givemisleading results. Fourthly, HopLand makes use of GP-LVM to extract a 2D latent space from a high-dimensional space which maysuffer from the high technical noise in the single-cell data. Althoughthe recovery of pseudotime relies on not only the reduced components, but also a third value, i.e. network energy, which can alleviatethe influence from the noise, we still recommend users to preprocesstheir data using some single-cell analysis techniques, e.g. PAGODA(Fan et al., 2016), scLVM (Buettner et al., 2015).The result of HopLand on the qPCR dataset is better than thoseon the scRNA-seq data. It is probably because the protocols ofDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i102/3953947by gueston 07 January 2018HopLand: single-cell pseudotime recoveryqPCR make data less prone to the dropout effect (Kalisky andQuake, 2011). Among the 5 RNA-seq datasets, HopLand has unstable performances, which may be partly caused by the differentscRNA-seq protocols used in generating the data (Ziegenhain et al.,2016). In the future, We will try to analyze HopLand in differentsequencing datasets and make it satisfy specific needs of differenttypes of sequencing technologies.FundingThis work has been supported by the MOE AcRF Tier 1 grant (RG120/15,2015-T1-002-094), Ministry of Education, Singapore, and the PhD scholarship from NTU Provost office for the SCE-BII joint PhD program.ReferencesAmit,I. et al. (2009) Unbiased reconstruction of a mammalian transcriptionalnetwork mediating pathogen responses. Science, 326, 257–263.Ay,A., and Arnosti,D.N. (2011) Mathematical modeling of gene expression:a guide for the perplexed biologist. Crit. Rev. Biochem. Mol. Biol., 46, 137–151.Baldi,P. (1995) Gradient descent learning algorithm overview: a general dynamical systems perspective. IEEE Trans. Neural Netw., 6, 182–195.Bendall,S.C. et al. (2014) Single-cell trajectory detection uncovers progressionand regulatory coordination in human b cell development. Cell, 157,714–725.Buettner,F. et al. (2015) Computational analysis of cell-to-cell heterogeneityin single-cell RNA-sequencing data reveals hidden subpopulations of cells.Nat. Biotechnol., 33, 155–160.Buganim,Y. et al. (2012) Single-cell expression analyses during cellular reprogramming reveal an early stochastic and a late hierarchic phase. Cell, 150,1209–1222.Chen,K.C. et al. (2005) A stochastic differential equation model for quantifying transcriptional regulatory network in Saccharomyces cerevisiae.Bioinformatics, 21, 2883–2890.Coifman,R.R. et al. (2005) Geometric diffusions as a tool for harmonic analysis and structure deﬁnition of data: diffusion maps. Proc. Natl. Acad. Sci.USA, 102, 7426–7431.Deng,Q. et al. (2014) Single-cell RNA-Seq reveals dynamic, random monoallelic gene expression in mammalian cells. Science, 343, 193–196.Fan,J. et al. (2016) Characterizing transcriptional heterogeneity through pathway and gene set overdispersion analysis. Nat. Methods, 13, 241–244.Guo,G. et al. (2010) Resolution of cell fate decisions revealed by single-cellgene expression analysis from zygote to blastocyst. Dev. Cell, 18, 675–685.Guo,J. et al. (2017) NetLand: quantitative modeling and visualiza-tion ofWaddington’s epigenetic landscape using probabilistic potential.Bioinformatics, DOI: 10.1093/bioinformatics/btx022.Haghverdi,L. et al. (2015) Diffusion maps for high-dimensional single-cellanalysis of differentiation data. Bioinformatics, 31, 2989–2998.Hopﬁeld,J.J. (1982) Neural networks and physical systems with emergent collective computational abilities. Proc. Natl Acad. Sci. USA, 79, 2554–2558.Hopﬁeld,J.J. (1984) Neurons with graded response have collective computational properties like those of two-state neurons. Proc. Natl. Acad. Sci.USA, 81, 3088–3092.Hyv€rinen,A., and Oja,E. (2000) Independent component analysis: algorithmsaand applications. Neural Netw., 13, 411–430.Islam,S. et al. (2011) Characterization of the single-cell transcriptional landscape by highly multiplex rna-seq. Genome Res., 21, 1160–1167.i109Kalisky,T., and Quake,S.R. (2011) Single-cell genomics. Nat. Methods, 8,311–314.Kalmar,T. et al. (2009) Regulated ﬂuctuations in nanog expression mediatecell fate decisions in embryonic stem cells. PLoS Biol., 7, e1000149.Kehat,I. et al. (2001) Human embryonic stem cells can differentiate into myocytes with structural and functional properties of cardiomyocytes. J. Clin.Invest,, 108, 407–414.Lang,A.H. et al. (2014) Epigenetic landscapes explain partially reprogrammedcells and identify key reprogramming genes. PLoS Comput. Biol., 10,e1003734.Lawrence,N.D. (2003) Gaussian process latent variable models for visualisation of high dimensional data. Proc. Advances in Neural InformationProcessing Systems (NIPS), 329–336.Li,Y. et al. (2005) Murine embryonic stem cell differentiation is promoted bysocs-3 and inhibited by the zinc ﬁnger transcription factor klf4. Blood, 105,635–637.Maetschke,S.R., and Ragan,M.A. (2014) Characterizing cancer subtypes asattractors of Hopﬁeld networks. Bioinformatics, 30, 1273–1279.Marco,E. et al. (2014) Bifurcation analysis of single-cell gene expression datareveals epigenetic landscape. Proc. Natl. Acad. Sci., 111, E5643–E5650.Martello,G. et al. (2012) Esrrb is a pivotal target of the gsk3/tcf3 axis regulating embryonic stem cell self-renewal. Cell Stem Cell, 11, 491–504.Rais,Y. et al. (2013) Deterministic direct reprogramming of somatic cells topluripotency. Nature, 502, 65–70.Ruder,S. (2016). An overview of gradient descent optimization algorithms.Web Page, .1–12.Sethian,J. (1999) Fast marching methods. SIAM Rev., 41, 199–235.Setty,M. et al. (2016) Wishbone identiﬁes bifurcating developmental trajectories from single-cell data. Nat. Biotechnol., 34, 637–645.Sozen,B. et al. (2014) Cell fate regulation during preimplantation development: a view of adhesion-linked molecular interactions. Dev. Biol., 395,73–83.Stegle,O. et al. (2015) Computational and analytical challenges in single-celltranscriptomics. Nat. Rev. Genet., 16, 133–145.Taherian Fard,A. et al. (2016) Not just a colourful metaphor: modelling thelandscape of cellular development using Hopﬁeld networks. npj Syst. Biol.Appl., 2, 16001.Tang,F. et al. (2009) mRNA-Seq whole-transcriptome analysis of a single cell.Nat. Methods, 6, 377–382.Titsias,M.K., and Lawrence,N.D. (2010) Bayesian gaussian process latentvariable model. AISTATS, 9, 844–851.Trapnell,C. et al. (2014) The dynamics and regulators of cell fate decisions arerevealed by pseudotemporal ordering of single cells. Nat. Biotechnol., 32,381–386.Waddington,C.H. (1957). The strategy of the genes: a discussion of some aspects of theoretical biology. With an appendix by H. Kacser. Allens &Unwin: London, UK.Wang,J.M. et al. (2008) Gaussian process dynamical models for human motion. IEEE Trans. Pattern Anal. Mach. Intell., 30, 283–298.Yan,L. et al. (2013) Single-cell RNA-seq proﬁling of human preimplantationembryos and embryonic stem cells. Nat. Struct. Mol. Biol., 20, 1131–1139.Zhang,H. et al. (2014) A comprehensive review of stability analysis ofcontinuous-time recurrent neural networks. IEEE Trans. Neural Netw.Learn. Syst., 25, 1229–1262.Ziegenhain,C. et al. (2017). Comparative analysis of single-cell RNA sequencing methods. Molecular Cell, 65, 631–643.Zwiessele,M., and Lawrence,N.D. (2016). Topslam: Waddington landscaperecovery for single cell experiments. bioRxiv, p. 057778.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i102/3953947by gueston 07 January 2018
5028881966002	PMID28881966	5028881966	https://watermark.silverchair.com/btx231.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881966.main.pdf	Bioinformatics, 33, 2017, i83–i91doi: 10.1093/bioinformatics/btx231ISMB/ECCB 2017DextMP: deep dive into text for predictingmoonlighting proteinsIshita K. Khan1, Mansurul Bhuiyan2 and Daisuke Kihara1,3,*1Department of Computer Science, Purdue University, West Lafayette, IN, USA, 2Department of Computer Science,Indiana University-Purdue University Indianapolis (IUPUI), Indianapolis, IN, USA and 3Department of BiologicalScience, Purdue University, West Lafayette, IN, USA*To whom correspondence should be addressed.AbstractMotivation: Moonlighting proteins (MPs) are an important class of proteins that perform morethan one independent cellular function. MPs are gaining more attention in recent years as they arefound to play important roles in various systems including disease developments. MPs also have asigniﬁcant impact in computational function prediction and annotation in databases. Currently MPsare not labeled as such in biological databases even in cases where multiple distinct functions areknown for the proteins. In this work, we propose a novel method named DextMP, which predictswhether a protein is a MP or not based on its textual features extracted from scientiﬁc literatureand the UniProt database.Results: DextMP extracts three categories of textual information for a protein: titles, abstracts fromliterature, and function description in UniProt. Three language models were applied and compared:a state-of-the-art deep unsupervised learning algorithm along with two other language models ofdifferent types, Term Frequency-Inverse Document Frequency in the bag-of-words and LatentDirichlet Allocation in the topic modeling category. Cross-validation results on a dataset of knownMPs and non-MPs showed that DextMP successfully predicted MPs with over 91% accuracy withsigniﬁcant improvement over existing MP prediction methods. Lastly, we ran DextMP with the bestperforming language models and text-based feature combinations on three genomes, human,yeast and Xenopus laevis, and found that about 2.5–35% of the proteomes are potential MPs.Availability and Implementation: Code available at http://kiharalab.org/DextMP.Contact: dkihara@purdue.edu1 IntroductionInvestigation of function of proteins is a central problem in bioinformatics as it is an essential step for unfolding obscurities of cellularprocesses. Although a majority of proteins are speculated to performa single function, over the past decade a significant number of multifunctional, or more popularly called ‘moonlighting’ proteins areemerging into attention in the biology community (Campbell andScanes, 1995; Jeffery, 1999; Weaver, 1998). Moonlighting proteins(MPs) are defined as proteins that perform multiple independent cellular functions within a single polypeptide chain. Functional diversity of these proteins are not due to gene fusions, multiple domainsin the same protein chain, multiple RNA splice variants or proteolytic fragments, families of homologous proteins or pleotropic effects (Huberts and Vander Klei, 2010; Jeffery, 1999, 2004; Maniet al., 2014). Most prominent examples of MPs are enzymes(Jeffery, 1999, 2004). The first of such findings was in late 1980s,crystallins (Piatigorsky and Wistow, 1989; Wistow and Kim, 1991),which are structural eye lens proteins that also have enzyme function. Since then, MPs are continued to be found in a wide variety ofgenomes with diverse cellular functions and molecular mechanismsfor switching functions.In parallel to serendipitous findings of MPs through experiments,bioinformatics approaches have been applied to characterize MPs inrecent years (Khan and Kihara, 2014). Existing studies investigateddifferent aspects of MPs such as sequence similarity (Gomez et al.,2003; Khan et al., 2012), conserved motifs/domains, structural disorder (Hern ndez et al., 2011), and protein-protein interaction (PPI)apatterns (Chapple et al., 2015; G mez et al., 2011; Pritykin et al.,o2015). We have recently developed a computational predictionmethod named MPFit, which predicts MPs and non-MPs using a diverse set of proteomics data (Khan and Kihara, 2016). Developmentof MPFit was based on our previous study where we presented a systematic characterizations of MPs in a computational framework(Khan et al., 2014). However, all these existing studies overlook aCV The Author 2017. Published by Oxford University Press.i83This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018i84major resource of information of protein function, i.e. text-based information that underlies in scientific literature and textual description of protein annotation in databases such as UniProt (UniProtConsortium, 2014). In most cases MPs are not explicitly labelled inthe database with ‘moonlighting’, ‘dual function’, ‘multitasking’, orother related words, even in cases where two distinct functions areknown and clearly stated in its database entry. To accommodate thecurrent limited knowledge of MPs, two online repositories of MPs(Hern ndez et al., 2014; Mani et al., 2014) were built on expertaknowledge with manual curation from literature. This situation convinced us that application of text mining techniques on MP literature would provide a major boost towards automatic MPannotation. In this work we propose a first text mining-based approach for predicting MPs, named DextMP (Deep dive into tEXTfor predicting Moonlighting Proteins).For the last decade, text mining techniques has been extensivelydeveloped to unravel non-trivial knowledge from structured/unstructured text data (Manning et al., 2008). Most of the existingworks are based on bag-of-words that leverages word-related statistics in the text (Joachims, 1998). The next generation of text-basedfeature learning models represent each text with a distribution of latent topics (Hoffman et al., 2010). In recent years, unsuperviseddeep learning-based feature construction has become popular in textmining (Mikolov et al., 2013). Such deep-learning-based methodsmap text into a condensed d-dimensional continuous vector spacesuch that semantically similar texts are embedded nearby eachother.DextMP consists of four logical steps: first, it extracts textual information of proteins from literature (publication titles or abstracts)and functional description in UniProt. Next, it constructs a k-dimensional feature vector from each text. In this step, a state-of-the-artdeep unsupervised learning algorithm is applied, which is calledparagraph vector (Le and Mikolov, 2014), along with two otherwidely used language models, Term Frequency-Inverse DocumentFrequency (TFIDF) in the bag-of-words category (Manning et al.,2008) and Latent Dirichlet Allocation (LDA) in the topic modelingcategory (Hoffman et al., 2010). Third, using four machine learningclassifiers, a text is classified to MP or to non-MP based on the textfeatures. Finally, prediction made to each literature for a protein issummarized to make a prediction to the protein. Cross-validationresults on the dataset of known MPs and non-MPs (control dataset)show that DextMP can successfully predict MPs with over 91% accuracy, with a significant improvement over existing MP predictionmethods. Among the different forms of text information, abstractstaken from literature and function description in UniProt showedbetter performance than the title of literature. Lastly, we ranDextMP with the best performing language models and text-basedfeature combinations on three genomes, Saccharomyces cerevisiae(yeast), Homo sapiens (human) and Xenopus laevis (African clawedfrog), and found that about 2.5–35% of the proteomes are potentialMPs.2 Materials and methodsI.K.Khan et al.manually curated MP database, MoonProt (Mani et al., 2014).Proteins that do not have a UniProt ID were discarded. In addition,five MPs were discarded because they have over 25% sequence identity to other proteins in the dataset. Non-MPs were selected usingthe following Gene Ontology (GO) function annotation-based criteria developed in our previous works (Khan et al., 2014; Khan andKihara, 2016). From the four most dominant genomes in the MPdataset, namely, human (45 MP, 17.1%), E.coli (29 MPs, 11%),yeast (23 MPs, 8.7%) and mouse (11 MPs, 4.2%), a protein was selected as a non-MP if a) it has at least eight GO term annotations, b)when GO terms in the Biological Process (BP) category were clustered using the semantic similarity score (Schlicker et al., 2006) nomore than one cluster was obtained at either the 0.1 threshold or the0.5 threshold, and c) no more than one cluster of MolecularFunction (MF) GO terms at semantic similarity scores of 0.1 and 0.5were formed. In essence, a protein is considered as a non-MP if ithas a sufficient number of GO annotations but they are not functionally diverse. We further ruled out non-MPs that had above 25%sequence identity with another non-MP sequences, and finally selected 162 non-MPs, among which 60 are from human (37.0%), 52from mouse (32.1%), 34 from yeast (20.9%) and 16 from E.coli(9.88%). In summary, 263 MP and 162 non-MP were selected asthe control dataset for the DextMP model.2.2 Text extractionFor each of the proteins in the control dataset, we extracted threecategories of text information from UniProt: a) the title of each reference paper of the protein entry; b) the abstract of each reference;and c) the summary description of the protein’s function in theFUNCTION field in UniProt. The text data for a) and c) were directly collected from the UniProt data dump (http://www.uniprot.org/downloads), and b) was collected by crawling web links in thePUBLICATION list of the entry. Table 1 shows the statistics of thedata size. Note that while one protein can have multiple titles andabstracts associated with it, it only has one function description. For49 MPs, no publication title was found, whereas 105 MPs did nothave a hyperlink directed to a publication abstract (Table 1). Figure1 shows the distribution of the number of abstracts per MP andnon-MP in the dataset.Obtained text data underwent three layers of data clean-up.First, redundant literature that appears both in MPs and non-MPswere discarded (this typically happens for papers that describe manyproteins, e.g. genome annotation). Second, from each text data, allstop words, punctuations, and special symbols including Greek letters were removed. Finally, stemming and lemmatization were performed (Manning et al., 2008) using the nltk package (Bird, 2006).As an example we briefly describe text data for a MP, phosphoglucose isomerase (PGI) in mouse (UniProt ID: P06745). It primarilyacts as an enzyme in the second step of glycolysis. This proteinmoonlights by acting as a cytokine/growth factor, and causes pre-Bcells to mature into antibody secreting cells, supports survival of embryonal neurons, and causes differentiation of some leukemia cellTable 1. Data size for the MP/non-MP datasetWe first explain text data and features used, then describe learningmodels of DextMP.2.1 Dataset of MPs and non-MPsThe dataset of MPs and non-MPs (i.e. negative example of moonlighting proteins) were taken from our previous work (Khan andKihara, 2016). The dataset contains 263 MPs selected from a#ProteinsMPnon-MP263162#Titlesa#Abstractsa#Functions2496 (214)1665 (162)1450 (158)1624 (162)194162aIn the parenthesis the number of proteins is shown for which the text datawas found. For example, out of 263 MPs, at least one publication title wasfound for 214 MPs.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018DextMP: moonlighting protein prediction by text miningi85Fig. 1. Distribution of the number of abstracts per protein. Black, MP; gray,non-MP in the control dataset. The ﬁrst bar is for 1 and 2 abstracts, next bar isfor 3 and 4 and so onlines. Among 14 references for this protein, one is entitled ‘tumorcell autocrine motility factor is the neuroleukin/phosphohexoseisomerase polypeptide’, which implies that this protein is an MP. Itbecomes apparent if one reads the abstract of this paper (http://www.uniprot.org/citations/8674049) or the function description ofthe entry, which says ‘besides its role as a glycolytic enzyme, mammalian PGI can function as a tumor-secreted cytokine and an angiogenic factor (AMF) that stimulates endothelial cell motility. PGI isalso a neurotrophic factor (Neuroleukin) for spinal and sensory neurons’. Despite this clear knowledge of moonlightness of this protein,the UniProt entry does not use any exact keyword, e.g. moonlighting proteins, multifunction, etc., which clearly indicates that this isan MP.Fig. 2. Schematic diagram of DextMP. The upper panel shows the text prediction process while the bottom panel is for the prediction model that uses predicted text labels to make the ﬁnal MP/non-MP classiﬁcation. P1, Protein 1,CL: Class Labelprobability from the text level prediction. The weighted majorityvote was applied to three classifiers, LR, RF and GBM. This latterpart of DextMP is not applied when function description of proteinswas used, since there is only one description for a protein and votingis not needed.2.3 Framework of DextMPThe overall framework of DextMP is shown in Figure 2. It is split intwo parts, MP/non-MP prediction to a text (the text predictionmodel, the top panel in Fig. 2) and prediction made to a query protein by combining prediction made for each text of the protein (thebottom panel).In the text prediction model, first, three different types of text information, titles, abstracts and function descriptions, are extracted.Then the data clean-up step is carried out. Next, the texts in thedataset are applied to a deep unsupervised feature construction technique (Le and Mikolov, 2014), a bag of words model (Manninget al., 2008), and topic modeling (Hoffman et al., 2010) to constructtext features. Finally, machine learning classification algorithms,namely, logistic regression (LR), random forest (RF), SupportVector Machine (SVM) and gradient boosted machine (GBM)(Pedregosa et al., 2011) are applied to the learned features to provide a MP/non-MP prediction on each text data.Once we have a MP/non-MP class prediction for each text, weuse the model shown in bottom panel of Figure 2 to obtain a classprediction for proteins (protein-level prediction). Each protein isassociated with a certain number of texts (titles/abstracts) that havepredicted class labels. To make the final MP/non-MP classification,two heuristics were applied: (i) A majority vote, where we simplytake the binary class label votes for the protein using different majority cutoffs, 50%, 70%, 80% and 90%. (ii) A weighted majorityvote, where a weight for a text is from the class prediction2.4 Learning features from textHere we explain the three language models used for feature construction from text (Fig. 2, top panel).1. Bag-of-words with TFIDF: Given a text corpus (collection ofsentences/texts), the bag-of-words model first computes the dictionary that contains all the words in the text corpus. Given a dictionaryof size N, a text can be represented as an N-dimensional real-valuedvector with TFIDF values for each word in the dictionary. For aword w, TFIDF is be computed as follows: TFIDF(w) ¼ TF(w) *IDF(w), where Term Frequency, TF(w) ¼ (number of times word wappears in a text)/(total number of words in the text); and InverseDocument Frequency, IDF(w) ¼ loge(total number of texts in thecorpus/number of texts with word w); Intuitively, TFIDF measuresthe importance of a keyword to a sentence with respect to its entiredictionary corpus.2. Topic Modeling with LDA: In principle, the bag-of-wordsmodel has two critical limitations: for a large dictionary, the size ofthe feature vector for each text can be huge, which makes it computationally expensive, and it does not take consideration of the wordordering in a text. To alleviate above two challenges, in topic modeling a text is modeled as a distribution of words for latent topics,where the number of topics is a user-defined parameter. LatentDirichlet Allocation (LDA) is one of the most popular topic modeling algorithms, which uses two Dirichlet-multinomial distributionsto model the mappings between documents and topics, and topicsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018i86and words. We used an open source Python implementation of LDA(Rurek and Sojka, 2010).3. Unsupervised Deep Language Model, DEEP and PDEEP: Asthe third language model, we used a deep learning-based unsupervised feature construction algorithm (Le and Mikolov, 2014). Thismodel maps texts into a continuous vector space of a dimension d,such that semantically similar texts appear close in the space. For asequence of words W ¼ (w0, w1, . . ., wn), where wi 2 D (D is the dictionary), suppose wi is an input word and rest of the words in thedictionary form the context wc ¼ (w0, w1, . .wi-1, wiþ1, . wn) . A neural network with a single hidden layer is trained so that for a vectorof D dimension with 1 for wi and 0 otherwise, the network outputsthe conditional probability that the other words co-appear in theneighborhood of wi in a text. Using the conditional probabilitiesthat each word co-appears with wi in the training dataset of texts asknown outputs, weights of hidden layers are trained with backpropagation, and the weights of hidden layers are considered as afeature vector of wi.Mikolov et al. extended the above model by modifying the probability expression to Pr[WjT]. Here, T is the text containing the sequence of words, and can be thought as another word. Similar tothe above model along with the task of maximizing the conditionalprobability, it outputs d-dimensional feature vector representationof each text, T. We used an open source Python implementation ofthe ‘paragraph vector’ deep learning model (Rurek and Sojka,2010).Using the deep learning method, we computed two models,DEEP and PDEEP (Pre-trained deep learning model). For DEEP, features were constructed on texts from the control dataset of MP andnon-MP only. For PDEEP, we used the entire text data fromUniProt. Concretely, we extracted a total of 1 060 520 titles and551 056 function descriptions from the UniProt data dump. Sincepublication abstracts are not available in the data dump, we omittedPDEEP training for abstracts.2.5 Parameter tuning of DextMPWe used a grid search to determine hyper-parameters for LDA andDEEP. In LDA, the ‘number of topics’ parameter was tuned by agrid search performed between 10 and 100 with a step size of 10 foreach text type. In DEEP, we tuned three hyper-parameters: the ‘minimum count’ parameter was tuned within a range of 1–5, ‘windowsize’ was tuned within 2–8, both with a step size of 1, and ‘dimension size’ was tuned in a range from 20 to 200 with an increasingstep size of 20. The parameter ‘minimum count’ indicates the minimum number of texts that the word must appear in, ‘window size’is the size of the convolution context, and ‘dimension size’ indicatesthe length of the feature vector representation. For PDEEP, we usedthe same parameters as DEEP.The hyper-parameters associated with the four classifiers ofDextMP were also determined using a grid search. For LR and SVMwe tuned the regularization, a cost parameter and a kernel function(linear or radial basis function), and used the default values for theother parameters in the models in the sklearn package (Pedregosaet al., 2011). For RF and GBM, we tuned the ‘number of trees’ parameter, the ‘learning rate’ parameter for GBM, and used the defaultfor the others.We performed a five-fold cross-validation. The dataset was splitinto five sub-groups, among which three sub-groups were used fortraining, another sub-group was used for validation, and the lastsub-group was used for testing. Given a vector of hyper-parametersfor a combination of a model and a classifier, where a model isI.K.Khan et al.either LDA/DEEP and a classifier be LR/RF/SVM/GBM, we performed a grid search for the optimal hyper-parameters over thetraining set, used the validation set to find the best hyper-parametervector, and ran the optimized model with the hyper-parameter values for the test set to report results. For example, when LDA and LRcombination was to be trained, for each of all the combinations ofthe ‘number of topics’ parameter for LDA and the regularizationparameter for LR, model parameters were optimized on a trainingset that consists of three sub-groups. Once the model was optimizedfor each of the all hyper-parameter combinations on the training set,the optimized models were tested on the validation set to determinethe best hyper-parameter combination and the model optimizedunder the hyper-parameters. Then, the selected model was tested onthe testing set to report the F-score for that parameter setting. Eachsub-group was used once for testing. We performed the above procedure for five test sub-groups independently, and finally reportedthe average F-score computed for the 5 test sub-groups.3 Results3.1 Text features of MPsTo begin with, we browsed abundant words in texts of MPs in thecontrol dataset. In Figure 3, word clouds of the three categories oftexts, publication titles, function descriptions and abstracts, areshown.From the word clouds, a few points came to light: the words ‘enzyme’, ‘kinase’ and ‘transcription’ appear in all three text types inFigure 3 (red circles). Word counts of ‘enzyme’ in titles, abstracts,UniProt function descriptions are 108/34, 562/107, 89/26, respectively for MP/non-MP. Counts for ‘kinase’ and ‘transcription’ were(102/16, 210/105, 44/9) and (87/59, 431/331, 75/34), respectively,for (titles, abstracts, UniProt function descriptions) of MP/non-MP.This is consistent with previous reports that many MPs were knownprimarily as enzymes when their secondary function, such as transcription factor, was discovered (Hern ndez et al., 2014; Jeffery,a2003; Khan and Kihara, 2016; Mani et al., 2014). The word ‘ribosome’, which appeared as the top word in Figure 3A (green circles),also agrees with our previous finding (Khan and Kihara, 2016) thatpredicted MPs were enriched in ribosomal pathways in the KEGGdatabase (Kanehisa and Goto, 2000), and found in literature (Wool,1996). Additionally, words that are clear indicators of MPs also appeared, such as ‘bifunctional’ (counts were 21/0, 29/5, 6/0 for MP/non-MP in titles, abstracts, function descriptions, respectively; blueFig. 3. Word clouds of text information of moonlighting protein dataset. Thesize of a word in the visualization is proportional to the number of times theword appears in the input text. (A–C): titles, function descriptions and abstracts, respectively. The images were generated at http://www.wordle.net/Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018DextMP: moonlighting protein prediction by text miningi87circle in Fig. 3A) and ‘multifunctional’ (12/0, 19/4, 4/0 for MP/nonMP in titles, abstracts, function descriptions, respectively).3.2 DextMP performance on text level predictionWe now show prediction results of the text-level MP prediction byDextMP on the control dataset (Table 2). A schematic diagram ofthis part of the DextMP model is described in the top panel ofFigure 2 and explained in Section 2.2. Along with the two differentdeep learning based models (DEEP and PDEEP), we used two othermethods in popular language model categories, TFIDF in the ‘bagof-words’ category and LDA in the ‘topic modelling’ category. Foreach language model, three forms of text information, titles, abstracts and UniProt function descriptions, were used and compared.Note that the abstracts-PDEEP combination was omitted, as it requires all publication abstracts for the entire protein corpus inUniProt for model training. Since UniProt does not maintain any filedump for publication abstracts, it requires running a web-crawlerand downloading abstract texts for all proteins in UniProt, whichbecame computationally very expensive. For learned features byeach language model, we further used four classifiers, LR, RF, SVMand GBM to make MP/non-MP classification (shown in right columns of Table 2).Among all the text-language_model-classifier combinations testedin Table 2, the highest F-score, 0.9371, was recorded by the combination of TFIDF and SVM when it was applied to literature abstracts(abstracts-TFIDF-SVM). The precision was 0.8920 and the recall was0.8640. Besides this best combination, seven more combinationsshowed an F-score over 0.850. Comparing the three text types, abstracts had the highest F-score (0.9371), and UniProt function descriptions was second highest (0.9184), and using titles had the lowest(0.8751).This order was consistent when the average F-score acrossdifferent model-classifier combinations for each text type was considered: the abstracts again showed the highest value of 0.8053 incomparison to the function descriptions (0.7138) and the titles(0.7141). We further counted which text type showed the highestF-score for combinations of language models (PDEEP was excluded)and classifiers, e.g. TFIDF-LR. Six combinations showed the highestTable 2. Summary of the text-level prediction with different combinations of text types, language models and classiﬁersText TypeLanguage ModelClassifiersLRTitlesAbstractsFunction DescriptionsTFIDFLDADEEPPDEEPTFIDFLDADEEPPDEEPTFIDFLDADEEPPDEEPRF0.77740.61280.76960.62620.92200.64190.7775–0.74120.61280.89290.70170.79420.68290.74020.54820.86820.69360.8119–0.74390.68290.89620.7211SVM GBM0.87510.65840.84290.48360.93710.65120.8480–0.77150.65820.91840.34740.72180.70650.80290.64450.83960.73490.7987–0.69470.70650.87880.6917Two-class weighted F-score was reported, where F-score of MP and nonMP was calculated and weighted average of them was taken, where theweights are the number of data points of each class. The values shown are theaverage of the test sets in the Five-fold cross-validation. LR, LogisticRegression; RF, Random Forest; SVM, Support Vector Machine; GBM,Gradient Boosted Machine.F-score when applied to abstracts, four combinations with functiondescriptions, and two were best with literature titles. These analysesshow that MP detection can be done better by using abstracts orUniProt function descriptions than simply using literature titles.Next we compare four language models. In Table 2, for eachtext type, the best performing language model under four classifiersis highlighted in bold. Surprisingly, that the simple TFIDF workedwell with titles and abstracts. This is likely because titles are simplerso that TFIDF can easily capture MP-specific words to make correctpredictions. DEEP showed superior performance in the function description category, which implies that complex semantic inter-wordrelations in the function descriptions require a complex model tocorrectly identify characteristics of MPs and non-MPs. DEEP clearlyoutperforms LDA in all three text information categories and allfour classifiers, while PDEEP had 3 (out of 8) wins over LDA.PDEEP was built as an extension from DEEP by enlarging its training set to the whole corpus in UniProt. This model showed a lowerF-score consistently for both the title and the function descriptioncategories. The reason for the lower accuracy of PDEEP is maybebecause the training data used for PDEEP is too large and somewhatgeneralized textual features unique for MPs.Comparing the four classifiers, SVM showed the best result in sixcases among the eleven different settings (rows in Table 2), GBM wonfor four cases and RF for one case. LR did not win a single setting.We have also tested DextMP’s prediction accuracy when text summaries were used as input. A summary of each abstract and eachUniProt function description was computed using a well-known algorithm, TextRank (Rada and Tarau, 2004) implemented in a generalpurpose python text summarization package, sumy (https://pypi.python.org/pypi/sumy). On average a summary reduced word counts ofan abstract from 178.3 to 152.7 and from 172.2 to 145.4 for a function description. Using computed summaries of abstracts, F-score forthe TFIDF model with the LR, SVM, GBM and RF classifiers reducedto 0.7937, 0.8043, 0.7259 and 0.7692, respectively. We used theTFIDF model for this comparison because it performed best amongthe language models used on the original abstract texts (Table 2). Forfunctional descriptions, using summaries also reduced F-score of theDEEP model with the four classifies to 0.8525, 0.8395, 0.7494 and0.8210, respectively. DEEP was used here since it performed best forfunction descriptions. The reduction of the F-score was about 5 to20% relative to when the original texts were used.In terms of computational time, DEEP takes substantially moretime in training relative to TFIDF and LDA, because the neural network needs to be trained (Table 3). However, since training can bepre-computed using a training dataset and reused later, in practiceDEEP does not take much time when applied to predictions of newdata relative to the time needed for training. In contrast, althoughTFIDF takes a short time for training, it takes a substantially longertime in prediction because the size of a feature vector becomes largeas more unique words appear, which exceeded 16 000.Computational time is classified into three steps of the DextMPalgorithm, training, feature generation and classification. Training isthe time needed on average for processing a text to compute parameters of a language model. Feature generation is the time needed tocompute a feature vector of a text to be classified. Classification isthe average time that each classifier took to make a classification toa text.3.3 DextMP performance on protein level predictionNext, we discuss the performance of DextMP on the final proteinlevel MP/non-MP classification using predictions made to each textDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018i88I.K.Khan et al.Table 3. Computational time (seconds)PhaseText TypeLanguage modelTFIDFTrainingFeature generationClassiﬁcationTitlesAbstractsFunction Dsc.TitlesAbstractsFunction Dsc.TitlesAbstractsFunction Dsc.LDADEEP5.8*10À53.3*10À46.3*10À47.8*10À43.2*10À32.2*10À35.1*10À21.2*10À17.0*10À21.0*10À32.9*10À31.5*10À23.3*10À46.6*10À41.0*10À33.8*10À34.0*10À35.3*10À34.4*10À19.0*10À11.21.8*10À42.4*10À42.0*10À49.2*10À31.3*10À26.1*10À3that belongs to proteins. This process is represented in the bottompanel in Figure 2. When UniProt function descriptions are used, aprotein-level prediction is identical to the text-level prediction, because a query protein has only one UniProt description. When titlesor abstracts of literature were used as text information, classificationlabels assigned to texts of a query protein were summarized using asimple majority vote or a weighted majority vote. As mentioned inSection 2.3, for a simple majority vote, four majority cutoffs, 50%,70%, 80%, 90%, were tested in cross-validation for each combination of (text type)-(language model)-(classifier), and the cutoff thatgave the largest F-score in the validation set was chosen and appliedto the testing set. In Figure 4 we compared F-scores of protein levelclassification of the 21 (text type)-(language model)-(classifier) combinations using the simple majority votes and the weighted majorityvotes. Out of all 44 combinations in Table 2, the 21 combinationsused in Figure 4 were LR, RF, and GBM classifiers applied to thetitle and abstract categories. The function descriptions category wasexcluded as a text type because it does not need voting. Among the21 combinations, the simple majority votes showed a larger F-scorefor 15 cases than the counterpart, although margins are not verylarge. Therefore, we only show the results with the simple voting forthe rest of this work.Table 4 summarizes F-scores of the protein-level MP prediction.The highest F-score was achieved when function descriptions wereused by a combination of DEEP-SVM (0.9184). Note that valuesfor function descriptions are identical to the text-level accuracy(Table 2) because one protein has only one description in UniProt.Comparing with the text-level prediction results in Table 2, a similar order of performance by different setting combinations wasobserved. However, a difference is that in almost all the cases thetext-level accuracy was higher than the protein-level, which indicatesthe voting step in the protein-level prediction decreased the accuracy.Following the highest F-score combination of function descriptionsDEEP-SVM, the next three top combinations were all with functiondescriptions, which kept the same values as the text-level prediction,using the DEEP language model. Similar to what was observed inTable 2, TFIDF showed the best results with all the classifiers in thetitles category, and also the best with three classifiers in the abstractscategory, as highlighted in bold. Precision and recall values were wellbalanced for the F-score results in Table 4. For example, for the titlesTFIDF-SVM combination, which showed an F-score of 0.8330, precision and recall were 0.8316 and 0.8479, respectively.For comparison, we ran a sequence-based function predictionmethod, PFP (Hawkins et al., 2006, 2009) on the control datasetand classified the proteins to MP/non-MP based on predicted GOterms. Following the GO term-based MP/non-MP classification performed in our previous study (Khan et al., 2014), a protein wasFig. 4. Protein-level cross-validation F-scores for weighted and non-weightedmajority votes. Results for 21 (text type)-(language model)-(classiﬁer) combinations are comparedTable 4. Summary of the protein-level predictionText TypeLanguage Model ClassifiersLRTitlesAbstractsFunction DescriptionsTFIDFLDADEEPPDEEPTFIDFLDADEEPPDEEPTFIDFLDADEEPPDEEPRFSVMGBM0.77030.56540.66510.66110.81320.54590.7650–0.74120.61280.89290.70170.74740.57230.66980.52780.82250.57390.8105–0.74390.68290.89620.72110.83300.58360.75570.43140.82080.53420.7747–0.77150.65820.91840.34740.69010.62270.68260.60210.78330.57130.7909–0.69470.70650.87880.6917F-score was reported. The values shown are the average of the test sets inthe ﬁve-fold cross validation. LR, Logistic Regression; RF, Random Forest;SVM, Support Vector Machine; GBM, Gradient Boosted Machine. For eachtext type, titles, abstracts and function descriptions, the best performing language model under four classiﬁers is highlighted in bold.classified into MP if more than one GO term in the BP category waspredicted with moderate or higher confidence scores (a PFP rawscore > 500), and if the GO terms were classified into more thantwo clusters using the relevance semantic similarity (SS_Rel) score(Schlicker et al., 2006) of 0.1 and more than four clusters at SS_Relof 0.5. This protocol predicted 127/52 MPs/non-MPs correctly outof 263/162 MPs/non-MPs, resulting in an F-score of 0.4472. Thus,DextMP showed a higher accuracy than the function predictionbased results.We think the prediction accuracy shown in Table 4 is sufficientlyhigh for practical use, particularly for a large scale screening considering that an alternative for finding MPs from text is for someoneto read texts one by one.3.4 Genome-scale MP prediction using DextMPFinally, we applied DextMP for predicting MPs in three genomes.Two genomes, S. cerevisiae (yeast) and H. sapiens (human), werechosen because MP prediction by MPFit (Khan and Kihara, 2016)were previously tested on them, so that we can compare DextMPwith MPFit. One more genome, X.laevis, was chosen becauseomics-data, such as gene expression and protein-protein interactionDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018DextMP: moonlighting protein prediction by text miningdata, are not available for this organism, and thus existing MP prediction methods (Chapple et al., 2015; G mez et al., 2011), whichorely on omics-data, cannot be used. Therefore DextMP can makeunique contributions. Among the eleven settings we tested in Table3, we used the top two models in the titles category and the top twoin the function descriptions category from Table 4, i.e. titlesTFIDF-SVM, titles-TFIDF-LR, function_descriptions-DEEP-RF andfunction_descriptions-DEEP-SVM, and took the consensus of thefour predictions. We did not use abstracts-based methods becauseabstracts were not directly available at UniProt and were not convenient for a large-scale prediction.In our previous work, we developed an omics-data-based MPprediction method, MPFit (Khan and Kihara, 2016) and demonstrated that it outperformed two existing methods, one of whichuses a target organism’s PPI network (Chapple et al., 2015) and another method that is based on GO term annotation of proteins(Pritykin et al., 2015). Therefore, in this section, we compareDextMP mainly with MPFit on the yeast and human genomes forwhich MPFit was applied.Table 5 summarizes predictions to the three genomes. For theyeast genome, out of 6721 proteins, 6500 had both title and function description in UniProt so that DextMP can run on them (coverage 96.73%). Among these proteins, 2316 (34.46% of the entireproteins in the genome) were predicted as MP by DextMP when aconsensus of three settings are considered, and 896 (13.33%) if aconsensus of the all four settings was considered. In our previouswork, MPFit predicted 10.97% of the yeast proteins are MPs, whichis similar to the current prediction with the full (four) consensus.Since yeast has 27 known MPs in the MoonProt database (Maniet al., 2014), we computed recall based on them. Out of 27 knownMPs, 24 and 20 are detected as MPs when a consensus of !3 and 4settings are considered, which give recall value of 0.889 and 0.741,respectively. MPFit recorded a recall of 0.8146 (22 out of 27) in theprevious work, which is between the two values in the current work.These two recall values are significantly higher than the GO termbased prediction by Pritykin et al. (2015), which was 0.4815.Besides the high recall value, DextMP also has a strong advantage ofhaving a higher coverage than both MPFit and the method byPritykin et al., because text information is in general more availablethan omics-data or GO annotations, which the two methods use asinput. The coverage for DextMP was 96.73%, while MPFit andPritykin et al. had a coverage of 69.56% and 68.69%, respectively.The human genome has a very high coverage of 98.06% (19 713proteins out of 20, 104 proteins), which have text information andwere subject to DextMP’s prediction. This is much higher than thecoverage for both MPFit (67.91%), the GO-based method byPritykin et al. (48.08%) and the PPI-based method (Chapple et al.,2015) (64.01%). Out of 45 known MPs in human, 42 were predicted correctly by DextMP (recall: 0.9333) when a consensus withthree or more settings was considered. With the full consensus of thefour settings, 31 MPs were correctly detected (recall: 0.689). Thesetwo recall values are higher than the two existing methods, the GObased method (recall: 0.4889) and the PPI-based method (recall:0.0667). Our previous method, MPFit, had a recall of 0.7333, whichis between the two recall values recorded in the current work.DextMP predicted that 23.78% to 8.37% of human proteins areMPs with the two cutoffs of consensus voting. The lower value,8.37%, is close to the MPFit’s prediction of 7.82% (Khan andKihara, 2016).As discussed above, a major advantage of DextMP is that itsolely relies on text information of proteins, unlike the other methods that cannot be applied for proteins that lack experimentali89Table 5. Genome-scale prediction by DextMPYeast# ProteinsCoverage# MPs (%) (vote ! 3)# MPs (%) (vote ¼ 4)# known MPsrecall (vote ! 3)recall (vote ¼ 4)HumanX.laevis672196.73%2316(34.46%)896(13.33%)230.8890.74120 10498.06%4781(23.78%)1682(8.37%)450.9330.68911 07830.54%600(5.42%)279(2.51%)–––Coverage, the percentage of proteins in a genome that have both literaturetitle and function descriptions, so that DextMP can run on them. Two prediction results are shown: the number of predicted MP proteins which are detected by three or more settings (vote ! 3) and the number of MPs detected bythe all four settings unanimously (vote ¼ 4). X.laevis does not have knownMPs. The fraction in parentheses was computed for predicted MPs among allthe proteins in the genome.studies (e.g. PPI) or well-curated GO term annotations. Capitalizingon this aspect, we ran another genome, X.laevis with DextMP as itis not applicable for MPFit or the two other existing predictionmethods because the genome lacks experimental studies. ForX.laevis, out of 11 078 proteins 30.5% have literature informationin UniProt. Due to the smaller number of proteins with literature information as compared with human and yeast, the fraction of predicted MPs in X.laevis, 2.51-5.42%, seems small, but the fractionagainst the 11 078 proteins with literature is in accordance with theresults for yeast and human.We now discuss three case studies where DextMP made correctprediction to known MPs while our previous method, MPFit, failed.The first example is a band 3 anion transport protein in human(UniProt ID: P02730). The primary function of this protein is transportation of inorganic anions across the plasma membrane while themoonlighting function is a scaffold providing binding sites for glycolytic enzymes (Low et al., 1993). MPFit failed to predict this proteinas an MP because this protein lacks four out of six omics-data features (i.e. PPI, phylogenetic profile, genetic interaction), whichMPFit imputes to complete input feature values but apparently itdid not work. In contrast, this protein has functional description inUniProt, which clearly depicts its two functions as follows: functionsboth as a transporter that mediates electroneutral anion exchangeacross the cell membrane and as a structural protein, and interactions of its cytoplasmic domain with cytoskeletal proteins, glycolytic enzymes, and hemoglobin. Based on this text, it was easy forDextMP to make a correct MP prediction.The second example is protein PHGPx (UniProt ID: P36969) inhuman. The primary function of this MP is cell protection againstmembrane lipid peroxidation and cell death while the moonlightingfunction is the protein’s structural role in mature spermatozoa(Scheerer et al., 2007). MPFit could not see characteristics of MPs inthis protein’s omics data, because some input features were notavailable and moreover, an important feature, functional divergenceof interacting proteins in its PPI network, was not observed.However, the protein’s functional description in UniProt indicatestwo functions, protects cells against membrane lipid peroxidationand required for normal sperm development and male fertility,which resulted in a correct MP prediction by DextMP.The last example is gephyrin (UniProt ID: Q9NQX3) in human.This protein anchors transmembrane receptors by connecting membrane proteins to cytoskeleton microtubule binding proteins. ItsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018i90moonlighting function is biosynthesis of the molybdenum cofactor(Stallmeyer et al., 1999). Similar to the previous two examples, thisprotein lacks several omics-data that are used as features in MPFit.Gene expression data showed that this protein has a similar expression pattern with genes with different functional classes, which is anindicator of an MP, but this information was diluted in combinationwith other omics-data. On the other hand, it is clear from itsfunctional description that it has two functions: It says microtubuleassociated protein involved in membrane protein-cytoskeleton interactions related to its first function, and catalyzes two steps in thebiosynthesis of the molybdenum cofactor, which is related to the second function.Lastly, we provide two examples where DextMP made novel MPprediction (i.e. proteins which are not recorded as known MPs inthe MoonProt database). The first example is aminoacyl tRNAsynthase complex-interacting protein 1 (UniProt ID: Q12904) inhuman. This protein is a non-catalytic component of the multisynthase complex, and among other multi-functionalities, it moonlights by binding to tRNA, possesses inflammatory cytokine activity,and is involved in glucose homeostasis, angiogenesis and wound repair (Han et al., 2006). The second example is exoribonuclease inyeast (UniProt ID: P22147). According to its function description inUniProt, this is also a multi-functional protein that exhibits severalindependent functions at different levels of the cellular processes. Itis a 50 –30 exonuclease component of the nonsense-mediated mRNAdecay (NMD), and has a role in multiple processes including DNAstrand exchange and exonuclease activities, preventing accumulation of potentially harmful truncated proteins, regulating the decayof wild-type mRNAs, degradation of mature tRNA, and defenseagainst viruses, among other functions (Johnson and Kolodner,1999; K€slin and Heyer, 1994).aI.K.Khan et al.In Table 5, we provided two MP estimations by using cutoffs ofthree or four votes. Using the three-vote criterion showed a better recall against known MPs by design, however, it is difficult to determine which estimations should be more trusted since the numberof known MPs are currently very limited. Thus these two criteriashould be considered as confidence levels of a prediction. The current prediction provides a rough estimates of MPs in genomes,which itself would be informative and useful to gain a large perspective of MPs. Ultimately, literature of all genes in genomes needs tobe manually checked to obtain the precise number of MPs, whereDextMP’s prediction can help in prioritizing genes to examine.The results of the genome-scale prediction nevertheless suggestthat MPs are not mere exceptions but common in organisms. Thisobservation triggers various interesting biological questions, for example, how proteins gain moonlighting functions during evolutionand biophysical mechanisms that enable a protein to have multiplefunctions. Correct annotation to proteins with dual functions alsoaffects to functional enrichment analysis (Wei et al., 2017), which iscommonly used in systems and network biology (Dotan-Cohenet al., 2009; Hawkins et al., 2010; Rachlin et al., 2006). This workis also relevant to computational biologists, particularly those whoare working on developing function prediction methods (Hawkinsand Kihara, 2007), genome annotation, function analysis on networks and curation of functional annotation in databases.Overall this work will help our understanding of the multifunctional nature of proteins at the systems level, and will aid inexploring the complex functional interplay of proteins in a cellularprocess.AcknowledgementsThe authors thank Sarah Rodenbeck for proofreading the manuscript.4 DiscussionWe developed DextMP that predicts MPs from text information,which is the first work of this kind. DextMP complements our earlier work, MPFit, which predicts MPs from their omics data-basedfeatures. DextMP showed significant improvement of predictionsover existing methods. Moreover, it is widely applicable because itonly needs the text information of target proteins. Since the study ofMPs is still in its early stage, even in cases that proteins are knownto have multiple distinct functions, they are not explicitly labeled asMPs in databases. DextMP will be a very useful tool for detectingpotential MPs from a vast amount of UniProt entries.It would be appropriate to discuss implication and technical nature of the provided genome-scale MP prediction. Since DextMPuses literature information of genes, the quantity and the quality ofavailable literature directly affects to prediction results. A smallernumber of MPs were detected in X. laevis than yeast and human apparently because only 30.54% of genes in the genome have literatureinformation. It is also noticed that the predicted fraction of MPs inyeast is larger than human, but this result is also at least partly reflecting the fact that yeast has one of the most well studied andannotated genomes as it is a model organism for systems biology.Another technical point to note is that the accuracy of DexMP wasconfirmed on the control set, where the numbers of positive andnegative data are balanced. Since this MP/non-MP distribution isdifferent in genomes from the control set, the accuracy of thegenome-scale prediction may be affected by that. Also the negativedata used in the control set have unavoidable uncertainty, becausenon-MPs in the dataset may be found as MPs in the future.FundingThis work was partly supported by the National Science Foundation(DBI1262189). DK also acknowledged supports from the National Institutesof Health (R01GM097528) and the National Science Foundation(IIS1319551, IOS1127027, DMS1614777).Conﬂict of Interest: none declared.ReferencesBird,S. (2006) NLTK: the natural language toolkit. COLING/ACL Interact.Present. Sessions, 69–72.Campbell,R.M. and Scanes,C.G. (1995) Endocrine peptides 0 moonlighting0 asimmune modulators: roles for somatostatin and GH-releasing factor.J. Endocrinol., 147, 383–396.Chapple,C.E. et al. (2015) Extreme multifunctional proteins identiﬁed from ahuman protein interaction network. Nat. Commnun., 6, 7412.Dotan-Cohen,D. et al. (2009) Biological process linkage networks. PLoSONE, 4, e5313.G mez,A. et al. (2011) Do protein-protein interaction databases identifyomoonlighting proteins? Mol. BioSyst., 7, 2379–2382.Gomez,A. et al. (2003) Do current sequence analysis algorithms disclose multifunctional (moonlighting) proteins? Bioinformatics, 19, 895–896.Han,J.M. et al. (2006) Structural separation of different extracellular activitiesin aminoacyl-tRNA synthetase-interacting multi-functional protein, p43/AIMP1. Biochem. Biophys. Res. Commun., 342, 113–118.Hawkins,T. et al. (2006) Enhanced automated function prediction using distantly related sequences and contextual association by PFP. Protein Sci., 15,1550–1556.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018DextMP: moonlighting protein prediction by text miningHawkins,T. et al. (2010) Functional enrichment analyses and construction offunctional similarity networks with high conﬁdence function prediction byPFP. BMC Bioinformatics, 11, 265–286.Hawkins,T. et al. (2009) PFP: Automated prediction of gene ontology functional annotations with conﬁdence scores using protein sequence data.Proteins Struct. Funct. Bioinf., 74, 566–582.Hawkins,T. and Kihara,D. (2007) Function prediction of uncharacterized proteins. J. Bioinf. Comput. Biol., 5, 1–30.Hern ndez,S. et al. (2011) Do moonlighting proteins belong to the intrinsicallyadisordered protein class? J. Proteomics Bioinf., 5, 262–264.Hern ndez,S. et al. (2014) MultitaskProtDB: a database of multitasking proateins. Nucleic Acids Res., 42, D517–D520.Hoffman,M. et al. (2010) Online learning for latent dirichlet allocation. Adv.Neural Inf. Process. Syst., 23, 856–864.Huberts,D.H. and Vander Klei,I.J. (2010) Moonlighting proteins: an intriguing mode of multitasking. Biochim. Biophys. Acta, 1803, 520–525.Jeffery,C.J. (2003) Moonlighting proteins: old proteins learning new tricks.Trends Genet., 19, 415–417.Jeffery,C. (1999) Moonlighting proteins. Trends Biochem. Sci., 24, 8–11.Jeffery,C. (2004) Moonlighting proteins: complications and implications forproteomics research. Drug Discov. Today TARGETS, 3, 71–78.Joachims,T. (1998) Text categorization with support vector machines: Learningwith many relevant features. Eur. Conf. Mach. Learn., 10, 137–142.Johnson,A.W. and Kolodner,R.D. (1999) Strand exchange protein 1 fromSaccharomyces cerevisiae. A novel multifunctional protein that contains DNAstrand exchange and exonuclease activities. J. Biol. Chem., 266, 14046–14054.Kanehisa,M. and Goto,S. (2000) KEGG: Kyoto encyclopedia of genes andgenomes. Nucleic Acids Res., 28, 27–30.K€slin,E. and Heyer,W.D. (1994) A multifunctional exonuclease from vegetaative Schizosaccharomyces pombe cells exhibiting in vitro strand exchangeactivity. J. Biol. Chem., 269, 14094–14102.Khan,I. et al. (2014) Genome-scale identiﬁcation and characterization ofmoonlighting proteins. Biol. Direct., 9, 1–29.Khan,I. and Kihara,D. (2014) Computational characterization of moonlighting proteins. Biochem. Soc. Trans., 42, 1780–1785.Khan,I. and Kihara,D. (2016) Genome-scale prediction of moonlighting proteins using diverse protein association information. Bioinformatics, 32,2281–2288.Khan,I. et al. (2012) Evaluation of function predictions by PFP, ESG, and PSIBLAST for moonlighting proteins. BMC Proceedings, 6, S5.Le,Q.V. and Mikolov,T. (2014) Distributed representations of sentences anddocuments. arXiv Preprint, 1405.4053.i91Low,P.S. et al. (1993) Regulation of glycolysis via reversible enzyme bindingto the membrane protein, band 3. J. Biol. Chem., 268, 14627–14631.Mani,M. et al. (2014) MoonProt: a database for proteins that are known tomoonlight. Nucleic Acids Res., 43, D277–D282.Manning,C.D. et al. (2008) Introduction to Information Retrieval. CambridgeUniversity Press, Cambridge.Mikolov,T. et al. (2013) Distributed representations of words and phrases andtheir compositionality. Adv. Neural Inf. Process. Syst., 26, 3111–3119.Pedregosa,F. et al. (2011) Scikit-learn: machine learning in Python. J. Mach.Learn. Res., 12, 2825–2830.Piatigorsky,J. and WistowG,J. (1989) Enzyme/crystallins: gene sharing as anevolutionary strategy. Cell, 57, 197–199.Pritykin,Y. et al. (2015) Genome-wide detection and analysis of multifunctional genes. PLoS Comput. Biol., 11, e1004467.Rachlin,J. et al. (2006) Biological context networks: a mosaic view of the interactome. Mol. Syst. Biol., 2, 66.Rada,M. and Tarau,P. (2004) TextRank: Bringing order into texts. In:Proceedings of EMNLP, Association for Computational Linguistics,Barcelona, Spain, pp. 404–411.Rurek,R. and Sojka,P. (2010) Software Framework for Topic Modelling withLarge Corpora. In: Proceedings of LREC 2010 workshop New Challengesfor NLP Frameworks, University of Malta, Valletta, Malta, pp. 45–50.Scheerer,P. et al. (2007) Structural basis for catalytic activity and enzyme polymerization of phospholipid hydroperoxide glutathione peroxidase-4(GPx4). Biochemistry, 46, 9041–9049.Schlicker,A. et al. (2006) A new measure for functional similarity of geneproducts based on Gene Ontology. BMC Bioinformatics, 7, 322.Stallmeyer,B. et al. (1999) The neurotransmitter receptor-anchoring protein gephyrin reconstitutes molybdenum cofactor biosynthesis in bacteria,plants, and mammalian cells. Proc. Natl. Acad. Sci. U. S. A., 96,1333–1338.UniProt Consortium. (2014) Activities at the Universal Protein Resource(UniProt). Nucleic Acids Res., 42, D191–D198.Weaver,D.T. (1998) Telomeres: moonlighting by DNA repair proteins. Curr.Biol., 8, R492–R494.Wei,Q. et al. (2017) NaviGO: Interactive tool for visualization and functionalsimilarity and coherence analysis with gene ontology. BMC Bioinformatics,18, 177.Wistow,G.J. and Kim,H. (1991) Lens protein expression in mammals:taxonspeciﬁcity and the recruitment of crystallins. J. Mol. Evol., 32, 262–269.Wool,I.G. (1996) Extraribosomal functions of ribosomal proteins. TrendsBiochem. Sci., 21, 164–165.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i83/3953945by gueston 07 January 2018
5028881965002	PMID28881965	5028881965	https://watermark.silverchair.com/btx230.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881965.main.pdf	Deep learning-based subdivision approach forlarge scale macromolecules structure recoveryfrom electron cryo tomogramsMin Xu1,*, Xiaoqi Chai2, Hariank Muthakana3, Xiaodan Liang4,Ge Yang2, Tzviya Zeev-Ben-Mordehai5 and Eric P. Xing41Computational Biology Department, 2Biomedical Engineering Department, 3Computer Science Department,4Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA and 5Division of StructuralBiology, Wellcome Trust Centre for Human Genetics, University of Oxford, Oxford OX3 7BN, UK*To whom correspondence should be addressed.AbstractMotivation: Cellular Electron CryoTomography (CECT) enables 3D visualization of cellular organ-ization at near-native state and in sub-molecular resolution, making it a powerful tool for analyzingstructures of macromolecular complexes and their spatial organizations inside single cells.However, high degree of structural complexity together with practical imaging limitations makesthe systematic de novo discovery of structures within cells challenging. It would likely require aver-aging and classifying millions of subtomograms potentially containing hundreds of highly hetero-geneous structural classes. Although it is no longer difficult to acquire CECT data containing suchamount of subtomograms due to advances in data acquisition automation, existing computationalapproaches have very limited scalability or discrimination ability, making them incapable of pro-cessing such amount of data.Results: To complement existing approaches, in this article we propose a new approach for subdi-viding subtomograms into smaller but relatively homogeneous subsets. The structures in thesesubsets can then be separately recovered using existing computation intensive methods. Our ap-proach is based on supervised structural feature extraction using deep learning, in combinationwith unsupervised clustering and reference-free classification. Our experiments show that, com-pared with existing unsupervised rotation invariant feature and pose-normalization basedapproaches, our new approach achieves significant improvements in both discrimination abilityand scalability. More importantly, our new approach is able to discover new structural classes andrecover structures that do not exist in training data.Availability and Implementation: Source code freely available at http://www.cs.cmu.edu/ mxu1/software.Contact: mxu1@cs.cmu.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionCellular processes are governed by macromolecules. Knowledge ofthe native structures and spatial organizations of macromoleculeswithin a single cell is a prerequisite for our understanding of cellularprocesses. Cellular Electron CryoTomography (CECT) (Gan andJensen, 2012; Grünewald et al., 2002; Lu ci c et al., 2013) enables the3D visualization of structures at close-to-native state and in sub-molecular resolution within single cells (Asano et al., 2015; Jinet al., 2008; Murata et al., 2010; Rigort et al., 2012). Therefore, ifwe knew how to systematically mine structures in cryo cellulartomograms, we would gain the desired knowledge on macromol-ecules’ native structures and organization in their cellular context(Nickell et al., 2006).However, systematic recovery of macromolecules structuresfrom cryo tomograms is a very difficult task for several reasons.First, the cellular environment is very crowded (Best et al., 2007;Frangakis et al., 2002) with macromolecules that typically adopt dif-ferent conformations as part of their function. Moreover, oneVC The Author 2017. Published by Oxford University Press. i13This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i13–i22doi: 10.1093/bioinformatics/btx230ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 2018macromolecule interacting with several different macromoleculescan dynamically form different complexes at every time point.Therefore, a cellular tomogram has very complex and highly hetero-geneous structural content. Second, the sizes of macromolecularcomplexes are typically smaller than 30 nm, which is only slightlylarger than the image resolution ( 4 nm). Finally, there are inherentpractical limitations in data acquisition, in the form of low signal tonoise ratio (SNR) and missing wedge effects.Given the above challenges, successful systematic analysis of themacromolecule structures in CECT data relies on processing largeamount of structurally highly heterogeneous particles (Asano et al.,2016), possibly at least millions of particles containing hundreds ofstructural classes. Nowadays, new imaging technologies and ad-vances in automation allow a research lab to obtain hundreds oftomograms within several days (Morado et al., 2016), potentiallycontaining millions of particles represented by 3D subimages (akasubtomograms). However, existing computational approaches havevery limited discrimination ability or scalability, making them gener-ally incapable for systematic de novo structural discovery on theseamounts of particles.Early works of analyzing the macromolecular complexes inCECT data focused at locating instances of macromolecular com-plexes in cells through template search (e.g. Beck et al., 2009; Böhmet al., 2000; Nickell et al., 2006). However, such approaches do notdiscover new structures. For the reconstruction of novel structuresrepeating within cryo tomograms (Förster et al., 2005), reference-free subtomogram averaging (Briggs, 2013), classification (e.g.Bartesaghi et al., 2008; Bharat et al., 2015; Chen et al., 2014; Xuet al., 2012; Scheres et al., 2009) and structural pattern mining (Xuet al., 2015) approaches have been developed. These approaches areessentially unsupervised clustering or constrained optimizationapproaches, and they do not rely on any training data containingsubtomograms with structural class labels. However, the scalabilityof such approaches is very limited, due to computationally intensivesteps such as subtomogram alignment or integration over the 6Drigid transformation space. For example, structural pattern mining(Xu et al., 2015) of 10 000 subtomograms containing 22 structuralclasses would take at least 2 days by running 300 parallel jobs on acomputer cluster.To complement the above approaches, rotation invariant feature(Xu et al., 2009, 2011; Chen et al., 2012), and pose normalization(Xu et al., 2015) methods have been developed and can be used tosubdivide highly heterogeneous subtomograms through unsuper-vised clustering. However, these approaches do not take into ac-count of the missing wedge effect, which introduces anisotropicresolution and is not rotation invariant. In addition, suchapproaches have limited structural discrimination ability in the pres-ence of high level of noise in the subtomograms.We aim to overcome the aforementioned challenges and limita-tions of structural mining in cellular tomograms by complementingwith existing approaches. In this article, we propose to use super-vised deep learning approach to subdivide a large number of struc-turally highly heterogeneous subtomograms into structurally morehomogeneous smaller subsets with significantly improved accuracyand scalability. After the subdivision, the computationally intensivereference-free structural recovery approaches can be separatelyapplied to selected subsets in a divide and conquer fashion, whichwould significantly reduce the overall computation cost.The major component of our new approach is a ConvolutionalNeural Network (CNN) classifier. Due to its superior scalabilityand good generalization ability, CNNs have made it computation-ally feasible to use a large number (e.g. billions) of parameters toapproximate the complex mapping inside massive training data. Inthis article, we propose tailored 3D variants (Section 2.2) of twopopular CNN image classification models. These two CNN modelshave achieved state-of-the-art supervised classification accuracy onpopular image classification benchmark datasets (e.g. ImageNetDataset; Russakovsky et al., 2015). The first model (Section 2.2.1) ischaracterized by relatively low depth and relatively complex parallellocal filter structure (i.e. inception structure (Szegedy et al., 2016b).The second model (Section 2.2.2) is characterized by relatively highdepth and very small simple convolution filters. In addition, becausethe inputs of the models are 3D gray-scale images (i.e. subtomo-grams) representing the 3D structures of particles contained in theimage, it is important for our CNN models to isotropically capturethe inherent 3D spatial structure in such 3D images. Therefore, inour models we use single channel 3D filters for convolution andpooling, instead of the 2D filters used in common deep learningbased computer vision applications.The above CNN models are designed for supervised classifica-tion. Since the native structures of most macromolecular complexesare unknown (Han et al., 2009; Xu et al., 2011), there is a particularneed for discovering macromolecular complex structures that do notexist in the training data. To do this, we combine CNN with un-supervised clustering (Section 2.3). First, we adapt the output layerof a trained CNN classifier to extract structural features that are in-variant to both rigid transforms and missing wedge effect. Suchstructural feature extraction is equivalent to performing a non-linearprojection of the testing subtomograms to the structural spacespanned by the structures in the training data, an analogy to metriclearning (e.g. Xing et al., 2002). Then, we subdivide the projectedsubtomograms using unsupervised clustering, and recover the struc-tures independently using reference-free classification and averaging(Frazier et al., 2017; Xu et al., 2012).Our experiments on realistically simulated subtomograms showthat the deep structural features extracted by the our CNN modelsare significantly faster and more robust to imaging noise and missingwedge effect than our previously used rotation invariant feature (Xuet al., 2009, 2011) approach. K-means clustering in the deep struc-tural feature space produced significantly more evenly distributedclusters than our previous approach of k-means clustering of posenormalized subtomograms (Xu et al., 2015). Our proof-of-principleexperiments on experimental subtomograms of purified macromol-ecular complexes also achieved competitive classification perform-ance. Therefore, our experiments validate that our deep learningbased approach is in practice a significantly better choice for subdi-viding millions of subtomograms. More importantly, our experi-ments (Section 3.3) on simulated data demonstrate that ourapproach is able to recover new structures that do not exist in thetraining data.2 Materials and methods2.1 BackgroundIn recent years, deep learning has emerged as a powerful tool formany computer vision tasks, such as image classification and objectdetection. Deep learning has achieved state-of-the-art supervisedimage classification performance on popular benchmark image data-sets such as ImageNet (Russakovsky et al., 2015), which containsmore than 14 million images separated into at least 1000 classes.The CNN (LeCun et al., 1998) is one of the most important tech-niques in deep learning. It is composed of multiple layers, and everylayer comprises a number of neurons that perform certain operation,i14 M.Xu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 2018such as convolution and pooling, on the output of previous layer.A typical CNN has alternating convolutional layers and poolinglayers, one or more fully connected layers, and lastly a softmaxlayer. By utilizing multiple stacked processing layers to representfeatures of data, it allows learning and extracting increasingly ab-stract image features at increasing scales.In particular, each convolutional layer consists of a set of learnablefilters in the form of neurons with shared weights. Each neuron in thislayer is connected to a region of neighboring neurons in the previouslayer, called receptive field. Intuitively, it captures the spatial informa-tion in the receptive field. For example, the 1D convolution of input xand a filter of size 2mþ 1 is defined as yi ¼Pmj¼ m wjxi j, where xi jis the i–jth input, wj is the jth weight of the convolutional filter. Afterthe convolution, a nonlinear activation function is applied, such as sig-moid, tanh or a rectified linear unit (ReLU) (Goodfellow et al., 2016).For example, the ReLU activation is defined as oReLuðxÞ ¼ maxf0; xg.The pooling layer is a form of down sampling used to reduce computa-tion cost and introduce a small amount of rotation and translation in-variance. Calculating the local maximum (max pooling) oraverage (average pooling) values are common forms of such pooling.For example, the 1D max pooling operation is defined asyi ¼ maxði 1Þm< j  imxj, where m is the size of the pooling windows.For another example, the 1D average pooling operation is defined asyi ¼ 1mPði 1Þm< j  imxj. After stacking several convolutional and pool-ing layers, one or more fully connected layers are usually added to ex-tract more global features. As the name suggests, each unit in theselayers connects to all units from the previous layer, defined as yi¼Pn 1j¼0 wijxj where wij is the weight between ith output yi and jth in-put xj, and n is the number of inputs. For multi-class classification tasks,the last output layer is usually a softmax activation layer (Equation 1),calculating a probability of a sample being assigned to each class.Given training data in form of input-output pairs, the training ofa CNN model optimizes weights through back-propagation so thatthe CNN best fits the training data. The optimization is often per-formed through variants of gradient descent approaches(Goodfellow et al., 2016) due to their superior scalability and sim-plicity for implementation.In 2012, the CNN architecture AlexNet proposed by Krizhevskyet al. (2012), first showed significant performance improvements onthe supervised image classification tasks compared with the trad-itional methods. Since then, CNN has become the dominant approachfor large scale supervised 2D image classification tasks, and moreadvanced architectures have been developed, such as GoogleNet (akainception network) (Szegedy et al., 2016a), VGG network (VGGNet)(Simonyan and Zisserman, 2014), and ResNet (He et al., 2016).2.2 CNN-based supervised subtomogram classificationWhen using CNN for subtomogram classification, the input of theCNN is a 3D subtomogram f, which is a 3D cubic image defined as afunction f : R3 ! R. The output of the CNN is a vectoro :¼ ðo1; . . . ;oLÞ, indicating the probability that f is predicted to beeach of the L classes defined in the training data. Each class correspondto one macromolecular complex. Given o, the predicted class isarg maxi oi.In this article, we propose two 3D CNN models based onGoogleNet and VGGNet for supervised subtomogram classificationand adapt them for structural feature extraction.(a) (b)Fig. 1. Architectures of our CNN models. These networks both stack multiple layers. Each box represents a layer in the network. The type and configuration oflayer are listed in each box. For example, ‘32-5   5   5-1 Conv’ denotes a 3D convolutional layer with 32 5   5   5 filters and stride 1. ‘2   2   2-2 MaxPool’ de-notes a 3D max pooling layer implementing max operation over 2   2   2 regions with stride 2. ‘FC-512’ and ‘FC-L’ denote a fully connected linear layer with 512and L neurons respectively, where every neuron is connected to every output of the previous layer. L is the number of classes in the training dataset. ‘ReLU’ and‘Softmax’ denote different types of activation layersDeep subtomogram subdivision i15Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 20182.2.1 Inception3D networkIn this section, we propose a 3D variant of tailored inception network(Szegedy et al., 2016a), denoted as Inception3D. Inception network isa recent successful CNN architecture that has the ability to achievecompetitive performance with relatively low computational cost(Szegedy et al., 2016a). The architecture of our model is shown inFigure 1a. It contains one inception module (Szegedy et al., 2016a),where 1   1   1, 3   3   3, and 5   5   5 3D filters are combinedwith 2   2   2 3D max pooling layer. The filters are implemented inparallel and concatenated, so that the features extracted at multiplescales using filters of different sizes are simultaneously presented tothe following layer. The 1   1   1 filters before the 3   3   3 and 5  5   5 convolutions are designed for dimension reduction. The in-ception module is followed by a 2   2   2 average pooling layer, thenby a fully connected output layer with the number of units equal tothe structure class number. All hidden layers are equipped with therectified linear (ReLU) activation. The output is a fully connectedlayer followed by a softmax activation layer.2.2.2 DSRF3D networkIn this section, we propose a 3D variant of tailored VGGNet(Simonyan and Zisserman, 2014), which is another CNN architec-ture that achieved top classification accuracy on popular imagebenchmark datasets. Our model is denoted as Deep Small ReceptiveField (aka DSRF3D). The architecture of our model is shown inFigure 1b. When compared with the Inception3D model, DSRF3Dis featured with deeper layers and very small 3D convolution filtersof size 3   3   3. The stacking of multiple small filters has the sameeffect of one large filter, with the advantages of less parameters totrain, and more non-linearity (Simonyan and Zisserman, 2014). Thearchitecture consists of four 3   3   3 3D convolutional layers andtwo 2   2   2 3D max pooling layers, followed by two fully con-nected layers, then followed by a fully connected output layer withthe number of units equal to the structure class number. All hiddenlayers are equipped with the ReLU activation layers. The output is afully connected layer with a softmax activation layer.2.3 Combination of supervised structural featureextraction and unsupervised clustering for structuraldiscovery2.3.1 Structural feature extractionFor the multi-class classification tasks in Section 2.2, the last fullyconnected layerst activation functions used in Sections 2.2.1 and2.2.2 are softmax functions:osoftmaxj ðxÞ ¼ PðjjxÞ ¼efjðxÞPLl¼1 eflðxÞ; (1)wherefjðxÞ ¼ xTwj; (2)x are the inputs of the last fully connected layer, wj are the weightsassociated with the jth class, fjðxÞ is the output of the last fully con-nected layer associated with the jth class, and PðjjxÞ is the probabil-ity of the subtomogram is assigned to class j.Designed for multi-class classification, the softmax activationosoftmaxj re-scales fj exponentially. Therefore, it encourages output to-wards binary values, which reduces the extracted structural featureinformation that are useful for precisely subdividing input subtomo-grams. Once a CNN is trained for the classification task, we removethe softmax activation layer to obtain the linear activation of thelast fully connected layer:olinearj ðxÞ ¼ fjðxÞ (3)Using linear activation, we obtain a more continuous representationof the tendency that a subtomogram is predicted to belong to a class.Such continuous outputs produce structural features that are invari-ant to rigid transformation and missing wedge effect, representing anonlinear projection of a subtomogram to a low dimension spacespanned by structural classes in the training data. In principle, suchfeatures can also be extracted from hidden layers, providing richerstructural descriptions, as long as they are invariant to rigid trans-formation of the particle, and invariant to missing wedge effect.2.3.2 Clustering and structure recoveryThe main goal of subdivision is for separating a collection of struc-turally highly heterogeneous subtomograms into subgroups of sub-tomograms containing similar structures. Structural recovery (e.g.Xu et al., 2012, 2015) often requires searching in the Cartesianproduct of the space of class membership and the space of rigidtransformations of subtomograms. The subdivision significantly re-duces the space of structural class membership. Therefore an accur-ate subdivision would significantly simplify the complexity ofstructural recovery of a subgroup of subtomograms. The subdivisionis usually performed through clustering. Successful subdividing mil-lions of subtomograms require the clustering to be both accurateand efficient.We improve the quality of clustering through supervised dimen-sion reduction. Specifically, after projecting the subtomograms tothe structural feature space using supervised feature extraction(Section 2.3.1), we over-partition the projected subtomograms usingk-means clustering to obtain a finer subdivision of subtomograms.The unsupervised reference free classification (e.g. Bartesaghi et al.,2008; Scheres et al., 2009; Xu et al., 2012) (or structural patternmining; Xu et al., 2015) is then independently applied to each clus-ter of subtomograms to recover the representative structures in thecluster.2.4 Implementation detailsThe CNN models and training and testing are implemented usingKeras (Chollet, 2015) and Tensorflow (Abadi et al., 2016). TheKeras_extras library (https://github.com/kuza55/keras-extras) isused for multiple GPU parallelization. A variant of our Tomominerlibrary (Frazier et al., 2017; Xu et al., 2015) is used for reference-free subtomogram classification and other processing. The experi-ments are performed on a computer equipped with two Nvidia GTX1080 GPUs, one Intel Core i7-6800K CPU, and 128GB memory.For the baseline methods, the calculation of rotation invariantfeatures is based on SHTools (Wieczorek et al., 2016). K-meansclustering and support vector machine (SVM) based supervisedmulti-class classification are performed using the Sklearn toolbox(Pedregosa et al., 2011).3 ResultsIn this section, we demonstrate two major advantages of our deeplearning subdivision approach through empirical study. First,efficient and accurate structural separation of millions of highly het-erogeneous particles is key for the systematic detection of the near-native structures and spatial organizations of large macromolecularcomplexes in cells captured by CECT data. In Section 3.2, wei16 M.Xu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 2018demonstrate that our deep learning based subtomogram subdivisionapproaches significantly outperform our previously used approachesin terms of scalability and discrimination ability, which would sig-nificantly facilitate such structural separation. Second, currently thenative structures of most of macromolecular complexes are un-known. Therefore it is necessary for a subtomogram subdivisionmethod to be useful for discovery of unknown structures. In Section3.3, we demonstrate that, although our deep learning approach isbased on supervised training, when combined with unsupervisedclustering and reference free subtomogram classification and averag-ing, our approach can be used to recover structures that do not existin the training data, therefore our approach can be used to discovernew structures in a systematic fashion.3.1 Datasets generation3.1.1 Simulated subtomograms from known structuresFor a reliable assessment of the approach, we generated subtomo-grams by simulating the actual tomographic image reconstructionprocess in a similar way as previous works (Beck et al., 2009;Förster et al., 2008; Nickell et al., 2005; Xu and Alber, 2013), withthe proper inclusion of noise, and missing wedge effect, and electronoptical factors, such as the contrast transfer function (CTF) andmodulation transfer function (MTF). Specifically, macromolecularcomplexes have an electron optical density proportional to the elec-trostatic potential. We used the PDB2VOL program from the Situs(Wriggers et al., 1999) package to generate volumes of 403 voxelswith a resolution and voxel spacing of 0.92 nm. The density mapsare used to simulate electron micrograph images through a set oftilt-angles. For this article, we set typical tilt-angle ranges of 660 ,650  and 640 . We added noise to electron micrograph images(Förster et al., 2008) to achieve the desired SNR levels, whose rangecover the SNRs estimated from experimental data (Section 3.1.2).Next we convoluted the electron micrograph images the CTF andMTF to simulate optical effects (Frank, 2006; Nickell et al., 2005).The acquisition parameters used are typical of those found in experi-mental tomograms (Zeev-Ben-Mordehai et al., 2016) (Section3.1.2), with spherical aberration of 2 mm, defocus of -5lm, andvoltage of 300 kV. The MTF is defined as sincðpx=2Þ where x is thefraction of the Nyquist frequency, corresponding to a realistic de-tector (McMullan et al., 2009). Finally a direct Fourier inversion re-construction algorithm (implemented in the EMAN2 library; Galaz-Montoya et al., 2015) is used to produce the simulated subtomo-gram from the tilt series. Figure 3 shows examples of such simulatedsubtomograms with different SNRs and tilt angle ranges.We collected 22 macromolecular complexes from the ProteinDatabank (PDB) (Berman et al., 2000) (Supplementary Table S1).We constructed a simulated dataset for each pair of SNR and tiltangle range parameters. Inside a dataset, for each complex, we gen-erated 1000 simulated subtomograms that contain randomly rotatedand translated particle of that complex. Furthermore, we also simu-lated 1000 subtomograms that contain no particle. As an outcome,dataset contains 23 000 simulated subtomograms of 23 structuralclasses.3.1.2 CryoEM data collection, tomogram reconstruction andpreparation of ground truthWe captured tomograms of purified Escherichia coli Ribosome andhuman 20S Proteasome through similar procedure as (Zeev-Ben-Mordehai et al., 2016). The imaging parameters have been opti-mized and successfully applied for structure separation of trimericconformations of natively membrane-anchored full-length herpessimplex virus 1 glycoprotein B (Zeev-Ben-Mordehai et al., 2016).Specifically, Cryo-Electron Microscopy was performed at 300 keVusing a TF30 ‘Polara’ electron microscope Field Electron and IonCompany (FEI) equipped with a Quantum postcolumn energy filter(Gatan) operated in zero-loss imaging mode with a 20-eV energy-selecting slit. Images were recorded on a postfilter 4000   4000K2-summit direct electron detector (Gatan) operated in countingmode with dose fractionation, with a calibrated pixel size of0.23 nm at the specimen level. Tilt series were collected usingSerialEM (Mastronarde, 2005) at defocus ranges of  6 to  5lm.During data collection, the autofocusing routine was iterated toachieve a very stable defocus through the tilt series with 100 nm ac-curacy. Tomographic reconstructions were performed usingweighted back-projection in IMOD program (Sandberg et al.,2003). The reconstructed tomograms were then four times binned toa voxel spacing of 0.92 nm.To prepare for ground truth, we performed template-free par-ticle picking similar to (Pei et al., 2016) through convoluting thetomograms with 3D Difference of Gaussian function with scalingfactor of r ¼ 7nm and scaling factor ratio K ¼ 1.1 to extract an ini-tial set of 3646 subtomograms of size 403 voxels. The extracted sub-tomograms were smoothed by convoluting with a Gaussian kernelof r ¼ 1 nm. We then aligned the subtomograms against Proteasomeand Ribosome templates. These templates were obtained from firstgenerating 4 nm resolution density maps from the PDB structuresusing PDB2VOL program (Wriggers et al., 1999), then convolutingthe density maps with proper CTF according to experimental data(Section 3.1.2). The subtomograms with high alignment scores wereselected. Finally, a set of 401 subtomograms were obtained, 201 and200 were labeled as Proteasome and Ribosome, respectively.To estimate SNR, for each structural class, we randomly selected100 pairs of subtomograms that were aligned with the correspond-ing template, and estimated the SNR given each subtomogram pairaccording to [Frank and Al-Ali, 1975]. The mean SNRs are 0.06and 0.08 for Proteasome and Ribosome, respectively.3.2 Classification performance3.2.1 On simulated dataTo assess the classification performance, for each dataset generatedin Section 3.1.1, we randomly separated the subtomograms into twoequal sized sets. We used one set for training, and the other set fortesting.The CNN models were trained using stochastic gradient descent(SGD) with Nesterov momentum of 0.9 to minimize the categoricalcross-entropy cost function. The initial learning rate was set to 0.01,with a decay factor of 1e-6. A 70% dropout (Srivastava et al., 2014)was implemented in Inception3D network to prevent over-fitting,i.e. a unit in network was retained with probability 70% during thetraining. SGD training was performed with a batch size of 64 for 20epochs.For the baseline method, we used spherical harmonics rotationinvariant feature (e.g. Xu et al., 2009, 2011) in combination withSVM with Radial Basis function kernel, denoted as RIF-SVM.The classification accuracy is summarized in the Table 1. It canbe seen that, at realistic SNR and tilt angle range levels, all CNNmodels achieved significantly higher classification accuracy than therotation invariant feature based method.We further measured the computation speed. On average, thetraining time took 0.0034 and 0.0055 s per subtomogram per epochfor Inception3D and DSRF3D networks respectively. Given trainedmodels, the feature extraction and classification take 0.0015 andDeep subtomogram subdivision i17Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 20180.0017 s per subtomogram for Inception3D and DSRF3D networksrespectively. Thus, after training, CNN based structural feature ex-traction and classification of one million subtomograms would take<1 h on a single affordable desktop computer with two affordableGPUs. In contrast, the unsupervised rotation invariant feature ex-traction took 0.1 second per subtomogram. With fixed subtomo-gram size and class number, the training of CNN models scaleslinearly respect to the number of subtomograms. By contrast, thetraining of SVM scales quadratically respect to the number ofsubtomograms.3.2.2 On experimental dataWe randomly split the subtomograms into two equal sized sets andused one set as training and the other set as testing. In the trainingset, 105 and 95 subtomograms are labeled as Proteasome andRibosome, respectively. In the testing set, 96 and 105 subtomogramsare labeled as Proteasome and Ribosome respectively.Although the number of samples was significantly smaller thanthe typical sample size in for deep learning tasks, the Inception3Dnetwork still achieved a classification accuracy of 0.905, which ishigher than the classification accuracy of 0.890 of the baselinemethod of rotation invariant feature in combination with SVM.DSRF3D network fail to converge during training due to small sam-ple size.3.3 Detection of new structuresIn this section, we test if our approach in Section 2.3 can be used tofacilitate the recovery of structures that do not exist in the trainingdata. The experiments were performed using subtomograms simu-lated at SNR 0.05 and tilt angle range 660  (Section 3.1.1).We prepared a training set Strain with all 23 structural classes ex-cept Proteasome (PDB ID: 3DY4), and a test set Stest with all 23structural classes. There are 500 subtomograms in each class in eachset. We trained an Inception3D network using Strain, then used thetrained network to extract the structural features by projecting thesubtomograms of Stest into a 22D deep structural feature space R22corresponding to the 22 classes in the training data. In such case,each subtomogram in Stest correspond to one point in R22. For visualinspection, we further embedded the points in R22 into a 2D spaceR2 using the T-SNE algorithm (Maaten and Hinton, 2008), which isparticularly well-suited for embedding high-dimensional data into aspace of two or three dimensions for visualization. Figure 2 showsthe embedded points. It is evident that samples are generally concen-trated in subregions according to their structural classes. Most im-portantly, although Proteasome subtomograms do not exist in theStrain, the Proteasome subtomograms in Stest are still concentrated atcertain subregion in R2 (Fig. 2), indicating the supervised structuralfeature extraction can potentially be used to characterize newstructures.Inspired by the above observations, we systematically examinedthe possibility of recovering new structures using our approach(Section 2.3) by conducting leave-one-out test to all 22 macromol-ecular complex structure classes. For each test, we removed subto-mograms of a class Ctrue from training data, then trained anInception3D model, we then used the trained model to project thesubtomograms of Stest into the deep structural feature space R22 ac-cording to Section 2.3. Then we performed k-means clustering inFig. 2. Subtomograms in the test set projected to the structural feature space of R22 through structural feature extraction (Section 2.3). The projected subtomo-grams were further embedded to R2 using T-SNE (Maaten and Hinton, 2008) only for visual inspection. The points were shaped and colored according to theirtrue class labels. The region enriched with Proteasome subtomograms (PDB ID: 3DY4) was highlighted using red circlei18 M.Xu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 2018R22, where k was chosen to be 100, significantly larger than the truenumber of classes. We then identified the cluster Lpred in which par-ticles of Ctrue were most enriched. Finally we applied unsupervisedreference-free subtomogram classification and averaging (Frazieret al., 2017; Xu et al., 2012) (with three classes, five iterations) tothe subtomograms of Lpred. Among the classes predicted by unsuper-vised subtomogram classification, we identified the class Cpred thatwas mostly enriched with particles of Ctrue. We then calculated thestructural discrepancy between the subtomogram average of Cpredand the true structure of Ctrue. Such structural discrepancy is meas-ured using Fourier Shell Correlation (Liao and Frank, 2010) with0.5 cutoff, representing the maximal size of the structural factorsthat are discrepant between two structures. When using a structuraldiscrepancy of 7 nm to determine whether the structure recovery issuccessful, we found 16 out of the 22 leave one out tests correctly re-covered structures of Ctrue, even Ctrue does not exist in Strain (Fig. 4).We further performed the same test using DSRF3D network, andwe were able to get similar results. Specifically, 18 out of 22 struc-tures were successfully recovered (Supplementary Table S2).We further inspected the cluster size distribution of the result ofk-means clustering of subtomograms of Stest projected to the featurespace R22 used for Figure 2. The cluster sizes did not vary too much(Supplementary Fig. S1a). In contrast, when applying our previouspose-normalization method (Xu et al., 2015) to subtomograms inStest, then perform k-means clustering on the pose normalized subto-mograms to subdivide the 11 500 subtomograms in the Stest into 100clusters, we found that most clusters are very small (SupplementaryFig. S1b). Specifically, there were 84 clusters whose size   10. On theother hand, there were 4 large clusters with size >1000, covering6089 subtomograms, with mixed particles of similar structural sizes.The largest cluster had a size of 2470. The highly uneven cluster sizedistribution was likely due to the reduced discrimination ability ofdistance matrics in high-dimensional space (curse of dimensionality)(Aggarwal et al., 2001). Because the true classes in Stest are also equalsized. In addition, when the data samples are uniformly distributed,the k-means algorithm, by definition, tends to produce an even div-ision of data samples. Therefore, compared with our previously usedpose-normalization approach, our supervised deep structural featureextraction approach produced subdivisions that are significantly moreconsistent with the algorithmic property of k-means clustering.REMARK: In our experiments, we used an arbitrary number of 100clusters for the clustering step. In principle, to efficiently over-partition the data, the cluster number should be chosen to be largerthan the number of expected representative structural classes amongthe collection of subtomograms to be subdivided, and be constrainedby the amount of computation affordable. Proper approaches for esti-mating real cluster number remain to be explored.4 DiscussionMacromolecular complexes are nano-machines that participate in awide range of cellular processes. To fully understand these proc-esses, it is necessary to know both native structures and spatial or-ganizations of these complexes inside individual cells. CECT iscurrently the preferred experimental tool to visualize macromolecu-lar complexes in near native conditions at sub-molecular resolution,when coupled with deep data mining it emerges as a very promisingtool for systematic detection of structures and spatial organizationsinside single cells. However, due to high level of structural complex-ity and practical imaging limitations, systematic de novo structuraldiscovery of macromolecules from such tomograms requires thecomputational analysis of large amount of subtomograms. Existingstructural recovery approaches are through reference-free subtomo-gram averaging (Briggs, 2013), classification (e.g. Xu et al., 2012),or structural pattern mining (Xu et al., 2015), and they have verylimited scalability. Therefore, efficient and accurate subdivision oflarge amount of highly heterogeneous subtomograms is a key stepfor scaling up such computational intensive structural recoveryapproaches. On the other hand, our previously used rotation invari-ant feature (Xu et al., 2009, 2011) and pose normalization (Xuet al., 2015) subdivision approaches have limited discriminationability and scalability. To complement existing approaches, as aproof-of-principle, in this work we propose to use deep learningbased supervised approach to significantly improve both scalabilityand discrimination ability of subtomogram subdivision. Our prelim-inary results demonstrated superior performance over our previouslyused subdivision approaches (Xu et al., 2009, 2011, 2015). An add-itional advantage of our deep learning approach over our previouslyused approaches (Xu et al., 2009, 2011, 2015) lies in its potentialfor handling the molecular crowding: even if a subtomogram con-tains not only a particle of interest but also neighbor structures dueto high molecular crowding (Xu and Alber, 2013), our deep learningFig. 3. Left: Isosurface of density map of yeast 20S proteasome (PDB ID:3DY4). Right: Center slices (in parallel with x–z plane) in the simulated subto-mograms with different degree of SNRs and tilt angle rangesTable 1. The classification accuracy of simulated datasets of subtomograms at different levels of SNR and tilt angle rangeSNR/Tilt angle range 660  650  640 Inception3D DSRF3D RIF-SVM Inception3D DSRF3D RIF-SVM Inception3D DSRF3D RIF-SVM1000 0.993 0.990 0.992 0.994 0.978 0.983 0.983 0.991 0.9670.5 0.975 0.972 0.929 0.964 0.967 0.885 0.931 0.951 0.8570.1 0.851 0.891 0.762 0.807 0.873 0.633 0.809 0.866 0.6490.05 0.757 0.767 0.592 0.682 0.728 0.455 0.637 0.684 0.4680.03 0.608 0.658 0.446 0.516 0.604 0.319 0.473 0.556 0.341Deep subtomogram subdivision i19Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 2018based approach by design is likely to be able to automatically focuson the particle of interest and extract structural features only fromthe particle of interest instead of from the neighbor structures, asshown by many deep learning based image classification tests. Sincelittle is known about the native structures of most macromolecularcomplexes in cells, it is therefore important for a subtomogram sub-division method to be able to be used for discovering new structures.We demonstrate that, by combining our supervised structural fea-ture extraction with unsupervised clustering and reference-free sub-tomogram classification and averaging, we are able to detect newstructures that do not exist in the training data.To our knowledge, this work is the first application of deep learn-ing for systematic structural discovery of macromolecular complexesamong large amount (millions) of structurally highly heterogeneousparticles captured by CECT. It represents an important step towardslarge scale systematic detection of native structures and spatial organ-izations of large macromolecular complexes inside single cells. Fromapplication perspective, potential uses of our approach are to quicklysubdivide the highly heterogeneous particles into subsets, and separ-ately recover the representative structures in each selected subset usingcomputation intensive unsupervised subtomogram classification orpattern mining approaches. Given a recovered structure, one can fur-ther verify whether it already exist in training data. The particles ofthe new structures can be further included into training data to train anew CNN model for more comprehensive disentangling of structuralfeatures with enhanced discrimination ability. Besides CECT dataanalysis, our approach can also be applied to similar analysis tasksarisen in cryo tomograms of cell lysate or purified complexes. OurCNN based classification approach can also be used for templatesearch or particle picking. In addition, our deep learning approachcan also be used in analyzing image patches in CECT images, whichare small 3D sub-images that are not necessarily cubic.Our approach is based on supervised learning. Therefore, as amain limiting factor, our method relies on the availability and qualityof training data. In practice, the training data can come from diversesources. They can be from cryo tomograms of purified complexes cap-tured in the same imaging condition as test samples. They can also befrom particles in CECT images located through different approaches,such as correlated super-resolution imaging (Chang et al., 2014;Johnson et al., 2015), template search (Beck et al., 2009; Kunz et al.,2015), unsupervised reference-free subtomogram classification (e.g.Xu et al., 2012), or structural pattern mining (Xu et al., 2015). Onthe other hand, the proper strategies of constructing and processingtraining data remain to be explored. In addition, the proposed CNNarchitectures remain to be further optimized for improved perform-ance. Furthermore, the size of the experimental data used in thisproof-of-principle study is much smaller than a typical settingrequired for deep learning. Extensive studies remain to be donethrough capturing large number of particles of multiple purifiedmacromolecular complexes and performing comprehensive analysisof the accuracy, robustness respect to sample size.AcknowledgementsWe thank Dr Robert Murphy for invaluable suggestions. We also thank MrKshitiz Dange for providing technical support.Fig. 4. The isosurfaces of true (left) and predicted (right) structures. The predicted structures were obtained by our approach (Section 2.3). The numbers in paren-theses were structural discrepancy between true and predicted structuresi20 M.Xu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 2018FundingThis work was supported in part by US National Institutes of Health (NIH)[Grant P41GM103712]. X.C. acknowledges support of a Ji-Dian LiangGraduate Research Fellowship. T.Z. acknowledges support of Sir Henry DaleFellowship jointly funded by the Wellcome Trust and the Royal Society[Grant 107578/Z/15/Z]. G.Y. acknowledges support of US National ScienceFoundation (NSF) Faculty CAREER [Grant DBI-1149494]. E.P.X. acknow-ledges supports of NIH [Grant R01GM114311], NIH [Grant P30DA035778]and Pennsylvania Department of Health CURE [Grant BD4BH4100070287].Conflict of Interest: none declared.ReferencesAbadi,M. et al. (2016). Tensorflow: A system for large-scale machine learning.In Proceedings of the 12th USENIX Symposium on Operating SystemsDesign and Implementation (OSDI). Savannah, Georgia, USA.Aggarwal,C.C. et al. (2001). On the surprising behavior of distance metrics inhigh dimensional space. In International Conference on Database Theory,London, UK, pp. 420–434. Springer.Asano,S. et al. (2015) A molecular census of 26s proteasomes in intact neu-rons. Science, 347, 439–442.Asano,S. et al. (2016) In situ cryo-electron tomography: a post-reductionistapproach to structural biology. J. Mol. Biol., 428, 332–343.Bartesaghi,A. et al. (2008) Classification and 3D averaging with missing wedgecorrection in biological electron tomography. J. Struct. Biol., 162, 436–450.Beck,M. et al. (2009) Visual proteomics of the human pathogen Leptospirainterrogans. Nat. Methods, 6, 817–823.Berman,H. et al. (2000) The protein data bank. Nucleic Acids Res., 28, 235.Best,C. et al. (2007) Localization of protein complexes by pattern recognition.Methods Cell Biol., 79, 615–638.Bharat,T.A. et al. (2015) Advances in single-particle electron cryomicroscopystructure determination applied to sub-tomogram averaging. Structure, 23,1743–1753.Böhm,J. et al. (2000) Toward detecting and identifying macromolecules in acellular context: template matching applied to electron tomograms. Proc.Natl. Acad. Sci. USA, 97, 14245–14250.Briggs,J.A. (2013) Structural biology in situthe potential of subtomogramaveraging. Curr. Opin. Struct. Biol., 23, 261–267.Chang,Y.W. et al. (2014) Correlated cryogenic photoactivated localizationmicroscopy and cryo-electron tomography. Nat. Methods, 11, 737–739.Chen,Y. et al. (2012). Detection and identification of macromolecular com-plexes in cryo-electron tomograms using support vector machines. InBiomedical Imaging (ISBI), 2012 9th IEEE International Symposium on,pp. 1373–1376. IEEE, Barcelona, Spain.Chen,Y. et al. (2014) Autofocused 3d classification of cryoelectron subtomo-grams. Structure, 22, 1528–1537.Chollet,F. (2015). keras. GitHub repository. https://github.com/fchollet/keras.Förster,F. et al. (2005) Retrovirus envelope protein complex structure in situstudied by cryo-electron tomography. Proc. Natl. Acad. Sci. USA, 102, 4729.Förster,F. et al. (2008) Classification of cryo-electron sub-tomograms usingconstrained correlation. J. Struct. Biol., 161, 276–286.Frangakis,A. et al. (2002) Identification of macromolecular complexes in cry-oelectron tomograms of phantom cells. Proc. Natl. Acad. Sci. USA, 99,14153–14158.Frank,J. (2006). Three-Dimensional Electron Microscopy of MacromolecularAssemblies. Oxford University Press, New York.Frank,J., and Al-Ali,L. (1975) Signal-to-noise ratio of electron micrographsobtained by cross correlation. Nature, 256, 376–379.Frazier,Z. et al. (2017) Tomominer and tomominer cloud: A software plat-form for large-scale subtomogram structural analysis. Structure, in press.Galaz-Montoya,J.G. et al. (2015) Single particle tomography in eman2.J. Struct. Biol., 190, 279–290.Gan,L., and Jensen,G.J. (2012) Electron tomography of cells. Quart. Rev.Biophys., 45, 27–56.Goodfellow,I. et al. (2016). Deep Learning. MIT Press, Cambridge,Massachusetts, USA. http://www.deeplearningbook.org.Grünewald,K. et al. (2002) Prospects of electron cryotomography to visualizemacromolecular complexes inside cellular compartments: implications ofcrowding. Biophys. Chem., 100, 577–591.Han,B.G. et al. (2009) Survey of large protein complexes in d. vulgaris revealsgreat structural diversity. Proc. Natl. Acad. Sci. USA, 106, 16580–16585.He,K. et al. (2016). Deep residual learning for image recognition. InProceedings of the IEEE Conference on Computer Vision and PatternRecognition, Las Vegas, Nevada, USA, pp. 770–778.Jin,L. et al. (2008) Applications of direct detection device in transmission elec-tron microscopy. J. Struct. Biol., 161, 352–358.Johnson,E. et al. (2015) Correlative in-resin super-resolution and electron mi-croscopy using standard fluorescent proteins. Sci. Rep., 5, Article 9583.Krizhevsky,A. et al. (2012). Imagenet classification with deep convolutionalneural networks. In Advances in Neural Information Processing Systems,Stateline, Nevada, USA, pp. 1097–1105.Kunz,M. et al. (2015) M-free: Mask-independent scoring of the reference bias.J. Struct. Biol., 192, 307–311.LeCun,Y. et al. (1998) Gradient-based learning applied to document recogni-tion. Proc. IEEE, 86, 2278–2324.Liao,H.Y., and Frank,J. (2010) Definition and estimation of resolution insingle-particle reconstructions. Structure, 18, 768–775.Lu ci c,V. et al. (2013) Cryo-electron tomography: The challenge of doing struc-tural biology in situ. J. Cell Biol., 202, 407–419.Maaten,L.V. and Hinton,G. (2008) Visualizing data using t-sne. J. Mach.Learn. Res., 9, 2579–2605.Mastronarde,D.N. (2005) Automated electron microscope tomography usingrobust prediction of specimen movements. J. Struct. Biol., 152, 36–51.McMullan,G. et al. (2009) Detective quantum efficiency of electron area de-tectors in electron microscopy. Ultramicroscopy, 109, 1126–1143.Morado,D.R. et al. (2016) Using tomoautoa protocol for high-throughputautomated cryo-electron tomography. J. Vis. Exp., 107, e53608.Murata,K. et al. (2010) Zernike phase contrast cryo-electron microscopy andtomography for structure determination at nanometer and subnanometerresolutions. Structure, 18, 903–912.Nickell,S. et al. (2005) TOM software toolbox: acquisition and analysis forelectron tomography. J. Struct. Biol., 149, 227–234.Nickell,S. et al. (2006) A visual approach to proteomics. Nat. Rev. Mol. CellBiol., 7, 225–230.Pedregosa,F. et al. (2011) Scikit-learn: Machine learning in python. J. Mach.Learn. Res., 12, 2825–2830.Pei,L. et al. (2016) Simulating cryo electron tomograms of crowded cell cyto-plasm for assessment of automated particle picking. BMC Bioinformatics,17, 405.Rigort,A. et al. (2012) Focused ion beam micromachining of eukaryotic cellsfor cryoelectron tomography. Proc. Natl. Acad. Sci. USA, 109, 4449–4454.Russakovsky,O. et al. (2015) Imagenet large scale visual recognition chal-lenge. Int. J. Comput. Vis., 115, 211–252.Sandberg,K. et al. (2003) A fast reconstruction algorithm for electron micro-scope tomography. J. Struct. Biol., 144, 61–72.Scheres,S. et al. (2009) Averaging of electron subtomograms and random con-ical tilt reconstructions through likelihood optimization. Structure, 17,1563–1572.Simonyan,K., and Zisserman,A. (2014). Very deep convolutional networksfor large-scale image recognition. arXiv preprint arXiv:1409.1556.Srivastava,N. et al. (2014) Dropout: a simple way to prevent neural networksfrom overfitting. J. Mach. Learn. Res., 15, 1929–1958.Szegedy,C. et al. (2016a). Inception-v4, inception-resnet and the impact of re-sidual connections on learning. arXiv preprint arXiv:1602.07261.Szegedy,C. et al. (2016b). Rethinking the inception architecture for computervision. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 2818–2826.Wieczorek,M. et al. (2016). Shtools/shtools: Version 4.0. doi:10.5281/zenodo.206114.Wriggers,W. et al. (1999) Situs: a package for docking crystal structures intolow-resolution maps from electron microscopy. J. Struct. Biol., 125,185–195.Xing,E.P. et al. (2002). Distance metric learning with application to clusteringwith side-information. In: Becker, S., Thrun, S., and Obermayer, K. (eds.)Deep subtomogram subdivision i21Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 2018Advances in Neural Information Processing Systems 15, pp. 521–528. MITPress, Cambridge, MA, USA.Xu,M. et al. (2009). 3d rotation invariant features for the characterization ofmolecular density maps. In Bioinformatics and Biomedicine, 2009. BIBMnfo.IEEE International Conference on, Washington, DC, USA, pp. 74–78. IEEE.Xu,M. et al. (2011) Template-free detection of macromolecular complexes incryo electron tomograms. Bioinformatics, 27, i69–i76.Xu,M. et al. (2012) High-throughput subtomogram alignment and classification byFourier space constrained fast volumetric matching. J. Struct. Biol., 178, 152–164.Xu,M. et al. (2015). De novo visual proteomics in single cells through patternmining. arXiv preprint arXiv:1512.09347.Xu,M. and Alber,F. (2013) Automated target segmentation and realspace fast alignment methods for high-throughput classification andaveraging of crowded cryo-electron subtomograms. Bioinformatics, 29,i274–i282.Zeev-Ben-Mordehai,T. et al. (2016) Two distinct trimeric conformations ofnatively membrane-anchored full-length herpes simplex virus 1 glycoproteinb. Proc. Natl. Acad. Sci., 113, 4176–4181.i22 M.Xu et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i13/3953944by gueston 07 January 2018
5028881964002	PMID28881964	5028881964	https://watermark.silverchair.com/btx229.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881964.main.pdf	Bioinformatics, 33, 2017, i75–i82doi: 10.1093/bioinformatics/btx229ISMB/ECCB 2017Orthologous Matrix (OMA) algorithm 2.0: morerobust to asymmetric evolutionary ratesand more scalable hierarchical orthologousgroup inferenceClement-Marie Train1,2,3, Natasha M. Glover1,2,3, Gaston H. Gonnet4,´Adrian M. Altenhoff2,4,* and Christophe Dessimoz1,2,3,5,6,*1Department of Ecology and Evolution, University of Lausanne, Lausanne, Switzerland, 2Swiss Institute ofBioinformatics, Lausanne, Switzerland, 3Center of Integrative Genomics, University of Lausanne, Lausanne,Switzerland, 4Department of Computer Science, ETH Zurich, Zurich, Switzerland, 5Department of Genetics,Evolution and Environment and 6Department of Computer Science, University College London, London, UK*To whom correspondence should be addressed.AbstractMotivation: Accurate orthology inference is a fundamental step in many phylogenetics and comparative analysis. Many methods have been proposed, including OMA (Orthologous MAtrix). Yetsubstantial challenges remain, in particular in coping with fragmented genes or genes evolving atdifferent rates after duplication, and in scaling to large datasets. With more and more genomesavailable, it is necessary to improve the scalability and robustness of orthology inference methods.Results: We present improvements in the OMA algorithm: (i) reﬁning the pairwise orthology inference step to account for same-species paralogs evolving at different rates, and (ii) minimizingerrors in the pairwise orthology veriﬁcation step by testing the consistency of pairwise distance estimates, which can be problematic in the presence of fragmentary sequences. In addition we introduce a more scalable procedure for hierarchical orthologous group (HOG) clustering, which areseveral orders of magnitude faster on large datasets. Using the Quest for Orthologs consortiumorthology benchmark service, we show that these changes translate into substantial improvementon multiple empirical datasets.Availability and Implementation: This new OMA 2.0 algorithm is used in the OMA database (http://omabrowser.org) from the March 2017 release onwards, and can be run on custom genomes usingOMA standalone version 2.0 and above (http://omabrowser.org/standalone).Contact: christophe.dessimoz@unil.ch or adrian.altenhoff@inf.ethz.ch1 IntroductionInferring evolutionary relationships between genes lies at the heart ofcomparative, phylogenetic, and functional analyses. Homologs aregenes that share a common ancestry (Fitch, 1970). They can be furtherclassified into: orthologs if they arose by speciation events, or paralogsif they arose by duplication events (Fitch, 1970; Fig. 1). These evolutionary relations are all defined among pairs of genes and—except forhomology—are not transitive. Many orthology inference methods havebeen proposed over the years, such as COGs (Tatusov et al., 1997), bidirectional best hits (Overbeek et al., 1999), Inparanoid (Remm et al.,2001), OrthoMCL (Li et al., 2003), Ensembl Compara (Vilella et al.,2008) or OrthoDB (Kriventseva et al., 2008).The Orthologous Matrix (OMA) algorithm infers orthologousgenes among multiple genomes on the basis of protein sequences(Dessimoz et al., 2005; Roth et al., 2008). In addition to inferringsuch pairwise evolutionary relationships, OMA infers two types oforthologous groups. The first, called ‘OMA groups’, are sets ofgenes in which every pair is inferred to be orthologous. The second,introduced more recently and called ‘hierarchical orthologousgroups’ (HOGs), are defined as set of genes that have all descendedCV The Author 2017. Published by Oxford University Press.i75This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i75/3953943by gueston 07 January 2018i76C.-M.Train et al.All pairs of protein sequencesTetrapods speciationAll-against-all comparisonDuplicationHomologs(“Candidate Pairs”)Mammals speciationFormation of Stable PairsMammals HOGTetrapods HOGPutative Orthologs(“Stable Pairs”)Fig. 1. Hierarchical Orthologous Groups. Labeled gene tree (left) and its relatedspecies tree (right) illustrating the evolutionary history of ﬁve genes all descended from a single common ancestral at the tetrapods level. Those homologscan be classiﬁed as orthologs if they start diverging by speciation (human versusdog genes of same color) or as paralogs if they start diverging by duplication(blue versus red genes). We can identify in this example HOGs at two taxonomiclevels: one larger HOG at the tetrapods level (dotted-line rectangle) containingall the homologous genes that emerged from the single tetrapod ancestral gene,and two HOGs at the mammalian level (solid-line rectangles), due to a duplication of the tetrapod ancestral gene before the mammals speciationfrom a single common ancestral gene at a specific taxonomic rangeof interest (Altenhoff et al., 2013; Fig. 1).When compared with most other methods, the OMA algorithmhas been shown to have high precision (i.e. low false-positive rate) butlow recall (i.e. high false-negative rate) in several benchmark studies(Altenhoff and Dessimoz, 2009; Altenhoff et al., 2016; Boeckmannet al., 2011; Trachana et al., 2011). Even so, predicting correct evolutionary relationships becomes more difficult due to complex mechanisms such as differential gene loss, asymmetric evolutionary rates,gene duplications and poor quality genomes. This can lead to spuriousor missing relationships (Dalquen and Dessimoz, 2013).The final stage of the OMA pipeline infers HOGs from pairwiseorthologs (Altenhoff et al., 2013). Such groups are useful for analyzing multiple genomes or genes, but require scalable clustering algorithms due to the complexity in reconstructing them.Here, we present two new improvements to our orthology inference algorithm in order to better handle rapidly evolving duplicatedgenes and to improve detection of asymmetric gene loss. In addition,we introduce a ‘bottom-up’ HOGs clustering algorithm that canscale up to thousands of genomes.Verification ofStable PairsDifferentially lostparalogs(“Broken Pairs”)Orthologs(“Verified Pairs”)Clique searchGETHOGsOMA Groups Hierarchical OrthologousGroups (HOGs)Fig. 2. Overview of the OMA pipeline. Boxes denote individual steps in thepipeline, while the text outside boxes denotes the input or output of theseprocesses and their terminology in OMAgenomes are upgraded to Stable Pairs. In order to include manyto-many orthologous relationships, Candidate Pairs found withina conﬁdence interval (corresponding to distance variance) arealso upgraded to Stable Pairs.iii. Witness of non-orthology veriﬁcation: At this point, somepairs of paralogs may still be misidentiﬁed as orthologs due todifferential gene loss (Dessimoz et al., 2006a). To avoid suchcases, a veriﬁcation step is added to assess the orthologous origin of a Stable Pair by using a third genome that retained bothorthologous copies, which thus act as witnesses of non-orthology. Pairs that pass this test are upgraded to Veriﬁed Pairs.iv. Ortholog clustering: Once the pairwise orthologs are inferred, aclustering algorithm is applied to group genes descending froma common ancestral gene into HOGs.2 Materials and methodsWe first provide an overview of the OMA algorithm, then present indetails the three refinements introduced in this new version, and finally provide methodological details about the benchmarking.2.1 Overview the OMA algorithmThe following section provides an overview of the existing OMA algorithm, of which the details are described in (Roth et al., 2008).The OMA algorithm infers pairs of orthologous genes from complete genomes in a four-step process (Fig. 2):i. Homology inference: Alignments are made with all possiblepairs of sequences from all genomes using local dynamic programming (Smith and Waterman, 1981), and pairs with sufﬁcient score and overlap are promoted to Candidates Pairs.ii. Ortholog and co-ortholog inference: Candidates Pairs that arethe mutually evolutionary closest sequences between a pair of2.2 Algorithmic refinements: taking into accountfast-evolving duplicated genes in the orthologyinference stepIn the current orthology inference step of the OMA algorithm, genesthat are mutually the closest pairs of sequences across genomes areconsidered as putative orthologs. Due to lineage-specific duplications, orthology relationships are however not necessarily one-toone (e.g. Dalquen and Dessimoz, 2013). Thus, OMA considers a tolerance interval during the mutually closest gene search to allow forinclusion of potential inparalogs.Specifically, the criterion originally used in OMA was as follows:a Candidate Pair xy between genomes X and Y is upgraded to aStable Pair if for all genes xi from X and for all genes yj from Y withxi 6¼ x and yj 6¼ y:Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i75/3953943by gueston 07 January 2018Orthologous Matrix (OMA) algorithm 2.0i77ÀÁdxyj À dxy > Àk Ã stdev dxyj À dxyandÀÁdxi y À dxy > Àk Ã stdev dxi y À dxywhere d is the pairwise maximum likelihood distance estimate, k thetolerance parameter of the standard deviation between the two distances, and where stdev() is the distance standard deviation of thedifference (Dessimoz et al., 2006a,b). This means that a CandidatePair xy is upgraded to a Stable Pair if and only if there are no otherpairs xyj or yxi with significantly smaller evolutionary distances.So far in the orthology inference step, only the distances betweengenes from different genomes are taken into account. However, if a duplicated gene evolved faster than its related in-paralog, searching for mutually closest genes between genomes can fail to identify it as an ortholog(Fig. 3A). Because of the distance asymmetry, the original algorithm doesnot detect the fast evolving gene as a co-ortholog, thus wrongly implyingan ancestral duplication as the origin of divergence (Fig. 3B).The refinement introduced here also takes into account the evolutionary distance between inparalogs. Inspired by other orthology algorithms detecting co-orthologs on the basis of alignment scores, suchas Inparanoid (Remm et al., 2001) or OrthoInspector (Linard et al.,2011), we added a new check that the distance between the two potential in-paralogous dog genes is significantly smaller than the distance between the closest genes (black and blue genes), as illustratedin the Figure 3A. More precisely, we retain as Stable Pairs allCandidate Pairs xy between genomes X and Y that were previouslydiscarded during orthology inference if, for any genes yj from Y withyj ¼ y there exists a gene yi that has a distance to y significantly closer6than the distance between the Candidate Pair genes x and y2:ÀÁdxy À dyyj > Àk Ã stdev dxy À dyyjwhere d is a pairwise maximum likelihood distance estimate, k theinparalogs tolerance parameter of the standard deviation betweenthe two distances and where the distance standard deviation stdev()is computed according to Dessimoz et al. (2006a,b).2.3 Algorithmic refinements: extended witnesses ofnon-orthology with verification of distances additivityAs mentioned earlier, the verification step of the OMA algorithm aimsat detecting paralogs resulting from differential gene losses (Fig. 4A).Lineage specific duplicationAncestral duplicationIndeed, paralogs can be the only remaining homologs between twogenomes and since they are mutually the closest genes across those genomes they can be wrongly inferred as orthologs. To prevent such cases,OMA searches for each pair of putative orthologs (‘Stable Pairs’)whether there might be a third genome that has retained paralogs thatcould act as a witness of non-orthology (Dessimoz et al., 2006a,b).This test is based on pairwise evolutionary distance comparisonof the gene quartet, without reconstructing the underlying gene tree(which, given the very large number of quartets of homologousgenes across many genomes, would be too time consuming).However, direct comparison of pairwise distances implies that thedistances among the four genes are additive, and by consequence,that a phylogenetic tree can be reconstructed from them. We havefound cases, particularly in the presence of fragmentary sequences,where additivity is far from being met.To ensure that the evolutionary distances do not depart excessively from additivity, in the verification of Stable Pair x1,y2 usingpotential witnesses of non-orthology z1,z2, we test a ‘soft’ variant ofthe four-point condition (Buneman, 1974), which allows for distance estimation uncertainty. We check that the sum of the distancesd(x1,z2) and d(y2, z1) is approximately equal to the sum of the distances d(x1, y2) and d(z1, z2). Indeed, considering the branch labelsdefined in Figure 4B, under the model and assuming no error, thefollowing equality holds:ðd þ c þ bÞ þ ða þ c þ eÞ ¼ ðd þ c þ aÞ þ ðe þ c þ bÞTaking inference uncertainty into account, we test the equality asfollows:j dx1 z2 þ dy2 z1 À dx1 y2 À dz1 z2 j <qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃÀÁÀÁ2Ãvarð dx1 z2 Þ þ var dy2 z1 þ var dx1 y2 þ varðdz1 z2 Þwhere x1 and y2 are the Stable Pair genes from genomes X and Y, z1and z2 are the witnesses of non-orthology in the third genome Z, d isa pairwise maximum likelihood distance estimate, and var(d(x,y)) isthe variance of the distance estimate between sequences x and y. Ifthe test fails, z1 and z2 are not used as witnesses of non-orthology.2.4 Algorithmic refinements: bottom-up HOG inferenceIn this section, we present improvements to the hierarchical orthologous group (HOG) clustering phase (Altenhoff et al., 2013). TheABBAClosest genesNot closest genesGene lossDuplicationSpeciationFig. 3. Putative evolutionary scenario for a gene triplet containing 1 human geneand 2 asymmetrically evolving dog genes. (A) Reconciled labeled gene tree for thegene triplet where the red dog gene (orthologous to the human gene) evolved atfaster rate of evolution. (B) Reconciled labeled gene tree for the gene triplet wherean ancestral duplication gave rise on one side to the blue dog gene and the blackhuman gene and on the other side only to the red dog gene, since the related grayhuman gene had been lost. The red dog gene is thus paralogous to the blackhuman geneFig. 4. Hidden paralogs example and witness of non-orthology gene quartet.(A) Example of labeled gene tree containing hidden paralogs due to asymmetric gene losses between human and mouse. This can occur when an ancestral duplication is ﬁrst followed by a speciation then by asymmetric geneslosses. The resulting paralogs are wrongly inferred as orthologs becausethey are the mutually closest pairs between two genomes (Human1, Mouse2sequences). OMA attempts to identify such cases through the use of a thirdspecies (here a monkey) that has retained both copies, which can act as witnesses of non-orthology. (B) The four extant genes form a quartet withbranches labeled a–eDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i75/3953943by gueston 07 January 2018i78work established a one-to-one correspondence between the connected components of a perfect orthology graph—i.e. containing nofalse positive or negative— and HOGs. Based on this, but allowingfor a noisy input, we introduced a heuristic called GETHOGs(‘Graph-based Efficient Technique for Hierarchical OrthologousGroups’), which used the min-cut algorithm to break down spuriousorthologous relationships before identifying HOGs as the connectedcomponents. This was performed for each taxonomic range of a reference phylogeny, starting from the root and walking down the treeto the most specific clades, in a ‘top-down’ fashion.Nevertheless, inconsistencies in the orthology graph due to spurious inferences or missing relations increase the probability of making errors during the clustering. Such mistakes in grouping are thenpropagated during the entire clustering procedure due to the greedynature of the algorithm, and can affect the final result. Furthermore,the original GETHOGs algorithm started at the root of the referencephylogeny, where the graph is largest (since it contains pairs oforthologs between all species instead of subsets of them) and mostuncertain (since it also contains orthologous relationships amongthe most distant species).Here, we introduce a ‘bottom-up’ variant of GETHOGs, whichinfers HOGs starting with the most specific taxonomy and incrementally merges them toward the root (Fig. 6). More specifically,the new approach reconstructs HOGs by applying the followingprocedure with each speciation node of the species tree as reference,from the leaves to the root:i. Build inter-HOG orthology graph (Fig. 5 BuildInterGraph, Fig.6D left): Deﬁne a graph in which the nodes are the HOGsinferred at the level of each child of the reference speciation. If achild is a leaf of the species tree (i.e. a child is an extant species),the HOGs deﬁned at this level are simply the individual sequences of that species. The edges of the graph represent one ormore pairwise orthology relationships between members of theHOGs, with the number of such relationships recorded asweights.ii. Remove spurious edges (Fig. 5 BuildInterGraph line 7–9, Fig.6D middle): Once the orthology graph is built, we next assesswhether each edge is well supported or not. For each edge, theFig. 5. Pseudocode of bottom-up GETHOGs algorithmC.-M.Train et al.algorithm computes the ratio of the number of pairwise orthologous relations (edge weight) to the maximum number of possible pairwise orthologous relations (equal to the product of thesize of the two HOGs connected by the edge). If the inputorthology graph is perfect (i.e. correct and complete), this ratiois one. A cutoff a (set to 0.8 throughout this article and bydefault) is then used to remove all edges with insufﬁcientconnections.iii. Search for connected components (Fig. 5 GETHOGSBottomUpline 10–12, Fig. 6D right)): The ﬁnal step searches for connectedcomponents inside the graph and clusters them together as a single HOG at the level of the speciation of reference.The asymptotic complexity is determined by the complexity of thespecies tree traversal and the complexity for the HOG inference ateach internal node of the species tree (i.e. inference for each taxonomic level). Tree traversal has a runtime complexity of O(n) wheren is the number of species, because there are n-1 internal nodes. Theruntime of the HOG inference at each level (steps 1–3 above) primarily depends on the number of pairwise orthology relationships.The total number of sequences is O(n) because we can expect a natural limit on the size of each genome. Thus, the total number of pairwise relationships is O(n2). Using Union-Find data structures,finding connected components in a graph of m edges is O(m)(Cormen, 2009). There are potentially O(n2) edges in each interHOG orthology graph, but since each orthology relationship onlyneed to be considered once in the entire traversal (at the speciationnode which induces them), the amortized complexity at each internal node is O(n) resulting in a total complexity of bottom-upGETHOGs of O(n2). This compares favorably to the top-downGETHOG algorithm, which has complexity O(n3Álog4n) (Altenhoffet al., 2013).2.5 Validation and benchmarkingWe used the Quest for Orthologs (QfO) reference proteomes dataset(Altenhoff et al., 2016) to benchmark our method and to analyzecase studies. It consists of 66 (40 eukaryotes, 20 bacteria, 6 archaea)proteomes, and contains more than 750 000 non-redundant proteinsequences. It includes a broad selection of genomes covering the treeof life, including model organisms of interest and those important inbiomedical or phylogeny research. In addition, as a reference tree weused a manually curated species tree for the 66 organisms containedin the QfO reference proteomes (Boeckmann et al., 2015).The orthology benchmarking service (http://orthology.benchmarkservice.org) is an automated web-based tool for orthology inferences quality assessment (Altenhoff et al., 2016). This servicetakes ortholog relations inferred on the QfO reference dataset as input, and after running a broad range of tests, it summarizes andplots the results. We focused on the generalized species tree discordance test for our benchmark analysis, as it is a robust way to assessthe quality of orthology predictions.The generalized species tree discordance test estimates the agreement between orthology predictions and a reference species tree.Since orthologs originate by speciation, comparing the similarity ofa tree reconstructed using pairwise orthology relations to a referencespecies tree is a way to assess the quality of the orthology predictions. We applied this procedure to a subset of the QfO references proteomes, covering different taxonomic ranges (LastUniversal Common Ancestor, Eukaryotes, Vertebrates and Fungi).The main results provided by this test are the ‘error rate’ (averageRobinson-Foulds distance between the reconstructed gene tree andreference species tree), the ‘number of complete trees sampled’Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i75/3953943by gueston 07 January 2018Orthologous Matrix (OMA) algorithm 2.0ABi79DCFig. 6. Bottom-up GETHOGs reconstruction example. (A) Orthology graph, where circles represent extant genes with a species-speciﬁc color and edges representpairwise orthologous relations between genes. The red edge represents a spurious orthologous relation between the mouse gene A and the monkey gene B1.(B) Reconciled gene trees corresponding to the orthology graph in (A). Extant genes are represented by squares, speciation events by circles and duplicationevents by stars. (C) Corresponding species tree. (D) HOGs reconstruction using bottom-up GETHOGs with a minimal edges removal threshold of 0.8. The algorithm starts by reconstructing HOGs at the level of the primates and ﬁnishes at the level of mammals. The left panel displays the sub-orthology graph composedof HOGs (or extant genes) as nodes connected by weighted edges according to the number of existing orthologous relations between HOG genes. In the middlepanel, to identify spurious edges, GETHOGs computes the fraction of orthologous pairs over the maximal number of possible pairs. The algorithm removes thered edge because the score is smaller than the minimal edge removal threshold. The right panel depicts the HOGs reconstructed from the connected componentof the corrected graph(number of trees fully reconstructed out of 50 k trials), and the‘number of predicted orthologs’.A3 ResultsBBefore presenting aggregate benchmarking results, we first presentdetailed examples of improvements obtained by the refinementsdescribed in the previous section. We begin with a case study of afamily containing fast-evolving genes, where we recover orthologousrelations and correct the orthology graph. We then present an example of the kind of improvement obtained by the new additivitytest.3.1 Fast-evolving duplicated genes case study: the haptoglobin familyThe first orthology inferences refinement we present aims to includefast evolving duplicated genes in orthology predictions by not onlylooking at evolutionary distances between genomes but also withingenomes.In order to investigate the performance of this refinement, weused the haptoglobin gene family as an example, which duplicatedin the primates (Fig. 7A). One branch of the primate paralogsevolved at a higher evolution rate than its sister branch, leading toasymmetry in the distance between the paralogs. As a result, although there is a one-to-many relationship between rodent haptoglobin and primate haptoglobin, the original OMA algorithm onlyuncovers the most conserved orthology pairs (Fig. 7B). By takinginto account the relatively short distance between the in-paralogouscopies (see section 2), the updated OMA algorithm now recoversboth copies as co-orthologs to their rodent counterparts (Fig. 7C).3.2 Additivity of distances in witnesses of nonorthology stepAs previously discussed in the section 2, the OMA algorithm attempts to uncover hidden paralogs (pairs of paralogs resulting fromdifferential gene losses, thus each lacking an ortholog in the otherCFig. 7. Analysis of haptoglobin gene family in mammals. (A) Phylogeneticlabeled gene tree of the haptoglobin family built using 6 proteins sequencesfrom 4 mammals (rat, mouse, human, chimpanzee). The dotted rectangle highlights the fast evolving primate paralogous genes. (B,C) Orthology graph of thehaptoglobin gene family shown in A. Nodes represent extant genes denoted bya species-speciﬁc color and their identiﬁer meanwhile the edges represent pairwise orthologous relations between genes. The orthology graph in B, relies onthe pairwise orthologous relations inferred using the classic OMA algorithm,while the orthology graph in C is built using the orthology relations includingthe reﬁnement for paralogs evolving at different rates. (UniProt IDs of the sequences involved Mouse!Q16646, Rat!A0A0H2UHM3, Human_a!HOY300,Chimpanzee_a!H2RAT6, Human_b!P00739, Chimpanzee_b!H2RB63)species). This step compares evolutionary distances among quartetsof genes without explicitly reconstructing their underlying phylogenetic gene tree (for performance reasons), under the assumption ofnear additivity of these distances.However, in some cases—typically in the presence of one ormore fragmented sequences—the assumption of additivity isstrongly violated. Figure 8 shows an example of a quartet of geneswith non-additive distances, where a Stable Pair between twomammal genes is erroneously discarded using two Arabidopsisgenes as witnesses of non-orthology. The underlying phylogeneticgene tree (Fig. 8A) indicates that the Arabidopsis genes are in factDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i75/3953943by gueston 07 January 2018i80C.-M.Train et al.ABFig. 8. Example of non additivity among gene quartet distances. (A). The twoArabidopsis genes arose from a duplication within the plants, which can beinferred from a tree inferred using a multiple sequence alignment.(B) However, if we consider pairwise distances estimated from independentpairwise alignments, one Arabidopsis gene appears to be closer to thehuman sequence, while the other appears to be closer to the opossum gene.In the original OMA algorithm, this would result in these Arabidopsis genesbeing erroneously used as witnesses of non-orthology; in the new algorithm,the non additivity of these distances (in Point Accepted Mutation units, withestimator variance in parentheses) is detected and the Arabidospsis genesare not used. (UniProt IDs of sequence involved: Human ! Q16874,Opossum ! F7FI80, Arabidopsis a ! Q93ZB2, Arabidopsis b ! Q9LNJ4)AFig. 10. Effect of the reﬁnements on pairwise orthology relationships (OMAPairs) in the generalized species tree discordance test at vertebrate level. Theasymmetric paralogs denotes the change in the OMA algorithm aiming to include fast evolving duplicated genes during orthology inferences. The additivity test denotes the new quartet consistency test added to the witness ofnon-orthology step. Error bars denote the 95% CI of the meanB3.3 QfO benchmarking resultsFig. 9. Example of non conservation of homologous sites across independentpairwise alignments. (A) Excerpts of three pairwise alignments between threesequences. (B) Graph-representation of the three alignments, where linesconnect aligned residues. The lines are depicted as full lines if the charactersare aligned consistently—thus forming closed triangles—and as dotted linesif they are aligned inconsistently—thus forming open triangles. (Sequencemapping to Uniprot Id: Human ! H. sapiensjQ16874, Opossum ! M.domesticajF7FI80, Arabidopsis ! A. thalianajQ93ZB2.)the result of a duplication within plants and not an ancestralduplication shared with the mammals in question. Without resorting to tree inference on a multiple sequence alignment (whichwould be prohibitively costly considering the number of quartetsneeded to verify every putative ortholog), the non-additivity ofthe pairwise distances in this quartet (Fig. 8B) can be detected byapplying the new condition (see section 2), which in this case isviolated:j191 þ 192 À 62 À 169j?<2Ãpﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ169 þ 193 þ 120 þ 121152 6< 2 Ã 24:55The equation does not hold, thus we cannot rely on this pair ofArabidopsis genes as witnesses of non-orthology.To understand how such non-additivity arises, consider that theevolutionary distances are computed independently during the allagainst-all phase. As a result, the pairs of residues aligned (thusinferred to be homologous) can be inconsistent across the differentsequences some inconsistencies and can appear within the pairwisealignments (non-conservation of homologous sites Fig. 9). In our example, the additivity test will fail; thus the Arabidopsis genes willnot be used as witnesses of non-orthology, and the orthologyinferred between the human and opossum sequence will stand (unless of course a different pair of witnesses, with additive distancesthis time, is found).To quantitatively assess the impact of the changes in the OMA algorithm, we submitted results obtained with them—individuallyand in combination—to the QfO orthology benchmark service(Altenhoff et al., 2016).We first consider the results at the level of pairwise orthology(‘OMA Pairs’). Applying the new handling of asymmetrically evolvingparalogs and the additivity test separately, we observe a significant increase in the number of predicted orthologs while maintaining a similaror even slightly better precision (Fig. 10). Precision here is measured interms of average topological distance between the reference species treeand the gene tree reconstructed from the inferred orthologs (the lowerthe better). When the two refinements are combined, there is an evenhigher increase in the number of predicted orthologs compared withthe current OMA predictions, while maintaining further the quality ofthe inferences. Consistent results are obtained for the different resolutions provided by the QfO benchmark service, though the increase inthe number of inferred pairs is more modest in the fungal dataset(http://orthology.benchmarkservice.org/cgi-bin/gateway.pl?f¼CheckResults&p1¼25fe02429dc60c51f81da2de).Next, we turn to the improvements in HOG inference. Asdescribed in more detail in the section 2, the new HOG inference approach (‘bottom-up GETHOGs’) implements several modificationscompared with the original version (Altenhoff et al., 2013): (i) Thetaxonomy is no longer traversed top-down but from the bottom-up,in a postfix traversal of the species tree; (ii) In the inter-HOG orthology graph considered for each clade, the nodes now representHOGs instead of single genes, thereby considerably reducing thecomplexity of these graphs; (iii) The edges are weighted according tothe number of orthology relations between two clusters of genes;(iv) Instead of cutting down spurious edges in the orthologous graphusing a minimum cut algorithm, the bottom-up HOG inference enables us to assess the support of orthologous relationships betweenHOGs in terms of the total number of orthologous relationshipsthat would be expected given perfect input pairwise orthologs.To assess the impact of the change, we first compared the topdown and bottom-up variants on the QfO ortholog benchmarkDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i75/3953943by gueston 07 January 2018Orthologous Matrix (OMA) algorithm 2.0i814 Discussion and conclusionFig. 11. Assessment of HOG inference on the generalized species tree discordancetest (eukaryotic dataset). Error bars denote the 95% CI of the mean. The data pointswith ‘original OMA’ refer to the algorithm used before this study and ‘new OMA’refer to the predictions produced by the reﬁnements introduced in section 2.3Fig. 12. Time performance of GETHOGs algorithm. CPU time to compute theHOGs reconstruction on dataset of different sizes. The timing is recorded on asingle instance running on a Intel(R) Xeon(R) CPU E5540 2.53GHzservice on the original OMA pairs as input (i.e. without new asymmetric paralogy and additivity tests). The bottom-up algorithm resulted in a substantial increase in the numbers of predictedorthologs with the latter, indicating higher recall (Fig. 11). On theEukaryotic, Vertebrate, and Fungal datasets, the error rate is alsomarkedly lower, while on the universal dataset (including bacteria,archaea and eukaryotes), the error rate is about the same (http://orthology.benchmarkservice.org/cgi-bin/gateway.pl?f¼CheckResults&p1¼98f077d9d00d3ab0375be957).Combining the new OMA pair inference with bottom-up HOGinference results in the largest increase in predicted orthologs. Onthe Eukaryotic dataset, the number of predicted orthologs almosttriples without negatively affecting precision (Fig. 11).In terms of time requirement, consistent with the asymptotictime complexity analysis (see section 2), the bottom-up approach isvastly more efficient and scalable (Fig. 12). With 100 genomes as input, the bottom up variant is already two orders of magnitude faster.In contrast to top-down GETHOGs, which is prohibitively expensive on very large protein families (Altenhoff et al., 2013), bottomup GETHOGs can process the entire public OMA database of 2024genomes and 10.5M sequences in 9 CPU hours.When compared with other methods, the OMA algorithm has oftenbeen reported to be stringent, yielding highly reliable inferences, butsuffering from low recall (Altenhoff et al., 2016; Ballesteros andHormiga, 2016; Trachana et al., 2011). This is certainly true of the‘OMA groups”, which require fully connected subgraphs of orthologs. For pairs and HOGs, however, we show with this new versionthat recall can be considerably improved without negatively affecting precision.Indeed, we introduced multiple improvements to the OMA algorithm, both in the inference of pairwise orthologs and in the inference of HOGs. At the pairwise level, the asymmetric paralogy testincreases the number of one-to-many and many-to-many orthologrelationships recovered when the paralogous copies evolve at different rates. Furthermore, the new additivity test reduces errors due toinconsistent distance computations in quartets of sequences (used toinfer differential gene losses in the OMA algorithm). These inconsistent distances often arise due to fragmentary sequences, typical ofdraft-quality genomes.The improvements in pairwise orthology are not only useful inand of themselves—they directly translate into better HOG inference. Combined with the more scalable and accurate bottom-upGETHOGs, the HOGs inferred by OMA are much more complete,with no or even positive impact on precision.Some of the ideas underlying these improvements are not new.Methods such as Inparanoid (Remm et al., 2001) or OrthoInspector(Linard et al., 2011) have long been exploiting distances betweeninparalogs—albeit using alignment score as a proxy—to increase therobustness of one-to-many or many-to-many orthology inference.Likewise, Hieranoid (Schreiber and Sonnhammer, 2013) also infersHOGs in a bottom-up fashion.However, the distinctive feature of the OMA algorithm hasbeen—and continues to be with this new version—its modular approach, with well-defined and testable objectives at each step of thepipeline (e.g. inference of pairwise orthologs, detection of differential gene losses, inference of HOGs from pairwise orthologs).OMA’s modular approach makes it possible to test and optimizeeach step in isolation, and to expect an overall improvement whenthese are combined—as the empirical benchmarks reported aboveclearly support. In contrast, ad hoc methods can prove difficult tomaintain and improve over time, with changes in one part of thepipeline affecting other parts in unexpected ways.Looking ahead, we see further opportunities for improvement.Unlike pairs and groups in OMA, inference of HOGs strongly relieson knowledge of the species tree. However, many parts of the tree oflife remain either poorly resolved or even misleading for some genefamilies due to incomplete lineage sorting, horizontal gene transferor hybridization (Philippe et al., 2011). Currently, we collapsebranches that are uncertain—however this means that gene duplication occurring within such multi-furcations (i.e. polytomies) confound the HOG inference. Approaches taking a more flexiblereading of species phylogeny, such as NOTUNG (Durand et al.,2006) or PHYLDOG (Boussau et al., 2012), may provide a betterway forward. We also see considerable potential in exploiting theparalogy graph to further improve HOG inference (Lafond and ElMabrouk, 2014).Meanwhile, this OMA 2.0 algorithm is used in the public OMAdatabase from the March 2017 release onwards (Altenhoff et al.,2015; http://omabrowser.org), and can be applied to custom genomes using the open source OMA standalone software version 2.0(http://omabrowser.org/standalone).Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i75/3953943by gueston 07 January 2018i82AcknowledgementsWe thank Georgina Bland for her contribution to the reﬁnement for asymmetrically evolving paralogs, and Karina Zile for her thoughtful review of the article. Computations were performed at the Vital-IT Center for highperformance computing of the SIB Swiss Institute of Bioinformatics, and onthe University College London Computer Science cluster.FundingC.D. acknowledges support by Swiss National Science Foundation grant150654 and UK BBSRC grant BB/M015009/1.Conﬂict of Interest: none declared.ReferencesAltenhoff,A.M. et al. (2013) Inferring hierarchical orthologous groups fromorthologous gene pairs. PLoS One, 8, e53786.Altenhoff,A.M. et al. (2016) Standardized benchmarking in the quest fororthologs. Nat. Methods, 13, 425–430.Altenhoff,A.M. et al. (2015) The OMA orthology database in 2015: functionpredictions, better plant support, synteny view and other improvements.Nucleic Acids Res., 43, D240–D249.Altenhoff,A.M. and Dessimoz,C. (2009) Phylogenetic and functional assessment of orthologs inference projects and methods. PLoS Comput. Biol., 5,e1000262.Ballesteros,J.A. and Hormiga,G. (2016) A new orthology assessment methodfor phylogenomic data: unrooted phylogenetic orthology. Mol. Biol. Evol.,33, 2481.Boeckmann,B. et al. (2011) Conceptual framework and pilot study to benchmark phylogenomic databases based on reference gene trees. Brief.Bioinformatics, 12, 423–435.Boeckmann,B. et al. (2015) Quest for orthologs entails quest for tree of life: insearch of the gene stream. Genome Biol. Evol., 7, 1988–1999.Boussau,B. et al. (2012) Genome-scale coestimation of species and gene trees.Genome Res., 23, 323–330.Buneman,P. (1974) A note on the metric properties of trees. J. Combin.Theory Ser. B, 17, 48–50.Cormen,T.H. (2009) Introduction to Algorithms MIT Press.Dalquen,D.A., and Dessimoz,C. (2013) Bidirectional best hits miss manyorthologs in duplication-rich clades such as plants and animals. GenomeBiol. Evol., 5, 1800–1806.C.-M.Train et al.Dessimoz,C. et al. (2006a) Detecting non-orthology in the COGs databaseand other approaches grouping orthologs using genome-speciﬁc best hits.Nucleic Acids Res., 34, 3309–3316.Dessimoz,C. et al. (2006b) Fast estimation of the difference between twoPAM/JTT evolutionary distances in triplets of homologous sequences. BMCBioinformatics, 7, 529.Dessimoz,C. et al. (2005) OMA, A comprehensive, automated project for theidentiﬁcation of orthologs from complete genome data: introduction and ﬁrstachievements. In: McLysaght, A. and Huson, D.H. (eds.) RECOMB 2005Workshop on Comparative Genomics. Springer Berlin Heidelberg, pp. 61–72.Durand,D. et al. (2006) A hybrid micro-macroevolutionary approach to genetree reconstruction. J. Comput. Biol., 13, 320–335.Fitch,W.M. (1970) Distinguishing homologous from analogous proteins. Syst.Zool., 19, 99–113.Kriventseva,E.V. et al. (2008) OrthoDB: the hierarchical catalog of eukaryoticorthologs. Nucleic Acids Res., 36, D271–D275.Lafond,M., and El-Mabrouk,N. (2014) Orthology and paralogy constraints:satisﬁability and consistency. BMC Genomics, 15(Suppl 6), S12.Li,L. et al. (2003) OrthoMCL: identiﬁcation of ortholog groups for eukaryoticgenomes. Genome Res., 13, 2178–2189.Linard,B. et al. (2011) OrthoInspector: comprehensive orthology analysis andvisual exploration. BMC Bioinformatics, 12, 11.Overbeek,R. et al. (1999) The use of gene clusters to infer functional coupling.Proc. Natl. Acad. Sci. USA, 96, 2896–2901.Philippe,H. et al. (2011) Resolving difﬁcult phylogenetic questions: why moresequences are not enough. PLoS Biol., 9, e1000602.Remm,M. et al. (2001) Automatic clustering of orthologs and in-paralogsfrom pairwise species comparisons. J. Mol. Biol., 314, 1041–1052.Roth,A.C.J. et al. (2008) Algorithm of OMA for large-scale orthology inference. BMC Bioinformatics, 9, 518.Schreiber,F., and Sonnhammer,E.L.L. (2013) Hieranoid: hierarchical orthology inference. J. Mol. Biol., 425, 2072–2081.Smith,T.F., and Waterman,M.S. (1981) Identiﬁcation of common molecularsubsequences. J. Mol. Biol., 147, 195–197.Tatusov,R.L. et al. (1997) A genomic perspective on protein families. Science,278, 631–637.Trachana,K. et al. (2011) Orthology prediction methods: a quality assessmentusing curated protein families. Bioessays, 33, 769–780.Vilella,A.J. et al. (2008) EnsemblCompara GeneTrees: Complete, duplicationaware phylogenetic trees in vertebrates. Genome Res., 19, 327–335.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i75/3953943by gueston 07 January 2018
5028881963002	PMID28881963	5028881963	https://watermark.silverchair.com/btx228.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881963.main.pdf	Bioinformatics, 33, 2017, i37–i48doi: 10.1093/bioinformatics/btx228ISMB/ECCB 2017Deep learning with word embeddings improvesbiomedical named entity recognitionMaryam Habibi1,*, Leon Weber1, Mariana Neves2, David Luis Wiegandt1and Ulf Leser11€Computer Science Department, Humboldt-Universitat zu Berlin, Berlin 10099, Germany and 2Enterprise Platformand Integration Concepts, Hasso-Plattner-Institute, Potsdam 14482, Germany*To whom correspondence should be addressed.AbstractMotivation: Text mining has become an important tool for biomedical research. The most fundamental text-mining task is the recognition of biomedical named entities (NER), such as genes,chemicals and diseases. Current NER methods rely on pre-deﬁned features which try to capturethe speciﬁc surface properties of entity types, properties of the typical local context, backgroundknowledge, and linguistic information. State-of-the-art tools are entity-speciﬁc, as dictionaries andempirically optimal feature sets differ between entity types, which makes their development costly.Furthermore, features are often optimized for a speciﬁc gold standard corpus, which makes extrapolation of quality measures difﬁcult.Results: We show that a completely generic method based on deep learning and statistical wordembeddings [called long short-term memory network-conditional random ﬁeld (LSTM-CRF)]outperforms state-of-the-art entity-speciﬁc NER tools, and often by a large margin. To this end,we compared the performance of LSTM-CRF on 33 data sets covering ﬁve different entityclasses with that of best-of-class NER tools and an entity-agnostic CRF implementation. On average, F1-score of LSTM-CRF is 5% above that of the baselines, mostly due to a sharp increase inrecall.Availability and implementation: The source code for LSTM-CRF is available at https://github.com/glample/tagger and the links to the corpora are available at https://corposaurus.github.io/corpora/.Contact: habibima@informatik.hu-berlin.de1 IntroductionText mining is an important tool for many types of large-scale biomedical data analysis, such as network biology (Zhou et al., 2014),gene prioritization (Aerts et al., 2006), drug repositioning (Wangand Zhang, 2013) or creation of curated databases (Li et al., 2015).The most fundamental task in biomedical text mining is the recognition of named entities (called NER), such as proteins, species, diseases, chemicals or mutations. To date, the best performing NERtools rely on specific features to capture the characteristics of the different entity classes. For instance, the suffix ‘-ase’ is more frequentin protein names than in diseases; species names often consist of twotokens and have latin suffixes; chemicals often contain specific syllabi like ‘methyl’ or ‘carboxyl’, and mutations usually are sequencesof letters and digits encoding genomic position, type of mutation,and base changes. Feature engineering, i.e. finding the set of featuresthat best helps to discern entities of a specific type from other tokens(or other entity classes) currently is more of an art than a science,incurring extensive trial-and-error experiments. On top of this costlyprocess, high-quality NER tools today need further entity-specificmodules, such as whitelist and blacklist dictionaries, which againare difficult to build and maintain. Defining these steps currentlytakes the majority of time and cost when developing NER tools(Leser and Hakenberg, 2005) and leads to highly specialized solutions that cannot be used for other entity types than the ones theywere designed for. On the other hand, the method used to identifyentities in a given text based on the defined features nowadays isCV The Author 2017. Published by Oxford University Press.i37This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018i38fairly homogeneous: conditional random fields (CRFs) (Laffertyet al., 2001), a statistical sequential classification method, is the defacto standard method. Here, we show that an entirely generic NERmethod based on deep learning and distributional word semanticsoutperforms such specific high-quality NER methods across different entity types and across different evaluation corpora.In the general field of information extraction, two recent developments lead to substantial improvements. First, word embeddings havebeen introduced to represent a single word by a low-dimensional vectorcapturing—in some way—the frequencies of co-occurring adjacentwords. When compared with the bag-of-words approach underlyingthe conventional methods outlined earlier, word embeddings capture semantic similarities between words (as mathematical similarities betweentheir vectors) that are not visible from their surface; for instance, thewords ‘enables’ and ‘allows’ are syntactically very different, yet theirmeaning is somewhat related, which leads to similar sets of cooccurring words, whereas the co-occurrences of the word ‘swim’ wouldbe completely different. The underlying idea of representing words ‘bythe company they keep’ (Mackin, 1978) is an old concept in linguistics,usually called distributional semantics; its recent popularity is based onthe novel idea that the embeddings are automatically adjusted such thatinformation extraction tools benefit the most. Second, it has beenshown that the application of artificial neural networks (ANNs), whichautomatically learn non-linear combinations of features, leads to betterrecognition results than the usage of CRFs, which can only learn (log)linear combinations of features. Deep neural networks, and especiallylong short-term memory networks (LSTM), perform this task particularly efficiently and effectively. As with word embeddings, this idea isnot new (Hochreiter and Schmidhuber, 1997), but only recent progressin the available data volumes and machine capabilities made it applicable to practically relevant problems (Pascanu et al., 2014).Following a suggestion from Lample et al. (2016), we combinedthe power of word embeddings, LSTMs and CRFs into a singlemethod for biomedical NER, called LSTM-CRF. This method iscompletely agnostic to the type of the entity; all it requires is anentity-annotated gold standard and word embeddings pre-computedon a large, entity-independent corpus (typically all PubMed abstracts). We assessed the performance of LSTM-CRF by performing33 evaluations on 24 different gold standard corpora (some with annotations for more than one entity type) covering five different entity types, namely chemical names, disease names, species names,genes/protein names, and names of cell lines. These corpora encompass patents and scientific articles and partly consist of abstracts andpartly of full texts. We compared the performance of LSTM-CRFwith that of best-of-class, entity-specific NER tools and with another generic NER method using a CRF with a typical NER featureset plus the word embeddings as input. LSTM-CRF turned out tohave the best F1-score on 28 of the 33 cases; on average, it is 5%better than the entity-specific NER tools and 3% better than theCRF method with word embeddings.M.Habibi et al.We first describe LSTM, a specific kind of ANN, and then discussthe architecture of LSTM-CRF in detail.An LSTM is a special kind of ANN which processes sequences ofarbitrary length and is able to model dependencies between sequenceelements even if they are far apart (Hochreiter and Schmidhuber,1997). The input to an LSTM unit is a sequence of vectors x1 ; x2 ; . . .; xT of length T, for which it produces an output sequence of vectorsh1 ; h2 ; . . . ; hT of equal length by applying a non-linear transformation learned during the training phase. Each ht is called the activation of the LSTM at token t. The exact formula to compute oneactivation of an LSTM unit in the LSTM-CRF model is providedbelow (Lample et al., 2016):it ¼ rðW xi xt þ W hi htÀ1 þ W ci ctÀ1 þ bi Þct ¼ ð1 À it Þ   ctÀ1 þ it   tanhðW xc xt þ W hc htÀ1 þ bc Þot ¼ rðW xo xt þ W ho htÀ1 þ W co ct þ bo Þht ¼ ot   tanhðct ÞIn the following sections, we give a technical explication of theLSTM-CRF approach and describe the competitor NER systems.Furthermore, we describe the corpora we used for evaluation, thedifferent embeddings evaluated, and details regarding text pre-processing and evaluation metrics.where all Ws and bs are trainable parameters, r denotes theelement-wise sigmoid function and   is the element-wise product.Such an LSTM-layer processes the input only in one direction andthus can only encode dependencies on elements that came earlier in thesequence. As remedy for this problem, we use another LSTM-layer processing in the reversed direction, which allows detecting dependencieson elements later in the text. The resulting neural network is called a bidirectional LSTM (Graves and Schmidhuber, 2005).The architecture of LSTM-CRF is shown in Figure 1. It is comprised of three main layers. The first layer is the embedding layer. Itreceives the raw sentence S made of the sequence of words w1 ; w2 ; . . .; wT as its input and produces an embedding (i.e. a dense vector representation) x1 ; x2 ; . . . ; xT for each word in S. The embedding vector xtof word wt is a concatenation of a word- and a character-level embedding. The word-level embedding is simply retrieved from a lookuptable (see the example in Fig. 1) of word embeddings trained on alarge corpus (see Section 2.3). The character-level embedding is obtained by applying a bi-directional LSTM to the character sequence ofeach word and then concatenating the last activations of both directions, as exemplified for the word ‘SH3’ in the left side of Figure 1.The resulting sequence of embeddings x1 ; x2 ; . . . ; xT is fed into another bi-directional LSTM-layer that produces a refined representation of the input, which is the input to a final CRF-layer. The classicalViterbi algorithm is used to obtain the final output from this layer. Allcomponents together form a single fully differentiable neural networkthat can be trained by backpropagation.In our experiments, we used a learning rate of 0.005 for all corpora except for BioSemantics where we set it to 0.0005, because themodel did not converge with the default one. Regarding the otherhyperparameters, we used the default values from (Lample et al.,2016) except for (i) the tag scheme which we set to IOB instead ofIOBES, and (ii) the dropout rate which we set to 0.3 instead of 0.5because this value was optimal for most corpora evaluated byLample et al. (2016).2.1 LSTM-CRF2.2 Competitor systemsLSTM-CRF (Lample et al., 2016) is a domain-independent NERmethod which does not rely on any kind of background knowledge.We compare the performance of LSTM-CRF against two types ofcompetitors: a CRF using a generic feature set for NER tasks plus2 Materials and methodsDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018Biomedical NER with deep learningi39•which does not build on a CRF, and it is also the only tool whichdoes not train a corpus-speciﬁc model.Diseases: we used DNorm (https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/tmTools/) (Leaman et al., 2013), a freelyavailable tool showing excellent performance in several studies(Leaman and Lu, 2016; Wei et al., 2015). It uses the rich featureset provided by BANNER and a dictionary of disease names created by the authors to train a ﬁrst-order CRF model. The outputof the model is also ﬁltered by manual rules like abbreviationresolution.Cell Lines: we used a model presented in (Kaewphan et al.,2016), reported to outperform many other available systems in across-corpus setting. It is based on the domain-independent CRFbased tool NERsuite (http://nersuite.nlplab.org/). In addition tothe pre-deﬁned NERsuite features, it uses a comprehensive dictionary of cell line names from multiple sources.Fig. 1. CRF-LSTM architecture. For instance, for the word wtÀ1 ¼ ‘SH3’ fromthe input sentence S, the character-based representation is computed byapplying a bi-directional LSTM onto the sequence of its characters ‘S’, ‘H’, ‘3’.The resulting embedding is concatenated with the corresponding wordembedding, trained on a huge corpus. This word representation is then processed by another bi-directional LSTM and ﬁnally by a CRF layer. The outputis the most probable tag sequence, as estimated by the CRF•word embeddings, and entity-specific NER tools for each class. Theformer should help to separate the impact of using word embeddingsfrom the impact of using the LSTM-CRF architecture. For the latterwe selected the current best-in-class entity-specific NER tools. Alltrainable baseline systems were retrained using the same corporaand folds.2.3 Gold standard corpora2.2.1 Baseline CRFWe used CRFSuite (http://www.chokkan.org/software/crfsuite/)(Okazaki, 2007) with default settings to train a first-order linearchain CRF model utilizing identical features for all entity types.These features were defined by the NER feature extractor shippedwith CRFsuite and were designed for domain-independent NER.Additionally, we provided as features (in turn) one of the wordembeddings described in Section 2.4.2.2.2 Baseline methodsFor each of the five entity classes considered in our evaluation, wechose the presumably best performing and publicly available currentNER tool:•••Chemicals: we employed tmChem (https://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/tmTools/) (Leaman et al., 2015)Model I, considered as the state-of-the-art chemical NER toolalso in other recent studies (Habibi et al., 2016; Leaman and Lu,2016;). tmChem Model I trains a ﬁrst-order CRF using featuresfrom BANNER (Leaman and Gonzalez, 2008) plus further onesobtained through a trial-and-error procedure, including character n-grams, chemical speciﬁc identiﬁers, and the output ofChemSpot (Rockt€schel et al., 2012) (another high-quality chemaical NER tool). The output of the model is ﬁltered by severaltype-speciﬁc post-processing steps for abbreviation resolution,enforcing of tagging consistency and balancing of parentheses.Genes/proteins: we utilized Gimli (http://bioinformatics.ua.pt/software/gimli/) (Campos et al., 2013), an open source retrainable gene/protein NER tool with competitive performance(Campos et al., 2012). Gimli trains both ﬁrst- and second-orderCRF models using a set of speciﬁc features, such as orthographic,morphological, linguistic-based, dictionary-based and a conjunction of features from adjacent tokens. Post-processing steps likeparentheses correction, abbreviation resolution, and name extension using a domain dictionary are also applied.Species: we used SPECIES (http://species.jensenlab.org/) (Paﬁliset al., 2013), a freely available dictionary-based tool with stateof-the-art tagging quality. Species is the only NER tool in our setWe performed our evaluations on five entity types: genes/proteins,chemicals, diseases, cell lines and species. We relied on a total of 24corpora, each containing manual annotations for one or more ofthese entity types, such as CHEMDNER patent (Krallinger et al.,2015a,b) for chemicals and genes/proteins, NCBI Disease (Do anget al., 2014) for disease names, Gellus (Kaewphan et al., 2016) forcell lines and S800 (Pafilis et al., 2013) for species. The corpora encompass two different genres of texts: (i) patent texts from theEuropean Patent Office (EPO) (http://www.epo.org/), WorldIntellectual Property Organization (WIPO) (http://www.wipo.int/),and United States Patent and Trademark Office (USPTO) (http://www.uspto.gov/) and (ii) scientific articles from PubMed Central(PMC) (http://www.ncbi.nlm.nih.gov/pmc) and PubMed (https://www.ncbi.nlm.nih.gov/pubmed/).Table 1 lists all corpora togetherwith important characteristics like the number of sentences, tokens,and annotated entities per entity class (measured after text pre-processing as described in Section 2.5).2.4 Word embeddingsWe utilized word embedding techniques to capture the semantics ofwords (and their similarities) based on their surrounding words. Weevaluated three different embedding models. The first model,denoted PubMed-PMC, was trained on a combination of PubMedabstracts (nearly 23 million abstracts) and PMC articles (nearly 700,000 full texts). The second model, denoted Wiki-PubMed-PMC,was trained using these two collections plus approximately four million English Wikipedia articles. The second model thus mixesdomain-specific texts with domain-independent ones. Both modelswere created by Pyysalo et al. (2013) using Google’s word2vec(http://bio.nlplab.org/); we use 200D vectors. To also be able tostudy the impact of text genre, we trained a third model with 50 dimensions, denoted Patent, on a set of roughly 20 000 European patents with biomedical topics using the Gensim toolkit. We optimizedthe hyper-parameters of Gensim for the F1-score of the CRF model(see Section 2.2) on the development set of the CHEMDNER patentcorpus.2.5 Text pre-processingAll corpora first were converted into a simple uniform format toease further processing. In this format, all texts and all annotationsare stored in one single file. Each document is represented by a document identifier, a tab separator and the entire document text in oneline. Annotations are given in the following lines, one annotationDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018PatentPatentPatentScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleScientiﬁc articleCHEBI (http://chebi.cvs.sourceforge.net/viewvc/chebi/chapati/)BioSemantics (Akhondi et al., 2014)CHEMDNER (Krallinger et al., 2015a)CDR (Li et al., 2016)BioCreative II GM (Smith et al., 2008)JNLPBA (Kim et al., 2004)CellFinder (Neves et al., 2012)OSIRIS (Furlong et al., 2008)DECA (Wang et al., 2010)Variome (Verspoor et al., 2013)PennBioIE (Kulick et al., 2004)FSU-PRGE (Hahn et al., 2010)IEPA (Ding et al., 2002)BioInfer (Pyysalo et al., 2007)miRNA (Bagewadi et al., 2014)NCBI Disease (Do an et al., 2014)gArizona Disease (Leaman et al., 2009)SCAI (Gurulingappa et al., 2010)S800 (Paﬁlis et al., 2013)LocText (Goldberg et al., 2015)Linneaus (Gerner et al., 2010)CLL (Kaewphan et al., 2016)Gellus (Kaewphan et al., 2016)Text genreCHEMDNER patent (Krallinger et al., 2015a,b)CorporaTable 1. The details of the gold standard corporaDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018AbstractAbstractAbstractAbstractAbstractFull-textAbstract, Full-textAbstract, Full-textAbstractAbstractAbstractAbstractAbstractAbstractAbstractFull-textFull-textAbstractAbstractFull-textFull-textAbstractAbstractAbstractText typeChemicalsGenes/proteinsChemicalsChemicalsChemicalsChemicalsDiseasesGenes/proteinsGenes/proteinsCell LinesGenes/proteinsSpeciesCell LinesGenes/proteinsGenes/proteinsGenes/proteinsDiseasesSpeciesGenes/proteinsGenes/proteinsGenes/proteinsGenes/proteinsGenes/proteinsDiseasesSpeciesDiseasesDiseasesDiseasesSpeciesSpeciesSpeciesCell LinesCell LinesEntity type35 58435 58410 638173 80889 67914 22814 22820 51022 56222 5622177217721771043546864716471647114 30535 4652431141270427042704763928844332793394917 78821511 221No. sentences1 465 4711 465 471314 1055 690 5182 235 435323 281323 281508 257597 333597 33365 03165 03165 03128 697138 034172 409172 409172 409357 313899 42615 17433 83265 99865 99865 998174 48776 489104 015195 19722 550473 1486547278 910No. tokens62 34462 34424 424208 326114 83723 06823 06850 86425 04625 046797779777977466914 51512 65912 65912 65921 08957 4982923530178217821782112 128735812 55820 526437134 396246825 628No. uniquetokens64 02612 59717 724368 09179 84215 41112 69420 70335 4604332134043535076860484309602518217 42756 087110941621006212372668813206222636462764077341640No. annotations20 8935835510972 75624 3213629345914 90610 7322528600516927523605966298434819 470209115040967147219211881048156439419311237No. uniqueannotationsi40M.Habibi et al.Biomedical NER with deep learningi41Table 2. Macro averaged performance values in terms of precision,recall and F1-score for CRF and LSTM-CRF methods with wordembedding features: (i) Patent, (ii) PubMed-PMC and (iii) WikiPubMed-PMCPrecision (%)(i)(ii)Recall (%)(iii)(i)(ii)F1-score (%)(iii)(i)(ii)(iii)CRF82.71 84.55 84.49 71.98 73.07 73.26 76.36 77.98 78.04LSTM-CRF 80.10 81.39 81.77 81.04 80.72 81.08 80.26 80.79 81.11The highest values for each method are represented in bold.per line, with start character position end character position, entitymention text, and entity type. An empty line indicates the end of adocument, and the next document starts after this empty line. Weconverted this file into the specific input format defined by eachbaseline NER tool. Moreover, we used Apache OpenNLP (https://opennlp.apache.org/) to split documents into sentences, sentencesinto tokens, and to assign part-of-speech tags. Finally, we created afile in CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) formatas input for the LSTM-CRF model and the CRFSuite tool.2.6 Evaluation metricsWe randomly divided each corpus into three disjoint subsets. 60%of the samples were used for training, 10%, as the development setfor the training of methods, and 30% for the final evaluation. Wecompared all methods in terms of precision, recall and F1-score onthe test sets. We performed exact matching to compute these performance values. We also performed an error analysis by comparingthe sets of false positives (FPs) and false negatives (FNs) of the different NER methods. To this end, we measured the number of FP andFN counts for each mention by each method and then calculated theoverlap between sets of FP or FN using fuzzy set operations thattake into account the frequency of mistakes per entity mention(Thole et al., 1979).3 ResultsWe assessed the performance of a novel method for entity-type independent NER, namely LSTM-CRF, on 33 different evaluation setscovering five different types of biomedical entities. LSTM-CRF usesas features only low-dimensional representations of the words in thevicinity of the to-be-classified tokens, created by mixing word-levelembeddings created in an unsupervised fashion with character-levelembeddings trained on the respective corpus. Results were compared with a traditional CRF using typical NER features and thesame word embeddings, and to type-specific baselines representingthe state-of-the-art in biomedical NER.3.1 Impact of different word embeddingsWe first studied the impact of using different word embeddings. Wecompared the results of three models, differing only in the unsupervised part, i.e. the text collections used for computing word-levelembeddings. For evaluation, we considered the LSTM-CRF and thepure CRF approach. The macro averaged performance values overall corpora in terms of precision, recall and F1-score are provided inTable 2; detailed performance values are given in Appendix A. Infive out of six cases, Wiki-PubMed-PMC achieves the best performance, and this model is also very close to the best one in the sixthcase. Based on this data, we used the Wiki-PubMed-PMC embeddings in all further evaluations.Fig. 2. F1-scores of baseline (B), generic CRF (C) and generic LSTM-CRF (L)for ﬁve entity types, each measured within 4–12 corpora. The score for eachcorpus per entity type is depicted by a speciﬁc colored circle3.2 Performance of LSTM-CRF, CRF, and baselinesWe compared the performance of five baseline NER tools, the CRF,and the LSTM-CRF using Wiki-PubMed-PMC embeddings on 33evaluation sets. Results in terms of F1-score for each entity type andeach corpus are shown Figure 2; exact precision, recall, and F1 values are given in Appendix A. LSTM-CRF achieves the best performance for 28 out of 33 evaluations and is very close to the bestmethod in the remaining five cases. On average (macro average),F1-scores are 81.11% for the generic LSTM-CRF method, 78.04%for the generic CRF method and 76.61% for the baselines. The improvements are mainly due to a strong increase in recall (mean81.08%, 73.26%, 75.13%) at the cost of decrease in precision(mean 81.77%, 84.49%, 80.38%).We also computed performance metrics aggregated per entitytype to see if methods are more suitable for some types than for theothers. Both macro and micro averaged performance values inTable 3 reveal that this does not seem to be the case; LSTM-CRFachieves the best average F1-score and recall for all entity types.An interesting observation is that the pure CRF method oftenperforms better than the entity-specific baseline algorithms. Thereare two explanations for this apparent contradiction to our introductory words, claiming that the best methods to date are CRF withtype-specific feature sets. First, NER methods are often developedfor a very specific sub-problem, and often only have excellent performance on a particular evaluation corpus. Our evaluation on amuch larger set of corpora reveals that such corpus-specific advantages cannot be simply extrapolated to other settings. Second, byusing word embeddings our CRF model represents words in acontext-encoding space, which enables it to recognize semantic similarity between words. This feature is missing in the word-basedbaseline tools.3.3 Error analysisWe compared the errors made by the three methods by computingintersections of the sets of FPs and FNs for each method and eachentity type. Results per entity class are shown in Figure 3.The Venn diagrams of FP sets or FN sets for the different entitytypes follow a similar pattern. Generally, error sets of CRF andLSTM-CRF are more similar to each other than to errors of thebaseline methods, probably due to the strong reliance of all baselineson entity type-specific dictionaries, creating their own specificerrors. Relying on dictionaries carries two types of dangers: first,they are notoriously incomplete, leading to FNs; second, dictionarymatching disregards context which leads to FP matches in case ofambiguous terms. This is particularly visible for species, where thebaseline only uses dictionary matching, leading to the worst precision among all entity types.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018i42M.Habibi et al.Table 3. Macro and micro averaged performance values in terms of precision, recall and F1-score for the baselines (B), the generic CRFmethod (C) and the generic LSTM-CRF method (L) over the corpora per each entity typea) Macro averaged performanceBChemicals79.8Diseases78.54Species72.75Gene/protein83.77Cell lines85.14Average80.38b) Micro averaged performanceChemicalsDiseasesSpeciesGene/proteinCell linesAverageB75.1679.2977.3381.3471.8276.4Precision (%)C85.8182.4889.5982.5484.0484.49L82.8281.6680.8481.5782.6581.77B80.7373.6779.6375.8761.3275.13Recall (%)C79.2673.4776.1574.6756.9173.26L84.7778.7087.6480.0673.2281.08B80.2275.8973.0379.3570.3576.61F1-score (%)C82.3277.5682.0978.1566.9678.04L83.7180.1183.6080.5877.281.11Precision (%)C83.4583.6288.9281.7970.2083.11L81.8182.5683.8481.5068.5981.72B79.775.9867.7477.4557.2078.87Recall (%)C79.1277.1776.6878.1457.6078.71L83.8681.6883.3382.7166.8483.45B77.3677.6072.2279.3563.6877.62F1-score (%)C81.2380.2682.3579.9263.2880.85L82.8282.1283.5982.1067.7082.58The highest values for each entity class are highlighted in bold.Table 4. The average length of errors from the lists of FPs and FNs,and the F1-scores of single-token and multi-token entities measured for baselines (B), generic CRF methods (C) and generic LSTMCRF methods (L) per entity typeF1-score (%)Mention lengthSingle-TokenChemicalsDiseasesFig. 3. Venn diagrams demonstrating the area of overlap among the FP setsor the FN sets of the three methods: the baseline (B), the generic CRF method(C) and the generic LSTM-CRF method (L) per entity typeSpeciesGenes/ProteinsWe also speculated that the length of an entity might have an impact on the performance of approaches. Very short entities are oftenabbreviations which are notoriously ambiguous; entities consistingof multiple tokens often produce errors at the border tokens, whichare severely punished in any exact matching evaluation (one FP andone FN). To this end, we measured the average length of a FP or FNmention in terms of non-space characters for the three methods perentity class. Furthermore, we computed the F1-scores of single- andmulti-token mentions separately. Results are shown in Table 4.First, LSTM-CRF is the best method regardless whether an entityname consists of only one or of multiple tokens. Only for species,the dictionary-based baseline method has a slightly better F1-scorefor multi-token entities, which is, however, outweighed by a muchworse F1-score for single entity names in the overall evaluation.This result shows that the LSTM-CRF method with word embeddings manages to tag precisely also multi-token entities, withoutrelying on any post-processing rules. However, we also observe thatthe performance of CRF and LSTM-CRF on single-token mentionsis considerably better than on multi-token entities, showing thatthere is still room for improvement regarding such entity names.Second, there is an interesting tendency that FPs tagged by LSTMCRF are slightly shorter than those found by the CRF, while FNsCell LinesLCBLCBLCBLCBLCBMulti-Token84.0282.5376.7786.1384.2080.4286.1185.5767.7286.4984.7783.2172.9464.7762.5579.9078.0776.8476.5474.9273.8079.1076.7579.8072.6469.3270.9964.9362.5264.24FP19.0725.119.1314.5115.0913.9010.6811.558.0012.8513.5011.7320.0219.0118.41FN20.6920.1116.4915.5814.6614.3612.3411.529.1514.1313.5211.9815.6515.2413.91The highest F1-scores are emphasized in bold.are slightly longer, indicating that LSTM-CRF seems to be biased towards shorter entity names.3.4 Precision-recall trade-offThe results in Table 3 show that, on average, LSTM-CRF significantly outperforms CRF and baselines in terms of recall at the expense of a less strong decrease in precision. Since, for someapplications, obtaining high precision is more important than highrecall, we also implemented and evaluated a method which assignsconfidence scores to the predictions made by LSTM-CRF, followedby an entity-type independent filter for low confidence predictionswith the aim to reduce the number of FP entities. The filter removesa percentage of entities with the lowest confidence values from theoutput of the generic LSTM-CRF method. First, the labels for all theDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018Biomedical NER with deep learningi43others on patent-derived corpora in terms of F1-score (recall)(Patent: 78.52% (82.75%), PubMed-PMC: 78.48% (81.85%),Wiki-PubMed-PMC: 77.84% (81.00%)) and the model derivedfrom Wiki-PubMed-PMC achieves a better F1-score (recall) on corpora consisting of scientific articles (Patent: 80.50% (80.81%),PubMed-PMC: 81.11% (80.57%) and Wiki-PubMed-PMC:81.56% (81.09%)). This observation again shows that patents aredifferent from scientific articles (Habibi et al., 2016) and that theiranalysis calls for specific resources.Fig. 4. Aggregated precision and recall of the generic LSTM-CRF method before (L) and after applying ﬁlters, removing 5 (L—5%), 10 (L—10%) and 15(L—15%) of entities, per entity type. The highest averaged precision and recallper entity type obtained by baselines or the generic CRF model are represented by the green and the orange dash line, respectivelytokens of a sentence are predicted by Viterbi decoding using thetrained model (as described in Section 2.1). Then, all tokens labeledas entity are assigned a confidence score which estimates the probability (given the LSTM-CRF model) of the predicted entity labelbeing part of the true labels of the sentence. The score for a given entity is obtained by summing the scores of all possible label sequencesof the sentence which produce this entity label normalized by thesum of scores of all the possible label sequences. These scores can beobtained by the constrained forward-backward algorithm (Culottaand McCallum, 2004) (The implementation is available online at“https://github.com/leonweber/tagger”.).We removed 5, 10 and 15% of the entities tagged by the genericLSTM-CRF model with lower confidence values and, on average,obtained precision (recall) scores of 83.62% (78.97%), 85.16%(76.22%) and 86.45% (73.05%), respectively; detailed performancevalues are given in Appendix A. Across all evaluations, the genericLSTM-CRF model without the 10% lowest confidence predictionsobtains, on average, higher precision and higher recall values thanthe generic CRF model and the baselines. In Figure 4, we compareprecision and recall values of the LSTM-CRF method before andafter applying filters together with CRF and baselines, macroaveraged by entity type. For four out of five entity types, precisionand recall values of this configuration is very close to or higher thanthose of the generic CRF method and of the baselines. The only exception is species, where even the most stringent filtering that weapplied cannot make the LSTM-CRF method reach a precisionhigher than that of the CRF method. We inspected these evaluationsin more detail. The lower precision values on average are mostly dueto the two corpora CellFinder and Variome. However, both corporamiss some of the species annotations. For instance, the 41 FP predictions of LSTM-CRF for Variome are 21 times the term ‘mouse’, 4times ‘mice’ and 16 times ‘human’—all of which are valid speciesnames, and all of which achieve high prediction confidence. Thesituation is similar for CellFinder, with tokens like ‘goat’, ‘human’or ‘EColi’ being not annotated as species.4 Discussion4.1 Genre and word embeddingsThe word embeddings we used for our study were either derivedfrom patents, from scientific articles or from a mixture of scientificarticles and Wikipedia entries. Since also some of our corpora arebased on patents and others on scientific articles, we wondered ifthe use of patent-derived embeddings is advantageous for patentcorpora yet less adequate for scientific articles and vice versa.Indeed, our patent-derived embeddings slightly outperform the4.2 Corpus size and word embeddingsThe performance values obtained in Section 3.1 show that bothCRF and LSTM-CRF achieve the best performance using WikiPubMed-PMC embeddings in most of the cases. Notably, this modelis derived from a collection of domain-specific texts (PubMed,PMC) mixed with domain-unspecific texts (Wikipedia). A possibleexplanation for its superiority is that it uses the largest text baseamong the three models; previous works have shown that wordembeddings tend to be the more effective the larger the text base is(Stenetorp et al., 2012). Furthermore, the use of general domain corpora in addition to the domain-specific ones may add more out-ofdomain information to the embeddings. However, more investigations are required to define optimal corpora for derivation of wordembeddings for concrete tasks.4.3 Related workTraditional biomedical NER methods have relied on rule- ordictionary-based approaches. The rule-based techniques recognizebiomedical entities using several rules manually defined based onthe textual patterns of entities (Narayanaswamy et al., 2003; Eltyeband Salim, 2014). These patterns vary depending on the specifictextual properties of an entity class. The definition of such entityspecific patterns is time consuming and requires domain-expertknowledge. Dictionary-based methods extract named entities bysearching them in dictionaries constructed for each entity type. Forinstance, Hettne et al. (2009) employ a dictionary-based approachfor the extraction of drugs, and Gerner et al. (2010) use adictionary-based approach for species names extraction. Again,building such dictionaries is time consuming and challenging (Liuet al., 2015a). Moreover, the recall obtained using these methods isgenerally low due to the inherent difficulty of the methods in capturing new entities; a strong advantage of dictionary-based methods isthat they directly solve the named entity normalization (NEN) problem, i.e. they can output a database identifier for each recognizedentity.Over the last years, pattern- and dictionary- based methods havebeen superseded by approaches relying on supervised machine learning, especially sequential classification algorithms, such as HiddenMarkov Models (Rabiner, 1989) and CRFs (Lafferty et al., 2001).CRFs have become the de-facto standard model, being the method ofchoice for essentially all tools winning recent NER competitions, suchas BioCreative IV (Krallinger et al., 2013) or i2b2 (Uzuner et al.,2011). Popular biomedical NER tools using CRFs are, for instance,ABNER (A Biomedical Named Entity Recognizer) (Settles, 2005) andBANNER (Leaman and Gonzalez, 2008). Hybrid methods combinemachine learning methods with dictionary- or rule-based techniques.For instance, ChemSpot (Rockt€schel et al., 2012) integrates resultsaof a CRF model with a dictionary-matching module for chemicalNER, and Gimli (Campos et al., 2013) applies post-processing stepslike parentheses balancing to the output of the CRF models.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018i44All these methods build upon pre-defined sets of features, whosecorrelations with other features and the target class of tokens arelearned from the gold standard annotations. In contrast, methodsbased on deep ANNs also consider non-linear combinations of feature values (Hastie et al., 2001). This drastically increases the searchspace, leading to the fact that ANNs for long were considered impractical for many applications. This situation changed only recentlydue to the steep increase in the compute power of machines. Whencombined with specifically trained word embeddings, deep learningwith ANNs has been shown to outperform other methods in manyareas, such as sentiment analysis (Dai and Le, 2015) or languagemodeling (Jozefowicz et al., 2016). These methods are also gradually entering the field of biomedical information extraction, yet results so far have been mixed. Segura-Bedmar et al. (2015) used wordembeddings as input to a CRF and reported only marginal effects inchemical NER. Liu et al. (2015b) found word embeddings to onlymarginally improve the performance of a CRF-based method usingcomprehensive dictionaries as features. Tang et al. (2014) comparedseveral ways of obtaining word embeddings and reported up to 2%increase in recall. The method we use in this paper, LSTM-CRF,was proposed as a general NER algorithm by Lample et al. (2016).A few previous works have applied LSTM-CRF for biomedical NER(e.g. Chalapathy et al., 2016a,b). However, all these evaluationsconsidered at most a handful of corpora. In contrast, our evaluationof biomedial NER methods based on 33 evaluations using 24 corpora to our knowledge is the most comprehensive one ever performed in this field. Larger previous evaluations we are aware ofwere performed in Campos et al. (2012), who measured the performance of different machine learning-based NER methods on 14corpora, and Batista-Navarro et al. (2015), who reported on the performance of a CRF-based NER tool using nine different corpora.5 ConclusionIn summary, our results indicate that LSTM-CRF improves considerably upon current biomedical NER methods. We find this excitingparticularly because the method is completely agnostic to entitytypes; thus, the costly development of specific tools using specificdictionaries could become superfluous. However, further research isnecessary to turn this observation into useful applications. In particular, LSTM-CRF only helps with the recognition of entities intexts; the next step in most text-mining applications, which is mapping recognized entities to standard nomenclature (e.g. Entrez-IDsfor genes, ChEBI-IDs for chemicals etc.), is not addressed. Thus,LSTM-CRF should be combined with generic NEN tools, as for instance presented in (Leaman et al., 2013). In the future, we plan tostudy the inclusion of background knowledge into the LSTMapproach, for instance in the form of post-processing rules to dealwith long multi-token entities; the hope is to achieve further improvements especially in terms of precision, though the price will beto lose the entity-independence. Nevertheless, our results imply thatthe advances in statistical NLP and machine learning can help tomake biomedical text mining (i) more accurate, (ii) less laborious todevelop and (iii) more robust with respect to the specific texts beingmined.AcknowledgementsWe are grateful to the Federal Ministry for Economic Affairs and Energy(BMWi) for its ﬁnancial support through the BioPatent project[KF2205219BZ4], the Federal Ministry for Research and Education (BMBF)for its ﬁnancial support through PREDICT project (031L0023A), and theM.Habibi et al.German Research Council (DFG) for funding through the Research TrainingGroup SOAMED (GRK1651).Conﬂict of Interest: The authors declare that they have no conﬂict of interests.ReferencesAerts,S. et al. (2006) Gene prioritization through genomic data fusion. Nat.Biotechnol., 24, 537–544.Akhondi,S.A. et al. (2014) Annotated chemical patent corpus: a gold standardfor text mining. PloS One, 9, 1–8.Bagewadi,S. et al. (2014) Detecting miRNA mentions and relations in biomedical literature. F1000Research, 3.Batista-Navarro,R. et al. (2015) Optimising chemical named entity recognition with pre-processing analytics, knowledge-rich features and heuristics.J. Cheminform., 7.Campos,D. et al. (2012) Theory and Applications for Advanced Text Mining,Chapter Biomedical Named Entity Recognition: A Survey of Machine-LearningTools. INTECH Open Access Publisher, Rijeka, Croatia, pp. 175–195.Campos,D. et al. (2013) Gimli: open source and high-performance biomedicalname recognition. BMC Bioinformatics, 14.Chalapathy,R. et al. (2016a) Bidirectional LSTM-CRF for clinical concept extraction. In Proceedings of the Clinical Natural Language ProcessingWorkshop, Osaka, Japan, pp. 7–12.Chalapathy,R. et al. (2016b) An investigation of recurrent neural architecturesfor drug name recognition. In Proceedings of the Seventh InternationalWorkshop on Health Text Mining and Information Analysis, Austin, TX.Culotta,A. and McCallum,A. (2004) Conﬁdence estimation for informationextraction. In Proceedings of NAACL-HLT, Boston, MA, pp. 109–112.Dai,A.M. and Le,Q.V. (2015) Semi-supervised sequence learning. In Advancesin Neural Information Processing Systems, Montreal, Canada, pp. 3079–3087.Ding,J. et al. (2002) Mining MEDLINE: abstracts, sentences, or phrases. InProceedings of the Paciﬁc Symposium on Biocomputing, Lihue, Hawaii,USA, pp. 326–337.Do an,R.I. et al. (2014) NCBI disease corpus: a resource for disease name recgognition and concept normalization. J. Biomed. Inform., 47, 1–10.Eltyeb,S. and Salim,N. (2014) Chemical named entities recognition: a reviewon approaches and applications. J. Cheminform., 6.Furlong,L.I. et al. (2008) OSIRISv1. 2: a named entity recognition system forsequence variants of genes in biomedical literature. BMC Bioinformatics, 9.Gerner,M. et al. (2010) LINNAEUS: a species name identiﬁcation system forbiomedical literature. BMC Bioinformatics, 11.Goldberg,T. et al. (2015) Linked annotations: a middle ground for manualcuration of biomedical databases and text corpora. BMC Proc., 9, 1–3.Graves,A. and Schmidhuber,J. (2005) Framewise phoneme classiﬁcation withbidirectional LSTM and other neural network architectures. Neural Netw.,18, 602–610.Gurulingappa,H. et al. (2010). An empirical evaluation of resources for theidentiﬁcation of diseases and adverse effects in biomedical literature. InProceedings of the 2nd Workshop on Building and evaluating resources forbiomedical text mining, Valletta, Malta, pp. 15–22.Habibi,M. et al. (2016) Recognizing chemicals in patents - a comparative analysis. J. Cheminform., 8, 1–15.Hahn,U. et al. (2010). A proposal for a conﬁgurable silver standard. InProceedings of the 4th Linguistic Annotation Workshop at ACL, Uppsala,Sweden, pp. 235–242.Hastie,T. et al. (2001) The Elements of Statistical Learning, Vol. 1. Springerseries in statistics Springer, Berlin.Hettne,K.M. et al. (2009) A dictionary to identify small molecules and drugsin free text. Bioinformatics, 25, 2983–2991.Hochreiter,S. and Schmidhuber,J. (1997) Long short-term memory. NeuralComput., 9, 1735–1780.Jozefowicz,R. et al. (2016). Exploring the limits of language modeling. arXivpreprint arXiv:1602.02410v2.Kaewphan,S. et al. (2016) Cell line name recognition in support of the identiﬁcation of synthetic lethality in cancer from text. Bioinformatics, 32, 276–282.Kim,J.-D. et al. (2004). Introduction to the bio-entity recognition task atJNLPBA. In Proceedings of the International Joint Workshop on NaturalDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018Biomedical NER with deep learningLanguage Processing in Biomedicine and its Applications, Geneva,Switzerland, pp. 70–75.Krallinger,M. et al. (2013) Overview of the chemical compound and drugname recognition (CHEDNER) task. In BioCreative Challenge EvaluationWorkshop, Washington, DC, vol. 2, pp. 2–33.Krallinger,M. et al. (2015a) The CHEMDNER corpus of chemicals and drugsand its annotation principles. J. Cheminform., 7, 1–17.Krallinger,M. et al. (2015b) Overview of the CHEMDNER patents task. InProceedings of the 5th BioCreative Challenge Evaluation Workshop,Sevilla, Spain, pp. 63–75.Kulick,S. et al. (2004) Integrated annotation for biomedical information extraction. In Proceedings of NAACL-HLT, Boston, MA, pp. 61–68.Lafferty,J. et al. (2001) Conditional random ﬁelds: Probabilistic models forsegmenting and labeling sequence data. In Proceedings of the EighteenthInternational Conference on Machine Learning, ICML, pp. 282–289.Lample,G. et al. (2016) Neural architectures for named entity recognition. InProceedings of NAACL-HLT, San Diego, CA, pp. 260–270.Leaman,R. and Gonzalez,G. (2008) BANNER: an executable survey of advances in biomedical named entity recognition. In Proceedings of the PaciﬁcSymposium on Biocomputing, Big Island, Hawaii, USA, pp. 652–663.Leaman,R. and Lu,Z. (2016) TaggerOne: Joint named entity recognition andnormalization with Semi-Markov models. Bioinformatics, 2839–2846.Leaman,R. et al. (2009). Enabling recognition of diseases in biomedical textwith machine learning: corpus and benchmark. In Proceedings of theSymposium on Languages in Biology and Medicine, Seogwipo-si, JejuIsland, South Korea, pp. 82–89.Leaman,R. et al. (2013) DNorm: disease name normalization with pairwiselearning to rank. Bioinformatics, 29, 2909–2917.Leaman,R. et al. (2015) tmChem: a high performance approach for chemicalnamed entity recognition and normalization. J. Cheminform., 7.Leser,U. and Hakenberg,J. (2005) What makes a gene name? Named entityrecognition in the biomedical literature. Brief. Bioinform., 6, 357–369.Li,G. et al. (2015) miRTex: A text mining system for miRNA-gene relation extraction. PLoS Comput. Biol., 11, 1–24.Li,J. et al. (2016) BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database 2016.Liu,S. et al. (2015a) Drug name recognition: approaches and resources.Information, 6, 790–810.Liu,S. et al. (2015b) Effects of semantic features on machine learning-baseddrug name recognition systems: word embeddings vs. manually constructeddictionaries. Information, 6, 848–865.Mackin,R. (1978) On collocations: Words shall be known by the companythey keep. In Honour of as Hornby, pp. 149–165.Narayanaswamy,M. et al. (2003) A biological named entity recognizer. InProceedings of the Paciﬁc Symposium on Biocomputing, Lihue, Hawaii,USA, pp. 427–438.Neves,M. et al. (2012) Annotating and evaluating text for stem cell research.In Proceedings of the 3rd Workshop on Building and Evaluation Resourcesfor Biomedical Text Mining at Language Resources and Evaluation,Istanbul, Turkey, pp. 16–23.i45Okazaki,N. (2007) CRFsuite: a fast implementation of conditional randomﬁelds (CRFs).Paﬁlis,E. et al. (2013) The SPECIES and ORGANISMS resources for fast andaccurate identiﬁcation of taxonomic names in text. PLoS One, 8, 1–6.Pascanu,R. et al. (2014) How to construct deep recurrent neural networks. InProceedings of the 2nd International Conference on Learning Representations,Banff, Canada.Pyysalo,S. et al. (2007) BioInfer: a corpus for information extraction in thebiomedical domain. BMC Bioinformatics, 8.Pyysalo,S. et al. (2013) Distributional semantics resources for biomedical textprocessing. In Proceedings of the 5th International Symposium onLanguages in Biology and Medicine, Tokyo, Japan.Rabiner,L.R. (1989) A tutorial on hidden markov models and selectedapplications in speech recognition. Proceedings of the IEEE, Vol. 77,pp. 257–286.Rockt€schel,T. et al. (2012) ChemSpot: a hybrid system for chemical namedaentity recognition. Bioinformatics, 28, 1633–1640.Segura-Bedmar,I. et al. (2015). Combining conditional random ﬁeldsand word embeddings for the CHEMDNER-patents task. In Proceedingsof the 5th BioCreative challenge evaluation workshop, Sevilla, Spain,pp. 90–93.Settles,B. (2005) ABNER: an open source tool for automatically tagging genes,proteins and other entity names in text. Bioinformatics, 21, 3191–3192.Smith,L. et al. (2008) Overview of BioCreative II gene mention recognition.Genome Biol., 9, 1–19.Stenetorp,P. et al. (2012). Size (and domain) matters: Evaluating semanticword space representations for biomedical text. In Proceedings of the 5thInternational Symposium on Semantic Mining in Biomedicine, Zurich,Switzerland, pp. 42–49.Tang,B. et al. (2014) Evaluating word representation features in biomedicalnamed entity recognition tasks. BioMed Res. Int., 2014, 1–6.Thole,U. et al. (1979) On the suitability of minimum and product operatorsfor the intersection of fuzzy sets. Fuzzy Sets Syst., 2, 167–180.Tjong Kim Sang,E.F. and De Meulder,F. (2003). Introduction to the CoNLL-2003shared task: Language-independent named entity recognition. In Proceedings ofthe 7th conference on Natural language learning at HLT-NAACL, Edmonton,Canada, pp. 142–147.€Uzuner,O. et al. (2011) 2010 i2b2/va challenge on concepts, assertions, and relations in clinical text. J. Am. Med. Inform. Assoc., 18, 552–556.Verspoor,K. et al. (2013) Annotating the biomedical literature for the humanvariome. Database 2013.Wang,X. et al. (2010) Disambiguating the species of biomedical named entities using natural language parsers. Bioinformatics, 26, 661–667.Wang,Z.-Y. and Zhang,H.-Y. (2013) Rational drug repositioning by medicalgenetics. Nat. Biotechnol., 31, 1080–1082.Wei,C.-H. et al. (2015) Overview of the BioCreative V chemical disease relation (CDR) task. In Proceedings of the 5th BioCreative challenge evaluationworkshop, Sevilla, Spain, pp. 154–166.Zhou,X. et al. (2014) Human symptoms-disease network. Nat. Commun., 5.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018a) Precision scoresChemicalsCHEMDNER patentCHEBIBioSemanticsCHEMDNERCDRGenes/ProteinsCHEMDNER patentBioCreative II GMJNLPBACellFinderOSIRISDECAVariomePennBioIEFSU-PRGEIEPABioInfermiRNADiseasesNCBI DiseaseCDRVariomeArizona DiseaseSCAImiRNASpeciesS800CellFinderVariomeLocTextLinneausmiRNACell linesCLLGellusCellFinderJNLPBAb) Recall scoresChemicalsCHEMDNER patent84.5280.5982.1790.491.0467.0176.4573.2874.3977.9276.8990.0888.1487.0488.2891.5783.4584.8183.5485.4279.5482.8581.5677.3393.1882.9895.4598.2692.6771.8893.694.1264.8CRF83.0982.5671.4672.5683.2389.1967.0279.9272.7481.9883.3381.8990.3288.1287.7990.6894.1687.3580.1180.6178.4276.1176.9979.0177.668232.1784.7887.472.5483.3393.1898.7265.34Baseline84.53Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 201883.9984.2978.5682.190.5291.4767.3377.0773.2177.2776.8876.689.5688.3286.7187.0891.0580.3184.2583.2684.6878.4181.4777.976.5589.981.2592.596.2185.0673.9786.2595.4564.19CRF (i)84.1484.4278.6881.9390.6691.8167.6677.2773.3984.0581.3878.189.9289.587.3390.1192.3885.1184.8783.3685.1780.9181.5180.2776.890.2283.3395.6597.7394.1977.7893.5597.0164.11CRF (ii)84.1484.2979.6781.9690.7992.3767.1377.2273.6583.7778.6177.6690.1289.5887.288.8992.0184.6985.1883.7285.2879.8981.379.5476.891.383.3394.3797.6394.1681.9793.4296.6764.11CRF (iii)86.2381.0971.5880.6487.3188.1961.0573.7971.8678.4974.0764.1187.6383.3181.6991.0187.7370.1581.779.2483.1372.6467.480.3969.6781.9850.4694.7493.2391.6776.4770.8989.4751.48LSTMCRF90.4182.2571.182.0787.2791.1264.0277.2273.0782.0176.5267.2287.6486.5185.0984.6790.8769.9486.1681.0683.4274.5274.9777.4473.3983.7666.2793.4891.5489.2982.4385.6487.3454.2LSTMCRF (i)87.7283.3572.9579.7288.2592.1865.7478.9974.7783.5676.7372.9186.5988.0286.0386.691.717586.4383.385.1279.1577.2180.8472.6781.6762.9290.3692.4986.1377.6390.2397.0659.83LSTMCRF (ii)87.5383.3369.580.8887.8392.5766.2377.574.8388.7378.0275.2787.4786.9787.2688.0192.6475.9585.3184.1984.5176.6478.4680.8674.5579.035991.0193.5787.8886.8489.5392.5261.73LSTMCRF (iii)Table A1. Precision, recall and F1-scores obtained for each corpus by the baselines and eight variants of the CRF and the LSTM-CRF methodsWe have provided precision, recall and F1-scores of each NER model for each corpus in Table A1.Appendix A85.2884.6671.382.489.9794.6167.379.676.5991.198076.7689.4189.3489.789.2194.7278.4787.4984.6386.2178.6280.182.8175.9580.5161.0592.9495.2792.9587.6791.4693.1463.54LSTM-CRF(iii) 5%82.1986.1472.8583.8191.7795.9868.3281.578.1693.1281.7378.6790.7291.2191.5189.3595.8680.3889.186.487.6281.1781.6183.9277.2783.0462.2292.5996.3894.5991.391.6195.8864.75LSTM-CRF(iii) 10%(continued)78.887.4474.5785.1193.4797.1669.2983.8179.7195.2882.7480.0591.3393.1192.8890.7696.7680.8290.488.1788.8982.8983.4985.5978.9983.0264.7193.4296.9994.2990.7794.5695.666.95LSTM-CRF(iii) 15%i46M.Habibi et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 2018Genes/Proteinsc) F1-scoresChemicalsCell linesSpeciesDiseasesGenes/ProteinsCHEMDNER patentCHEBIBioSemanticsCHEMDNERCDRCHEMDNER patentBioCreative II GMJNLPBACellFinderOSIRISCHEBIBioSemanticsCHEMDNERCDRCHEMDNER patentBioCreative II GMJNLPBACellFinderOSIRISDECAVariomePennBioIEFSU-PRGEIEPABioInfermiRNANCBI DiseaseCDRVariomeArizona DiseaseSCAImiRNAS800CellFinderVariomeLocTextLinneausmiRNACLLGellusCellFinderJNLPBATable A1. (continued)83.5369.0975.3584.1589.0168.477.4972.7571.3572.0666.8778.3785.0888.8369.8575.2172.7763.1763.4958.2493.5182.6184.1485.1889.4472.9379.6978.3674.6659.370.679.4169.0887.8598.4886.6664.5671.1577.9249.860.6356.95Baseline83.869.1880.183.985.8765.5771.9673.364.2459.8560.6178.1278.2781.2564.1967.9773.3156.5348.5855.1290.280.5481.2575.3388.4242.7676.6273.2181.257.6448.4565.0555.7977.3659.0968.4879.3789.159.7447.3725.8155.31CRF84.1471.0380.1585.2287.9267.5173.273.5769.6966.0564.8178.2980.5184.6367.6969.6973.9363.4757.8957.792.2481.8282.2778.6789.9854.7779.7476.4781.4762.6359.673.1860.0983.9659.0980.4384.6494.8770.1355.8733.8757.13CRF (i)84.2870.6380.1285.8888.8368.2673.9174.1774.8570.3464.0678.3881.5886.0368.8870.8374.9667.4761.9461.2593.4782.3583.427990.0656.5480.6979.1581.6464.7561.0271.1159.7278.360.6171.7490.7893.5963.6458.752.4259.14CRF (ii)84.2271.9480.1186.0689.3167.6573.9274.3975.5570.5465.5878.3481.886.4668.1770.8975.1468.863.9761.0993.0682.7683.448090.1458.6681.5979.1781.8264.362.0171.9760.9379.2560.6172.8390.3492.9564.9457.4946.7758.47CRF (iii)83.5871.5179.9683.3886.8765.1773.1273.5549.3664.2271.4479.2979.885.5869.972.4675.323656.6867.6891.5682.9884.258190.0649.8275.8676.9884.3865.0556.0770.9362.985.8583.3378.2676.2191.6767.5361.1341.1363.44LSTMCRF86.1475.4182.3286.190.4870.2277.4775.9771.3976.5280.2782.5684.9689.8677.7677.7279.1163.276.5273.4594.5687.0987.5584.6792.7280.5783.0683.1584.8270.876.5578.3764.9592.4583.3393.4894.0396.1579.2267.6155.6567.94LSTMCRF (i)85.4875.6382.0185.6391.0570.8178.5776.9373.1676.4278.5284.4283.1789.9476.7478.1679.2265.0776.1166.8296.685.4488.2388.3391.7879.5182.9283.3886.3270.6570.3476.6469.3592.4584.8581.5287.5395.5176.6263.5679.8466.7LSTMCRF (ii)85.3873.9581.9986.6290.6370.0577.8277.2575.0875.577983.1485.4588.7774.3478.1379.8265.0773.2866.1696.8786.5487.2485.6789.5978.0983.5882.7987.6472.4770.4875.2669.8192.4589.3988.0493.2492.9585.7162.3579.8464.98LSTMCRF (iii)84.9774.0381.4286.5490.269.9677.8677.174.8475.3776.9980.4783.3686.1872.8476.1977.6263.4771.2664.0894.1584.4785.7982.6787.0175.9781.4581.548570.6568.2273.3667.5789.6287.8885.8790.2592.9583.1260.7376.6163.54LSTM-CRF(iii) 5%84.1273.6780.5585.7988.9369.1777.5276.5773.9574.7374.5277.5480.5482.8470.0473.975.0461.3368.8362.2190.4881.782.9178.3383.4173.8578.5678.8781.8269.1465.8270.4265.1487.7484.8581.5286.3989.7481.8257.497561.34LSTM-CRF(iii) 10%(continued)82.973.2979.2784.7387.2568.1877.3375.8173.0373.4272.0574.3777.4879.1867.171.7872.2759.265.9959.7885.9978.7779.4875.3379.569.9675.376.0178.3866.7263.5667.8262.983.0283.3377.1782.0984.6276.6256.2870.1659.9LSTM-CRF(iii) 15%Biomedical NER with deep learningi47Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i37/3953940by gueston 07 January 201868.0691.8885.2785.9287.8491.7379.4979.8979.4676.4966.6673.6579.273.1184.8248.4985.774.2671.8380.5464.9175.1260.86Baseline64.2190.1484.1784.0581.2989.9756.5480.5178.0383.2666.8461.1472.3864.8284.5469.0379.7587.8190.8565.2562.940.5159.68CRF65.8290.8884.9484.4382.6690.5265.1381.9479.7283.0469.6468.8475.4767.3386.8368.4286.0590.0589.77267.815060.46CRF (i)68.6591.6685.7885.3384.1991.267.9482.7381.283.3771.9369.7975.4167.1983.8470.1881.9994.1393.897072.1468.0661.52CRF (ii)68.3991.5786.0485.2784.2191.0769.3183.3581.3883.5171.2570.3575.5767.9584.8570.1882.2193.8493.5572.4671.1863.0461.16CRF (iii)65.8589.5583.1482.9585.7188.8858.2678.6778.183.7568.6461.2275.3766.1183.8762.8685.7183.8691.6771.7265.6556.3556.84LSTMCRF70.290.9786.886.384.6791.7974.8884.5882.0984.1172.6175.7577.968.9187.8973.8393.4892.7792.5980.7975.5767.9860.3LSTMCRF (i)69.7391.3286.7187.1287.4691.7577.1984.6483.3485.7174.6673.6178.6970.9786.7372.2685.7189.9490.5877.1274.5887.6163.08LSTMCRF (ii)70.4291.9386.7587.2586.8291.097784.4483.4986.0574.4974.2677.9672.185.2271.0889.593.490.3486.2773.5185.7163.31LSTMCRF (iii)69.8591.7286.8487.785.8190.777.284.3683.0685.674.4273.6877.871.5184.8272.0589.2792.792.9585.3372.9984.0763.54LSTM-CRF(iii) 5%69.4890.686.198783.4889.2176.9883.582.4684.6274.6772.8776.5870.6985.3271.7986.7191.1192.1186.370.6584.1663LSTM-CRF(iii) 10%68.4588.5885.3485.6682.3387.297582.1681.6483.373.9372.1775.6870.0383.0272.8584.5288.9289.1983.170.5680.9363.23LSTM-CRF(iii) 15%CRF and LSTM-CRF stand for the CRF and the LSTM-CRF methods without word embedding features. The other ones represented by numbers are using one of the three word embeddings: (i) Patent, (ii) PubMed-PMC and(iii) Wiki-PubMed-PMC. The ones represented by percentage numbers indicate the performance of the method after removing a percentage of predicted entities with lower conﬁdence values.Cell linesSpeciesDiseasesDECAVariomePennBioIEFSU-PRGEIEPABioInfermiRNANCBI DiseaseCDRVariomeArizona DiseaseSCAImiRNAS800CellFinderVariomeLocTextLinneausmiRNACLLGellusCellFinderJNLPBATable A1. (continued)i48M.Habibi et al.
5028881962002	PMID28881962	5028881962	https://watermark.silverchair.com/btx227.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881962.main.pdf	Applying meta-analysis to genotype-tissueexpression data from multiple tissues to identifyeQTLs and increase the number of eGenesDat Duong,1,* Lisa Gai,1 Sagi Snir,2,3 Eun Yong Kang,1 Buhm Han,4,5Jae Hoon Sul6,† and Eleazar Eskin1,7,*,†1Department of Computer Science, University of California, Los Angeles, CA 90095, USA, 2Institute of Evolution,University of Haifa, Haifa 3498838, Israel, 3Department of Evolutionary and Environmental Biology, University ofHaifa, Haifa 3498838, Israel, 4Department of Convergence Medicine, University of Ulsan College of Medicine,Seoul 05505, Republic of Korea, 5Asan Institute for Life Sciences, Asan Medical Center, Seoul 05505, Republic ofKorea, 6Department of Psychiatry and Biobehavioral Sciences, University of California, Los Angeles, CA 90095,USA and 7Department of Human Genetics, University of California, Los Angeles, CA 90095, USA*To whom correspondence should be addressed.†The authors wish it to be known that, in their opinion, the last two authors should be regarded as Joint Authors.AbstractMotivation: There is recent interest in using gene expression data to contextualize findings fromtraditional genome-wide association studies (GWAS). Conditioned on a tissue, expression quanti-tative trait loci (eQTLs) are genetic variants associated with gene expression, and eGenes are geneswhose expression levels are associated with genetic variants. eQTLs and eGenes provide greatsupporting evidence for GWAS hits and important insights into the regulatory pathways involvedin many diseases. When a significant variant or a candidate gene identified by GWAS is also aneQTL or eGene, there is strong evidence to further study this variant or gene. Multi-tissue gene ex-pression datasets like the Gene Tissue Expression (GTEx) data are used to find eQTLs and eGenes.Unfortunately, these datasets often have small sample sizes in some tissues. For this reason, therehave been many meta-analysis methods designed to combine gene expression data across manytissues to increase power for finding eQTLs and eGenes. However, these existing techniques arenot scalable to datasets containing many tissues, like the GTEx data. Furthermore, these methodsignore a biological insight that the same variant may be associated with the same gene acrosssimilar tissues.Results: We introduce a meta-analysis model that addresses these problems in existing methods.We focus on the problem of finding eGenes in gene expression data from many tissues, and showthat our model is better than other types of meta-analyses.Availability and Implementation: Source code is at https://github.com/datduong/RECOV.Contact: eeskin@cs.ucla.edu or datdb@cs.ucla.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionExpression quantitative trait loci (eQTLs) studies find eQTLs, whichare genetic variants associated with gene expression, and eGenes,which are genes whose expression levels are associated with at leastone genetic variant. eQTL studies are related to traditional genome-wide association studies (GWAS) which find variants associatedwith disease.Both eQTL studies and GWAS focus on single nucleotide poly-morphisms (SNPs). Many SNPs found by GWAS are located inintergenic regions, and their relationship to the disease phenotype isoften not obvious. Gene expression is an intermediate phenotype be-tween a causal SNP and a disease (Huang et al., 2014). Thus, eQTLstudies may provide biological insights into the mechanism throughwhich disease occurs. If a significant SNP identified by GWAS isfound to be an eQTL, there is a strong evidence to further study theVC The Author 2017. Published by Oxford University Press. i67This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i67–i74doi: 10.1093/bioinformatics/btx227ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i67/3953939by gueston 07 January 2018variant. For this reason, top hits in GWAS that are also eQTLs areof special interest. In fact, recent GWAS have confirmed that manydisease-causing variants are eQTLs (Albert, 2016; Liu et al., 2016;Nieuwenhuis et al., 2016). Similarly, genes near GWAS-significantSNPs that are identified as eGenes may warrant further study as can-didate causal genes. Thus, eQTL studies provide great supportingevidence for GWAS results and important insights into the regula-tory pathways involved in many diseases.The underlying approach behind eQTL studies and GWAS is thesame. In an eQTL study, one performs association tests between thegenotype data and the gene expression (instead of disease statuses) toidentify variants that are associated with the gene expression. eQTLsand eGenes may be specific to only one or a group of tissues, as a geneis not always uniformly expressed in every tissue. For example, SNPsassociated with schizophrenia have been found to be eQTLs in only thebrain tissues, indicating that schizophrenia affects how the brain func-tions (Fromer et al., 2016). For this reason, there have been recentlarge-scale studies to collect gene expression data in many tissues, suchthe Genotype-Tissue Expression (GTEx) project (The GTExConsortium, 2015). This GTEx dataset contains gene expression datain 44 tissues and genotypes of 5 million SNPs for over 300 individuals.To find eQTLs from the GTEx data and other multi-tissue data-sets, one can apply the traditional tissue-by-tissue (TBT) approach,in which a separate eQTL study is done for each tissue. However,many tissues do not have enough samples to detect SNPs that areweakly associated with the gene expressions. To address this issue,there have been many efforts in developing different types of meta-analysis, which gather data from many tissues to increase the totalsample size and power to detect eQTLs. Two notable methods areMeta-Tissue and eQTLBma. Both have been shown to outperformthe traditional TBT method (Flutre et al., 2013; Sul et al., 2013).Meta-Tissue and eQTLBma have an important limitation thatreduces their applicability to large gene expression datasets such asthe GTEx data. Both methods are computationally intensive andshould used for datasets containing at most 10 or 20 tissues, respect-ively (Flutre et al., 2013; Sul et al., 2013). Meta-Tissue uses both lin-ear mixed models (LMMs) and fixed (or random) effects meta-analysis to combine data from many tissues. Meta-Tissue must esti-mate the variance components in its LMM setup for every pair ofvariant and gene expression; thus, its runtime is impractical whenthere are thousands of genes or too many tissues (Sul et al., 2013).eQTLBma uses a Bayesian approach that considers all possible com-binations of tissues in which a SNP is an eQTL. This setup corresponds to2T configurations where T is the number of tissues, making the methodinfeasible when T is 44 like in the GTEx data (Flutre et al., 2013).As an alternative to Meta-Tissue and eQTLBma, the GTEx con-sortium used a meta-analysis software called Metasoft, introducedby Han and Eskin (2011). Metasoft is equivalent to Meta-Tissuewithout the LMM setup (Han and Eskin, 2011, 2012). Metasoft ex-tends the random effects (REs) meta-analysis model; this extendedmodel is named RE2 (Han and Eskin, 2011).However, Meta-Tissue, eQTLBma and RE2 assume that a SNPhas independent effect on a given gene’s expression in each tissue.This ignores the fact that the same SNP tends to have similar effectsin related tissues (The GTEx Consortium, 2015).Recently, Acharya et al. (2016) introduced a method thatamends this shortcoming in Meta-Tissue, eQTLBma and RE2. Themodel developed by Acharya et al. (2016) requires genotype andgene expression data for each individual in each tissue. Their imple-mentation in R, using the JARGUAR library, requires loading allthese data into memory. When there are many genes and tissues, thisapproach can be memory intensive.In this article, we present a novel meta-analysis method namedRECOV. Unlike Meta-Tissue and eQTLBma, RECOV is applicableto large gene expression datasets and can analyze all 44 tissues inthe GTEx data. Like JARGUAR, RECOV considers the biologicalinsight that a variant may have similar effects on a gene across tis-sues. However, unlike JARGUAR, RECOV needs only the summarystatistic (i.e. SNP effect and its variance) at each SNP in each tissueand not the complete genotype and gene expression data for each in-dividual. RECOV is based on the RE2 meta-analysis framework anduses a covariance (COV) matrix to explicitly model the correlationof a SNP effect on the same gene’s expression in similar tissues.In the Section 2, we describe RECOV in detail and demonstratehow it can be used to identify eGenes from eQTL studies in morethan one tissue. In the Results section, we use simulated datasets toshow that RECOV has correct false positive rate (FPR). We thenapply RECOV to real multi-tissue expression data from the GTExdataset. Our results show that RECOV detects more eGenes thanprevious RE2 and TBT methods.2 Materials and methodsWe begin by introducing the notations in this article. We use x 2 Rnto specify a vector x with dimension n, and Z 2 Rn m to specify amatrix Z with dimension n   m. We use xi to denote the ith elementin x, and likewise, Zij to specify entry ij in Z. We denote an item k inthe set K by k 2 K, and a set fa1 . . . aKg indexed by k by usingfakgk2K, where the subscript k 2 K is omitted whenever the contextis clear. The size of the set K is denoted as jKj.2.1 Detecting one eGene via an eQTL study2.1.1 eQTL study in one tissueWe begin with an eQTL study in one tissue t. An eQTL study findsevery eQTL associated with the expression level of a specific gene g.To do this, the study tests each variant v in the set V against the ex-pression of g in a sequential fashion. To set up the problem, supposewe represent the gene expression for m individuals in tissue t as avector q 2 Rm, and we want to find the effect of variant v on g. Let s2 Rm be the standardized genotypes of this v. The eQTL study as-sumes the following modelq ¼ bvgtsþ  vgt (1)where   2 Rm is the vector of sampling errors     N 0; r2  I   , and bvgt2 R is the true effect size of the variant v on g in tissue t (Eskin,2015). The estimate bvgt of the true value bvgt can be computedusing the basic least squares equation bvgt ¼ argminbvgt jjq  bvgtsjj22(Abraham and Ledolter, 2006). This solution isbvgt ¼ s>s    1s>q where bvgt   N bvgt; s>s    1r2    (2)By using Equation (2) and writing the null hypothesis H0: bvgt ¼ 0,one can do a hypothesis test to assert if v has an effect on g. To dothis test, compute the estimate br2 vgt of r2 vgt bybr2 vgt ¼1m  1Xmi¼1 qi   bvgtsi   2 (3)and estimate the variance dvgt of bvgt bydvgt ¼ s>s    1br2 vgt (4)then compute the p-value pvgt ¼ p  value bvgt   (Abraham andLedolter, 2006; Eskin, 2015). If pvgt is less than some significancelevel, then we reject H0: bvgt ¼ 0, and conclude that v is an eQTL ofi68 D.Duong et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i67/3953939by gueston 07 January 2018g in tissue t. When many variants are tested, we must apply a mul-tiple testing correction; for example, one can apply Bonferroni cor-rection by using the threshold a=jVj where a is the significance levelfor the whole family of tests. The Bonferroni correction is conserva-tive when there is linkage disequilibrium (LD) in set V. There existother methods that can handle LD better than the Bonferroni correc-tion (Darnell et al., 2012; Joo et al., 2014, 2016; Hormozdiari et al.,2015).2.1.2 Using an eQTL study in one tissue to discover one eGeneBecause an eQTL study tests each variant v 2 V against gene g in atissue t, from a single eQTL study we have a set of p-valuesfpvgtgv2V . The minimum pgt ¼ minv2Vfpvgtg is defined to be theobserved eGene statistic at gene g in tissue t (The GTExConsortium, 2015). Define apgt ¼ p  value pgt   to be the eGenep-value (The GTEx Consortium, 2015). The eGene p-valuedepends on two important factors: the number of variants jVj,and the LD of the variants. In practice, apgt is computed by doing apermutation test (Sul et al., 2015; Duong et al., 2016). In brief, inthe kth permutation, one would permute the gene expressionsamong the individuals, and compute a new p kð Þgt ¼ minv2Vfpkð Þvgtg. apgtis the ranking of the observed pgt with respect to the densitycreated from many p kð Þgt . If its eGene p-value apgt is less thansome desired threshold, one can then conclude that g is an eGenein tissue t.2.2 TBT analysis to find one or many eGenesWhen there are genotype-tissue expression data from many tissues,TBT analysis is the standard method to find eGenes (Sul et al., 2013;The GTEx Consortium, 2015). TBT tests whether or not the genehas at least one eQTL in each tissue by examining each tissue indi-vidually. Suppose the gene is expressed in T tissues. Then TBT per-forms T eQTL studies (one test in each tissue). The null hypothesisis that the gene is not an eGene in any tissue. This hypothesis isequivalent to saying that no eQTL is found for this gene in anytissue.Three layers of multiple testing correction are required sinceTBT performs one test per gene in each tissue. The first layer ofmultiple testing correction is applied within a tissue and correctsfor LD of the variants tested against the gene. This correction canbe done by using the permutation test to compute the eGenep-value for the gene in the tissue (Duong et al., 2016; Sul et al.,2013, 2015).The second layer of multiple testing correction adjusts for thefact that we may test more than one gene within a tissue. For ex-ample, the GTEx pilot study tested thousands of genes within onetissue, and then transformed eGene p-values into eGene q-values tocontrol for this multiple testing (Dabney et al., 2010; The GTExConsortium, 2015). This second layer of multiple testing correctionis not needed if only one gene is tested in each tissue.The third layer of multiple testing correction takes into accountthe fact that one gene is tested T number of times (once per tissue)(Sul et al., 2013). In this article, we apply Bonferroni correction sothat the false-positive threshold for any eGene q-value in each tissueis a=T, where a is 5% for example. In this layer, other multiple test-ing correction methods such as the Benjamini-Hochberg correctioncan be used instead of the Bonferroni correction. However, thispaper focuses on the meta-analysis model, and measuring the per-formances of various multiple testing correction methods is beyondits scope.2.3 Meta-analysis models for combining eQTL studiesacross tissuesWe motivate the application of meta-analysis for combining eQTLstudies across tissues. An eQTL is defined not only with respect to agene, but also with respect to the tissue in which the gene expressionis measured. eQTL studies of the same gene have been analyzed sep-arately at the tissues level (The GTEx Consortium, 2015). We canbetter detect the effect of a variant on the gene by combining eQTLresults across many tissues and modeling the relatedness of the effectsizes of one variant among the tissues.When using meta-analysis to find many eGenes, it is importantto emphasize that one would need only two layers of multiple testingcorrection. The first layer is applied within a gene to correct for LDbecause one tests many variants against the gene. The second layeris applied at the gene level because there is usually more than onegene being tested.We define the notations to be used later. Suppose we have TeQTL studies (one study per tissue) that test the association of avariant v at a gene g. As before, denote the effect of this variant inthe study (i.e. tissue) t as bvgt, where bvgt is computed usingEquation (2). Denote the variance of bvgt in the study t as dvgt wheredvgt is computed using Equation (4). Let bvg 2 RT contain the ef-fects in these T studies, so that bvg ¼ bvg1 bvg2       bvgT  >. LetDvg ¼ diag dvg1 . . .dvgT   .2.3.1 REs and the RE2 modelThe maximum likelihood procedure in the RE model assumes that bvghas the form (Han and Eskin, 2011; Thompson and Sharp, 1997)bvg ¼ kvg þ  vg (5)The random sampling errors  vg are estimated from the data andassumed to be  vg   N 0;Dvg   . kvg 2 RT in the RE model is a ran-dom variable, i.e. kvg   N lvg1; s2vgI   with lvg 2 R and s2vg 2 Rþ.Here the number 1 denotes a vector with all entries equal to 1. Theeffect kvg is thus known as the random effect. lvg is the commontrue underlying effect that all the studies inherit. The term s2vg is theheterogeneity among the effects of the variant in T tissues.Clearly, bvg comes from the distributionbvg   N lvg1; s2vgIþDvg   (6)The traditional RE model assumes that if the effect of the variantdoes not exist in any tissue, then lvg ¼ 0. However, it has beenshown that this traditional null hypothesis does not yield optimalstatistical power in detecting eQTLs (Han and Eskin, 2011, 2012).For this reason, the RE2 model assumes a different null hypothesis,that if the effect of the variant does not exist in any tissue, then lvg¼ 0 and s2vg ¼ 0. The fact that s2vg ¼ 0 is a result of lvg ¼ 0 becausewhen the effect does not exist, its variance must not exist (Han andEskin, 2011, 2012; Kang et al., 2014). We will compare our methodagainst the RE2 model.The null hypothesis H0 in RE2 isH0 : lvg ¼ 0 s2vg ¼ 0 (7)The log likelihood ratio for testing this hypothesis becomesllrvg ¼ 2 logsuplvg ;s2vg L bvgjlvg; s2vg   L bvgjlvg ¼ 0; s2vg ¼ 0    (8)The function L denotes the likelihood function of the random vari-able bvg. The numerator suplvg ;s2vg L bvgjlvg; s2vg   may be estimatedMeta-analysis on multiple tissues i69Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i67/3953939by gueston 07 January 2018using numerical methods or other heuristic methods. Here, we applythe Nelder-Mead method, which is a heuristic derivative-free searchmethod.In finding the supremum, one implicitly enforces s2vg   0. Due tothis restricted parameter space, the asymptotic density of the likeli-hood ratio is an average of a v21 and v22 (Self and Liang, 1987; Hanand Eskin, 2011). To find the p-value of this likelihood ratio whenT is large, one can use this asymptotic density.Otherwise, one may compute the likelihood ratio p-value by creat-ing a density of likelihood ratios under the null hypothesis and rank-ing the observed likelihood ratio with respect to this density. This nulldensity is made by sampling many instances of bvg using Equation (6)with lvg ¼ s2vg ¼ 0, and computing their corresponding llrvg. If thep-value of llrvg is significant, then v is an eQTL with respect to g in atleast one tissue. Because we have 44 tissues in the GTEx data, we willuse the asymptotic distribution of the likelihood ratio.2.3.2 RECOV: REs model with a COV termHere we present an extension to the RE model. We firstdiscuss the COV term. Equation (5) of the RE model assumes kvg  N lvg1; s2vgI   so that the effects of variant v toward gene g are in-dependent across the tissues. However, tissues from the same bodypart are similar; in fact, many eQTLs are found to be consistentamong many tissues (Flutre et al., 2013). From this observation, wemust acknowledge that kvg   Nðlvg1;RvgÞ where Rvg is not diag-onal. The term Rvg 2 RT T models the COV of effect sizes ofv among tissues conditioned on the gene g. The matrix Rvg containsT   T unknown parameters which are to be estimated. In practice,one has to assume a simpler form for Rvg. Here, we assumeRvg  cvgUvg. The matrix Uvg is estimated from the data. The termcvg   0 is an unknown scaling constant and is optimized jointly withthe mean of regression coefficient lvg.In this article, we compute the Uvg at each variant-gene pair asfollows. Denote Bg ¼ b1g b2g       bjVjg  so that Bg 2 RT jVj. Acolumn in Bg contains the effects of a variant in 44 tissues. To avoidreusing the data when testing a single SNP, we remove its effects inthe 44 tissues when estimating its COV term. To do this, we divideall cis-variants of g into 10 separate segments according to theirphysical locations on the chromosome, and use the 9 segments thatdo not contain v to compute Uvg. In particular, denote B vg as thematrix Bg without the effect sizes of the variants that belong to thesame segment as v. Uvg can be estimated as Uvg ¼ B vgB> vg (afterproper scaling is applied to B vg). This computation is similar tohow one would compute a kinship matrix using the genotype matrix(Eskin, 2015). In this scheme, we observe that the variants in strongLD with v are also removed, so that there are fewer vectors in B vgthat resemble bvg when computing Uvg. This further helps reducingthe problem of data reusing. Supplementary Table S2 shows that byremoving SNPs in the same segment as v, we retain fewer SNPs thatare in strong LD with v.Now, we are ready to introduce this COV term Uvg to the REmodel. We extend the RE model so that when testing a variant vagainst gene g, we havebvg ¼ kvg þ  vg (9)wherekvg   N lvg1; cvgUvg    vg   N 0;Dvg   (10)Like before, the matrix Dvg is known because it contains theobserved variances of the SNP effects estimated by Equation (4).This form for Dvg is standard in meta-analysis (Thompson andSharp, 1997). We now havecov bvg   ¼ cvgUvg þDvg (11)bvg   N lvg1; cov bvg      (12)The null hypothesis that v does not affect g in all T tissues isH0 : lvg ¼ 0 cvg ¼ 0 (13)The alternative hypothesis implies that v has an effect in at least oneof the T tissues.Under this setting, the log likelihood ratio to test the hypothesisbecomesllrvg ¼ 2 logsuplvg ; cvg L bvgjlvg; cvg   L bvgjlvg ¼ 0; cvg ¼ 0    (14)Like in the RE model, in finding the supremum in the alternative,one enforces cvg   0. Due to this restricted parameter space, theasymptotic density of the likelihood ratio is an average of a v21 andv22. Alternatively, one can compute the empirical p-value of this like-lihood ratio with a permutation test. In any case, if the p-value ofthe likelihood ratio is significant, then v is an eQTL with respect tog in at least one tissue.2.4 Using meta-analysis of eQTL in many tissues toidentify eGenesIn practice, a set of variants V is tested against g. Here we describehow one can combine the meta-analysis result at each variant v 2 Vto determine if g is an eGene.Define pvg ¼ p   value llrvg   so that from many variants, wehave the set of p-values fpvggv2V . The observed statistic at gene g ispg ¼ minv2Vfpvgg. To determine if pg is significant, one needs tocompute its eGene p-value apg (The GTEx Consortium, 2015). Tocontrol for multiple testing when LD exists between the variants,one can compute apg using a permutation test (Duong et al., 2016;Sul et al., 2015; The GTEx Consortium, 2015). The permutationtest creates a distribution of the observed pg under the null hypoth-esis, which can then be used to compute the eGene p-value apg of pg.This permutation test can be done as follows. Let K be the num-ber of permutations. In the kth permutation, permute the gene ex-pression of g among the individuals in each of the T tissues so thatthere are T permuted datasets. This permutation reflects the hypoth-esis that the gene is not an eGene in any tissue. Next, redo the meta-analysis at each variant v 2 V so that a new p kð Þg ¼ minv2Vfp kð Þvg g iscomputed. apg is the fraction of times the observed pg is less thanp kð Þg . The gene g is an eGene in at least one tissue if its eGene p-valueapg is below some threshold a.In the pilot GTEx analysis, a set of genes G is being tested atonce, so that one has a set aG ¼ fapggg2G. To control for the familywise error rate, one can apply Bonferroni correction to get thethreshold a=jGj. Any gene g 2 G with apg < a=jGj is an eGene in atleast one of the T tissues.2.4.1 Estimating eGene p-valueThe permutation test above must be performed at every pair of vari-ant v 2 V and gene g 2 G in a tissue t. The entire permutation testrequires KjVjjGjT permuted datasets, which is time consuming.Here, we introduce an alternate method to estimate the eGene p-value. In essence, the permutation test estimates a function f thati70 D.Duong et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i67/3953939by gueston 07 January 2018maps a test statistic to its p-value. There is evidence that the correl-ation of test statistics at two variants is equal to their LD (Hanet al., 2009). This holds true when the test statistics are effect sizes(Han et al., 2009).In our meta-analysis, to properly estimate the eGene p-value apg ,we must consider the effect of LD in the set of variants V on theobserved statistic at each variant v. At each variant, it does not mat-ter whether we treat its llrvg or its pvg as the observed statistic, be-cause the likelihood ratio and its p-value are two equivalent entitiesfor two reasons. First, the likelihood ratio of each variant v has thesame distribution and degree of freedom. Second, the p-value func-tion is 1-to-1 and strictly monotone. Thus, having a null density forthe maxv2Vfllrvgg is equivalent to having a null density for the min-imum pg.We empirically find that on average the correlation of the likeli-hood ratios at any two variants is roughly equal to their LD; that is,on average cor llrug; llrvg    LD u; vð Þ for any variant u; v 2 V(Fig. 1). For this reason, any function f that accounts for LD andmaps an observed test statistic of a gene into an eGene p-valuewould be applicable in our case. We can use such a function to con-vert the observed test statistics pg at the gene g to eGene p-value apgwithout doing the permutation test. Each gene g has its own LDstructure and requires its own function f, because the cis-variants ofeach gene are non-identical.We apply MVN-EGENE to estimate the function f for eachgene. MVN-EGENE is a software that tests if a gene is an eGene inone tissue. MVN-EGENE is designed so that one does not need todo the permutation test when estimating an eGene p-value. MVN-EGENE is unable to simultaneously consider more than one tissues,as would a meta-analysis would.To compute the function f at a gene, we apply MVN-EGENE atthat gene in a tissue (Sul et al., 2015). We assume that the LD doesnot change much between tissues, and it does not matter muchwhich particular tissue is chosen, as long as it has many samples.In MVN-EGENE, the test statistic for a gene is the most signifi-cant effect size taken over all cis-variants. The p-value of this teststatistic depends on the LD of the cis-variants. Instead of doing apermutation test to compute this p-value, MVN-EGENE simulatesdata under the null hypothesis using a multivariate normaldistribution. In brief, in one simulation, MVN-EGENE samples theeffect sizes of the cis-variants of a gene in a tissue using zero as themean effect and LD as the COV matrix. In this simulation, the mostsignificant effect among these effect sizes is taken to be the test stat-istic at the gene. After many simulations, one can create a null distri-bution for the observed test statistic. One can easily convert aneffect size into a p-value using a normal distribution. By having anull density of the most significant effect size taken over all the vari-ants, one also has the null density of the minimum p-value takenover all the variants. This null density of the minimum p-value inMVN-EGENE properly handles LD at the gene. Here, we use thisdistribution of minimum P-values as our null density to convert theobserved minimum likelihood ratio p-value pg to its eGene P-valueapg in both RECOV and RE2.2.4.2 Estimating genomic controlIn the GTEx dataset and other multi-tissue gene expression datasets,the same individual may provide samples for many tissues (Fig. 2).Sharing of samples from the same individuals among tissues isknown to inflate the FPR in a meta-analysis (Han et al., 2016).Before testing whether the RECOV outcome is affected by the factthat tissues share individuals, we test if RECOV inflates the FPRwhen the data is absolutely free of any spurious statistical associ-ation. These signals can be due to LD, shared individuals in tissues,batch effects, or correlated expressions of the same gene (or betweengenes) across tissues. It is important to mention that in the realGTEx data, batch effects have been dealt with by the GTEx consor-tium by applying PEER factors on the gene expression in each tissue(The GTEx Consortium, 2015). Subsection 2.4.1 above describeshow RECOV and RE2 handle LD in the variants. We now describehow we use a genomic control (GC) factor to remove the effect ofshared individuals in the tissues from the meta-analysis results. ThisGC factor is clearly data dependent as different datasets will requiredifferent GC values.Here, we focus on finding the GC factor for the GTEx data. Todo this, we simulate two types of datasets and compare their behav-iors. The first type does not contain any spurious statistical signals.The second type contains only signal due to sharing of samplesamong the tissues, and the number of people shared between pairsof tissues is taken from the GTEx data. Our goal is to apply RECOVand RE2 to the GTEx data; to avoid data reusing, the SNPs and thegene expressions in both types of datasets are simulated and thus areindependent of the values in the GTEx data.When there is not any spurious statistical association in the data,any alternative hypothesis must be rejected more often than the nullhypothesis. We simulate data to demonstrate that RECOV does notinflate FPR in this case. In each simulated dataset, the number of in-dividuals per tissue is taken from the GTEx data, but we do not lettissues share individuals. We generate 1000 SNPs at various minorallele frequency (MAF) without LD, and a random gene expressionin each tissue. We generate gene expressions where the expression ofthe same gene is not correlated between any two tissues. We com-pute the p-value of the likelihood ratio at each SNP using bothRECOV and RE2 model. We repeat this simulation 1000 times toobtain 1 000 000 p-values each for RECOV and RE2. The histo-grams of these p-values in both RECOV and RE2 indicate that thenull hypothesis is more favored than the alternative hypothesis (Fig.3A and B).To measure the effect strictly caused by shared individuals, wesimulate datasets as above, but now allow tissues to share individ-uals. The number of people shared between pairs of tissues is taken●●●●●●●●●●●●●●●●●●●●●0.0 0.5 1.00.00.51.0Correlation of llr vs LDLDMean correlation of RECOV llr●●●●●●●●●●●●●●●●●●●●●0.0 0.5 1.00.00.51.0Correlation of llr vs LDLDMean correlation of RE2 llrFig. 1. Correlation for the likelihood ratios of a pair of cis-variants versus theirLD. Denote corðllru ; llrv Þ as the correlation for the likelihood ratios of variantsu and v over all genes where both are cis-SNPs. Empirically, corðllru ; llrv Þ isclose to the LD of u and v. To show this, we randomly select many pairs ofcis-SNPs from the gene ENGS00000204219.5 that also appear together in atleast two other genes. These pairs are then grouped into bins by their LD (binwidth 0.05). We compute the likelihood ratio for each SNP in each pair overall the genes in which they are cis-variants. Using these likelihood ratios, weestimate corðllru ; llrv Þ for the pair u, v. We average corðllru ; llrv Þ over all pairsu, v in each LD bin. We then plot the absolute value of this average againstthe LD value. The identity line is shown in red. Plots for additional pairschosen from other genes are shown in Supplementary Figure S1Meta-analysis on multiple tissues i71Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i67/3953939by gueston 07 January 2018from the GTEx data. In each simulation, we compute the likelihoodratio p-values at 1000 SNPs, and repeat the simulation 1000 timesto obtain 1 000 000 P-values. We observe that these p-values shifttoward 0 when the tissues share samples that are from the same indi-viduals (Fig. 3C and D). In this case, we estimate the RECOV andRE2 GC factor to be 1.2947 and 1.1045, respectively. These GCfactors are used to remove the effect caused by shared individuals intissues that may inflate the FPR. To compute a GC factor, one con-verts the median of the observed p-values into a chi-square statistic,then finds a multiplying factor to scale this new statistic to a chi-square random variable that has p-value at 0.50 (Devlin andRoeder, 1999).3 Results3.1 RECOV controls FPRWhen using any meta-analysis method to find an eGene, one needsto apply the method at every cis-variant of the specified gene inorder to determine if that gene has at least one eQTL. Thus, the glo-bal FPR of RECOV and RE2 depends on the FPR at a single cis-variant. For this reason, we measure the FPR of RECOV and RE2 ata single variant.To obtain the FPR at one variant, we simulate 1000 datasets fora single variant under the null hypothesis where the variant is notassociated with the gene expression in any of 44 tissues. The MAFof the SNP is randomly chosen and kept the same in all 1000 data-sets. Then in each dataset, the genotype for this SNP and the geneexpression are simulated independently of the values in the GTExdata.To make the simulated data more realistic, we first let each tissuehave the same number of individuals as in the GTEx data, and eachpair of tissues have the same number of shared samples as in theGTEx data. Second, we set expression levels of the same gene fromthe same individual to be correlated with an average correlation of0.5 across tissues, using the sampling method described in (Sul et al.,2013). This correlation of expression can occur when the tissues ofan individual have been exposed to the same environmental factors.In each of the 1000 datasets, we estimate the effect size and vari-ance of this single variant on the gene expression in each tissue.RECOV and RE2 take these effect sizes and variances and produce ameta-analysis p-value for this variant. The GC factor estimated inSubsection 2.4.2 is used to transform this p-value in each simulation.This removes only the effect of shared individuals, which is not ex-plicitly modeled in RECOV and RE2. The FPR of this single variantis the fraction of times its transformed p-values are significant.We repeat this experiment for 1000 independent variants, sothat we have 1000 measures of FPR for RECOV and RE2. We usethe significance level of 0.05 (a ¼ 0.05). We find that RECOV at-tains correct FPR for the majority of variants tested. In RECOV, themedian FPR among the 1000 variants is 0.05, and the 75 and 95%quantiles are 0.06 and 0.09. In RE2, the median FPR is 0.05, andthe 75 and 95% quantiles are 0.07 and 0.10. These results demon-strate that RECOV and RE2 control the FPRs in a realistic setting.3.2 RECOV discovers more eGenes in GTEx dataWe apply RECOV, RE2 and TBT to the real multi-tissue eQTLdataset from the GTEx consortium. We use GTEx Pilot Dataset V6released on 12 January 2015. The GTEx consortium has performedRNA-seq on 44 tissues from hundreds of individuals, and we select15 336 genes that have expression data in all 44 tissues. The consor-tium has already applied PEER factors to every gene expression ineach tissue to remove any batch effects (The GTEx Consortium,2015). For genotype data, we use the GTEx imputed genotype datathat contains 5 million SNPs for each individual. Like in the originalGTEx pilot study, for each gene, we use its cis-SNPs, which aredefined to be located within 1Mb from its transcription start site(The GTEx Consortium, 2015). Not all variants are genotyped inevery tissue, because the 44 tissues contain samples from differentindividuals. We use only cis-variants that are genotyped in all 44 tis-sues. The median number of cis-variants tested per gene is 3744.For each of the 15 336 genes, we apply RECOV, RE2, and TBTto every cis-SNP. For each cis-SNP of a gene, our test statistic is thelog likelihood ratio (for RECOV and RE2) or SNP-effect (for TBT).These test statistics are converted into p-values by using a chi-squaredistribution (for RECOV and RE2) or normal distribution (forFig. 2. Shared individuals among the 44 tissues in the GTEx dataset. Degreeof sample sharing between two tissues is measured using the Jacquardindex00.510 0.5 1Likelihood ratio p−valuesDensityRECOV. No shared individuals in tissues.A00.510 0.5 1Likelihood ratio p−valuesDensityRE2. No shared individuals in tissues.BC D00.510 0.5 1Likelihood ratio p−valuesDensityRECOV. Shared individuals in tissues.00.510 0.5 1Likelihood ratio p−valuesDensityRE2. Shared individuals in tissues.Fig. 3. (A) RECOV and (B) RE2 applied to datasets where the tissues do notshare individuals. (C) RECOV and (D) RE2 applied to datasets where the tis-sues share individualsi72 D.Duong et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i67/3953939by gueston 07 January 2018TBT). These p-values are then transformed using the GC factors toremove the effect of shared individuals in the tissues. Finally, themost significant p-value among all cis-variants is converted into aneGene p-value by method in 2.4.1 (for RECOV and RE2) or byEGENE-MVN (for TBT).After computing the eGene p-values for 15 336 genes, we useBonferroni correction to control for multiple testing correction at5% level to identify significant eGene p-values; thus, each gene has asignificance threshold of 0.05/15 336.Figure 4A shows the Venn diagram of the numbers of eGenesfound by TBT, RE2 and RECOV. The majority of tested genes arefound to be candidate eGenes. This is expected because there aremany tissues tested. It is likely that a gene contains at least oneeQTLs in some tissue, which significantly increases the total numberof eGenes detected. Both RE2 and RECOV find more candidateeGenes than TBT. This result agrees with previous findings whereapplying meta-analysis to multi-tissue datasets yields better outcomethan the simple TBT approach (Flutre et al., 2013; Sul et al., 2013).RECOV detects the highest number of eGenes among the threemethods. Out of the 15 336 genes tested, RECOV finds that 81.40%of those genes are eGenes while TBT and RE2 find 61.90 and78.45% of genes are eGenes, respectively. This shows that our ap-proach detects 3% more eGenes than RE2 and about 20% moreeGenes than TBT.Next, we apply each method to a case study in order to under-stand the circumstances where one method outperforms the othertwo. We begin with the simple TBT method. In Figure 4A, there are252 genes detected only by the TBT method. Previous publicationshave reported TBT to be the most powerful option to detect geneswith eQTLs that are found in only one tissue (Sul et al., 2013; TheGTEx Consortium, 2015). In the TBT method, one analyzes eachtissue independently, and is able to determine the number of tissuesin which a gene is an eGene. In our result, out of these 252 genes,225 are eGenes in only 1 tissue, 25 are in 2 tissues, and only 2 are in3 tissues. This finding agrees with Figure 2 in Sul et al. (2013).Of the 452 genes discovered by only RECOV, the averageRECOV eGene p-value is 8:52E 9 61:51E 8   ; whereas theaverage RE2 eGene p-value is 4:18E 3 62:85E 2   . To understandwhy RECOV discovers genes that are not found by TBT andRE2, consider the protein-coding gene CABLES1 (Ensembl idENSG00000134508.8) which is only detected by RECOV. From theGTEx portal, CABLES1 is expressed mostly in brain tissues, yet itdoes not have any brain-specific eQTLs. RECOV is a meta-analysismethod that pools samples across tissues to increase signals ofeQTLs. Thus, when the sample size per tissue is small enough thateQTL signals may be undetected, RECOV outperforms TBT. UnlikeRE2, the meta-analysis of RECOV considers correlation of the cis-variants across the tissues; thus RECOV would be better than RE2 ifCABLES1 has a consistent correlation pattern. This is indeed thecase (Fig. 4B). CABLES1’s RECOV and RE2 eGene p-value are 4:94E 13 and 5E 5, respectively.Of the 88 genes discovered by only RE2, the average RE2 eGenep-value is 1:15E 8 61:44E 8   ; whereas the average RECOV eGenep-value is 1:85E 4 62:32E 4   . We suspect that these 88 genes aregenes with eQTLs in multiple tissues. However, due to low samplesize, these eQTLs signals may be undetected or do not produce aneGene q-value less than the significance threshold in TBT analysis.As a case study, consider the protein-coding gene GALNT11(Ensembl id ENSG00000178234.8) which is detected by only RE2.Like CABLES1, GALNT11 is expressed mostly in the brain tissues(The GTEx Consortium, 2015). Unlike CABLES1, GALNT11 haseQTL signals in the frontal cortex brain tissue, but these signals pro-duce an eGene q-value of 0.0189 which is higher than the TBT sig-nificance threshold. In this case, a meta-analysis approach is moresuitable because it combines data from many tissues to improve theeGene p-value. GALNT11’s cis-variants have correlated effect sizesacross the brain tissues, but this pattern does not stand out from therest of the tissues when compared with that of CABLES1 (Fig. 4C).For this reason, GALNT11’s RECOV p-value is higher than its RE2p-value (3:50E 4 versus 7:08E 8). RE2 may also have betterperformance than RECOV in cases where the cis-variants do nothave an obvious correlation pattern across the 44 tissues. As an ex-ample, consider the pseudogene RP11-34P13.16 (Ensembl idENSG00000269981.1), which is not tissue-specific (The GTExConsortium, 2015). The effect sizes of its cis-variants appear to berandomly correlated (Fig. 4D), and its RECOV and RE2 p-valuesare 1:50E 4 and 1:37E 8, respectively. Altogether, these attributesmay have caused the different results produced by RECOV andRE2.4 DiscussionIn this article, we introduce a new REs meta-analysis method namedRECOV. Our approach is motivated by the insight that the sameSNP may have similar effect on the same gene in related tissues. Weexplicitly model these phenomena by adding a COV matrix to theexisting RE2 model introduced by Han and Eskin (2011). Whenapplied to the GTEx data, RECOV controls the FPR at the SNPlevel. More importantly, using no additional data, RECOV finds3% more eGenes than the TBT and RE2 methods.A B C DFig. 4. (A) Venn diagram of the numbers of eGenes found by TBT, RE2 and RECOV. (B) The correlation of SNP-effects for the gene ENSG00000134508.8 in 44tissues (tissue names are omitted). The correlation is computed by using the matrix Bg in Subsection 2.3.2 where the formula is BgB>g (after proper scaling and re-moval of nearby SNPs). Black box indicates the brain tissues. ENSG00000134508.8 is found to be an eGene by only the RECOV method. The correlation of SNP-ef-fects for gene (C) ENSG00000178234.8 and (D) ENSG00000269981.1 in 44 tissues (tissue names are omitted). ENSG00000178234.8 and ENSG00000269981.1 arefound to be eGenes by only the RE2 methodMeta-analysis on multiple tissues i73Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i67/3953939by gueston 07 January 2018RECOV scales well to large numbers of tissues compared withprevious meta-analysis methods for gene expression data. For ex-ample, Meta-Tissue and eQTLBma can only handle up to 10 and 20tissues, respectively (Flutre et al., 2013; Sul et al., 2013). RECOValso requires only the summary statistics for the SNP effect on thegene expression in each tissue. These summary statistics are oftenreadily available in gene expression data. Thus, unlike the model byAcharya et al. (2016), RECOV requires minimal data preparation.RECOV, and the RE2 it extends, require optimizing two param-eters in the log likelihood ratio. These unknowns are the mean effectsize and the scaling factor for the COV matrix, both of which can beestimated using efficient heuristic methods. We note that the TBTmethod avoids this optimization. This is a speed-performance trade-off. This study and others show that the meta-analysis approach isbetter than TBT when applied to multi-tissue data (Acharya et al.,2016; Flutre et al., 2013; Sul et al., 2013;). Unlike TBT, RECOVdoes not provide information about the specific subset of tissues inwhich the gene is an eGene. This problem is inherent to all meta-analysis methods, which only test whether a gene is an eGene in atleast one tissue.Next, we address our use of the GC factor for RECOV. The GCfactor is traditionally used to correct for inflation due to populationstructure in classic GWAS, but in this paper, we use it to correct for in-flation from any unmodeled source. We show that this inflation is dueto tissues containing samples from the same individuals. This problemof sample sharing is not the same as the problem of population struc-ture in GWAS (Han and Eskin, 2011, 2012). The value of the GC fac-tor depends on the choice of the COV matrix Uvg in the model. Asshown in this article, when Uvg ¼ I for RE2 the GC factor is 1.1045,whereas when Uvg ¼ B vgB> vg for RECOV the GC factor is 1.2947.RECOV is a general framework for meta-analysis that can beused with any COV matrix. The COV matrix used in this article(described in Subsection 2.3.2) reflects our assumptions about thebehavior of the same SNP in different tissues. Namely, we assume aSNP has correlated effects on a gene’s expression across tissues.There are many ways to select this COV matrix, and other optionsmay better fit different assumptions about the data. For example, ifwe instead assume the same SNP has correlated effects on the ex-pressions of different genes across the tissues, we can estimate Uvgby combining information from neighboring genes of g, using know-ledge from a gene–gene interaction network. The problem of select-ing the most suitable COV matrix for RECOV is a rich topic forfuture work.FundingD.D., L.G., E.K. and E.E. are supported by National Science Foundation[grants 0513612, 0731455, 0729049, 0916676, 1065276, 1302448,1320589 and 1331176], National Institutes of Health [grants K25-HL080079, U01-DA024417, P01-HL30568, P01-HL28481, R01-GM083198, R01-ES021801, R01-MH101782 and R01-ES022282] andNINDS Informatics Center for Neurogenetics and Neurogenomics [grant P30NS062691]. B.H. is supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government (MSIP) [grant2016R1C1B2013126]. E.E. is supported in part by the NIH BD2K award[grant U54EB020403] D.D. is supported by the NIH Training Grant inGenomic Analysis and Interpretation [grant T32HG002536].Conflict of Interest: none declared.ReferencesAbraham,B. and Ledolter,J. (2006). Introduction to regression modeling.Thomson Brooks/Cole, Belmont, CA, Thomson Brooks/Cole.Acharya,C.R. et al. (2016) Exploiting expression patterns across multiple tis-sues to map expression quantitative trait loci. BMC Bioinformatics, 17,257.Albert,F.W. (2016) Brains, genes and power. Nat. Neurosci., 19, 1428–1430.Dabney,A. et al. (2010). qvalue: Q-value estimation for false discovery ratecontrol. R package version, 1(0).Darnell,G. et al. (2012) Incorporating prior information into association stud-ies. Bioinformatics, 28, i147–i153.Devlin,B. and Roeder,K. (1999) Genomic control for association studies.Biometrics, 55, 997–1004.Duong,D. et al. (2016) Using genomic annotations increases statistical powerto detect eGenes. Bioinformatics, 32, i156–i163.Eskin,E. (2015) Discovering genes involved in disease and the mystery of miss-ing heritability. Commun. ACM, 58, 80–87.Flutre,T. et al. (2013) A statistical framework for joint eQTL analysis in mul-tiple tissues. PLoS Genet., 9, e1003486.Fromer,M. et al. (2016) Gene expression elucidates functional impact of poly-genic risk for schizophrenia. Nat. Neurosci., 19, 1442–1453.Han,B. and Eskin,E. (2011) Random-effects model aimed at discovering asso-ciations in meta-analysis of genome-wide association studies. Am. J. Hum.Genet., 88, 586–598.Han,B. and Eskin,E. (2012) Interpreting meta-analyses of genome-wide asso-ciation studies. PLoS Genet., 8, e1002555.Han,B. et al. (2009) Rapid and accurate multiple testing correction and powerestimation for millions of correlated markers. PLoS Genet., 5, e1000456.Han,B. et al. (2016) A general framework for meta-analyzing dependent stud-ies with overlapping subjects in association mapping. Hum. Mol. Genet.,25, 1857–1866.Hormozdiari,F. et al. (2015) Identification of causal genes for complex traits.Bioinformatics, 31, i206–i213.Huang,Y.-T. et al. (2014) Joint analysis of snp and gene expression data ingenetic association studies of complex diseases. Ann. Appl. Stat., 8, 352.Joo,J.J. et al. (2014) Effectively identifying regulatory hotspots whilecapturing expression heterogeneity in gene expression studies. GenomeBiol., 15, r61.Joo,J.W.J. et al. (2016) Multiple testing correction in linear mixed models.Genome Biol., 17,Kang,E.Y. et al. (2014) Meta-analysis identifies gene-by-environment inter-actions as demonstrated in a study of 4,965 mice. PLoS Genet., 10,e1004022.Liu,G. et al. (2016) Cis-eQTLs regulate reducedLST1gene andNCR3gene ex-pression and contribute to increased autoimmune disease risk: Table 1.Proc. Natl. Acad. Sci. USA, 113, E6321–E6322.Nieuwenhuis,M.A. et al. (2016) Combining genomewide association studyand lung eQTL analysis provides evidence for novel genes associated withasthma. Allergy, 71, 1712–1720.Self,S.G. and Liang,K.Y. (1987) Asymptotic properties of maximum likeli-hood estimators and likelihood ratio tests under nonstandard conditions.J. Am. Stat. Assoc., 82, 605–610.Sul,J.H. et al. (2013) Effectively identifying eQTLs from multiple tissues bycombining mixed model and meta-analytic approaches. PLoS Genet.,9, e1003491.Sul,J.H. et al. (2015) Accurate and fast multiple-testing correction in eQTLstudies. Am. J. Hum. Genet., 96, 857–868.The GTEx Consortium. (2015) The genotype-tissue expression (GTEx) pilotanalysis: Multitissue gene regulation in humans. Science, 348, 648–660.Thompson,S.G. and Sharp,S.J. (1997) Explaining heterogeneity in meta-analysis: A comparison of methods. Stat.. Med., 18, S82.i74 D.Duong et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i67/3953939by gueston 07 January 2018
5028881961002	PMID28881961	5028881961	https://watermark.silverchair.com/btx226.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28881961.main.pdf	Rectified factor networks for biclustering ofomics dataDjork-Arné Clevert,1,* Thomas Unterthiner,2 Gundula Povysil2and Sepp Hochreiter2,*1Bioinformatics Department, Bayer AG, Berlin, Germany and 2Institute of Bioinformatics, Johannes KeplerUniversity Linz, Linz, Austria*To whom correspondence should be addressed.AbstractMotivation: Biclustering has become a major tool for analyzing large datasets given as matrix ofsamples times features and has been successfully applied in life sciences and e-commerce fordrug design and recommender systems, respectively. Factor Analysis for Bicluster Acquisition(FABIA), one of the most successful biclustering methods, is a generative model that representseach bicluster by two sparse membership vectors: one for the samples and one for the features.However, FABIA is restricted to about 20 code units because of the high computational complexityof computing the posterior. Furthermore, code units are sometimes insufficiently decorrelated andsample membership is difficult to determine. We propose to use the recently introduced unsuper-vised Deep Learning approach Rectified Factor Networks (RFNs) to overcome the drawbacks ofexisting biclustering methods. RFNs efficiently construct very sparse, non-linear, high-dimensionalrepresentations of the input via their posterior means. RFN learning is a generalized alternatingminimization algorithm based on the posterior regularization method which enforces non-negativeand normalized posterior means. Each code unit represents a bicluster, where samples for whichthe code unit is active belong to the bicluster and features that have activating weights to the codeunit belong to the bicluster.Results: On 400 benchmark datasets and on three gene expression datasets with known clusters,RFN outperformed 13 other biclustering methods including FABIA. On data of the 1000 GenomesProject, RFN could identify DNA segments which indicate, that interbreeding with other homininsstarting already before ancestors of modern humans left Africa.Availability and implementation: https://github.com/bioinf-jku/librfnContact: djork-arne.clevert@bayer.com or hochreit@bioinf.jku.at1 IntroductionBiclustering is widely used in statistics (Kasim et al., 2016), machinelearning (O’Connor and Feizi, 2014; Kolar et al., 2011; Lee et al.,2015) and bioinformatics (Cheng and Church, 2000; Hochreiter,2013; Madeira and Oliveira, 2004; Povysil and Hochreiter, 2014,2016), e.g. for analyzing large dyadic data given in matrix form,where one dimension are the samples and the other the features.A matrix entry represents a feature value for the according sample.A bicluster is a pair of a sample set and a feature set for which thesamples are similar to each other on the features and vice versa.Biclustering simultaneously clusters rows and columns of a matrix.It clusters row elements that are similar to each other on a subset ofcolumn elements. In contrast to standard clustering, the samples of abicluster are only similar to each other on a subset of features.Furthermore, a sample may belong to different biclusters or to nobicluster at all. Thus, biclusters can overlap in both dimensions. Forexample, in drug design biclusters are compounds which activatethe same gene module and thereby indicate a side effect. In this ex-ample, different chemical compounds are added to a cell line andthe gene expression is measured (Verbist et al., 2015). If multiplepathways are active in a sample, it belongs to different biclustersand may have different side effects.FABIA (Factor Analysis for Bicluster Acquisition, Hochreiteret al., 2010) evolved into one of the most successful biclusteringmethods. A detailed comparison has shown FABIA’s superiorityover existing biclustering methods both on simulated data and real-world gene expression data (Hochreiter et al., 2010). FABIA outper-formed nonnegative matrix factorization with sparseness constraintsVC The Author 2017. Published by Oxford University Press. i59This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33, 2017, i59–i66doi: 10.1093/bioinformatics/btx226ISMB/ECCB 2017Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i59/3953934by gueston 07 January 2018and state-of-the-art biclustering methods. It has been applied to gen-omics, where it identified in gene expression data task-relevant bio-logical modules (Xiong et al., 2014). In the large drug design projectQuantitative Structure Transcriptional Activity Relationships(QSTAR), FABIA, was used to extract biclusters from a data matrixthat contains bioactivity measurements across compounds (Verbistet al., 2015). FABIA has been applied to genetic data, where it hasbeen used to identify DNA segments that are identical by descent(IBD) in different individuals because these individuals inherited thesegment from a common ancestor (Hochreiter, 2013; Povysil andHochreiter, 2014). FABIA is a generative model that enforces sparsecodes (Hochreiter et al., 2010) and, thereby, detects biclusters.Sparseness of code units and parameters is essential for FABIA tofind biclusters, since only few samples and few features belong to abicluster. Each FABIA bicluster is represented by two membershipvectors: one for the samples and one for the features. These member-ship vectors are both sparse since only few samples and only few fea-tures belong to the bicluster.However, FABIA has shortcomings, too. A disadvantage ofFABIA is that it is only feasible with about 20 code units (the biclus-ters) because of the high computational complexity which depends cu-bically on the number of biclusters, i.e. the code units. If less codeunits would be used, only the large and common input structureswould be detected, thereby occluding the small and rare ones.Another shortcoming of FABIA is that units are insufficiently decorre-lated and, therefore, multiple units may encode the same event or partof it. A third shortcoming of FABIA is that the membership vectors donot have exact zero entries, i.e. the membership is continuous must bethresholded for clear membership assignment. This threshold is diffi-cult to adjust. A forth shortcoming is that biclusters can have largepositive but also large negative members of samples (i.e. positive ornegative code values). In this case, it is not clear whether the positivepattern or the negative pattern has been recognized.Rectified factor networks (RFNs; Clevert et al., 2015) overcome theshortcomings of FABIA. The first shortcoming of only few code units isavoided by extending FABIA to thousands of code units in a computa-tionally feasible way. RFNs introduce rectified units to FABIA’s poster-ior distribution and, thereby, allow for fast computations on graphicalprocessing units (GPUs). Even though rectification is well established inDeep Learning by rectified linear units, the RFN approach is the firstmethod which applies rectification to the posterior distribution of factoranalysis and matrix factorization. RFNs transfer the methods for rec-tification from the neural network field to latent variable models.Addressing the second shortcoming of FABIA, RFNs achieve decorrela-tion by increasing the sparsity of the code units using dropout(Srivastava et al., 2014), a method used in Deep Learning to avoid co-adaptation of latent variables. RFNs also address the third shortcomingof FABIA: because the rectified posterior means yield exact zero values,membership to biclusters can be readily assigned to all non-zero values.Since RFNs only have non-negative code units, the forth problem ofseparating the negative from the positive pattern disappears, too.2 Identifying biclusters by RFNsWe propose to use the recently introduced RFNs (Clevert et al.,2015) for biclustering to overcome the drawbacks of the FABIAmodel. The factor analysis model and the construction of a biclustermatrix are depicted in Figure 1. RFNs efficiently construct verysparse, non-linear, high-dimensional representations of the input.RFN models identify rare and small events in the input, have a lowinterference between code units, have a small reconstruction errorand explain the data covariance structure.RFN learning is a generalized alternating minimization algo-rithm (Gunawardana and Byrne, 2005) derived from the posteriorregularization method (Ganchev et al., 2010) which enforces non-negative and normalized posterior means. These posterior means arethe latent code of the input data. The RFN code can be computedvery efficiently. For non-Gaussian priors, the computation of theposterior mean of a new input requires either to numerically solvean integral or to iteratively update variational parameters. In con-trast, for Gaussian priors the posterior mean is the product betweenthe input and a matrix that is independent of the input. RFNs use arectified Gaussian posterior therefore; they have the speed ofGaussian posteriors but lead to sparse codes via rectification.The RFN model is a factor analysis modelv ¼ Wh þ   ; (1)which extracts the covariance structure of the data. The priorh   N 0; Ið Þ of the hidden units (factors) h 2 Rl and the noise    N 0;WÞð of visible units (observations) v 2 Rm are independent.The model parameters are the weight (factor loading) matrixW 2 Rm l and the noise covariance matrix W 2 Rm m.RFN model selection is done via the posterior regularizationmethod, that introduces a variational distribution Q hjvð Þ 2 Q froma family Q, which approximates the posterior p hjvð Þ. We choose Qto constrain the posterior means to be non-negative and normalized.The full model distribution p h; vð Þ contains all model assumptionsand, thereby, defines which structures of the data are modeled.Q hjvð Þ contains data dependent constraints on the posterior, there-fore on the code.For data fvg ¼ fv1; . . . ; vng, it maximizes the objective F :1nXni¼1 log p við Þ  1nXni¼1 DKL Q hijvið Þ k p hijvið Þð Þ; (2)where DKL is the Kullback-Leibler distance. Maximizing F achievestwo goals simultaneously: (i) extracting desired structures and infor-mation from the data as imposed by the generative model and(ii) ensuring sparse codes via Q from the set of rectified Gaussians.In the variational framework, Q is the variational distribution andF is called the negative free energy Neal and Hinton (1998). Ifp hjvð Þ 2 Q, then Q hjvð Þ ¼ p hjvð Þ and we obtain the classical EM al-gorithm. The EM algorithm maximizes the lower bound F on thelog-likelihood as seen at the first line of Equation (2) and ensures inits E-step Q hjvð Þ ¼ p hjvð Þ.For Gaussian posterior distributions, and mean-centered datafvg ¼ fv1; . . . ; vng, the posterior p hijvið Þ is Gaussian with mean vec-tor lp   iand covariance matrix Rp:lp   i¼ I þ WTW 1W    1WTW 1 vi;Rp ¼ I þ WTW 1W    1:(3)For rectified Gaussian posterior distributions, Rp remains as in theGaussian case, but minimizing the second DKL of Equation (2) leadsto the constrained optimization problem (see Clevert et al. 2015) fora detailed description of the RFN objective and the algorithm’s cor-rectness and convergence.)minli1nXni¼1 li   lp   i   TR 1p li   lp   i   s:t: 8i : li   0 ; 8j :1nXni¼1 l2ij ¼ 1 ;(4)where ‘ ’ is component-wise. In the E-step of the generalized alter-nating minimization algorithm (Gunawardana and Byrne, 2005),i60 D.-A.Clevert et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i59/3953934by gueston 07 January 2018which is used for RFN model selection, we only perform a step ofthe gradient projection algorithm (Bertsekas, 1976; Kelley, 1999), inparticular a step of the projected Newton method for solvingEquation (4) (Clevert et al., 2015).Therefore, RFN model selection is extremely efficient but stillguarantees the correct solution. Additional speed-up is generated byimplementing RFNs on GPUs.2.1 RFN biclusteringFor an RFN model, each code unit represents a bicluster, wheresamples for which the code unit is active, belong to the bicluster. Onthe other hand, features that activate the code unit belong to thebicluster, too. The vector of activations of a unit across all samplesis the sample membership vector. The weight vector which activatesthe unit is the feature membership vector. The unconstrained poster-ior mean vector is computed by multiplying the input with a matrixaccording to Equation (3). The constrained posterior of a code unitis obtained by multiplying the input by a vector and subsequentlyrectifying and normalizing the code unit (Clevert et al., 2015).To keep feature membership vectors sparse, we introduce aLaplace prior on the parameters of the original RFN model.Therefore, only few features contribute to activating a code unit,that is, only few features belong to a bicluster. Sparse weights W iare achieved by a component-wise independent Laplace prior forthe weights:p W ið Þ ¼1ffiffiffi2p  n Ynk¼1 e ffiffi2pjWki j (5)The weight update for RFN (Laplace prior on the weights) isW ¼ W þ g U S 1  W     a sign Wð Þ: (6)Whereby the sparseness of the weight matrix can be controlled bythe hyper-parameter a and U and S are defined as U ¼ 1nPni¼1 vilTiand S ¼ 1nPni¼1 lilTi þ R, respectively. To enforce more sparsenessof the sample membership vectors, we introduce dropout of codeunits. Dropout means that during training some code units are set tozero at the same time as they get rectified. Dropout avoids co-adaptation of code units and reduces correlation of code units—an-other problem of FABIA which is solved.RFN biclustering does not require a threshold for determiningsample memberships to a bicluster since rectification sets code unitsto zero. Further crosstalk between biclusters via mixing up negativeand positive memberships is avoided; therefore spurious biclustersappear less often.2.2 Extraction of IBD segments from RFN biclustersRFN biclusters that result from applying RFN to genotype data, rep-resent individuals that are similar to each other because they shareminor alleles of a subset of SNVs (single nucleotide variants).However, a bicluster does not automatically represent an IBD seg-ment because RFN does not regard the physical location or the tem-poral order of the features (SNVs). Only shared minor alleles thataccumulate locally constitute IBD segments as shown in Hochreiter(2013). To distinguish random minor allele matches extracted byRFN from true IBD segments, we compute a histogram of counts ofthe RFN model SNVs and calculate the probability of observing k ormore counts by chance. Let p be the probability of a random minorallele match between t individuals. If n SNVs are in a segment ofDNA, the probability of observing k or more model SNVs by chancein this segment is given by:Pr K   kð Þ ¼Xni¼k ni  pi 1   pð Þn i : (7)Towards this end, the routine implemented in HapFABIA(Hochreiter, 2013) was adjusted to extract IBD segments from RFNbiclusters. The binomial test (Equation 7) is used as a first step toidentify local accumulations of minor alleles that were extracted byRFN. In a second step IBD segments are disentangled and individ-uals reassigned. Later-on, spuriously correlated minor alleles areremoved based on an exponential test on long physical distances.Finally, similar IBD segments that were separated in the first step be-cause of their length are rejoined in the last step.Pairwise IBD detection methods like fastIBD Browning andBrowning (2011) or GERMLINE Gusev et al. (2009) directly lookfor shared continuous DNA segments and incorporate the likelihoodof IBD in their original model. In contrast to that we first look forshared minor alleles in multiple individuals via biclusters and only insubsequent steps use local accumulations and likelihood computa-tions to extract IBD segments from the biclusters.(a) (b)Fig. 1. Left: Factor analysis model: hidden units (factors) h, visible units v, weight matrix W , noise  . Right: The outer product whT of two sparse vectors results ina matrix with a bicluster. Note that the non-zero entries in the vectors are adjacent to each other for visualization purposes onlyRFN biclustering i61Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i59/3953934by gueston 07 January 20183 ExperimentsIn this section, we will present numerical results on multiple syn-thetic and real-world datasets to verify the performance of our RFNbiclustering algorithm, and compare it with various other bicluster-ing methods.3.1 Methods comparedTo assess the performance of RFNs as unsupervised biclusteringmethods, we compare the following 14 biclustering methods:1. RFN: rectified factor networks (Clevert et al., 2015),2. FABIA: factor analysis with Laplace prior on the hidden units(Hochreiter et al., 2010; Hochreiter, 2013),3. FABIAS: factor analysis with sparseness projection (Hochreiteret al., 2010),4. MFSC: matrix factorization with sparseness constraints(Hoyer, 2004),5. plaid: plaid model (Chekouo et al., 2015; Lazzeroni and Owen,2002),6. ISA: iterative signature algorithm (Ihmels et al., 2004),7. OPSM: order-preserving sub-matrices (Ben-Dor et al., 2003),8. SAMBA: statistical-algorithmic method for bicluster analysis(Tanay et al., 2002),9. xMOTIF: conserved motifs (Murali and Kasif, 2003),10. Bimax: divide-and-conquer algorithm (Prelic et al., 2006),11. CC: Cheng-Church d-biclusters (Cheng and Church, 2000),12. plaid_t: improved plaid model (Turner et al., 2003),13. FLOC: flexible overlapped biclustering, a generalization of CC(Yang et al., 2005) and14. spec: spectral biclustering (Kluger et al., 2003).For a fair comparison, the parameters of the methods were opti-mized on auxiliary toy datasets. If more than one setting was closeto the optimum, all near optimal parameter settings were tested. Inthe following, these variants are denoted as method_ variant (e.g.plaid_ss). For RFN we used the following parameter setting: 13 hid-den units, a dropout rate of 0.1, 500 iterations with a learning rateof 0.1, and set the parameter a (controlling the sparseness on theweights) to 0.01.3.2 Simulated datasets with known biclustersIn the following subsections, we describe the data generation processand results for synthetically generated data according to either amultiplicative or additive model structure.3.2.1 Data with multiplicative biclustersWe assumed n ¼ 1000 features and l ¼ 100 samples and implantedp ¼ 10 multiplicative biclusters. The bicluster datasets with p biclus-ters are generated by the following model:X ¼Xpi¼1 ki zTi þ ! ; (8)where ! 2 Rn l is additive noise; ki 2 Rn and zi 2 Rl are the biclus-ter membership vectors for the ith bicluster. The ki’s are generatedby (i) randomly choosing the number Nki of genes in bicluster ifrom f10; . . . ; 210g, (ii) choosing Nki features randomly fromf1; . . . ; 1000g, (iii) setting ki components not in bicluster i to N0; 0:22   random values, and (iv) setting ki components that are inbicluster i to N 63; 1ð Þ random values, where the sign is chosen ran-domly for each gene. The zi’s are generated by (i) randomly choos-ing the number Nzi of samples in bicluster i from f5; . . . ; 25g,(ii) choosing Nzi samples randomly from f1; . . . ; 100g, (iii) setting zicomponents not in bicluster i to N 0; 0:22   random values and (iv)setting zi components that are in bicluster i to N 2; 1ð Þ random val-ues. Finally, we draw the ! entries (additive noise on all entries) ac-cording to N 0;32   and compute the data X according to Equation(8). Using these settings, noisy biclusters of random sizes between10   5 and 210   25 (features   samples) are generated. In all ex-periments, rows (features) were standardized to mean 0 and variance 1.3.2.2 Data with additive biclustersIn this experiment, we generated biclustering data where biclustersstem from an additive two-way ANOVA model:X ¼Xpi¼1 hi   ki zTi Þ þ !; (9)where hikj ¼ li þ aik þ bij and   is the element-wise product ofmatrices and both ki and zi are binary indicator vectors which indi-cate the rows and columns belonging to bicluster i. The ith biclusteris described by an ANOVA model with mean li, kth row effect aik(first factor of the ANOVA model), and jth column effect bij (secondfactor of the ANOVA model). The ANOVA model does not haveinteraction effects. Although the ANOVA model is described for thewhole data matrix, only the effects on rows and columns belongingto the bicluster are used in data generation. Noise and bicluster sizesare generated as in previous Subsection 3.2.1. Data were generatedfor three different signal-to-noise ratios which are determined by thedistribution from which li is chosen: A1 (low signal) N 0;22   , A2(moderate signal) N 62; 0:52   and A3 (high signal) N 64; 0:52   ,where the sign of the mean is randomly chosen. The row effects akiare chosen from N 0:5; 0:22   and the column effects bij are chosenfrom N 1; 0:52   .3.2.3 Results on simulated datasetsFor method evaluation, we use the previously introduced bicluster-ing consensus score for two sets of biclusters (Hochreiter et al.,2010), which is computed as follows:1. Compute similarities between all pairs of biclusters by theJaccard index, where one is from the first set and the other fromthe second set.2. Assign the biclusters of one set to biclusters of the other set bymaximizing the assignment by the Munkres algorithm.3. Divide the sum of similarities of the assigned biclusters by thenumber of biclusters of the larger set.Step (3) penalizes different numbers of biclusters in the sets. Thehighest consensus score is 1 and only obtained for identical sets ofbiclusters.Table 1 shows the biclustering results for these datasets. RFNsignificantly outperformed all other methods (t-test and McNemartest of correct elements in biclusters).3.2.4 Runtime comparisonOur open-source implementation of RFN offers high-performanceCPU and GPU versions. In a runtime comparison on synthetic datadisplayed in Figure 2, we can clearly see how execution times forRFN is much lower and scales much better with the number ofbiclusters than its main competitor FABIA. This comparison wasrun on an Intel i5-3470 CPU and an NVIDIA Titan X GPU.3.3 Gene expression datasetsIn this experiment, we test the biclustering methods on gene expres-sion datasets, where the biclusters are gene modules. The genes thati62 D.-A.Clevert et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i59/3953934by gueston 07 January 2018are in a particular gene module belong to the according biclusterand samples for which the gene module is activated belong to thebicluster. We consider three gene expression datasets which havebeen provided by the Broad Institute and were previously clusteredby Hoshida et al. (2007) using additional datasets as side informa-tion. Note, that Hoshida et al.’s clustering may include falsely as-signed cluster memberships, which could affect the benchmarkresults.1. The ‘breast cancer’ dataset (vanta Veer et al., 2002) was aimedat a predictive gene signature for the outcome of a breast cancer ther-apy. We removed the outlier array S54 which leads to a dataset with97 samples and 1213 genes. In Hoshida et al. (2007), three biologic-ally meaningful sub-classes were found that should be re-identified.2. The ‘multiple tissue types’ dataset (Su et al., 2002) are gene ex-pression profiles from human cancer samples from diverse tissuesand cell lines. The dataset contains 102 samples with 5565 genes.Biclustering should be able to re-identify the tissue types.3. The ‘diffuse large-B-cell lymphoma (DLBCL)’ dataset(Rosenwald et al., 2002) was aimed at predicting the survival afterchemotherapy. It contains 180 samples and 661 genes. The threeclasses found by Hoshida et al. (2007) should be re-identified.For methods assuming a fixed number of biclusters, we chosefive biclusters—slightly higher than the number of known clusters toavoid biases towards prior knowledge about the number of actualclusters. Besides the number of hidden units (biclusters) we used thesame parameters as described in Section 3.1. The performance wasassessed by comparing known classes of samples in the datasets withthe sample sets identified by biclustering using the consensus scoredefined in Subsection 3.2.3—here the score is evaluated for sampleclusters instead of biclusters. The biclustering results are summar-ized in Table 2. In two out of three datasets, RFN biclusteringyielded significantly better results than all other methods and wason second place for the third dataset (significantly according to aMcNemar test of correct samples in clusters).3.4 1000 Genomes datasetsIn this experiment, we used RFN for detecting DNA segments thatare IBD. A DNA segment is IBD in two or more individuals, if it isidentical because they have inherited it from a common ancestor, thatis, the segment has the same ancestral origin in these individuals.Biclustering is well-suited to detect such IBD segments in a genotypematrix (Hochreiter, 2013; Povysil and Hochreiter, 2014, 2016),which has individuals as row elements and genomic SNVs as columnelements. Entries in the genotype matrix usually count how often theminor allele of a particular SNV is present in a particular individual.Table 1. Results are the mean of 100 instances for each simulateddatasetMult. model Add. modelMethod M1 A1 A2 A3RFN 0.643 6 7e-4 0.475 6 9e-4 0.640 6 1e-2 0.816 6 6e-7FABIA 0.478 6 1e-2 0.109 6 6e-2 0.196 6 8e-2 0.475 6 1e-1FABIAS 0.564 6 3e-3 0.150 6 7e-2 0.268 6 7e-2 0.546 6 1e-1SAMBA 0.006 6 5e-5 0.002 6 6e-4 0.002 6 5e-4 0.003 6 8e-4xMOTIF 0.002 6 6e-5 0.002 6 4e-4 0.002 6 4e-4 0.001 6 4e-4MFSC 0.057 6 2e-3 0.000 6 0e-0 0.000 6 0e-0 0.000 6 0e-0Bimax 0.004 6 2e-4 0.009 6 8e-3 0.010 6 9e-3 0.014 6 1e-2plaid_ss 0.045 6 9e-4 0.039 6 2e-2 0.041 6 1e-2 0.074 6 3e-2CC 0.001 6 7e-6 4e-4 6 3e-4 3e-4 6 2e-4 1e-4 6 1e-4plaid_ms 0.072 6 4e-4 0.064 6 3e-2 0.072 6 2e-2 0.112 6 3e-2plaid_t_ab 0.046 6 5e-3 0.021 6 2e-2 0.005 6 6e-3 0.022 6 2e-2plaid_ms5 0.083 6 6e-4 0.098 6 4e-2 0.143 6 4e-2 0.221 6 5e-2plaid_t_a 0.037 6 4e-3 0.039 6 3e-2 0.010 6 9e-3 0.051 6 4e-2FLOC 0.006 6 3e-5 0.005 6 9e-4 0.005 6 1e-3 0.003 6 9e-4ISA 0.333 6 5e-2 0.039 6 4e-2 0.033 6 2e-2 0.140 6 7e-2spec 0.032 6 5e-4 0.000 6 0e-0 0.000 6 0e-0 0.000 6 0e-0OPSM 0.012 6 1e-4 0.007 6 2e-3 0.007 6 2e-3 0.008 6 2e-3Datasets M1 and A1–A3 were multiplicative and additive bicluster, re-spectively. The numbers denote average consensus scores with the true biclus-ters together with their standard deviations in parentheses. The best resultsare printed bold and the second best in italics (‘better’ means significantly bet-ter according to both a paired t-test and a McNemar test of correct elementsin biclusters).Fig. 2. Runtime comparison of FABIA and RFN for 10, 30, 100, 300 and 500biclusters on synthetic inputs of n ¼ 500 features and l ¼ 1000 samples for100 iterations each. Shown data are the median of five measurements, errorbars are standard errors of the meanTable 2. Results on the (A) breast cancer, (B) multiple tissue sam-ples, (C) DLBCL datasets measured by the consensus score(A) breast cancer (B) multiple tissues (C) DLBCLmethod sco #bc #g #s sco #bc #g #s sco #bc #g #sRFN 0.57 3 73 31 0.77 5 75 33 0.35 2 59 72FABIA 0.52 3 92 31 0.53 5 356 29 0.37 2 59 62FABIAS 0.52 3 144 32 0.44 5 435 30 0.35 2 104 60MFSC 0.17 5 87 24 0.31 5 431 24 0.18 5 50 42plaid_ss 0.39 5 500 38 0.56 5 1903 35 0.30 5 339 72plaid_ms 0.39 5 175 38 0.50 5 571 42 0.28 5 143 63plaid_ms5 0.29 5 56 29 0.23 5 71 26 0.21 5 68 47ISA_1 0.03 25 55 4 0.05 29 230 6 0.01 56 26 8OPSM 0.04 12 172 8 0.04 19 643 12 0.03 6 162 4SAMBA 0.02 38 37 7 0.03 59 53 8 0.02 38 19 15xMOTIF 0.07 5 61 6 0.11 5 628 6 0.05 5 9 9Bimax 0.01 1 1213 97 0.10 4 35 5 0.07 5 73 5CC 0.11 5 12 12 nc nc nc nc 0.05 5 10 10plaid_t_ab 0.24 2 40 23 0.38 5 255 22 0.17 1 3 44plaid_t_a 0.23 2 24 20 0.39 5 274 24 0.11 3 6 24spec 0.12 13 198 28 0.37 5 395 20 0.05 28 133 32FLOC 0.04 5 343 5 nc nc nc nc 0.03 5 167 5An ‘nc’ entry means that the method did not converge for this dataset. Thebest results are in bold and the second best in italics (‘better’ means signifi-cantly better according to a McNemar test of correct samples in clusters). Thecolumns ‘sco’, ‘#bc’, ‘#g’, ‘#s’ provide the consensus score, the numbers ofbiclusters, their average numbers of genes, and their average numbers of sam-ples, respectively. RFN is two times the best method and once on secondplace.RFN biclustering i63Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i59/3953934by gueston 07 January 2018Individuals that share an IBD segment are similar to each other be-cause they also share minor alleles of SNVs (tagSNVs) within the IBDsegment, therefore IBD segments can be seen as biclusters.For our IBD-analysis we used the next generation sequencingdata from the 1000 Genomes Phase 3 (The 1000 Genomes ProjectConsortium, 2015) [ftp://ftp.1000genomes.ebi.ac.uk/Vol03325/ftp/release/20130502/ (31 October 2014, date last accessed)]. This data-set consists of low-coverage whole genome sequences from 2504 in-dividuals of the main continental population groups (Africans,East Asians, South Asians, Europeans and Admixed Americans).Individuals that showed cryptic first degree relatedness to otherswere removed so that the final dataset consisted of 2493 individuals(see Povysil and Hochreiter, 2016). High-coverage genomes of theAltai Neanderthal and Denisovan were provided by the Max PlanckInstitute for Evolutionary Anthropology (Meyer et al., 2012; Prüferet al., 2014) [http://cdna.eva.mpg.de/denisova/ (2 February 2012,date last accessed) and http://cdna.eva.mpg.de/neandertal/altai/, 23May 2013, date last accessed]. Furthermore, we used the sequenceof the reconstructed common ancestor of human, chimpanzee, gor-illa, orang-utan, macaque and marmoset genomes which was part ofthe 1000 genomes project data.Like Povysil and Hochreiter (2016), we restricted the analysis toSNVs and removed repeat regions and CpGs. RFN IBD detectionis based on low frequency and rare variants (minor allele frequency< 0.05), therefore we removed common and private SNVs prior tothe analysis. Afterwards, all chromosomes were divided into inter-vals of 10 000 SNVs with adjacent intervals overlapping by 5000SNVs. RFN was applied to the unphased genotype data and IBDsegments were extracted from biclusters as described in Section 2.2.To distinguish true IBD segments from random findings we de-fine an IBD score as the total sum of minor allele presences of indi-viduals that share the IBD segment and tagSNVs that were extractedby RFN. True IBD segments should have an IBD score close to thenumber of individuals times the number of tagSNVs. To determinethe significance of a finding, we calculate the empirical distributionof IBD scores based on 10E þ 5 randomly sampled DNA segmentsof the same size as the detected segment. This allows us to calculatethe P-value of our detected IBD segments under this H0 distribution.To get randomly sampled DNA segments of the same size, we firstsample the same number of individuals from the total set of individ-uals and a start SNV that can be anywhere in the genome.Afterwards, we extract the genotype matrix consisting of thesampled individuals and a number of SNVs equal to the number ofSNVs between the first and the last tagSNV of the IBD segment, be-ginning from the sampled start SNV. Finally, we sample tagSNVsfrom these SNVs and calculate the IBD score as described above.The depicted IBD segment in Figure 3 has a highly significant IBDscore (P-value < 1E-5).In the data of the 1000 Genomes Project Phase 3, we found >1.5million IBD segments. About 70% of the IBD segments were onlyFig. 3. Example of an IBD segment matching the Neanderthal genome shared among Africans and Admixed Americans. The rows represent all individuals thathave the IBD segment, and columns represent consecutive SNVs. Major alleles are shown in yellow, minor alleles of tagSNVs in violet, and minor alleles of otherSNVs in cyan. The row labeled model L indicates tagSNVs identified by RFN in violet. The rows Ancestor, Neanderthal and Denisova show bases of the respectivegenomes in violet if they match the minor allele of the tagSNVs (in yellow otherwise). For the Ancestor genome we used the reconstructed common ancestor se-quence that was provided as part of the 1000 Genomes Project datai64 D.-A.Clevert et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i59/3953934by gueston 07 January 2018shared by Africans, while <1% were shared by individuals from allfive continental populations. In contrast to HapFABIA, which wasused for the analyses in Hochreiter (2013) and Povysil andHochreiter (2016), IBD segments found with RFN require less post-processing because RFNs can extract much more biclusters andtherefore IBD segments in a single run. Thus, problems caused bythe iterative approach of HapFABIA can be avoided. To gain in-sights into the genetic relationships between humans, Neanderthalsand Denisovans, we compared the detected IBD segments with therespective ancient genomes as described in Povysil and Hochreiter(2016). Furthermore, we excluded segments that were already presentin the reconstructed ancestral sequence of all primates to distinguishIBD segments stemming from this ancestor from such that are due tolater interbreedings. We could confirm that a surprisingly high numberof IBD segments is shared between Africans and Neanderthals/Denisovans (see Fig. 3 for an example of an IBD segment that matchesthe Neanderthal genome). Neanderthal- and Denisova-matching IBDsegments only observed in Africans are clearly shorter than IBD seg-ments shared between non-Africans and the ancient genomes (5500versus 12 500 bp and 5000 versus 12 000 bp, respectively forNeanderthal- and Denisova-matching segments). Since shorter segmentsare assumed to be older than longer ones (Povysil and Hochreiter,2014), this is an indication of very early interbreedings within Africathat involved ancestors of Neanderthals and Denisovans, as well as an-cestors of modern Africans (Povysil and Hochreiter, 2016).4 ConclusionWe have introduced RFNs for biclustering and benchmarked it with13 other biclustering methods on artificial and real-world datasets.On 400 benchmark datasets with artificially implanted biclus-ters, RFN significantly outperformed all its competitors includingFABIA. On three gene expression datasets with previously verifiedground-truth, RFN biclustering yielded twice significantly better re-sults than all other methods and was once the second best perform-ing method. On data of the 1000 Genomes Project, RFN couldidentify IBD segments that previous IBD detection methods were un-able to discover. Those detected segments support the hypothesisthat interbreedings between ancestors of humans and other ancienthominins already have taken place in Africa.RFN biclustering is geared to large datasets, sparse coding, manycoding units and distinct membership assignment. Thereby RFNbiclustering overcomes the shortcomings of FABIA and has the po-tential to become the new state of the art biclustering algorithm.AcknowledgementWe thank the NVIDIA Corporation for supporting this research with severalTitan X GPUs.FundingThis work was was funded by the Institute of Bioinformatics.Conflict of Interest: none declared.ReferencesBen-Dor,A. et al. (2003) Discovering local structure in gene expression data:the order-preserving submatrix problem. J. Comput. Biol., 10, 373–384.Bertsekas,D.P. (1976) On the Goldstein-Levitin-Polyak gradient projectionmethod. IEEE Trans. Automat. Control, 21, 174–184.Browning,B.L. and Browning,S.R. (2011) A fast, powerful method for detect-ing identity by descent. Am. J. Hum. Genet., 88, 173–182.Chekouo,T. et al. (2015) The gibbs-plaid biclustering model. Ann. Appl. Stat.,9, 1643–1670.Cheng,Y. and Church,G.M. (2000) Biclustering of expression data. InProceedings of the International Conference on Intelligent Systems forMolecular Biology, Vol. 8, San Diego, U.S.A., pp. 93–103.Clevert,D.-A. et al. Rectified factor networks. (2015) In: Cortes, C., Lawrence,N. D., Lee, D. D., Sugiyama, M. and Garnett, R. (eds.) Advances in NeuralInformation Processing Systems 28 (NIPS), 2015, Montreal, Canada,Curran Associates, Inc.Ganchev,K. et al. (2010) Posterior regularization for structured latent variablemodels. J. Mach. Learn. Res., 11, 2001–2049.Gunawardana,A. and Byrne,W. (2005) Convergence theorems forgeneralized alternating minimization procedures. J. Mach. Learn. Res., 6,2049–2073.Gusev,A. et al. (2009) Whole population, genome-wide mapping of hidden re-latedness. Genome Res., 19, 318–326.Hochreiter,S. (2013) HapFABIA: Identification of very short segments of iden-tity by descent characterized by rare variants in large sequencing data.Nucleic Acids Res., 41, e202.Hochreiter,S. et al. (2010) FABIA: factor analysis for bicluster acquisition.Bioinformatics, 26, 1520–1527.Hoshida,Y. et al. (2007) Subclass mapping: Identifying common subtypes inindependent disease data sets. PLoS One, 2, e1195.Hoyer,P.O. (2004) Non-negative matrix factorization with sparseness con-straints. J. Mach. Learn. Res., 5, 1457–1469.Ihmels,J. et al. (2004) Defining transcription modules using large-scale geneexpression data. Bioinformatics, 20, 1993–2003.Kasim,A. et al. (2016) Applied Biclustering Methods for Big and High-Dimensional Data Using R. Chapman and Hall/CRC.Kelley,C.T. (1999) Iterative Methods for Optimization. Society for Industrialand Applied Mathematics (SIAM), Philadelphia.Kluger,Y. et al. (2003) Spectral biclustering of microarray data: coclusteringgenes and conditions. Genome Res., 13, 703–716.Kolar,M. et al. Minimax localization of structural information in large noisymatrices. (2011) In: Shawe-Taylor, J., Zemel, R.S., Bartlett, P.L., Pereira, F.and Weinberger, K.Q. (eds.) Advances in Neural Information ProcessingSystems 24, pp. 909–917. Curran Associates, Inc.Lazzeroni,L. and Owen,A. (2002) Plaid models for gene expression data. Stat.Sinica, 12, 61–86.Lee,J.D. et al. (2015) Evaluating the statistical significance of biclusters. In:Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M. and Garnett, R. (eds.)Advances in Neural Information Processing Systems 28 (NIPS), 2015,Montreal, Canada, pp. 1324–1332. Curran Associates, Inc.Madeira,S.C. and Oliveira,A.L. (2004) Biclustering algorithms for biologicaldata analysis: a survey. IEEE ACM Trans. Comput. Biol. Bioinform., 1,24–45.Meyer,M. et al. (2012) A high-coverage genome sequence from an archaicdenisovan individual. Science, 338, 222–226.Murali,T.M. and Kasif,S. (2003) Extracting conserved gene expression motifsfrom gene expression data. In Pacific Symposium on Biocomputing, pp.77ges.Neal,R. and Hinton,G.E. (1998) A view of the EM algorithm that justifies in-cremental, sparse, and other variants. In: Jordan, M.I. (ed.) Learning inGraphical Models. MIT Press, Cambridge, MA, pp. 355–368.O’Connor,L. and Feizi,S. (2014) Biclustering using message passing. In:Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D. and Weinberger,K.Q. (eds.), Advances in Neural Information Processing Systems 27 (NIPS),2014, Montreal, Canada, Curran Associates, Inc., pp. 3617–3625.Povysil,G. and Hochreiter,S. (2014) Sharing of Very Short IBD Segments betweenHumans, Neandertals, and Denisovans. bioRxiv. doi: 10.1101/003988.Povysil,G. and Hochreiter,S. (2016) IBD Sharing between Africans,Neandertals, and Denisovans. Genome Biol. Evol., 8, 3406.Prelic,A. et al. (2006) A systematic comparison and evaluation of biclusteringmethods for gene expression data. Bioinformatics, 22, 1122–1129.Prüfer,K. et al. (2014) The complete genome sequence of a Neanderthal fromthe Altai Mountains. Nature, 505, 43–49.RFN biclustering i65Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i59/3953934by gueston 07 January 2018Rosenwald,A. et al. (2002) The use of molecular profiling to predict survivalafter chemotherapy for diffuse large-B-cell lymphoma. N. Engl. J. Med.,346, 1937–1947.Srivastava,N. et al. (2014) Dropout: a simple way to prevent neural networksfrom overfitting. J. Mach. Learn. Res., 15, 1929–1958.Su,A.I. et al. (2002) Large-scale analysis of the human and mouse transcrip-tomes. Proc. Natl. Acad. Sci. USA, 99, 4465–4470.Tanay,A. et al. (2002) Discovering statistically significant biclusters in gene ex-pression data. Bioinformatics, 18(Suppl. 1), S136–S144.The 1000 Genomes Project Consortium (2015) A global reference for humangenetic variation. Nature, 526, 68–74. ISSN 0028-0836.Turner,H. et al. (2003) Improved biclustering of microarray data demonstratedthrough systematic performance tests. Comput. Stat. Data Anal., 48: 235–254.van’t Veer,L.J. et al. (2002) Gene expression profiling predicts clinical out-come of breast cancer. Nature, 415, 530–536.Verbist,B. et al. (2015) Using transcriptomics to guide lead optimization indrug discovery projects: lessons learned from the QSTAR project. DrugDiscov. Today, 20, 505–513. ISSN 1359-6446.Xiong,M. et al. (2014) Identification of transcription factors for drug-associatedgene modules and biomedical implications. Bioinformatics, 30, 305–309.Yang,J. et al. (2005) An improved biclustering method for analyzing gene ex-pression profiles. Int. J. Artif. Intell. Tools, 14, 771–790.i66 D.-A.Clevert et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/i59/3953934by gueston 07 January 2018
5028369339002	PMID28369339	5028369339	https://watermark.silverchair.com/btx152.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28369339.main.pdf	Bioinformatics, 33(14), 2017, 2199–2201doi: 10.1093/bioinformatics/btx152Advance Access Publication Date: 22 March 2017Applications NoteGenome analysisHiC-spector: a matrix library for spectral andreproducibility analysis of Hi-C contact mapsKoon-Kiu Yan,1,2,* Galip Gurkan Yardımcı,3 Chengfei Yan,1,2¨3,4William S. Noble, and Mark Gerstein1,2,5,*1Program in Computational Biology and Bioinformatics, 2Department of Molecular Biophysics and Biochemistry,Yale University, New Haven, CT, USA, 3Department of Genome Sciences, 4Department of Computer Science andEngineering, University of Washington, Seattle, WA, USA and 5Department of Computer Science, Yale University,New Haven, CT, USA*To whom correspondence should be addressed.Associate Editor: Bonnie BergerReceived on November 16, 2016; revised on March 11, 2017; editorial decision on March 13, 2017; accepted on March 21, 2017AbstractSummary: Genome-wide proximity ligation based assays like Hi-C have opened a window to the3D organization of the genome. In so doing, they present data structures that are different fromconventional 1D signal tracks. To exploit the 2D nature of Hi-C contact maps, matrix techniques likespectral analysis are particularly useful. Here, we present HiC-spector, a collection of matrix-relatedfunctions for analyzing Hi-C contact maps. In particular, we introduce a novel reproducibility metricfor quantifying the similarity between contact maps based on spectral decomposition. The metricsuccessfully separates contact maps mapped from Hi-C data coming from biological replicates,pseudo-replicates and different cell types.Availability and Implementation: Source code in Julia and Python, and detailed documentation isavailable at https://github.com/gersteinlab/HiC-spector.Contact: koonkiu.yan@gmail.com or mark@gersteinlab.orgSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionGenome-wide proximity ligation assays such as Hi-C have emergedas powerful techniques to understand the 3D organization of thegenome (Kalhor et al., 2011; Lieberman-Aiden et al., 2009).Although these techniques offer new biological insights, theydemand different data structures and present new computationalquestions (Ay and Noble, 2015; Dekker et al., 2013). For instance, afundamental question of practical importance is, how can we quantify the similarity between two Hi-C data sets? In particular, giventwo experimental replicates, how can we determine if the experiments are reproducible?Data from Hi-C experiments are usually summarized by socalled chromosomal contact maps. By binning the genome intoequally sized bins, a contact map is a matrix whose elements storethe population-averaged co-location frequencies between pairs ofloci. Therefore, mathematical tools like spectral analysis can beextremely useful in understanding these chromosomal contact maps.Our aim is to provide a set of basic analysis tools for handling Hi-Ccontact maps. In particular, we introduce a simple but novel metricto quantify the reproducibility of the maps using spectraldecomposition.2 AlgorithmsWe represent a chromosomal contact map by a symmetric and nonnegative adjacency matrix W. The matrix elements represent the frequencies of contact between genomic loci. Recent single-cell imagingexperiment suggests that the frequency serves as a reasonable proxyof spatial distance (Wang et al., 2016). In principle, the largerthe value of Wij , the closer is the distance between loci i and j.The starting point of spectral analysis is the Laplacian matrix L,which is defined as L ¼ D À W. Here D is a diagonal matrix inCV The Author 2017. Published by Oxford University Press.2199This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2199/3078603by gueston 07 January 20182200K.-K.Yan et al.Pwhich Dii ¼ j Wij (the coverage of bin i in the context of Hi-C).The Laplacian matrix further takes a normalized form ‘ ¼ DÀ1=2 LDÀ1=2 (Chung, 1997). It can be verified that 0 is an eigenvalue of ‘,and the set of eigenvalues of ‘ (0k0k1...knÀ1 Þ isreferred to as the spectrum of ‘:Given two contact maps W A and W B , we propose to quantifytheir similarity by decomposing their corresponding Laplacianmatrices ‘A and ‘B respectively and then comparing their eigenvectors. Let fkA ; kA ; . . . ; kA g and fkB ; kB ; . . . ; kB g be the spectra01nÀ101nÀ1of ‘A and ‘B , and fvA ; vA ; . . . ; vA g and fvB ; vB ; . . . ; vB g be their01nÀ101nÀ1sets of normalized eigenvectors. A distance metric Sd is defined asSd ðA; BÞ ¼XrÀ1i¼0k vA À vB k :ii(1)Here k k represents the Euclidean norm. The parameter r is thenumber of leading eigenvectors picked from ‘A and ‘B . In general,Sd provides a metric to gauge the similarity between two contactmaps. vA and vB are more correlated if A and B are two biologicaliireplicates as compared with the case when they are two different celllines (see Supplementary Fig. S1).For the choice of r, like any principal component analysis, theleading eigenvectors are more important than the lower rankedeigenvectors. In fact, we observe that the Euclidean distance between a pair of high-order eigenvectors is the same as the distancebetween a pair of unit vectors whose components are randomlysampled from a standard normal distribution (see SupplementaryFig. S2). In other words, the high-order eigenvectors are essentiallynoise terms, whereas the signal is stored in the leading vectors. As arule of thumb, we found the choice r ¼ 20 is good enough for practical purposes. Furthermore, as the distance between a pair of randomly sampled unit vectors presents a reference, we linearly rescalethe distance metric into a reproducibility score Q ranges from 0 to 1(see the Supplementary Material).We used HiC-spector to calculate the reproducibility scores formore than a hundred pairs of Hi-C contact maps. As shown inFigure 1, the reproducibility scores between pseudo-replicates aregreater than the scores for real biological replicates, which aregreater than the scores between maps from different cell lines (seethe Supplement). It is worthwhile to point out that two contactmaps can be compared in terms of features like topologically associating domains (TADs) and loops. It depends strongly on the choicesof methods and parameters. Nevertheless, what we refer to, as ‘reproducibility’ is a direct comparison of the contact maps.Mathematically there are different ways to compare two matrices. For instance, one could assume all matrix elements are independent and define a distance metric using Spearman correlation.The intuition behind Sd is essentially a better way to decompose acontact map. The normalized Laplacian matrix is closely related toa random-walk-process taking place in the underlying graph of W:The leading eigenvector refers to the steady state distribution; thenext few eigenvectors correspond to the slower decay modes of therandom walk process and capture the densely interacting domainsthat are highly significant in contact maps. In fact, HiC-spector canbetter separate biological replicates and non-replicates comparedwith the correlation coefficient (see Supplementary Fig. S3).Apart from the reproducibility score, HiC-spector provides a number of matrix algorithms useful for analyzing contact maps. For instance, to perform a widely used normalization procedure for contactmaps (Imakaev et al., 2012), we include the Knight-Ruiz algorithm(Knight and Ruiz, 2012), which is a newer and faster algorithm formatrix balancing. Also, we have included the functions for estimatingFig. 1. Reproducibility scores for three sets of Hi-C contact maps pairs.Contact maps came from Hi-C experiments performed in 11 cell lines.Biological replicates refer to a pair of replicates of the same experiment.Pseudo replicates are obtained by pooling the reads from two replicates together performing down sampling. There are 11 biological replicates, 33 pairsof pseudo replicates, and 110 pairs of maps between different cell types. Eachbox shows for a pair the distribution of Q in 23 chromosomes, with crossesas the outliersthe average contact frequency with respect to the genomic distance, aswell as identifying the so-called A/B compartments (Lieberman-Aidenet al., 2009) using the corresponding correlation matrix.3 Implementation and benchmarkHiC-spector is a library written in Julia, a high-performance language for technical computing. A Python script for the reproducibility score is also provided. The bottleneck for evaluating Q is matrixdiagonalization. The runtime is very efficient but depends on thesize of contact maps (see Supplementary Fig. S5 for details).4 Materials and methodsHi-C data are generated by the ENCODE consortium (see theSupplementary Material). Contact maps were generated using thetool cworld (https://github.com/dekkerlab/cworld-dekker).AcknowledgementsWe want to thank the 3D Nucleome subgroup in the ENCODE consortiumfor processing the Hi-C data and discussion.FundingThis work has been supported by NIH award [U41 HG007000].Conﬂict of Interest: none declared.ReferencesAy,F. and Noble,W.S. (2015) Analysis methods for studying the 3D architecture of the genome. Genome Biol., 16, 183.Chung,F. (1997). Spectral Graph Theory. American Mathematical Society,Providence, Rhode Island.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2199/3078603by gueston 07 January 2018HiC-spectorDekker,J. et al. (2013) Exploring the three-dimensional organization of genomes:interpreting chromatin interaction data. Nat. Rev. Genet., 14, 390–403.Imakaev,M. et al. (2012) Iterative correction of Hi-C data reveals hallmarks ofchromosome organization. Nat. Methods, 9, 999–1003.Kalhor,R. et al. (2011) Genome architectures revealed by tethered chromosome conformation capture and population-based modeling. Nat.Biotechnol., 30, 90–98.2201Knight,P.A. and Ruiz,D. (2013) A fast algorithm for matrix balancing. IMA JNumer. Anal., 33, 1029–1047.Lieberman-Aiden,E. et al. (2009) Comprehensive mapping of long-range interactions reveals folding principles of the human genome. Science, 326,289–293.Wang,S. et al. (2016) Spatial organization of chromatin domains and compartments in single chromosomes. Science, 353, 598–602.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2199/3078603by gueston 07 January 2018
5028369191002	PMID28369191	5028369191	https://watermark.silverchair.com/btx143.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28369191.main.pdf	Bioinformatics, 33(14), 2017, 2218–2220doi: 10.1093/bioinformatics/btx143Advance Access Publication Date: 22 March 2017Applications NoteGene expressionThe RNASeq-er API—a gateway to systematicallyupdated analysis of public RNA-seq dataRobert Petryszak*, Nuno A. Fonseca, Anja Fullgrabe, Laura Huerta,¨Maria Keays, Y. Amy Tang and Alvis BrazmaFunctional Genomics Group, European Molecular Biology Laboratory, European Bioinformatics Institute, EMBL-EBI,Hinxton, UK*To whom correspondence should be addressed.Associate Editor: Bonnie BergerReceived on September 10, 2017; revised on February 24, 2017; editorial decision on March 13, 2017; accepted on March 20, 2017AbstractMotivation: The exponential growth of publicly available RNA-sequencing (RNA-Seq) data posesan increasing challenge to researchers wishing to discover, analyse and store such data, particularly those based in institutions with limited computational resources. EMBL-EBI is in an ideal position to address these challenges and to allow the scientiﬁc community easy access to not just raw,but also processed RNA-Seq data. We present a Web service to access the results of a systematically and continually updated standardized alignment as well as gene and exon expression quantiﬁcation of all public bulk (and in the near future also single-cell) RNA-Seq runs in 264 species inEuropean Nucleotide Archive, using Representational State Transfer.Results: The RNASeq-er API (Application Programming Interface) enables ontology-powered searchfor and retrieval of CRAM, bigwig and bedGraph ﬁles, gene and exon expression quantiﬁcation matrices (Fragments Per Kilobase Of Exon Per Million Fragments Mapped, Transcripts Per Million, rawcounts) as well as sample attributes annotated with ontology terms. To date over 270 00 RNA-Seqruns in nearly 10 000 studies (1PB of raw FASTQ data) in 264 species in ENA have been processedand made available via the API.Availability and Implementation: The RNASeq-er API can be accessed at http://www.ebi.ac.uk/fg/rnaseq/api. The commands used to analyse the data are available in supplementary materials andat https://github.com/nunofonseca/irap/wiki/iRAP-single-library.Contact: rnaseq@ebi.ac.uk; rpetry@ebi.ac.ukSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionThe pattern of rapid growth of RNA-sequencing (RNA-Seq) data,observed in recent years, is set to continue as costs of sequencing experiments decrease and novel technologies and analysis methodsreach maturity, e.g. single-cell RNA-Seq (Linnarson et al., 2016).Figure 1 highlights sustained exponential growth in the number ofpublic bulk RNA-Seq runs in European Nucleotide Archive (ENA).A ‘run’ is a unit of biological assay performed on a sequencingmachine for a single, de-multiplexed sequencing library preparation.Figure 2 shows the number of runs in the top 20 RNA-Seq data-richspecies in ENA.This sustained growth only exacerbates the challenges facing researchers wishing to discover, analyse and store available RNA-Seqdata, particularly those based in institutions with limited computational resources. EMBL-EBI is in an ideal position to address thesechallenges and to allow the scientific community easy access to not justraw, but also processed RNA-Seq data. We have therefore undertakenthe task of on-going standardized alignment and gene and exon expression quantification of all public bulk (and in the near future also singlecell) RNA-Seq data in ENA (Silvester et al., 2014) in 264 species withgenome references in Ensembl (Cunningham et al., 2015), EnsemblGenomes (Kersey et al., 2014) and WormBase Parasite (Howe et al.,CV The Author 2017. Published by Oxford University Press.2218This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permitsunrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2218/3078600by gueston 07 January 2018RNASeq-er API—a gateway to public RNA-seq dataFig. 1. Cumulative number of public bulk RNA-Seq runs in ENA, in speciescovered by the API2219records are used instead. This API has also been incorporated intoBioServices Python Package (Cokelaer et al., 2013) and CPAN Perlpackage (http://search.cpan.org/dist/Bio-EBI-RNAseqAPI/). The analysis pipeline behind the RNASeq-er API offers an important service toresearchers performing RNA-Seq experiments that choose to submittheir data to ArrayExpress via https://www.ebi.ac.uk/fg/annotare submission tool: the deposited studies are not only described by rich,ontology-annotated experimental metadata; the associated raw data isalso analysed for free, and for qualifying studies, is subsequently visualized in Expression Atlas (via private access if pre-publication). Thiscombined metadata-rich deposition, analysis and visualization serviceaims to make data depositions not only easily discoverable, but also tofacilitate understanding and reproducibility of the underlying researchresults. The results of our analysis can also inform and feed into thesubmitters’ own downstream analyses well before the paper is readyfor submission to a journal.2 ImplementationFig. 2. The number of sequencing runs in the top 20 RNA-Seq data-rich species in ENA2016), depositing the results on the public EMBL-EBI FTP server, andmaking them discoverable via the RNASeq-er API (ApplicationProgramming Interface). Our fully automated analysis pipeline processes new RNA-Seq runs as soon as they become public in ENA andmakes the results available via the API shortly after. In addition, allRNA-Seq runs in a given species are re-processed when a new genomeassembly is released. While the initial processing of the bulk of publicRNA-Seq data took around 6 months, the pipeline (utilising 2000 coresin parallel) is capable of processing around 500-1000 sequencing runsper day and thus provides results for any new run in ENA within daysof it becoming public. The re-processing for new genome assembly typically takes a week or 2, with the exception of human and mouse (dueto the sheer volume of data) and of large genome species (it took overa month to re-process all wheat runs after the new TGACv1 genomereference was released). The RNASeq-er API enables ontologypowered search for and retrieval of CRAM, bigwig and bedGraph filesat individual ENA run level, and of gene and exon expression quantification matrices [Fragments Per Kilobase Of Exon Per MillionFragments Mapped (FPKM), Transcripts Per Million (TPM), rawcounts] at ENA study level. The API returns data in tab-delimited andJSON formats, and provides additional search filter by the minimumpercentage of reads mapped to the genome reference in a given run.The API also provides access to baseline gene expression quantifications, aggregated across all runs in each of over 4000 normal tissue,cell type, developmental stage, sex and strain conditions in 61 species.Please note that it is up to the user of the API to specify the minimumdesired percentage of mapped reads—no such filtering is employed bythe API a priori. To facilitate discoverability and to allow for interpretation of the analysed data, the API also provides sample attributes perrun, including corresponding ontology terms derived from manual curation in ArrayExpress (Kolesnikov et al., 2015) and Expression Atlas(Petryszak et al., 2016). Where manually curated sample annotationsare not available, BioSamples database (Faulconbridge et al., 2013)The analysis of each sequencing run is performed using the iRAPpipeline (Fonseca et al., 2014). First quality-filtered (Petryszak et al.,2014, Supplementary Material) reads are aligned to the latest genomereference via TopHat 2 (Kim et al., 2013). Note that so far we haveused STAR (Dobin et al., 2013) for the wheat genome reference, butnow that TopHat 2 has been improved to handle large genome references, we plan to use TopHat 2 only for all species. Then the resulting BAM (Li et al., 2009) file is converted to CRAM (Fritz et al.,2011) format; bigWig (https://genome.ucsc.edu/goldenpath/help/bigWig.html) and bedGraph (https://genome.ucsc.edu/goldenpath/help/bedgraph.html) genome track files are also generated. Wheregroups of technical replicates corresponding to a single biologicalsample were identified via manual curation in ArrayExpress, the corresponding CRAM, bigWig and bedGraph files are aggregated foreach such biological replicate. The expressions (raw counts) of genesand exons defined in the corresponding GTF file (obtained from thesame source as the genome reference) are quantified using HTSeq(Anders et al., 2015) and DEXSeq (Anders et al., 2012) respectively.FPKM and TPM are then calculated. The gene lengths are based onthe union of exons. Finally, for each gene the median TPM expression and coefficient of variation are calculated across all runs thathave the same unique combination of sample attributes, including tissue, cell type, developmental stage, sex and strain.The full API documentation is available in the Supplementarydata. The latest API documentation is also available at http://www.ebi.ac.uk/fg/rnaseq/api/(html) and http://www.ebi.ac.uk/fg/rnaseq/api/doc (pdf).AcknowledgementsWe would like to thank our colleagues at EMBL-EBI: Paul Kersey for obtaining the initial BBSRC funding, the Ensembl Genomes team for their assistancein deﬁning the API speciﬁcation; the Ensembl and WormBase ParaSite teamsfor facilitating timely access to the genome references; the ENA Team fortheir assistance in retrieval the raw RNA-Seq data; and ﬁnally to the Samples,Phenotypes and Ontologies Team for the provision of tools for retrieval of thesequencing metadata from BioSamples database and of an up-to-date annotation of sequencing meta-data to ontologies.FundingThe development of the pipeline to align and the API to access the results forpublic plant RNA-Seq data was funded by BBSRC. The further extension toDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2218/3078600by gueston 07 January 20182220the analysis of non-plant species was supported by the European MolecularBiology Laboratory (EMBL) member states; Funding for open access chargewas provided by EMBL.Conﬂict of Interest: none declared.ReferencesAnders,S. et al. (2014) HTSeq—a Python framework to work with highthroughput sequencing data. Bioinformatics Print, 31, 166–169.Anders,S. et al. (2012) Detecting differential usage of exons from RNA-seqdata. Genome Res., 22, 2008–2017.Cokelaer,T. et al. (2013) BioServices: a common Python package to accessbiological Web Services programmatically. Bioinformatics, 29, 3241–3242.Cunningham,F. et al. (2015) Ensembl 2015. Nucleic Acids Res., 2015 43,D662–D669.Dobin,A. et al. (2013) STAR: ultrafast universal RNA-seq aligner. Bioinformatics,29, 15–21.Faulconbridge,A. et al. (2013) Updates to BioSamples database at EuropeanBioinformatics Institute. Nucleic Acids Res., 42, D50–D52.Fonseca,A.N. et al. (2014) iRAP - an integrated RNA-seq Analysis Pipeline.bioRxiv, DOI: 10.1101/005991.Fritz,M.H.Y. et al. (2011) Efﬁcient storage of high throughput DNA sequencing data using reference-based compression. Genome Res., 21, 734–740.R.Petryszak et al.Howe,K.L. et al. (2016) WormBase 2016: expanding to enable helminth genomic research. Nucleic Acids Res., 44, D774–D780.Kersey,J.P. et al. (2014) Ensembl Genomes 2013: scaling up access to genomewide data. Nucleic Acids Res., 42, D546–D552.Kolesnikov,N. et al. (2015) ArrayExpress update-simplifying data submissions. Nucleic Acids Res., 43, D1113–D1116.Li,H. et al. (2009) The Sequence Alignment/Map format and SAMtools.Bioinformatics, 25, 2078–2079.Kim,D. et al. (2013) TopHat2: accurate alignment of transcriptomes inthe presence of insertions, deletions and gene fusions. Genome Biol.,14, 1.Kolesnikov,N. et al. (2015) ArrayExpress update—simplifying data submissions. Nucleic Acids Res., 43, (D1): D1113–D1116.Linnarson,S. and Teichmann,S.A. (2016) Single-cell genomics: coming of age.Genome Biol., 17, 97.Petryszak,R. et al. (2016) Expression Atlas update—an integrated database ofgene and protein expression in humans, animals and plants. Nucleic AcidsRes., 44, D746–D752.Petryszak,R. et al. (2014) Expression Atlas update—a database of gene andtranscript expression from microarray- and sequencing-based functionalgenomics experiments. Nucleic Acids Res., 42, D926–D932.Silvester,N. et al. (2014) Content discovery and retrieval services at theEuropean Nucleotide Archive. Nucleic Acids Res., 43, D23–D29.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2218/3078600by gueston 07 January 2018
5028369168002	PMID28369168	5028369168	https://watermark.silverchair.com/btx148.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28369168.main.pdf	Bioinformatics, 33(14), 2017, 2209–2211doi: 10.1093/bioinformatics/btx148Advance Access Publication Date: 22 March 2017Applications NoteSequence analysisConKit: a python interface to contactpredictionsFelix Simkovic, Jens M. H. Thomas and Daniel J. Rigden*Department of Biochemistry, Institute of Integrative Biology, University of Liverpool, Liverpool L69 7ZB, UK*To whom correspondence should be addressed.Associate Editor: Janet KelsoReceived on November 30, 2016; revised on March 10, 2017; editorial decision on March 13, 2017; accepted on March 14, 2017AbstractSummary: Recent advances in protein residue contact prediction algorithms have led to the emergence of many new methods and a variety of ﬁle formats. We present ConKit, an open source,modular and extensible Python interface which allows facile conversion between formats and provides an interface to analyses of sequence alignments and sets of contact predictions.Availability and Implementation: ConKit is available via the Python Package Index. The documentation can be found at http://www.conkit.org. ConKit is licensed under the BSD 3-Clause.Contact: hlfsimko@liverpool.ac.uk or drigden@liverpool.ac.ukSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionResidue–residue contact predictions are becoming an increasinglypopular and dynamic field of bioinformatics research as well assource of information in structural biology. Over recent years, greatadvances have been made to facilitate highly accurate predictions(e.g. Jones et al., 2015; Marks et al., 2011), which enabled these predicted contacts to, for example, guide accurate structure predictions(e.g. Michel et al., 2014; Ovchinnikov et al., 2015), identify functional sites (e.g. Grigolon et al., 2016; Hopf et al., 2012; Parenteet al., 2015), or predict modes of protein interaction (e.g. Hopfet al., 2014; Ovchinnikov et al., 2014).The many prediction algorithms and pipelines currently available have adopted a variety of different file format conventions.Additionally, metapredictors rely on file conversions to either combine various predictions, or standardize the output in their chosenstyle. These various file formats lead to a dilemma for software developers writing tools to utilize predicted contacts. Although astandardized format—Casp RR—exists, it has not been widelyadopted. Therefore, software developers must either insist on agiven format for their tools, or develop an extensive library of conversion algorithms to handle multiple formats.Users of contact prediction methods are often faced with thechallenge of estimating the quality of a prediction. In co-evolutionbased methods, the quality typically depends on the MultipleSequence Alignment depth (Jones et al., 2015; Ovchinnikov et al.,2015). However, the quantification of this depth, commonlyreferred to as number of effective sequences, and other importantmeasures, such as the sequence coverage in the alignment, often requires further software packages or manual method development.With this motivation, ConKit was developed to satisfy many ofthe issues outlined earlier and to provide additional functionalityuseful to a variety of software developers and users.2 Materials and methodsConKit is a cross-platform package written in the Python programming language. It is based in part on the NumPy (Oliphant, 2015),SciPy (http://www.scipy.org), BioPython (Cock et al., 2009) and matplotlib (Hunter, 2007) packages. ConKit’s modular design enables itto have numerous applications as a standalone package. It is currentlymade up of four distinct packages—the data model, input/outputstructure, plotting facility and command-line application wrappers.ConKit also easily integrates into larger software packages and it is already distributed with CCP4 v7.0.032 (Winn et al., 2011) and CCPEM beta (update January 7, 2017) (Wood et al., 2015).2.1 Data modelThe underlying data model in ConKit stores contact information ina three-tier hierarchy, which provides easy access to the contact information stored within. At its lowest level, ConKit stores individualCV The Author 2017. Published by Oxford University Press.2209This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permitsunrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2209/3078601by gueston 07 January 20182210contact pairs in the Contact class. All Contact instances are combined and held in a ContactMap class, which provides routinefunctions to handle all contacts in a single prediction. At the toplevel, ConKit allows users to store multiple ContactMap instancesin the ContactFile class. Each tier has attributes and functionsrelating to the data stored within, such as the sequence attribute inthe ContactMap class, which allows users to easily associate a sequence with a contact prediction.Alongside the data model for contact information, aSequenceFile hierarchy was implemented. Although BioPython’sSeqIO and AlignIO packages (Cock et al., 2009) already providesuch a data structure, ConKit’s hierarchy enables customized interactions for the models. Two tiers are currently implemented, withthe SequenceFile class storing one or more Sequence classes.Both hierarchies provide storage, modification and analysismethods. For example, a ContactMap instance allows users to calculate the precision value of a given contact map when comparedagainst the contacts extracted from a protein structure (Morcoset al., 2011). This feature is essential for assessing the quality of contact predictions when structural information is available (Joneset al., 2015; Monastyrskyy et al., 2016). It also has potential valuein scoring the quality of ab initio models based on the number oflong-range contacts fulfilled in a model (de Oliveira et al., 2016;Ovchinnikov et al., 2017).In comparison, a SequenceFile instance enables users to calculate the alignment depth, a key estimate for assessing the usefulness ofan alignment in co-evolution based predictions (Monastyrskyy et al.,2016). A SequenceFile instance also provides the functionality todetermine the sequence coverage in the stored alignment, which canprove useful when trimming alignments to the core region of a proteindomain.2.2 Input/outputManually constructing a data model in ConKit is generally not necessary. Four sequences and 17 contacts file format parsers havebeen implemented to allow read and write functionality.Importantly, the modular design of ConKit allows for an easy addition of new file format parsers in the future. In general, the twomethods, read() and write(), are the access points to the parsers.To make file format conversions more accessible a third notable I/Ofunction has been implemented, namely convert(), which acts asa wrapper encapsulating read() and write(). For a full list ofavailable file formats, refer to ConKit’s documentation.2.3 Data visualizationBesides the analysis functionality outlined previously, ConKit also provides an interface for data visualization. It uses the matplotlib (Hunter,2007) package and ConKit’s data models to extract and visualize data.Using the matplotlib package enables native support for many file formats, such as Portable Network Graphics or Scalable Vector Graphics.In ConKit, all plots are created via Python classes, thus enabling fullcustomizability via simple class attribute setting. ConKit provides plotting classes for both SequenceFile and ContactMap hierarchies,such as the SequenceCoverageFigure class to illustrate the sequence coverage (Fig. 1a) in a Multiple Sequence Alignment or theContactMapFigure for the commonly used contact map visualization (Supplementary Fig. S1). ConKit also provides thePrecisionEvaluationFigure class for a stepwise evaluation ofprecision scores when comparing a predicted contact map to proteinstructure (Fig. 1b). For a full list of available plots alongside usage examples, refer to the documentation.F.Simkovic et al.Fig. 1. Data visualization in ConKit. (a) Sequence coverage in an exampleMultiple Sequence Alignment. (b) Precision score evaluation of a contact prediction ﬁle at various contact cutoff values where L ¼ sequence length(rounded down to the nearest whole number of contacts). Data for both examples are based on PDB entry: 1DTX2.4 Command-line application interfaceConsidering the number of different features of ConKit, we believethat the command-line application interface could be particularlyuseful to create metapredictor pipelines. To date, these wrapperscomprise the following executables: HHblits (Remmert et al.,2012); Jackhmmer (Johnson et al., 2010); HHfilter (Remmert et al.,2012); CCMpred (Seemayer et al., 2014); PSICOV (Jones et al.,2012); and bbcontacts (Andreani and Soding, 2015). All wrappers¨are based on the AbstractCommandline class in BioPython, andthus a fully working version of the package is required for thisConKit sub-package.3 UsageConKit can be used in two distinct ways. To access all features, usersdo require some familiarity with the Python programming language.All packages outlined earlier are available via Python’s import system and relevant classes are exposed. To circumvent this requirementfor non-programmers and make ConKit a more general tool, predefined routines in the form of scripts are automatically installed giving the general user access to ConKit’s key features from the command line. All scripts are written in Python making them operatingsystem independent. All scripts have the prefix conkit and a oneword suffix based on its function. For example, conkit-msatooland conkit-convert can be used to analyse alignments and convert contact prediction files, respectively, while conkit-precisioncalculates the precision value given a contact prediction, the corresponding sequence and a protein structure. The conkit-plot scriptexposes ConKit’s plotting package for simple figure generation. For afull list of available scripts, refer to ConKit’s documentation.4 ConclusionsWe present ConKit, an extensible and modular Python interface forhandling and manipulating residue–residue contact predictions,multiple sequence alignments and contact maps. Its core functionality is enhanced by the provision of command line scripts and application wrappers.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2209/3078601by gueston 07 January 2018ConKitAcknowledgementsThe authors would like to acknowledge Stefan Seemayer for the provision ofhelpful software libraries.FundingThis work was supported by the Biotechnology and Biological SciencesResearch Council grant [BB/L008696/1]. CCP4 co-funds FS’s studentship.Conﬂict of Interest: none declared.ReferencesAndreani,J. and Soding,J. (2015) bbcontacts: prediction of b-strand pairing¨from direct coupling patterns. Bioinformatics, 31, 1729–1737.Cock,P.J.A. et al. (2009) Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics, 25,1422–1423.de Olivera,S.H. et al. (2016) Comparing co-evolution methods and their application to template-free protein structure prediction. Bioinformatics, 33,373–381.Grigolon,S. et al. (2016) Identifying relevant positions in proteins by CriticalVariable Selection. Mol. Biosyst., 12, 2147–2158.Hopf,T.A. et al. (2012) Three-dimensional structures of membrane proteinsfrom genomic sequencing. Cell, 149, 1607–1621.Hopf,T.A. et al. (2014) Sequence co-evolution gives 3D contacts and structures of protein complexes. Elife, 3, e03430.Hunter,J.D. (2007) Matplotlib: a 2D graphics environment. Comput. Sci.Eng., 9, 90–95.Johnson,L.S. et al. (2010) Hidden Markov model speed heuristic and iterativeHMM search procedure. BMC Bioinformatics, 11, 431.Jones,D.T. et al. (2012) PSICOV: precise structural contact prediction usingsparse inverse covariance estimation on large multiple sequence alignments.Bioinformatics, 28, 184–190.2211Jones,D.T. et al. (2015) MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins.Bioinformatics, 31, 999–1006.Marks,D.S. et al. (2011) Protein 3D structure computed from evolutionary sequence variation. PLoS One, 6, e28766.Michel,M. et al. (2014) PconsFold: improved contact predictions improve protein models. Bioinformatics, 30, i482–i488.Monastyrskyy,B. et al. (2016) New encouraging developments in contact prediction: assessment of the CASP11 results. Proteins, 84 (Suppl 1), 131–144.Morcos,F. et al. (2011) Direct-coupling analysis of residue coevolution captures native contacts across many protein families. Proc. Natl. Acad. Sci.USA, 108, E1293–E1301.Oliphant,T.E. (2015) A Guide to NumPy, 2nd edn. Continuum Press. Austin,USA.Ovchinnikov,S. et al. (2014) Robust and accurate prediction of residue–residue interactions across protein interfaces using evolutionary information.Elife, 3, e02030.Ovchinnikov,S. et al. (2015) Large-scale determination of previously unsolvedprotein structures using evolutionary information. Elife, 4, e09248.Ovchinnikov,S. et al. (2017) Protein structure determination using metagenome sequence data. Science, 355, 294–298.Parente,D.J. et al. (2015) Amino acid positions subject to multiple coevolutionary constraints can be robustly identiﬁed by their eigenvector networkcentrality scores. Proteins, 83, 2293–2306.Remmert,M. et al. (2012) HHblits: lightning-fast iterative protein sequencesearching by HMM-HMM alignment. Nat. Methods, 9, 173–175.Seemayer,S. et al. (2014) CCMpred—fast and precise prediction of proteinresidue–residue contacts from correlated mutations. Bioinformatics, 30,3128–3130.Winn,M.D. et al. (2011) Overview of the CCP4 suite and current developments. Acta Crystallogr. D Biol. Crystallogr., 67, 235–242.Wood,C. et al. (2015) Collaborative computational project for electron cryomicroscopy. Acta Crystallogr. D Biol. Crystallogr, 71, 123–126.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2209/3078601by gueston 07 January 2018
5028334395002	PMID28334395	5028334395	https://watermark.silverchair.com/btx136.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28334395.main.pdf	Bioinformatics, 33(14), 2017, 2207–2208doi: 10.1093/bioinformatics/btx136Advance Access Publication Date: 9 March 2017Applications NoteSequence analysispoRe GUIs for parallel and real-time processingof MinION sequence dataRobert D. Stewart1 and Mick Watson1,2,*1Department of Genetics and Genomics and 2Edinburgh Genomics, The Roslin Institute and R(D)SVS, University ofEdinburgh, Easter Bush EH25 9RG, UK*To whom correspondence should be addressed.Associate Editor: Bonnie BergerReceived on December 19, 2016; revised on February 14, 2017; editorial decision on March 3, 2017; accepted on March 8, 2017AbstractMotivation: Oxford Nanopore’s MinION device has matured rapidly and is now capable of producing over one million reads and several gigabases of sequence data per run. The nature of theMinION output requires new tools that are easy to use by scientists with a range of computationalskills and which enable quick and simple QC and data extraction from MinION runs.Results: We have developed two GUIs for the R package poRe that allow parallel and real-time processing of MinION datasets. Both GUIs are capable of extracting sequence- and meta- data fromlarge MinION datasets via a friendly point-and-click interface using commodity hardware.Availability and Implementation: The GUIs are packaged within poRe which is available onSourceForge: https://sourceforge.net/projects/rpore/ﬁles/.Documentation is available on GitHub: https://github.com/mw55309/poRe_docs.Contact: mick.watson@roslin.ed.ac.uk1 IntroductionNanopore sequencing is the only sequencing technology that measures an actual single molecule of DNA, rather than incorporationevents into a template strand (Goodwin et al., 2016; Loman andWatson, 2015). Early access to Oxford Nanopore’s MinION, a portable DNA sequencer approximately six inches in length, began in2014. The MinION may be considered a mature platform, havingbeen used to sequence bacterial genomes (Loman et al., 2015; Risseet al., 2015); resolve repeats in the human genome (Jain et al., 2015);study cDNA structure (Hargreaves and Mulley, 2015; Bolisettyet al., 2015); detect base modifications (Rand et al., 2016; Karlssonet al., 2015; Stoiber et al., 2016); detect antibiotic resistance (Ashtonet al., 2014); perform real-time enrichment (‘read until’; Loose et al.,2016) and provide surveillance in a human disease outbreak (Quicket al., 2016). The latest chemistry release, R9.4, has seen the firsthigh-coverage human genome data released (https://github.com/nanopore-wgs-consortium/NA12878; https://github.com/nanoporetech/ONT-HG1), with several MinION flowcells from the two projectsproducing over 4 gigabases (Gb) of sequence data.The MinION has been designed to enable mobile, real-timesequencing. As soon as a sequencing library is placed onto the device,the MinION begins sequencing. Each channel/nanopore reports asynchronously, creating a single file per channel per read. These are createdin HDF5, a compressed binary hierarchical data format (https://www.hdfgroup.org/). Depending on the sequencer and chemistry version,these HDF5 files include raw or event-level signal data, recorded as aDNA molecule passed through the pore. There are a range of basecalling options, including cloud-based Metrichor, local MinKNOWbase-calling and open-source alternatives (David et al., 2016; Bo azet al., 2016), that will convert the signal data into DNA sequences.With 512 pores and a sequencing speed of several hundred basesper-second, each MinION flowcell has the capacity to produce severalmillion reads in a 48-hour run. As each read presents as two files (oneraw, one base-called) MinION runs represent huge challenges for researchers without sufficient computational skills. Tools exist, such aspoRe(Watson et al., 2015) and poretools (Loman and Quinlan, 2014),to assist with this, but many are command-line based, and there is a needfor easy-to-use, GUI-based tools for MinION data QC and analysis.CV The Author 2017. Published by Oxford University Press.2207This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permitsunrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2207/3064508by gueston 07 January 20182208R.D.Stewart and M.WatsonFig. 1. Screenshot of the pore parallel GUI, which as a Shiny App will open in the user’s browser2 Materials and methodsWe have designed and built two graphical-user-interfaces (GUIs) forMinON data processing, organization and extraction. Both are built asShiny apps and released as part of the package poRe (Watson et al.,2015). At present the original poRe and the new GUII code are separate, but we envisage merging the functions over time. Both are availablethrough the R package poRe. The poRe real-time GUI is designed toextract data (FASTQ, FASTA and metadata) during a run, or duringbase-calling. A source and destination folder are required. The softwarethen monitors the source folder for new FAST5 files; as FAST5 files arrive in the folder, they are processed, data are extracted and output tothe destination folder. The poRe real-time GUI saves researchers a hugeamount of time as data can be extracted while the MinION is running.The poRe real-time GUI is accessed by running the command pore_rt().The pore parallel GUI (Fig. 1) is designed to extract data from runsthat have already finished. Again, the software expects a source and destination folder; in addition, the user can select which data to extract, andthe number of cores to use. The software then extracts FASTQ, FASTAand metadata from all files in the source folder into files in the destinationfolder; using the number of cores specified by the user, via the parallelpackage. The poRe parallel GUI is accessed via the pore_parallel()command.3 ResultsThe poRe parallel GUI was able to simultaneously extract FASTQ,FASTA and metadata from 209 819 FAST5 files downloaded fromthe ‘cliveome’ project in just 37 min on our 16-core Linux server, ata rate of approx. 90 FAST5 files per second.FundingThis work was supported by The Biotechnology and Biological SciencesResearch Council (BBSRC) including institute strategic support to The RoslinInstitute (BB/M020037/1, BB/J004243/1, BB/J004235/1, BBS/E/D/20310000).Conflict of Interest: The authors have received free flowcells and reagentsfrom Oxford Nanopore as part of the MAP. Mick Watson has attendedOxford Nanopore events and had his travel paid for by ONT.ReferencesAshton,P.M. et al. (2014) MinION nanopore sequencing identiﬁes the position andstructure of a bacterial antibiotic resistance island. Nat. Biotechnol., 33, 296–300.Bolisetty,M.T. et al. (2015) Determining exon connectivity in complexmRNAs by nanopore sequencing. Genome Biol., 16, 204.Bo a,V. et al. (2016) DeepNano: Deep Recurrent Neural Networks for BasezCalling in MinION Nanopore Reads. https://arxiv.org/abs/1603.09195.David,M. et al. (2016) Nanocall: An Open Source Basecaller for OxfordNanopore Sequencing Data. Bioinformatics., 33, 49–55.Goodwin,S. et al. (2016) Coming of age: ten years of next-generation sequencing technologies. Nat. Rev. Genet., 17, 333–351.Hargreaves,A.D., and Mulley,J.F. (2015) Assessing the utility of the OxfordNanopore MinION for snake venom gland cDNA sequencing. PeerJ, 3, e1441.Jain,M. et al. (2015) Improved data analysis for the MinION nanopore sequencer. Nat. Methods, 12, 351–356.Karlsson,E. et al. (2015) Scaffolding of a bacterial genome using MinIONnanopore sequencing. Sci. Rep., 5, 11996.Loman,N.J. et al. (2015) A complete bacterial genome assembled de novousing only nanopore sequencing data. Nat. Methods, 12, 733–735.Loman,N.J., and Quinlan,A.R. (2014) Poretools: a toolkit for analyzing nanopore sequence data. Bioinformatics, 30, 3399–3401.Loman,N.J., and Watson,M. (2015) Successful test launch for nanoporesequencing. Nat. Methods, 12, 303–304.Loose,M. et al. (2016) Real-time selective sequencing using nanopore technology. Nat. Methods, 13, 751–754.Quick,J. et al. (2016) Real-time, portable genome sequencing for Ebola surveillance. Nature, 530, 228–232.Rand,A.C. et al. (2016) Cytosine variant calling with high-throughput nanopore sequencing cold. Spring Harbor Labs J. http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.4189.html.Risse,J. et al. (2015) A single chromosome assembly of Bacteroides fragilis strainBE1 from Illumina and MinION nanopore sequencing data. Gigascience, 4, 60.Stoiber,M.H. et al. (2016) De novo identiﬁcation of DNA modiﬁcationsenabled by genome-guided nanopore signal processing. http://biorxiv.org/content/early/2016/12/15/094672.Watson,M. et al. (2015) poRe: an R package for the visualization and analysisof nanopore sequencing data. Bioinformatics, 31, 114–115.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2207/3064508by gueston 07 January 2018
5028334344002	PMID28334344	5028334344	https://watermark.silverchair.com/btx139.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28334344.main.pdf	Bioinformatics, 33(14), 2017, 2232–2234doi: 10.1093/bioinformatics/btx139Advance Access Publication Date: 11 March 2017Applications NoteSystems biologyEstimating gene regulatory networkswith pandaRDaniel Schlauch1,2,†, Joseph N. Paulson1,2,†, Albert Young1,2,Kimberly Glass3 and John Quackenbush1,2,*1Department of Computational Biology and Biostatistics, Dana-Farber Cancer Institute, 2Department ofBiostatistics, Harvard T.H. Chan School of Public Health and 3Channing Division of Network Medicine, Brighamand Women’s Hospital, Boston, MA 02115, USA*To whom correspondence should be addressed.†The authors wish it to be known that, in their opinion, the ﬁrst two authors should be regarded as Joint First Authors.Received on December 22, 2016; revised on March 7, 2017; editorial decision on March 7, 2017; accepted on March 9, 2017AbstractPANDA (Passing Attributes between Networks for Data Assimilation) is a gene regulatory network inference method that begins with a model of transcription factor–target gene interactions and usesmessage passing to update the network model given available transcriptomic and protein–proteininteraction data. PANDA is used to estimate networks for each experimental group and the networkmodels are then compared between groups to explore transcriptional processes that distinguish thegroups. We present pandaR (bioconductor.org/packages/pandaR), a Bioconductor package that implements PANDA and provides a framework for exploratory data analysis on gene regulatory networks.Contact: johnq@jimmy.harvard.edu or dschlauch@fas.harvard.eduAvailability and Implementation: PandaR is provided as a Bioconductor R Package and is availableat bioconductor.org/packages/pandaR.1 Introduction2 Materials and methodsWhile correlation-based networks are widely used in transcriptomicdata analysis, such networks do not explicitly model the biologicalmechanisms involved in regulating gene expression, such as thebinding of transcription factors (TFs) to the regulatory regions of agene. Passing Attributes between Networks for Data Assimilation(PANDA) (Glass et al., 2013) is an integrative network inferencemethod that explicitly models interactions between TFs and theirputative target genes. PANDA starts with an initial network modelderived from motif-based TF–target mapping to the genome, anduses a message-passing framework to refine that initial model ineach phenotype given gene expression and other data. PANDA doesnot directly incorporate co-expression information between regulators and targets. Instead, edges in PANDA networks reflect the overall consistency between a TF’s regulatory profile with the targetgene’s co-expression. In a number of applications, PANDA has provided insight into the regulatory context of genes and TFs associatedwith disease and other phenotypes (Glass et al. 2014, 2015; Laoet al., 2015; Vargas et al., 2016).2.1 PANDAPANDA’s regulatory network model is fundamentally a bipartitegraph in which TFs are connected to target genes. In PANDA’s message passing model, the edge weights are calculated based on the evidence that information from a particular TF is successfully passed toa particular gene. This evidence comes from the agreement betweentwo estimated quantities on each edge, referred to as the availabilityand the responsibility (Fig. 1).The availability is an estimate of the responsiveness of a gene j toTF i. The assumption in calculating the availability is that geneswith correlated expression are likely to be regulated by a commonTF. Hence, the availability is based on correlation in expression between gene j and other genes with the strength of evidence for regulatory interactions (edges) between TF i and other genes. Analogousto this, the responsibility is an estimate of the influence of TF i ongene j, and models the fact that TFs that form a complex are morelikely to regulate the same target gene. The estimated responsibilityis therefore based on the concordance between the set of TFs knownCV The Author 2017. Published by Oxford University Press.2232This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permitsunrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2232/3066290by gueston 07 January 2018pandaR Bioconductor package2233Fig. 1. The PANDA algorithm takes as input protein–protein interaction data,a transcription factor–gene interaction network prior and gene expressiondata. Three networks representing inferred TF–TF co-operativity, TF–generegulatory processes and gene co-regulation are then iteratively updatedusing message passing until the model convergespandaResultCase), which presents a scatterplot of the edgeweights between two inferred networks. This function integrates withggplot2, allowing the user to define graphics based on genes and TFsto easily identify functionally relevant sets of differential edge weights.We also implemented a function calcDegreeDifference() tocalculate a gene’s degree or the degree difference between regulatorynetworks. Since it is important to benchmark the predicted edges ofpandaR and compare its performance with alternative methods, wehave included a function, validateNetwork(), which integratesthe package ROCR and can be used to compare PANDA’s network inference results against a known reference standard. Finally, users canalso use the function lioness(), which uses a unique linear interpolation method to estimate network models for each individual sample in a population (Kuijjer et al., 2015). Unlike other methods thatproject gene expression onto an existing network, the lioness()function uses a leave-one-out method to estimate each individual edgeweight in the network.2.2 Data inputto interact with TF i (based on protein–protein interaction (PPI)data) and the respective strength of evidence of a regulatory association between those other TFs and gene j. Both the availability andresponsibility are estimated using a modified version of theTanimoto similarity. We define this similarity between two nodes, iPx yk k kP 2 P 2 Pand j, as Ti;j ¼ pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ. In the case of availability, xkkxk þkyk Àjkxk yk jis the correlation between gene j and gene k, and yk is the edgeweight between TF i and gene k. In the case of responsibility xk represents whether a PPI exists between TF i and TF k, and yk is theedge weight between TF k and gene j.The edge weight between TF i and gene j is then updated based onthe mean of the responsibility and availability. PANDA iteratively estimates edge weights until the process converges to the most parsimonious structure for information flow given the data. The result is anetwork model, represented by edge weights for every pairwise combination of TFs and genes, based on evidence from gene expression,sequence motif and PPI data. In analysis of simulated data, and datafrom systems in which confirmatory ChIP evidence are available,PANDA has been shown to outperform other competing methods(Glass et al., 2013).pandaR implements the PANDA algorithm in an easy-to-useBioconductor package. Beginning with gene expression, TF gene interaction priors, and optional PPIs, pandaR generates a regulatory network for m TFs regulating n genes and presents it as an m Â nadjacency matrix. It also provides the user with an estimated TF-by-TF‘cooperativity network’ and gene-by-gene ‘co-regulation network’, bothestimated by PANDA and reported as complete graphs representing theevidence for TF cooperation and gene co-regulation, respectively.pandaR also provides a number of exploratory data visualizationsof the inferred network’s properties and diagnostic tools. Our hypothesis in developing PANDA was that gene regulatory networks differbetween biological states and that changes in the network are linkedto phenotypic differences. Therefore, the pandaR package extends thePANDA network inference model by including a number of functionsthat can aide in comparing network structures between phenotypes.For example, the function plotGraph(topSubnet) integrates withigraph to generate a bipartite visualization of the PANDA networks.Because networks are often organized into functionally coherentcommunities, users can investigate and plot community structureusing plotCommunityDetection(topNet). Additional networkcomparison functions include plotZ(pandaResultControl,pandaR accepts input data in a variety of formats. Gene expressiondata can be input as either a data.frame, matrix, or as aBioconductor ExpressionSet. TF gene interaction priors for the regulatory network can be input as a matrix or data.frame, with tripletcolumns specifying a putative regulatory edge from a TF (column 1)to gene (column 2) with a defined weight (column 3), typically initialized as 1.0; the regulatory prior is generally based on mappingTF motifs to target genes based on genomic sequence information.PPI data are not required but can be input as a either a matrix ordata.frame that includes protein pairs and an interaction weight.Annotation type is not restricted except that node names forgenes in the regulatory file must match node names in the gene expression file and TF names must match in both the regulatory andPPI inputs.2.3 ExampleAn example data set generated from a subset of human gene expression data is available by running: data(pandaToyData).The primary function in pandaR is called usingpandaResult <– panda(pandaToyData$motif,pandaToyData$expression, pandaToyData$ppi)where pandaResult is a ‘panda’ object that contains matricesdescribing the complete bipartite gene regulatory network and complete networks for gene co-regulation and TF cooperation. Due tothe completeness of the input data, edge weights for the regulatorynetwork are reported for all m Â n TF–gene edges.The distribution of these edge weights has approximate mean 0and standard deviation 1. The edges are therefore best interpreted in arelative sense. Strongly positive values are indicative of relativelygreater evidence of a regulatory TF–gene association and smaller ornegative values can be interpreted as lacking evidence of regulatoryinteraction. Consequently, users often want to see only a high edgeweight subset of the complete network in order to focus on the moststrongly supported regulatory interactions. This filtering is performedusing the topedges function. A network containing the top 1000edge scores as binary edges can be obtained using the commandtopNet <– topedges(pandaResult, 1000)The network can be further simplified to a TF set of interest by usingthe subnetwork method,Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2232/3066290by gueston 07 January 20182234trFactors <– c(“TLX1”,”VDR”,”RXRA”,”PPARG”)topSubnet <– subnetwork(topNet, trFactors)to limit the output to a subset of TFs and the genes that they arefound to regulate.FundingThis work was supported by the US National Institutes of Health, includinggrants from the National Heart, Lung and Blood Institute [5P01HL105339,5R01HL111759, 5P01HL114501, K25HL133599]; the National CancerInstitute [5P50CA127003, 1R35CA197449, 1U01CA190234, 5P30CA006516];and the National Institute of Allergy and Infectious Disease [5R01AI099204].Conflict of Interest: none declared.D.Schlauch et al.Glass,K. et al. (2014) Sexually-dimorphic targeting of functionally-relatedgenes in COPD. BMC Syst. Biol., 8, 118.Glass,K. et al. (2015) A network model for angiogenesis in ovarian cancer.BMC Bioinformatics, 16, 115.Kuijjer,M.L. et al. (2015) Estimating Sample-speciﬁc Regulatory Networks.arXiv Preprint arXiv:1505.06440.Lao,T. et al. (2015) Haploinsufﬁciency of hedgehog interacting protein causesincreased emphysema induced by cigarette smoke through network rewiring. Genome Med., 7, 12.Vargas,A.J. et al. (2016) Diet-induced weight loss leads to a switch in generegulatory network control in the rectal mucosa. Genomics, 16,126–133.Youn,A. et al. (2010) Learning transcriptional networks from the integration ofChIP–chip and expression data in a non-parametric model. Bioinformatics,26, 1879–1886.ReferencesGlass,K. et al. (2013) Passing messages between biological networks to reﬁnepredicted interactions. PLoS ONE, 8, e64832.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2232/3066290by gueston 07 January 2018
5028334301002	PMID28334301	5028334301	https://watermark.silverchair.com/btx141.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28334301.main.pdf	Bioinformatics, 33(14), 2017, 2194–2196doi: 10.1093/bioinformatics/btx141Advance Access Publication Date: 11 March 2017Applications NoteGenome analysisDynamix: dynamic visualization by automaticselection of informative tracks from hundreds ofgenomic datasetsMatthias Monfort, Eileen E. M. Furlong and Charles Girardot*European Molecular Biology Laboratory, Genome Biology Unit, Heidelberg D-69117, Germany*To whom correspondence should be addressed.Associate Editor: John HancockReceived on January 23, 2017; revised on March 8, 2017; editorial decision on March 9, 2017; accepted on March 10, 2017AbstractMotivation: Visualization of genomic data is fundamental for gaining insights into genome function. Yet, co-visualization of a large number of datasets remains a challenge in all popular genomebrowsers and the development of new visualization methods is needed to improve the usabilityand user experience of genome browsers.Results: We present Dynamix, a JBrowse plugin that enables the parallel inspection of hundreds ofgenomic datasets. Dynamix takes advantage of a priori knowledge to automatically display datatracks with signal within a genomic region of interest. As the user navigates through the genome,Dynamix automatically updates data tracks and limits all manual operations otherwise needed toadjust the data visible on screen. Dynamix also introduces a new carousel view that optimizesscreen utilization by enabling users to independently scroll through groups of tracks.Availability and Implementation: Dynamix is hosted at http://furlonglab.embl.de/Dynamix.Contact: charles.girardot@embl.deSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionThe significant drop in sequencing costs made large genome-scalestudies like ENCODE (Consortium, 2012) a reality. Such projectshave resulted in thousands of publicly available genomics datasetsthat represent an invaluable source for new discoveries. To this end,human visual inspection of the data remains essential as newhypotheses often result from a meticulous exploration of newlyacquired data and its comparison with established datasets. In popular genome browsers (Hung and Weng, 2016; Stein, 2013;Thorvaldsdottir et al., 2013; Westesson et al., 2013; Yates et al.,2016), datsets are visualized as tracks where a track represents botha dataset and its visualization options. Visualized tracks are alignedto a common reference sequence and stacked on top of eachother (in the track container) to allow for direct comparison(Supplementary Fig. S1). Users typically interact with the data byzooming in and out, panning left and right or using embeddedsearch features (by genomic feature names or location). Althoughthousands of datasets are publicly available, it remains challengingto visualize more than a couple of dozens tracks in current web genome browsers. Indeed, stacking too many tracks usually comes atthe cost of smooth navigation due to the time needed to render thedata and the limited size of the screen (which forces the user to constantly scroll up and down and re-organize the tracks). JBrowse(Westesson et al., 2013) has brought significant improvements tothe former point through, for instance, the implementation of clientside rendering, query-efficient data structures and smooth—«Google Maps like»—navigation system. Desktop applications likeIGV (Thorvaldsdottir et al., 2013) also eliminate the downside ofshared-server resources using the user machine resources. However,the limitations related to the user’s hardware (RAM, CPU) and thescreen size remains and severely limit the number of tracks that canbe displayed. While a set of tracks may be relevant to display at agiven genomic location, the same tracks may no longer be relevantto display a few hundred base pairs downstream where the signaldrops to background level. As the navigation continues, the usermust scroll up and down, swap tracks in the track container andCV The Author 2017. Published by Oxford University Press.2194This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permitsunrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2194/3066292by gueston 07 January 2018drag tracks next to each other. To circumvent these issues, we present Dynamix, a JBrowse plugin that takes advantage of a prioriknowledge to automatically display relevant tracks as the userbrowses the genome and offers new innovative ways to browsetracks organized in logical groups.2195chr2R8,794,209:8,812,865Dynamix2 ResultsA key concept behind Dynamix is to anticipate users’ actions and execute them automatically. Practically, for a given genomic windowDynamix removes tracks without features or containing only background level signal (locus-specific empty tracks) from the track container and replaces them with tracks containing features or signal abovebackground (locus-specific interesting tracks). Quantitative signal datasets (e.g. 4C-seq or ChIP-seq coverage) are usually associated with feature datasets (e.g. peak lists) that describe the regions containing signal.This is obviously true for published datasets but also for in-house dataonce data processing has taken place. In Dynamix, a quantitative signaldataset is always registered together with its companion feature dataset,which describes the interesting regions with significant signal. Uponregistration, Dynamix uses both datasets to create standard JBrowsetracks and remembers them as a dependent track set. A special dynamix_features track compiles all features from all track sets registered inDynamix. As the user interacts with the browser, the dynamix_featurestrack is queried to assemble the list of locus-specific interesting tracksand update JBrowse’s track container (i.e. missing interesting tracks areadded following a pre-defined order while empty locus-specific tracksare removed, Fig. 1 and Supplementary Fig. S2). In addition, summaryinformation is communicated to the user through the Dynamix widget(Fig. 1, Supplementary Figs S2, S3A). Dynamic track management onlyoccurs for tracks registered in Dynamix and it is always possible tomanually add static tracks (JBrowse tracks not registered in Dynamix)in the track container, as with standard viewers.To provide more flexibility, and complement Dynamix’ automated track updating, Dynamix also offers a manual mode thatswitches off the automatic updating of the track container. This results in genome browsing with static track selection (as conventionalbrowser), but with a Dynamix widget that keeps the user informed(Supplementary Fig. S3A). In manual mode, explicit user interactionwith the Dynamix widget is required to update the track container(Supplementary Fig. S3A).2.2 Track groups and carousel browsingJuxtaposing related datasets in the browser is very common to bettercompare the data at hand, and some genome browsers offer definingtrack groups (Hung and Weng, 2016; Stein, 2013; Thorvaldsdottiret al., 2013; Yates et al., 2016). Dynamix brings this feature toJBrowse allowing managed tracks to be organized into different track groups. Each track group has its own configuration(Supplementary Fig. S3A) and its browsing mode can be individuallyset to automatic, manual or disabled (disabled mode instructsDynamix to simply ignore the track group).Controlling where tracks are automatically added into the trackcontainer is extremely important to achieve a consistent and reproducible user experience and Dynamix obeys different rules toachieve this goal. First, the tracks of a track group are always rendered in a continuous block (static tracks and tracks from aDynamix track group remain separate). Second, the display order oftracks within a group is pre-defined and fully customizable. Finally,track group anchoring within the track container can be configuredchr2R8,878,094 - 8,896,7502.1 Dynamic track displayFig. 1. Dynamic update of the track container. Dynamix-enabled JBrowse conﬁguration showing (from top to bottom) Flybase genes followed by 7 tracksmanaged by Dynamix altogether deﬁned as a group. Top, screenshot for thegenomic range 8 794 209–8 812 865 of chr2R where all 7 Dynamix managedtracks are displayed. Bottom, screenshot showing the same conﬁgurationafter the user moved 84 Kb away to the genomic range 8 878 094–8 896 750 ofchr2R. Dynamix still displays the Flybase genes (not managed by Dynamix)while only 3 of the original 7 tracks managed by Dynamix (Track 2, 4 and 7)remain visible; the four missing tracks have been automatically hidden fromthe track container by Dynamix. The visible track group (i.e. Track 1 to 7 in A)was conﬁgured to be anchored below the ‘Flybase Genes’ track. Red dashedboxes (pointed by red arrows) indicate the Dynamix speciﬁc menu and control widgetsuch that a track group is always displayed right below a particularstatic track or track group (Fig. 1, Supplementary Fig. S2A).At particular genomic locations, the number of interesting trackscan be large and exceed the user’s hardware capabilities, possibly resulting in a browser crash. To address this issue, the maximum number of tracks to display can be configured both globally (to matchuser’s hardware capabilities) and for each track group (to fine-tuneuser’s screen usage, Supplementary Fig. S3B). When the number ofinteresting tracks exceeds the configured maxima, only a fraction ofthe interesting tracks is rendered and a visual cue warns the userthat data is missing from the display (Supplementary Figs S3A andS4A). While this simple solution prevents the browser from crashing, the user has no chance to view the hidden interesting tracks. InDynamix, we solve this issue by introducing carousel browsingwhich allows infinite scrolling through all interesting tracks of agroup. Upon each click on the carousel control, half of the visibleinteresting tracks are replaced with hidden tracks following the predefined track order (Supplementary Fig. S4).2.3 Comparison with other genome browsersOrganizing tracks into broad categories (e.g. Genes, Variation,Repeats) is a common feature of Ensembl, UCSC and GBrowse genome browsers. JBrowse natively offers a similar (yet more general)solution based on track filtering using user-defined track annotations. In GBrowse, related subtracks (e.g. RNA-seq time series) canbe grouped into a single track, subtracks can then be individuallytoggled in the track and their display order controlled. In contrast,Dynamix track grouping also offers control over the display order ofthe tracks; in addition, it provides the carousel view and automatedselection of informative tracks instead of manual filtering of the subtracks. Dynamic browsing and carousel browsing are, to the best ofour knowledge, unique to Dynamix.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2194/3066292by gueston 07 January 20182196M.Monfort et al.3 ConclusionReferencesDynamix enables scientists to browse hundreds of datasets using newvisualization concepts. Readers can try Dynamix using the 4CBrowser(http://furlonglab.embl.de/4CBrowser) and the DynamixDemo serverwhich demonstrates how Dynamix can be used for rich displays (http://furlonglab.embl.de/DynamixDemo). Visualization of genomics datasetsremains a challenging area of research, we hope that Dynamix will encourage the development of innovative visualization methods.Consortium,T.E.P. (2012) An integrated encyclopedia of DNA elements in thehuman genome. Nature, 489, 57–74.Hung,J.-H. and Weng,Z. (2016) Visualizing genomic annotations with theUCSC genome browser. Cold Spring Harb. Protoc., 2016, pdb.prot093062.Stein,L.D. (2013) Using GBrowse 2.0 to visualize and share next-generationsequence data. Brief. Bioinf., 14, 162–171.Thorvaldsdottir,H. et al. (2013) Integrative Genomics Viewer (IGV): highperformance genomics data visualization and exploration. Brief. Bioinf.,14, 178–192.Westesson,O. et al. (2013) Visualizing next-generation sequencing data withJBrowse. Brief. Bioinf., 14, 172–177.Yates,A. et al. (2016) Ensembl 2016. Nucleic Acids Res., 44, D710–D716.AcknowledgementsWe thank Jelle Scholtalbers, Markus Fritz and the Furlong Lab for theircomments.Conﬂict of Interest: none declared.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2194/3066292by gueston 07 January 2018
5028334291002	PMID28334291	5028334291	https://watermark.silverchair.com/btx137.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28334291.main.pdf	Bioinformatics, 33(14), 2017, 2229–2231doi: 10.1093/bioinformatics/btx137Advance Access Publication Date: 9 March 2017Applications NoteSystems biologySmartR: an open-source platform for interactivevisual analytics for translational research dataSascha Herzinger1, Wei Gu1, Venkata Satagopam1, Serge Eifes1,2,Kavita Rege1, Adriano Barbosa-Silva1, Reinhard Schneider1,*and On behalf of the eTRIKS Consortium1Luxembourg Centre for Systems Biomedicine, University of Luxembourg, Esch/Belval, Luxembourg andInformation Technology for Translational Medicine (ITTM) S.A, Esch/Belval, Luxembourg2*To whom correspondence should be addressed.Associate Editor: John HancockReceived on January 25, 2017; revised on March 1, 2017; editorial decision on March 3, 2017; accepted on March 8, 2017AbstractSummary: In translational research, efﬁcient knowledge exchange between the different ﬁelds ofexpertise is crucial. An open platform that is capable of storing a multitude of data types such asclinical, pre-clinical or OMICS data combined with strong visual analytical capabilities will signiﬁcantly accelerate the scientiﬁc progress by making data more accessible and hypothesis generationeasier. The open data warehouse tranSMART is capable of storing a variety of data types and has agrowing user community including both academic institutions and pharmaceutical companies.tranSMART, however, currently lacks interactive and dynamic visual analytics and does not permitany post-processing interaction or exploration. For this reason, we developed SmartR, a plugin fortranSMART, that equips the platform not only with several dynamic visual analytical workﬂows,but also provides its own framework for the addition of new custom workﬂows. Modern web technologies such as D3.js or AngularJS were used to build a set of standard visualizations that wereheavily improved with dynamic elements.Availability and Implementation: The source code is licensed under the Apache 2.0 License and isfreely available on GitHub: https://github.com/transmart/SmartR.Contact: reinhard.schneider@uni.luSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionTranslational research can be described as an ‘interdisciplinarybranch of the biomedical field supported by three main pillars:benchside, bedside and community’ (Cohrs et al., 2015). One of themost difficult, yet most important, tasks in this field is the propercommunication and knowledge exchange between the differentfields of expertise. An information system that integrates all level ofdata (pre-clinical, clinical, OMICS, etc.) generated in research andthat provides an interactive interface to explore, visualize and analyze those data will substantially increase the efficiency of knowledge exchange and hypothesis generation.In the context of the eTRIKS (European Translational Information& Knowledge Management Services) consortium (https://www.etriks.org/, 2017) academia and pharma seek to combine their interdisciplinary knowledge to provide secure data environments and open sourcetools that help to answer important biological questions and enablethe discovery of new scientific facts within existing studies. ThetranSMART platform (Athey et al., 2013) addresses these requirements, supports a multitude of data types, has a well-established community and provides APIs, that make connections with a variety ofother services possible. Detailed reasons for this choice and comparisons to other existing analytics platforms are described elsewhere(Satagopam et al., 2016). A major problem currently present intranSMART is the lack of interactive visual-analytical functionality,which is essential for a collaborative knowledge management platform. In the present state, analytical workflows are restricted to theCV The Author 2017. Published by Oxford University Press.2229This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2229/3064509by gueston 07 January 20182230Fig. 1. The SmartR Heat Map. Seen is the interactive heat map in tranSMARTbased on the breast cancer mRNA data of the GEO study GSE4382 (Sorlieet al., 2003)displaying of static images generated by the statistical programminglanguage R. The static nature of this approach makes it very difficultto apply any post-processing analysis or to do further exploration,such as selecting a certain feature for further investigation. When confronted with this problem, one might first attempt to use existing webvisualization libraries like Highcharts (http://www.highcharts.com/,2017), Plotly.js (Plotly Technologies Inc., 2015), or one of many BioJS(G mez et al., 2013) components. Browsing existing heat map impleomentations or other basic visualizations, one can see that, althoughvisually appealing, they lack deep integration of more than the mostbasic statistics. Because of the analytical limitations of the webbrowser and the missing analytical engine, they cannot re-computeinitial input values and therefore often chose not to display many statistics in the first place to preserve their dynamism. Another approach isto combine custom visualizations with an analytical engine as shownin DIVE (Rysavy et al., 2014), BRAVIZ (Angulo et al. 2016),HitWalker2 (Bottomly et al., 2015), or Shiny (https://shiny.rstudio.com/, 2017).Using custom visualizations with a supporting analysis component enables the researcher to iteratively explore the data with eachanalysis step, in contrast to hypothesis-driven research. To makethis methodology available for translational researchers, we developed SmartR, a new, highly modular, analytical framework fortranSMART, that equips the platform with interactive and dynamicvisualization capabilities, built using recent web technologies.2 Materials and methodsThe tranSMART platform uses Grails (https://grails.org/, 2017) as aweb-framework, which provides a plugin architecture of its own.Therefore, it was a natural choice to use Grails for the back-end ofour plugin as well. This has the benefit of having direct access to internal services and APIs to ensure consistent database access acrossthe different Oracle and Postgres versions of the platform, whichhelps to keep maintenance low.To support the user graphics with non-trivial statistics, for instance clustering information, it was necessary to properly integratea language for statistical computation, such as R (R DevelopmentCore Team, 2011), into Grails. Because Grails uses Groovy, whichintegrates well with most standard Java libraries, we could use theS.Herzinger et al.Java client for Rserve, ‘a TCP/IP server which allows other programs to use facilities of R’ (Urbanek, 2003). In other words, thisallows direct read and write access with respect to the RSession viathe back-end of our application.As a base for the front-end, we decided to use the frameworkAngularJS (https://angularjs.org/, 2017) to enforce a MVC (ModelView-Controller) structure for each workflow. Besides the usual advantages with regard to maintenance and testability, the enforcement of a specific workflow structure helps to keep a similarstructure to each workflow, even with multiple contributing developers with different levels of experience. This was a high prioritygoal from the outset, because it would allow the formation of asmall community, which could contribute their own ideas and requirements to the plugin. Another reason for building upon an almost completely decoupled framework, rather than integratingSmartR directly into tranSMART, are the regular changes ofthe platform’s code base and the long list of partially outdateddependencies.The visualizations are implemented as AngularJS directives,which enables arbitrary placement of the plots in HTML.Technically, most JavaScript visualization libraries can be usedwithin such a directive, but we focused on the low-level libraryD3.js, ‘a JavaScript library for manipulating documents based ondata’ (Bostock et al., 2011). While the coding effort to create evenbasic visualizations is quite high, D3.js gives a high level of freedomfor customization and creativity to the developer. This allowed us toimplement features which we found useful that were not providedby other visualization libraries. An example for this is the dynamicheat map that we created.3 ResultsThe framework itself equips the tranSMART platform with a newanalytical engine that is testable, maintainable, and expandable. Wealso provide a series of prebuilt, commonly used visual-analyticalworkflows. In the following, we will focus on one of these workflows, namely the interactive heat map, as an example to illustratethe interactive and dynamic nature of the platform. Videos, screenshots and links to public test servers for all created visual analyticscan be found in the Supplementary Material.Since several decades, heat maps are a common tool for analyzing gene expression data, but displaying non-static heat maps withthe limited resources of a web-browser is a challenge. The SmartRheat map provides a solution by implementing a lazy-loading approach, where initially only the 100 most significant genes accordingto user defined ranking criteria are displayed. This reduction of displayed data lets us treat the single fields of the heat map as movabledynamic elements, rather than a static image. Doing so leads to several useful features, such as the possibility to change the clusteringon-the-fly, select various color sets for different data types and accessibility (color blindness), or to sort rows and columns of the heatmap. Another feature is the possibility to ‘expand’ the heat map byoverlaying non-array, one-dimensional data types (see Fig. 1), e.g.phenotypic data like ‘Age’ (numerical) or ‘Tumor Type—T0’ (categorical). In this way one can directly relate clusters or single samples to certain user defined groups within the selected cohort(s),leading to a much better understanding of the data across differentdata types. All displayed genes can be directly linked to external annotation databases like Gene Cards and the EMBL-EBI database.This function allows the user to link the findings to much broaderknowledge bases with a single click. Similar functionality is revealedDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2229/3064509by gueston 07 January 2018SmartR: analytics for translational research datawhen we apply a clustering to the heat map and click on one of theresulting dendrogram nodes. This will gather all genes in the respectivesub-tree and trigger a KEGG pathway enrichment analysis via externaltools like BioCompendium (http://biocompendium.embl.de/, 2017).This allows us to link a cluster directly to a possibly related KEGGpathway. Besides the interactive heat map, we have also provided afew other commonly used analyses like correlation analysis, where theuser can select regions on the correlation plot and get updated analysisinstantly, box plots, volcano plots, and line graphs for visualizinglongitudinal-like data. We could not exhaust all possible analyses during our implementation but we would like to emphasis that SmartRnot only provides a list of pre-built analyses but also provides a framework for easy implementation of customized workflows. To trulygrasp the dynamic nature of this approach, we highly recommendwatching the related videos available in the Supplementary Material.AcknowledgementsThe authors would like to thank Florian Guitton (ICL), Axel Oehmichen (ICL),The Hyve team, Manfred Hendlich (Sanoﬁ), Heike Schuermann (Sanoﬁ), RogerioMartins (Sanoﬁ) and Annick Peleraux (Sanoﬁ) for their help and support.FundingThe eTRIKS consortium receives funding from the European Union and fromthe European Federation of Pharmaceutical Industries and Associations as anIMI JU funded project (no.). 115446.Conﬂict of Interest: none declared.2231ReferencesAngulo,D.A. et al. (2016) A multi-facetted visual analytics tool for exploratoryanalysis of human brain and function datasets. Front Neuroinform., 10, 36.Athey,B.D. et al. (2013) tranSMART: an open source and community-driveninformatics and data sharing platform for clinical and translational research. AMIA Jt Summits Transl. Sci. Proc., 2013, 6–8.Bostock,M. et al. (2011) D3 data-driven documents. IEEE Trans. Vis.Comput. Graph., 17, 2301–2309.Bottomly,D. et al. (2016) HitWalker2: visual analytics for precision medicineand beyond. Bioinformatics, 32, 1253–1255.Cohrs,R.J. et al. (2015) Translational Medicine deﬁnition by the EuropeanSociety for Translational Medicine. Nhtm, 2, 86–88.G mez,J. et al. (2013) BioJS: an open source JavaScript framework for biologiocal data visualization. Bioinformatics, 29, 1103–1104Plotly Technologies Inc. Collaborative Data Science. Montreal, QC, 2015.´R Development Core Team (2011) R: A Language and Environment forStatistical Computing. Vienna, Austria: the R Foundation for StatisticalComputing. ISBN: 3-900051-07-0.Rysavy,S.J. et al. (2014) DIVE: A graph-based visual-analytics framework forbig data. IEEE Comput. Graph. Appl. Mag., 34, 26–37.Satagopam,V. et al. (2016) Integration and visualization of translational medicine data for better understanding of human diseases. Big Data, 4, 97–108.Sorlie,T. et al. (2003) Repeated observation of breast tumor subtypes in independent gene expression data sets. Proc. Natl. Acad. Sci. USA., 100,8418–23.Urbanek,S. (2003) Rserve: a fast way to provide R functionality to applications. In: Hornik,F.L.K. and Zeileis,A. (Eds.) Proceedings of the 3rdInternational Workshop on Distributed Statistical Computing (DSC 2003),Vienna, Austria, pp. 20–22.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2229/3064509by gueston 07 January 2018
5028334267002	PMID28334267	5028334267	https://watermark.silverchair.com/btx134.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28334267.main.pdf	Bioinformatics, 33(14), 2017, 2205–2206doi: 10.1093/bioinformatics/btx134Advance Access Publication Date: 14 March 2017Applications NoteSequence analysisResistoMap—online visualization of human gutmicrobiota antibiotic resistomeKonstantin S. Yarygin1,2,*, Boris A. Kovarsky2, Tatyana S. Bibikova3,Damir S. Melnikov3, Alexander V. Tyakht1,2 and Dmitry G. Alexeev1,21Moscow Institute of Physics and Technology (MIPT), 9 Institutskiy per, Dolgoprudny, Moscow Region 141700,Russian Federation, 2Federal Research and Clinical Centre of Physical-Chemical Medicine (FRCC PCM), MalayaPirogovskaya 1a, Moscow 119435, Russian Federation and 3Data Laboratory, Kostycheva 64-12, Bryansk 241037,Russian Federation*To whom correspondence should be addressed.Associate Editor: Inanc BirolReceived on September 27, 2016; revised on February 6, 2017; editorial decision on March 6, 2017; accepted on March 13, 2017AbstractWe created ResistoMap—a Web-based interactive visualization of the presence of genetic determinants conferring resistance to antibiotics, biocides and heavy metals in human gut microbiota.ResistoMap displays the data on more than 1500 published gut metagenomes of world populationsincluding both healthy subjects and patients. Multiparameter display ﬁlters allow visual assessment of the associations between the meta-data and proportions of resistome. The geographicmap navigation layer allows to state hypotheses regarding the global trends of antibiotic resistanceand correlates the gut resistome variations with the national clinical guidelines on antibioticsapplication.Availability and Implementation: ResistoMap was implemented using AngularJS, CoffeeScript,D3.js and TopoJSON. The tool is publicly available at http://resistomap.rcpcm.org.Contact: yarygin@phystech.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionMicrobial drug resistance is a global healthcare problem caused bythe extensive uncontrolled use of antibiotics in medicine and agriculture. It is predicted that toward 2050 around 10 million people willdie annually for reasons connected with drug resistance (O’Neill,2016). Although resistant pathogens are the main concern, the global microbial channels of gene exchange existing between unrelatedmicrobial taxa allow commensal microbes to share resistance geneswith opportunists.Human gut microbiota is increasingly viewed as a clinically important reservoir of drug resistance (Willmann and Peter, 2017).Among other human-associated communities, this one is the largestand most intimately involved in host health. The pool of antibioticresistance (AR) genes that increases in abundance during antibiotictreatment becomes a ‘tinderbox’: the transmission of these genes toa pathogen has dire consequences for both the patient and society.Semi-quantitative analysis of the functional composition of microbiota using ‘shotgun’ metagenomics allows the assessment of relativeabundance of AR genes in human microbiota and thus provides a personalized prediction for the capacity of microbiota to contribute tothe onset of resistant pathogens. Vast volumes of metagenomes havebeen published that provide the opportunity to estimate the variationof resistome between the subjects, healthy populations of the world aswell as the clinical cohorts. However, there is a lack of visual tools forexploratory analysis of such data; moreover, there is no integrateddatabase of gut resistome profiles.Here we present ResistoMap, an interactive tool for comprehensive visualization of the gut resistome in populations of the world.The displayed features include the relative abundance of AR genes,AR-conferring mutations as well as genes conferring resistance to biocides and heavy metals. ResistoMap is a perspective tool for exploringthe global landscape of gut resistome in order to identify nationalCV The Author 2017. Published by Oxford University Press.2205This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2205/3069860by gueston 07 January 20182206K.S.Yarygin et al.traits in antibiotic intake, correlating the resistome composition withvarious external factors and generate biomedical hypotheses that canhelp to control the drug resistance on a global scale.2.3 User contributions2 Implementation3 Data processing2.1 NavigationThe relative abundance of resistance-conferring genes was evaluatedby mapping the metagenomic reads to the CARD database v.1.0.5(McArthur et al., 2013) and normalizing the gene coverage. Thepotential resistance-conferring mutations were analyzed using the published list of such mutations (Elbehery et al., 2016) and PATRIC database (Wattam et al., 2013). The levels of resistance to heavy metalsand biocides were assessed using the BacMet database (Pal et al.,2014). Details of these steps are described in the Supplementary Data.ResistoMap contains two main interactive work fields—a geographic map and a heatmap. The heatmap displays the median relative levels of determinants conferring resistance to each of theantibiotic groups (columns) in each selected cohort of subjects(rows). The values were precomputed by classifying the gut metagenomic reads from 12 publicly available studies (see section 2.2). Thenumber of metagenomes included in the cohort is displayed on theleft from the heatmap below the colour key.The heatmap contains four vertical sections corresponding to different types of resistome quantification:1.2.3.4.levels of AR-conferring geneslevels of mutations in target geneslevels of genes conferring resistance to biocides (total)levels of genes conferring resistance to heavy metal (total)Using the drop-down lists at the top of the screen, users can choosethe antibiotic group of interest, the study(-ies) and/or the country(-ies) to be displayed on the heatmap. To filter the cohort, userscan use ‘Pool by’ checkboxes that allow the stratification of cohorts by country of origin, gender, age and diagnosis (where applicable). It is possible to sort the rows of the heatmap by thenumber of samples and resistance level (by mean value or a selected antibiotic). For convenience of comparison between closevalues, clicking on a cell ‘freezes’/‘unfreezes’ the displayed abundance value on the right.2.2 Metagenomic dataThe datasets for the analysis of resistome included 1593 gut metagenomes from the individuals from 12 studies covering 15 countries (Clemente et al., 2015; Consortium et al., 2012; Karlssonet al., 2013; Nielsen et al., 2014; Nishijima et al., 2016; ObregonTito et al., 2015; Qin et al., 2012; Rampelli et al., 2015; Tyakhtet al., 2013; Yap et al., 2013; Zeevi et al., 2015; Zeller et al.,2014). For each metagenome, additional factors including countryof origin, gender, age and clinical status were considered (whereavailable).Using ResistoMap, one can estimate the global landscape of resistance potential to different groups of antibiotics as well as inferassociations between specific drugs and clinical meta-data. It is important to provide a diverse reference database of the resistancegenes in order to draw adequate conclusions from metagenomicdata; such databases are constantly expanding, and already, usingcurrent data we have discovered a number of interesting patterns,some of which are in agreement with available epidemiological datawhile others are non-intuitive and await interpretations from a clinical perspective (see Supplementary Material). Although the precision of the resistome profile comparison between different studiesmay be limited due to variations in sample preparation protocols,the exploratory analysis of global gut resistome using ResistoMapshows global trends that will gain new insights and contribute to thespread of antibiotic stewardship and rational use of antimicrobialsubstances in agriculture.It is possible to contribute published datasets to ResistoMap by following the instructions on the site (“Add your data” button).FundingThis work was supported by the Russian Scientific Foundation [grant number15-14-00066].Conflict of Interest: none declared.ReferencesClemente,J.C. et al. (2015) The microbiome of uncontacted Amerindians. Sci.Adv., 1, e1500183.Consortium,H.M.P. et al. (2012) Structure, function and diversity of thehealthy human microbiome. Nature, 486, 207–214.Elbehery,A.H. et al. (2016) Antibiotic resistome: Improving detection and quantiﬁcation accuracy for comparative metagenomics. Omics, 20, 229–238.Karlsson,F.H. et al. (2013) Gut metagenome in European women with normal, impaired and diabetic glucose control. Nature, 498, 99–103.McArthur,A.G. et al. (2013) The comprehensive antibiotic resistance database. Antimicrobial Agents and Chemotherapy, 57, 3348–3357.Nielsen,H.B. et al. (2014) Identiﬁcation and assembly of genomes and geneticelements in complex metagenomic samples without using reference genomes. Nat. Biotechnol., 32, 822–828.Nishijima,S. et al. (2016) The gut microbiome of healthy Japanese and its microbial and functional uniqueness. DNA Res., 23, 125–133.Obregon-Tito,A.J. et al. (2015) Subsistence strategies in traditional societiesdistinguish gut microbiomes. Nat. Commun., 6, 6505.O’Neill,J. (2016). Tackling Drug-Resistant Infections Globally: ﬁnal Reportand Recommendations. London: Wellcome Trust & HM Government.Pal,C. et al. (2014) BacMet: antibacterial biocide and metal resistance genesdatabase. Nucleic Acids Res., 42, D737–D743.Qin,J. et al. (2012) A metagenome-wide association study of gut microbiota intype 2 diabetes. Nature, 490, 55–60.Rampelli,S. et al. (2015) Metagenome sequencing of the Hadza huntergatherer gut microbiota. Curr. Biol., 25, 1682–1693.Tyakht,A.V. et al. (2013) Human gut microbiota community structures inurban and rural populations in Russia. Nat. Commun., 4,Wattam,A.R. et al. (2013) PATRIC, the bacterial bioinformatics database andanalysis resource. Nucleic Acids Res, 42, D581–D591.Willmann,M. and Peter,S. (2017) Translational metagenomics and the humanresistome: confronting the menace of the new millennium. J. Mol. Med.(Berl.), 95, 41–51.Yap,G. et al. (2013). Comparative evaluation of the bacterial gene compositionof stool microbiota in infants with and without eczema preliminary ﬁndings.In: Allergy. Vol. 68. Wiley-Blackwell, NJ USA, 706–706.Zeevi,D. et al. (2015) Personalized nutrition by prediction of glycemic responses. Cell, 163, 1079–1094.Zeller,G. et al. (2014) Potential of fecal microbiota for early-stage detection ofcolorectal cancer. Mol. Syst. Biol., 10, 766.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2205/3069860by gueston 07 January 2018
5028334237002	PMID28334237	5028334237	https://watermark.silverchair.com/btx135.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28334237.main.pdf	Bioinformatics, 33(14), 2017, 2191–2193doi: 10.1093/bioinformatics/btx135Advance Access Publication Date: 11 March 2017Applications NoteGenome analysisDNA Compass: a secure, client-side site fornavigating personal genetic informationCharles Curnin1, Assaf Gordon1 and Yaniv Erlich1,2,3,*1New York Genome Center, New York, NY 10013, USA, 2Department of Computer Science, Fu Foundation School ofEngineering, Columbia University, New York, NY 10027, USA and 3Center for Computational Biology andBioinformatics, Columbia University, New York, NY 10032, USA*To whom correspondence should be addressed.Associate Editor: Bonnie BergerReceived on September 28, 2016; revised on February 19, 2017; editorial decision on March 6, 2017; accepted on March 8, 2017AbstractMotivation: Millions of individuals have access to raw genomic data using direct-to-consumercompanies. The advent of large-scale sequencing projects, such as the Precision MedicineInitiative, will further increase the number of individuals with access to their own genomic information. However, querying genomic data requires a computer terminal and computational skill to analyze the data—an impediment for the general public.Results: DNA Compass is a website designed to empower the public by enabling simple navigationof personal genomic data. Users can query the status of their genomic variants for over 1658markers or tens of millions of documented single nucleotide polymorphisms (SNPs). DNACompass presents the relevant genotypes of the user side-by-side with explanatory scientiﬁcresources. The genotype data never leaves the user’s computer, a feature that provides improvedsecurity and performance. More than 12 000 unique users, mainly from the general genetic genealogy community, have already used DNA Compass, demonstrating its utility.Availability and Implementation: DNA Compass is freely available on https://compass.dna.land.Contact: yaniv@cs.columbia.edu1 IntroductionWe have entered the era of ubiquitous genomic information. Today, approximately three million people worldwide have access to theirgenome-wide autosomal information via direct to consumer (DTC) genomic companies such as 23andMe, AncestryDNA, and FamilyTreeDNA(Khan and Mittelman, 2013). Recent studies have predicted that by2025, at least 100 million individuals will have their genomes sequenced(Stephens et al., 2015) and whole genome sequencing will become a routine part of newborn screening (Burn and Flinter, 2013).Interpretation of this data, however, remains difficult. While recentethics studies have highlighted the importance of returning results to research participants, most studies are reluctant to provide any interpretation due to regulatory complications (Jarvik et al., 2014; Evans andRothschild, 2012). As an alternative, a growing number of entities,including DTC companies or Genes for Good, return raw data to participants. However, the scale of genomic information precludes evensimple analysis by people without knowledge of bioinformatics. VCFfiles of personalized genomic tests can reach gigabytes of data; althoughthese files are textual and human-readable, they are largely inaccessibleto individuals who lack basic command-line skills. We encountered thisproblem recently in connection with our website DNA.Land (https://dna.land) (Erlich, 2015), which crowdsources genomic datasets directlyfrom people who were tested by DTC companies.One of the features of DNA.Land is genomic imputation of theuser’s DTC file to report 39 million variants. We anticipated thatthis unique feature would be well-received by the 30 000 participants of DNA.Land. However, most participants who downloadedthe data expressed a high level of frustration after repeatedly crashing their Excel spreadsheet or word processor when attempting toopen the imputed VCF file.Here, we present DNA Compass (https://compass.dna.land), a freewebsite that enables the navigation of personal genomic informationCV The Author 2017. Published by Oxford University Press.2191This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permitsunrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2191/3065932by gueston 07 January 20182192C.Curnin et al.Fig. 1. DNA Compass report for some of the height-associated genotypes of a user (left) along with information from SNPedia (right). By clicking on each SNP inthe list (left), the user can select relevant information related to the SNP from SNPedia, PubMed, dbSNP, GWAS Central or Google. The search ﬁeld (top left) supports over 1658 markers. The SNP ﬁeld (top right) allows the query of tens of millions of variants. The data presented belongs to the senior author of thismanuscriptwithout command-line knowledge. Our website aims to empower thegrowing number of individuals who have access to their genomic datasets but are uninterested in pursuing a bioinformatics or computer science degree just to search for a particular SNP or trait in their raw data.To test the website, we announced its launch on the Facebook pages ofseveral genetic genealogy groups. We had nearly 2500 unique userswithin the first 2 days of operation, demonstrating the wide interest insuch a tool. Since launching, we have had more than 13 000 uniqueusers. Overall, we received very positive feedback.(Fig. 1). Crucially, we do not send any private information to any ofthese websites. This model allows the user to access relevant scientific knowledge with maximum efficiency. If the user chooses theGoogle search option, a new tab is opened in the browser where thesearch term is the SNP’s rsID.In addition to presenting the data in tabular format, the user canalso navigate the SNPs through a digital karyotype diagram.There are several options for user support. First, an example VCFTabix pair is available for those who want to test the website. In addition, we include an extensive FAQ section and guide. Finally, thewebsite is dynamic and provides quick feedback for the user’s actions.2 User experienceTo use DNA Compass, a user must specify two files: a compressedVCF file and a corresponding Tabix index file (Danecek et al.,2011). We selected these formats due to their wide popularity: theyare reported to DTC participants on DNA.Land and are popular inwhole genome sequencing projects. DNA Compass supportsGRCh37 and GRCh38 (with newer genome builds once available).DNA Compass quickly processes the genomic information andvalidates the format. First, the site infers the genome build versionfrom the decompressed VCF file’s header. If the file’s build versioncannot be determined, the site alerts the user and attempts to extractthe genotypes according to GRCh37hg19. If a user selects a VCFwith an unsupported genome build, an error message is presented.Once the data is retrieved from the VCF file, the user can specify anindividual SNP by rsID or enter a particular condition/trait (e.g. heightor lupus) from more than 1658 categories. To assist the user to find thetopic of interest quickly, the website completes the text as the user typesand also offers the full list of categories. Next, DNA Compass presentsa table with the desired information. For each SNP, we present the following information: the user’s genotype, chromosome, cytoband, riskallele and effect size. Risk allele and effect size information is based onthe GWAS Catalog of the European Bioinformatics Institute (EBI).Importantly, the website does not directly interpret genomic information. Instead, for each SNP, users can navigate five publiclyavailable resources, namely SNPedia (Cariaso and Lennon, 2012),PubMed, dbSNP (Sherry et al., 2001), GWAS Central (Beck et al.,2014) and Google. With the exception of Google, clicking on any ofthese resources opens an iframe window that presents the selectedwebsite side-by-side with the user’s own genetic information3 Architecture and compatibilityAside from their popularity, we chose the VCF and Tabix file formatbecause a compressed VCF can be stored and managed easily, whilethe Tabix index file makes it easy to navigate to and decompress slices of the VCF. DNA Compass can accept nearly any valid VCF file,including those that are not from DNA.Land. The site can even process VCF files which contain nonstandard or inaccurate rsIDs orSNP markers—even files which contain no markers at all—as longas the chromosome and position information is accurate.DNA Compass operates almost exclusively on the client-side. Thesole role of the server is to provide static webpages and informationregarding SNP positions and associations. Importantly, the clientnever transmits information from the user’s VCF and Tabix files. Thishas two advantages. First, it obviates the need for transferring massiveVCF files across the web, which means a quick response to userqueries. Tests with a standard Macbook Air laptop show that it takesa few seconds on either Chrome or Firefox to return the results of aquery. Second, by restricting the processing of data to the client side,we mitigate genetic privacy issues related to the management andstorage of personal genomic data (Erlich and Narayanan, 2014).The server-side uses DreamFactory and SQLite3 to store the coordinates (chromosome and position in GRCh37/hg19 andGRCh38/hg38) for each SNP rsID in dbSNP141. The client-sidefunctionality employs jQuery, Bootstrap, D3.js and jsf-local-aerial.To facilitate future developments, we also released the entire sourcecode on GitHub (https://github.com/TeamErlich/dna-land-compass)under the BSD license.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2191/3065932by gueston 07 January 2018DNA Compass21934 ConclusionReferencesWith the advent of large-scale public efforts to making DNAsequencing more accessible, a massive number of individuals withgenomic data, and the increasing integration of genomics in medicine, we envision a growing demand from the general public fortechnological solutions that will allow them navigate their raw data.DNA Compass aims to increase genetic literacy and empowerresearch participants to understand their personal genomic data andadvocate for themselves.Beck,T. et al. (2014) GWAS Central: a comprehensive resource for the comparison and interrogation of genome-wide association studies. Eur. J. Hum.Genet., 22, 949–952.Burn,J., and Flinter,F. (2013) Should we sequence everyone’s genome? BMJ,346, 16–17.Cariaso,M., and Lennon,G. (2012) SNPedia: a wiki supporting personal genome annotation, interpretation and analysis. Nucleic Acids Res., 40,D1308–D1312.Danecek,P. et al. (2011) The variant call format and VCFtools.Bioinformatics, 27, 2156–2158.Erlich,Y. (2015) DNA.Land: A community-wide platform to collect millionsof genomes-phenomes (Abstract). http://www.ashg.org/2015meeting/pages/online-planner.shtml.Erlich,Y., and Narayanan,A. (2014) Routes for breaching and protecting genetic privacy. Nat. Rev. Genet., 15, 409–421.Evans,J.P., and Rothschild,B.B. (2012) Return of results: not that complicated?. Genet. Med., 14, 358–360.Jarvik,G.P. et al. (2014) Return of genomic results to research participants: theﬂoor, the ceiling, and the choices in between. Am. J. Hum. Genet., 94, 818–826.Khan,R., and Mittelman,D. (2013) Rumors of the death of consumer genomics are greatly exaggerated. Genome Biol., 14, 1.Sherry,S.T. et al. (2001) dbSNP: the NCBI database of genetic variation.Nucleic Acids Res., 29, 308–311.Stephens,Z.D. et al. (2015) Big Data: Astronomical or Genomical?. PLoSBiol., 13, e1002195.AcknowledgementWe thank the DNA Compass users who tested the website and helped us tofurther develop the website.FundingY.E. holds a Career Award at the Scientiﬁc Interface from the BurroughsWellcome Fund. This study was supported by a generous gift by Andria andPaul Heafy.Conﬂict of Interest: During the peer-review process of this work, the seniorauthor (Y.E.) became a paid consultant of MyHeritage, a genealogy companythat offers DTC services. The company was not involved in the study and thetool was not tested with datasets that were generated by the company.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2191/3065932by gueston 07 January 2018
5028334194002	PMID28334194	5028334194	https://watermark.silverchair.com/btx118.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28334194.main.pdf	Genome analysisswga: a primer design toolkit for selectivewhole genome amplificationErik L. Clarke1,*,†, Sesh A. Sundararaman1,2,*,†, Stephanie N. Seifert3,Frederic D. Bushman1, Beatrice H. Hahn1,2 and Dustin Brisson31Department of Microbiology, 2Department of Medicine and 3Department of Biology, University of Pennsylvania,Philadelphia, PA 19104, USA*To whom correspondence should be addressed.†The authors wish it to be known that these authors contributed equally.Associate Editor: John HancockReceived on October 14, 2016; revised on February 17, 2017; editorial decision on February 20, 2017; accepted on February 24, 2017AbstractMotivation: Population genomic analyses are often hindered by difficulties in obtaining sufficientnumbers of genomes for analysis by DNA sequencing. Selective whole-genome amplification(SWGA) provides an efficient approach to amplify microbial genomes from complex backgroundsfor sequence acquisition. However, the process of designing sets of primers for this method hasmany degrees of freedom and would benefit from an automated process to evaluate the vast num-ber of potential primer sets.Results: Here, we present swga, a program that identifies primer sets for SWGA and evaluatesthem for efficiency and selectivity. We used swga to design and test primer sets for the selectiveamplification of Wolbachia pipientis genomic DNA from infected Drosophila melanogaster andMycobacterium tuberculosis from human blood. We identify primer sets that successfully amplifyeach against their backgrounds and describe a general method for using swga for arbitrary targets.In addition, we describe characteristics of primer sets that correlate with successful amplification,and present guidelines for implementation of SWGA to detect new targets.Availability and Implementation: Source code and documentation are freely available on https://www.github.com/eclarke/swga. The program is implemented in Python and C and licensed underthe GNU Public License.Contact: ecl@mail.med.upenn.eduSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionSelective whole-genome amplification (SWGA) provides a means ofobtaining sufficient numbers of genomes from a target organism toperform whole-genome sequence analysis, even in the presence ofoverwhelming DNA from other organisms (Leichty and Brisson,2014). Difficulties in isolating a target of interest are common in mi-crobial population genomics, which requires acquiring adequategenomic DNA from a target while limiting the amount of non-targetDNA (Mardis, 2008). Often, the genomes of interest represent onlya fraction of a percent of the total nucleic acids in a sample, and sodirect sequencing is inefficient and expensive. Laboratory culture ofthe target microbe is the traditional solution, but many microbesreplicate poorly or not at all in in vitro conditions (Amann et al.,1990; Ghazanfar et al., 2010; Schmeisser et al., 2007).SWGA allows sequence acquisition without culture of the targetorganism or extensive purification of target DNA. It achieves this bypreferentially amplifying the target genome using a set of selectiveprimers and phi 29 polymerase-based multiple displacement amplifi-cation (MDA) (Dean et al., 2002; Leichty and Brisson, 2014). Sinceits introduction, this method has been used to study Wolbachiapipientis in Drosophila melanogaster (Leichty and Brisson, 2014),and to understand the evolution and drug resistance of PlasmodiumVC The Author 2017. Published by Oxford University Press. 2071This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comBioinformatics, 33(14), 2017, 2071–2077doi: 10.1093/bioinformatics/btx118Advance Access Publication Date: 27 February 2017Original PaperDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2071/3056525by gueston 07 January 2018falciparum (Guggisberg et al., 2016; Oyola et al., 2016;Sundararaman et al., 2016) and Plasmodium vivax (Cowell et al.,2017). Further applications of SWGA to population genomics mayhelp reconstruct epidemic transmission patterns, characterize pat-terns of inter-host viral transmission, detect escape from antimicro-bial agents, and delineate the evolutionary dynamics of immuneescape (Hume et al., 2003; Luikart et al., 2003; Mart ınez et al.,2012; Nelson et al., 2008; Nunes et al., 2012; Stack et al., 2012).Implementation of SWGA has been complicated by the difficultyin identifying an effective set of selective primers, as there are manyconstraints and degrees of freedom in the composition of potentialprimer sets. These primers must reflect DNA sequence motifs com-mon in the target genome but rare in the background DNA. Theyalso must have binding sites sufficiently near each other to enablethe branching and displacement actions of the PHI 29 polymerasethat are essential for MDA. A previously published method used aset of Perl scripts (Leichty and Brisson, 2014) to identify primerswith the highest ratio of binding frequencies in the target genomeversus the background DNA. However, choosing a set by the abovemethod is suboptimal: for one, the primers may form heterodimerswith each other or homodimers with themselves; they may be indi-vidually selective but in aggregate bind too frequently to the back-ground DNA; or, they may bind to the target’s telomeric ormitochondrial DNA, and not be sufficiently evenly distributedacross the genome. There are aspects of the primer sets that have anunknown effect on the efficiency of the reaction, including the an-nealing and melting temperature of the primer sequences, the even-ness of the binding sites across the target genome, and the density ofbinding sites. The Perl scripts mentioned above are unable to evalu-ate many of these criteria, requiring extensive manual effort andtrial-and-error to create workable designs.Here we present swga, a program that identifies selective primersets for a given target genome and background. swga evaluates allpotential primer sequences and forms sets of valid primers that meetthe above criteria. It automatically calculates a variety of metrics foreach set that potentially affect the efficacy and selectivity of the reac-tion. These sets are then are ranked and presented to the user, ena-bling the selection of primer sets most likely to succeed. Nearly alloperating parameters of the program are user-specifiable but initial-ized with reasonable defaults based on the target and backgroundgenomes selected, reducing the work needed to get started.We demonstrate the use of swga to design primer sets and testthem on two biological systems: Wolbachia pipientis from infectedDrosophila melanogaster, and Mycobacterium tuberculosis DNAspiked into human blood. For each system, we designed multiple pri-mer sets to explore the effect of various aspects of the primer sets onreaction efficacy, such as primer melting temperature, binding dens-ity on the target genome, and the evenness of binding sites. These ex-perimental results clarify the relative importance of each and allowus to describe an effective workflow for using swga.2 Materials and methods2.1 Program overviewThe swga program can be divided into four modules (Fig. 1).2.1.1 Primer identificationThe user starts by defining the target and background sequencesusing swga init. At this point, a set of sequences can be suppliedthat define a priori where primers should not bind, such as a mito-chondrial genome or plasmids (the ‘exclusionary sequences’). Theswga count command then uses DSK (Rizk et al., 2013) to identifyall nucleotide sequences in the size range specified by parametersmin_size and max_size that exist in the target genome and do notexist in the exclusionary sequences (if provided). These primers areused to populate a local SQLite database for later retrieval. The se-lectivity of these primers is determined by their frequency in the tar-get genome versus the background DNA, so swga count saves thefrequency that each primer appears in the target and background aswell. Primers that appear extremely rarely in the target and overlyfrequently in the background (as defined by user-editable param-eters, with defaults set by swga init), are not saved to help speedup downstream steps. Additionally, primers that would form in-ternal hairpins or homodimers with themselves are omitted.2.1.2 Primer filteringThe command swga filter ranks and filters potential primers bytheir melting temperature, selectivity, and evenness of binding in the tar-get genome. First, primers that bind too sparsely to the target genome(lower than parameter min_fg_bind) or too frequently to the back-ground (max_bg_bind) are removed. Next, the melting temperature isapproximated using nearest-neighbor thermodynamics (Allawi andSantaLucia, 1997) with corrections for mono- and divalent cations.Primers with melting temperatures outside the range defined by min_tmand max_tm are removed. The evenness of binding then is calculated byfinding the Gini index (Gini, 1912) of the distances between each primerbinding site on the target. The Gini index varies between 1 and 0, where1 represents extremely uneven and 0 represents perfectly even. A primerwith a low Gini index has binding sites that are each separated by simi-lar distances, whereas a primer with a high Gini index may reflect onewhere many of the primer binding sites are clumped together (e.g. ontandem repeat regions). Primers with Gini indices higher than max_giniare removed. Finally, primers are ranked by the ratio of target bindingfrequency to background binding frequency and those primers with thehighest ratio are identified for downstream use (by default, this identi-fies the top 200 primers, and is modifiable via the max_primers param-eter). The thresholds for each filter are user-editable, and the swgafilter command caches results so that it can be quickly re-run to ex-plore different results.Fig. 1. An overview of the swga workflow. The program begins by countingall nucleotide sequences of length k (k-mer) in both the target and back-ground genomes for a given range of k (e.g. 8–12 bp). The k-mers are then fil-tered by criteria that include the binding frequencies in the background andtarget genome, their melting temperatures, and the likelihood of hairpin orhomodimer formation. The best k-mers are then used to form compatiblesets, in which no k-mer would likely form a heteroduplex with any other inthe set. These sets are then evaluated for multiple criteria including bindingfrequencies and evenness. The results can be exported into common formatsfor downstream use and visualization2072 E.L.Clarke et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2071/3056525by gueston 07 January 20182.1.3 Primer set evaluationThe swga find sets command is then used to find sets of compat-ible primers from the ones identified in the last step of swga fil-ter. Brute force evaluation of all primer sets is computationallyinfeasible: given n primers and a set size of k, the total number ofpossible sets is (n choose k). With the default parameters of n¼200and k ¼ 2   7, there are over 2:4   1016 possible sets. Fortunately,not all of these sets are usable for swga. A pair of primers are incom-patible if they form heterodimers (calculated by the number of con-secutive complimentary bases), or if one primer is a subsequence ofanother. swga find sets calculates the pairwise compatibility ofall selected primers and stores the results as a graph. In this graph,primers are vertices and compatible primers are connected withedges. The problem of finding compatible sets then reduces to aproblem of finding sets of vertices in the graph that are all intercon-nected (a ‘clique’ in graph theory). swga also stores the average dis-tance between binding sites on the background as a ‘weight’ on eachvertex. This allows the program to prioritize cliques that have highertotal weights, representing sets of primers that bind infrequently tothe background.To find these cliques, the swga find_sets command uses amodified version of the program cliquer by Niskanen and €Ostergård(2003). The branch-and-bound algorithm in cliquer is a computa-tionally efficient way of finding cliques in a graph. We have ex-tended the algorithm to find only cliques that meet certain criteria.By specifying the desired criteria a priori the algorithm can skip setsthat do not meet the requirements and save computation time.These criteria include the minimum distance between binding sitesin the background (min_bg_bind_dist) and maximum distance be-tween binding sites on the target (max_fg_bind_dist). In addition,the algorithm can explore a range of set sizes (min_size and max_-size) in order to find valid sets. By specifying a broad range of setsizes, the algorithm is able to find sets with a broad range of charac-teristics independent of the number of primers.Primer sets that meet these criteria are further evaluated on metricsincluding the average and maximum distance between primer bindingsites on the target genome and the Gini index of all binding sites in theset. These sets and their accompanying metrics are then saved.Even with the above optimizations, the number of valid sets canbe quite large. For this reason, swga find_sets can be safelystopped after evaluating and storing a sufficient number of sets. Inour usage, we generally stop after 1–5 million sets have been saved.2.1.4 Primer set output and visualizationThe saved primer sets can be explored and exported using swga ex-port. This command allows the user to order the sets by any of theevaluated metrics, export all or some of the sets of interest to Excel-compatible formats, or export a set to a BedGraph or BedFile formatfor visualization in a genome browser (Kent et al., 2002).2.2 Empirical primer set testingTo evaluate swga, we used it to design primer sets for amplificationof W. pipientis DNA against a background of D. melanogaster andof M. tuberculosis against a background of H. sapiens. We evaluatedprimer sets on their ability to selectively and evenly amplify the tar-get genome.2.2.1 Designing primer sets for W. PipientisWe created four primer sets for W. pipientis against D. melanogaster,varying each by melting temperature range, selectivity, and evennessof binding sites on the target genome. We first initialized swga on theW. pipientis genome with D. melanogaster as the background, andran swga count to store all potential primers.For the first two sets, we used swga filter with the ‘standard’temperature range established in Leichty and Brisson (2014), anddefault in swga, of 15–45 C. This range we named Tm Low, orTmL. After running swga find_sets and storing 1 million sets, weused swga export to output the set with the lowest target to back-ground binding distance ratio, which we called Set TmL/Selective.We then used swga export again to output the set with the lowestGini index, which we called Set TmL/Even.The next two sets were designed with a higher melting tempera-ture range. We re-ran swga filter with a Tm range of 35–55 C,which we named Tm High, or TmH. As above, we then re-ran swgafind_sets on the new primers and chose the most selective andmost even sets from the results. These are called TmH/Selective andTmH/Even, respectively. The complete parameter listing is includedin Supplementary File 1. The primers belonging to each set are givenin Supplementary Table 1.2.2.2 Designing primer sets for M. tuberculosisWe created ten primer sets for M. tuberculosis using swga. Our tar-get genome was M. tuberculosis strain H37Rv (NC_000962.3) andour background was the human genome, version GRCh38. For thissystem, we ran swga filter with a temperature range constant at15–45 C, and imposed a maximum per-primer Gini index of 0.6.We stopped swga find_sets after storing five million sets and ex-ported all of them to CSV format using swga export. The sets werefiltered to only sets with mean distance between target binding sites<5000 bases. We selected ten sets with the most extreme combin-ations of mean target binding distance and evenness (via the metricsfg_dist_mean and fg_dist_gini, respectively). These sets we namedMtb1 through Mtb10. The distribution of these sets in the pool isvisualized in Supplementary Figure S1. In addition, we selected fromthe original five million the set with the highest Gini index (most un-even) and highest mean target binding distance as negative compari-sons, named MtbUneven and MtbSparse, respectively. The fullparameter listing is included in Supplementary File 2. The primersbelonging to each set are given in Supplementary Table 2.2.2.3 Selective whole-genome amplification and sequencingThe Wolbachia-specific primer sets were tested on pooled genomicDNA extracted from 10 Wolbachia-infected D. melanogaster (strainDmelnw118). Pooling was performed to eliminate inter-fly variabilityin Wolbachia infection levels, and each primer set was tested in trip-licate using 40 ng of input DNA per reaction, except as noted foradditional tests of the TmL/Even Wolbachia primer set. For consist-ency with the approach used in Leichty and Brisson (2014), thepooled genomic extract was digested with NarI (NEB, New EnglandBiolabs, Inc., Ipswich, MA, USA) at 37 C for 30 minutes, in orderto suppress mitochondrial amplification. This step is likely unneces-sary in the general case because swga includes an option to omitmitochondrial sequences from primer formation.Mycobacterium primer sets were tested on purified M. tubercu-losis DNA (strain H37Rv, ATCC 27294D-2), diluted to 1% inhuman genomic DNA extracted from cultured CD4þT cells. Primersets were tested in triplicate.Selective whole-genome amplification was performed as pre-viously described (Sundararaman et al., 2016), with slight modifica-tions. Reactions were performed in a volume of 50 lL using inputDNA, 3.5 mM total of SWGA primers, 1  phi29 buffer (NewEngland Biolabs), 1 mM dNTPs and 30 units phi29 polymerasePrimer design for selective whole genome amplification 2073Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2071/3056525by gueston 07 January 2018(New England Biolabs). Amplification conditions included a 1 hramp-down step (35–30 C), followed by a 16 h amplification step at30 C. Phi29 was then denatured for 10 min at 65 C.Amplified samples were purified using AmpureXP beads(Beckman Coulter), prepared for Illumina sequencing as described inKryazhimskiy et al. (2014), and sequenced on an Illumina MiSeq(150 bp, paired end). We also sequenced the unamplified pool to es-tablish a baseline for amplification efficiency. Illumina-specificadapter and primer sequences were removed from the readsusing Cutadapt (Martin, 2011). In both systems, reads were firstaligned to the background (D. melanogaster or human) using smalt(Ponstingl and Ning, 2010). Unmapped reads were then mappedto the target genome (W. pipientis or M. tuberculosis, respectively),also using smalt. Analysis of sequence coverage of the target gen-ome and sequencing rarefaction analyses were performed using R(R Core Team, 2015). All code used in the analysis and to generatethe figures is available online at https://github.com/eclarke/swga_paper.3 ResultsWe used swga to design four primer sets for amplifying Wolbachiaagainst a background of D. melanogaster, which tested the effect ofmelting temperature ranges, selectivity, and evenness. We designedtwelve primer sets for amplifying M. tuberculosis against a backgroundof human DNA with varying primer binding evenness and density onthe target. Ten sets tested were various combinations of high densityand evenness Two, for comparison, were the most uneven and mostsparse. For M. tuberculosis, we compared amplification using randomhexamers (e.g. standard MDA) to the swga-designed primer sets.3.1 Evaluation of primer sets for W. pipientisThe four primer sets for W. pipientis were designed with two differ-ent temperature ranges (TmL: 15–45 C, TmH: 35–55 C). From thesets identified in each temperature range, we chose the set with thehighest selectivity, defined by the lowest target to background bind-ing distance ratio (TmL/Selective and TmH/Selective). We also chosethe sets with the most even distribution of binding sites (TmL/Evenand TmH/Even). As a control, we included the primer set fromLeichty and Brisson (2014). The composition and metrics for eachof these five sets is shown in Table 1.The pooled genomic DNA contained 4.7% W. pipientis DNA, asdetermined by sequencing of the unamplified control. We recovered 200 Mbp of sequence for each amplicon. The proportion ofsequencing reads that were derived from W. pipientis was at least2.5 times greater in all amplified samples than the sequencing readsfrom the unamplified genomic extract (Supplementary Fig. S2). Wefound that the primer sets with the higher melting temperatures(TmH/Selective and TmH/Even) yielded more Wolbachia reads as atotal percentage, with some replicates as high as 77.8%. However,these primer sets failed to reach 10  coverage on even 10% of theW. pipientis genome (Fig. 2). This was most likely due to unevenamplification of the target genome, as shown in SupplementaryFigure S3.In contrast, the sets designed with the standard, lower meltingtemperature range (TmL) yielded more even coverage across the gen-ome (Supplementary Fig. S3). The TmL/Even primer set, selected forhaving the most even distribution of primer sites across theWolbachia genome, gave high, even coverage across the target(Fig. 3; Supplementary Fig. S3). Moreover, the TmL/Even setreduced the sequencing effort required to achieve 10  coverageacross 90% of the genome by 10-fold relative to the unamplifiedcontrol (Fig. 2), extrapolating from the still-rising unamplified con-trol’s rarefaction curve. While the final two sets—TmL/Selective andthe Leichty set—provided more even coverage of the genome thanthe TmH sets, they ultimately did not outperform the unamplifiedcontrol. The previously-published primer set from Leichty andBrisson (2014) yielded low total amplification efficiency (12.1–27.7%) and uneven coverage, while the TmL/Selective set had highamplification efficiency (50–60%) but similarly uneven coverage.We had originally expected that high numbers of primer bindingsites in local regions of the genome would provide better coverageof that region. This was not seen in any of the sets tested(Supplementary Fig. S4). In each of the five sets tested, we did notdetect a correlation between the number of primer binding sites andcoverage. However, in primer sets with an overall higher density ofbinding sites on the target (as measured by a low average distancebetween binding sites), we had generally higher coverage acrossmore of the Wolbachia genome (compare Table 1 and Fig. 2).Table 1. Characteristics of primer sets chosen for selective whole-genome amplification of Wolbachia from infected Drosophila DNARatio # Primers Gini Mean target dist Mean bg. distTmL/Selective 0.0544 9 0.654 5.33Eþ03 9.78Eþ04TmL/Evena 0.1050 7 0.537 6.85Eþ03 6.53Eþ04TmH/Even 0.0075 2 0.537 1.31Eþ04 1.73Eþ06TmH/Selective 0.0005 2 0.66 1.21Eþ04 2.43Eþ07Leichty 2014 0.0163 2 0.712 5.31Eþ03 3.25Eþ05aThe set that most effectively amplified Wolbachia. ‘Ratio’ is ratio of theaverage distance between binding distances in the target and background.Fig. 2. Selective whole genome amplification reduces the sequencing effortnecessary to achieve at least  10 coverage across the W. pipientis genome.Each color represents an individual technical replicate; dashed lines representthe unamplified control. Lines above the unamplified control represent bettersequencing efficiency in that they yielded greater coverage of the target gen-ome with less sequencing effort. Sequencing 100 million bases from unamp-lified genomic DNA extracted from 10 flies resulted in 10-fold or greatersequencing coverage in only 2.8% of the W. pipientis genome. In contrast,the TmL/Even primer set resulted in 10-fold or greater coverage of 60–75% ofthe W. pipientis genome with similar sequencing effort. This fraction wasincreased further to 72–91% when the TmL/Even primer set was used to amp-lify W. pipientis from 20 ng (rather than 40 ng) of total fly extract DNA (empir-ically, using lower total starting DNA can yield higher relative amplificationwhen using phi29). The TmL/Selective primer set and the manually chosenset (Leichty and Brisson, 2014) improved W. pipientis sequence coveragerelative to the unamplified sample. However, both of these sets failed to im-prove sequencing efficiency due an unevenness of coverage. The high Tmsets enriched only small portions of the genome and thus did not improvethe genome coverage relative to the control2074 E.L.Clarke et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2071/3056525by gueston 07 January 2018In summary, the primer set with the lowest Gini index andstandard melting temperature (TmL/Even) was the best at selectivelyand evenly amplifying Wolbachia. While other sets provided ahigher percentage of Wolbachia DNA (Supplementary Fig. S3), theoverall coverage of these sets was low and amplification mostlyoccurred in specific regions (Fig. 3). This suggests that evenness ofprimer binding sites on the target is a major factor in the efficacy ofthe primer set.3.2 Evaluation of primer sets for M. tuberculosisFor M. tuberculosis, we restricted the primer pool to only those witha low Gini index (<0.6). We let the program identify five millionprimer sets and then selected only those sets whose mean distancebetween binding sites on the M. tuberculosis genome was <5 kb.From the resulting pool of primer sets, we selected ten sets with themost extreme combinations of primer set binding evenness anddensity to test the contributions of each. These ten will be referredto as our positive tests (Mtb1-10), and the distribution of thesepoints on the total pool of sets is shown in Supplementary Figure S1.We also selected the least selective set and the most uneven set fromthe five million set pool as negative controls (MtbSparse andMtbUneven, respectively). The composition and metrics for each ofthese 12 sets is shown in Table 2.The four sets with the lowest mean binding distance (sets Mtb4,Mtb6, Mtb8 and Mtb9) on the M. tuberculosis genome performedbetter than the unamplified controls, six other positive tests, bothnegative tests and the random hexamers (Fig. 4 and Table 2). Thesesets reached 1  coverage across 38–60% of the M. tuberculosis gen-ome with 200 megabases of sequence, while the remaining six posi-tive tests did not perform better than the negative controls (Fig. 4;Supplementary Fig. S5). These four sets yielded higher coverageacross most of the Mycobacterium genome than the unamplifiedcontrols, while the remaining sets either only amplified certain re-gions or did no better than unamplified (Supplementary Fig. S6).Deeper sequencing of these four sets’ amplicons showed that thesets reached 10  coverage over 29–50% of the target by 1.5 Gbpof sequencing effort, with the unamplified controls only reaching10  coverage on 2.5% of the target for the same sequencingeffort (Fig. 5).For Mycobacterium, we found that sets with smaller distancesbetween primer binding sites on the target genome outperformedthose optimized for lower Gini index. Nine out of the ten positivetest Mycobacterium sets, including the four best sets, had lower Giniindices than the sets for Wolbachia. This suggests that after a certainthreshold the Gini index becomes secondary to the primer bindingsite density. Therefore, pre-selecting primers with a low Gini indexduring swga filter and then choosing sets with high bindingdensity in swga export allows the optimization of both attributes,and should yield effective primer sets.4 DiscussionSelective whole-genome amplification provides a way to preferentiallyamplify a target genome from a complex background. However, im-plementation of the SWGA method has been limited due to the diffi-culties in designing an effective set of primers. Assembling a primer setwhere all of the primers are compatible with each other, selective forFig. 3. Sequencing coverage of two swga-chosen sets, and the set fromLeichty and Brisson (2014), across the W. pipientis genome. The depth ofsequencing coverage per 1 Mb of sequencing effort (1 Mb * coverage depth/total bp sequenced) is shown for representative replicates of? TmH/Even andTmL/Even (red lines) relative to the unamplified control (black lines). SWGAusing the TmL/Even primer set improves depth of coverage across the major-ity of the W. pipientis genome by 10- to 100-fold, relative to the unamplifiedcontrol. SWGA using the Leichty and Brisson (2014) (top panel) or TmH/Even(middle panel) sets also improve depth of coverage but over smaller regionsof the genome, with the TmH/Even set resulting in high but localized amplifi-cation. Depth of coverage plots for all primer sets and replicates are shown inSupplementary Figure S3Table 2. Characteristics of primer sets chosen for selective whole-genome amplification of M. tuberculosis from human DNA,ordered by ratioRatio # Primers Gini Mean target dist. Mean bg. dist.Mtb6a 0.0057 7 0.501 1.95Eþ03 3.41Eþ05Mtb9a 0.0058 7 0.538 1.78Eþ03 3.05Eþ05Mtb4a 0.0062 7 0.512 1.88Eþ03 3.04Eþ05Mtb8a 0.0062 7 0.533 1.80Eþ03 2.92Eþ05Mtb7 0.0066 7 0.499 2.03Eþ03 3.09Eþ05Mtb2 0.0095 7 0.484 3.29Eþ03 3.45Eþ05Mtb5 0.0155 6 0.480 5.00Eþ03 3.22Eþ05Mtb1 0.0171 7 0.476 4.97Eþ03 2.90Eþ05Mtb3 0.0172 7 0.478 4.99Eþ03 2.90Eþ05Mtb10 0.0181 7 0.479 4.29Eþ03 2.37Eþ05MtbUneven 0.0140 2 0.623 1.14Eþ04 8.10Eþ05MtbSparse 0.0387 3 0.505 2.60Eþ04 6.71Eþ05‘Ratio’ indicates the ratio between the average distance between bindingdistances in the target and background. Primer sequences are listed inSupplementary Table 2.aThe sets that most effectively amplified Mycobacterium for SWGA.Ratio: 5.7e−03Gini: 0.50Size: 7Ratio: 9.5e−03Gini: 0.48Size: 7MtbSparse MtbUneven RandomMtb2 Mtb5 Mtb1 Mtb3 Mtb10Mtb6 Mtb9 Mtb4 Mtb8 Mtb7Ratio: 3.9e−02Gini: 0.50Size: 30 100 200 0 100 200 0 100 2000 100 200 0 100 2000%25%50%75%100%0%25%50%75%100%0%25%50%75%100%Ratio: 5.8e−03Gini: 0.54Size: 7Ratio: 1.6e−02Gini: 0.48Size: 6Ratio: 1.4e−02Gini: 0.62Size: 2Ratio: 6.2e−03Gini: 0.51Size: 7Ratio: 1.7e−02Gini: 0.48Size: 7Ratio: NAGini: NASize: NARatio: 6.2e−03Gini: 0.53Size: 7Ratio: 1.7e−02Gini: 0.48Size: 7Ratio: 6.6e−03Gini: 0.50Size: 7Ratio: 1.8e−02Gini: 0.48Size: 7Sequencing effort (Mbp sequenced)Percent of genome covered (1x)Fig. 4. Selective amplification of Mycobacterium using swga-designed setsthat prioritized primer-level evenness and set-level binding density and se-lectivity. Curves indicate the percent of the target covered at 1 depth. Setsare ordered by the ratio of average distance between primer binding sites onthe target to average binding distance on the background. The coloring indi-cates individual replicates, and the black dashed line indicates the unampli-fied control. The sets with the lowest ratios returned greater coverage of thetarget genome compared to unamplified controls than those with higherratios, as shown by the rarefaction curves of these sets being higher than thedashed linesPrimer design for selective whole genome amplification 2075Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2071/3056525by gueston 07 January 2018the target genome, and rare in the background is a problem withmany degrees of freedom. The swga program addresses this difficultyby automatically identifying and evaluating primer sets by specifiedcriteria, allowing the user to select only those sets most likely to suc-ceed in selective amplification of the target.We used swga to design primer sets for W. pipientis and M. tu-berculosis that selectively amplified each in the presence of theirhost’s genome. These sets had varying binding evenness and selectiv-ity for the target genome, allowing us to compare these attributes tothe performance of each set. In addition, we demonstrated potentialclinical utility of the swga program by amplifying DNA from theM. tuberculosis pathogen spiked into human blood. While in theseexperiments we used target/background pairs with clearly definedgenomes, there is no reason the background cannot be a heterogen-ous mixture of DNA, such as stool or soil. In this case, the back-ground could be approximated by whole-genome shotgunsequencing of the mixture, and subtracting any reads belonging tothe target, if present.Based on these results, it appears that primer binding evenness(as measured by the Gini index), primer set binding selectivity, andthe density of binding sites on the target genome each play an im-portant role in the set’s efficacy. In the W. pipientis study, we estab-lished that the temperature range of 15–45 C for the primers andprioritizing evenness of binding led to more even amplification ofthe target genome. In addition, the swga-designed sets performedbetter at selectively amplifying the target than the hand-designed setin Leichty and Brisson (2014). In fact, the Leichty primer set wasnot generated by swga because the maximum distance between pri-mer sites on the Wolbachia genome was greater than the specifiedcutoff. In M. tuberculosis, by starting with a pool of primers thatbind relatively evenly to the target, we constrained the range of setbinding evenness by removing primers that cluster on repeat regions.After controlling the range of binding evenness at the primer level,the sets with the highest target binding density (i.e. lowest mean dis-tance between binding sites) achieved highest coverage, suggestingthat further refinements of the sets for evenness is not necessary.These sets consequently had the lowest ratio of target to backgroundaverage binding distances. This ratio, as a more complete represen-tation of the set’s selectivity than just the binding density on thetarget, had a strongly inverse correlation with the amount of thegenome covered after sequencing (Fig. 6). Because both attributesare closely related, it is difficult to disentangle the effects of bindingdensity from the effects of a low ratio, and it may be that either orboth of these attributes contribute to the success of these primersets. Furthermore, some sets had relatively similar ratios (e.g. Mtb7versus Mtb8), but Mtb8 yielded greater genome coverage. This indi-cates that there are likely other set attributes not considered herethat also contribute to set efficacy. To compensate for this, we sug-gest selecting five to ten sets with low ratios to test experimentally,and then selecting the best-performing of those sets.The swga program does not consider a specific number of pri-mers for each set. Instead, swga considers primer sets of differentsizes, and reports suggested sets. By exploring a range of sizes, theswga program allows the user to find sets with desirable attributeswithout having to guess what the ideal set size will be in advance.SWGA is best suited to large scale population genomics studiesand may not be cost effective in some smaller studies. Developingthe SWGA primer set requires up-front costs that need to be re-covered in later applications for the method to be cost effective. Adetailed cost-benefit analysis over multiple applications is presentedin Supplementary File 2. SWGA is most useful when large numbersof samples are to be sequenced, when the target genome is rare inthe unamplified sample, and when higher sequencing coverage ofthe target genome is desired.Our experiments so far suggest a general workflow that can beused to design primer sets for other systems. In particular, we recom-mend the following guidelines:1. During swga filter, set the max_gini parameter as low as pos-sible while still yielding 200 or more primers.2. For swga find_sets, set the max_sets to 1–5 million to explorea wide range of set attributes.3. Use swga export to export the sets ordered by the distance be-tween binding sites on the target (attribute fg_mean_dist).4. Pick the five to ten sets with lowest fg_mean_dist to test ex-perimentally. Barcode each amplicon separately, then pool andsequence with low depth to assess performance. Once a high-performing set is identified, sequence that amplicon more deeply.Once a high-performing set is identified, it is usable in any samplesthat have similar target/background combinations.We expect best practices to evolve as SWGA is used more fre-quently. To facilitate this, we have set up a web page on the project’ssource repository and a user mailing list. A tutorial on the program’soperation and more extensive documentation on each parameterand module is available on the web page as well.Mtb4 Mtb6 Mtb8 Mtb9500 1000 1500 500 1000 1500 500 1000 1500 500 1000 15000%25%50%75%100%Sequencing effort (Mbp sequenced)Percent of genome covered (10x)Fig. 5. Deeper sequencing of four primer sets yields greater coverage of M.tuberculosis genome. The colored lines indicate individual replicates and thegreen dashed line is the pooled total. All four sets yield  10-fold increases inefficiency over the unamplified samples (black dashed line). The primer setsreach  10 coverage on between 28 and 50% of the target genome while theunamplified controls were at< 2.5%  10 coverage with 1.5 Gbp ofsequencingMtb6Mtb9Mtb4Mtb8Mtb7Mtb2Mtb5Mtb3Mtb10 MtbSparseMtbUneven10%20%30%0.00 0.01 0.02 0.03 0.04Target to background ratioPercent genome covered at 1x (50Mbp depth)Fig. 6. The percentage of the Mycobacterium genome covered by each setat 1 coverage after 50 Mbp of sequencing is inversely correlated to the set’starget to background binding distance ratio (e.g. selectivity). The smoothedline of best fit (LOESS) is shown by the dotted line. The points and whiskersrepresent the median and standard deviation of the technical replicates.Positive tests are in blue, while negative controls are in orange. The randomhexamers did not have a definable ratio and are not displayed2076 E.L.Clarke et al.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2071/3056525by gueston 07 January 2018AcknowledgementsWe thank Michael Parisi for his generous donation of Wolbachia-infectedDrosophila strains, as well as Alex Berry and other early swga users for theirfeedback.FundingThis work was supported by grants from the National Institutes of Health[R01 AI097137, R01 AI076342, R01 AI091595, R37 AI050529, T32AI007532, P30 AI045008, R01 AI100877, R01 HL113252 and R01HL087115]; and the Burroughs Wellcome Fund [1012376].Conflict of Interest: none declared.ReferencesAllawi,H.T., and SantaLucia,J. (1997) Thermodynamics and NMR of internalGT mismatches in DNA. Biochemistry, 36, 10581–10594.Amann,R.I. et al. (1990) Combination of 16S rRNA-targeted oligonucleotideprobes with flow cytometry for analyzing mixed microbial populations.Appl. Environ. Microbiol., 56, 1919–1925.Cowell,A.N. et al. (2017) Selective whole-genome amplification is a robustmethod that enables scalable whole-genome sequencing of plasmodiumvivax from unprocessed clinical samples. mBio, 8, e02257–16.Dean,F.B. et al. (2002) Comprehensive human genome amplification usingmultiple displacement amplification. Proc. Natl. Acad. Sci. U.S.A., 99,5261–5266.Ghazanfar,S. et al. (2010) Metagenomics and its application in soil microbial com-munity studies: biotechnological prospects. Malar. J., 6, 611–622.Gini,C. (1912). Variabilit a e mutuabilit a. contributo allo studio delle distribu-zioni e delle relazioni statistiche (variability and mutability. contribution tothe study of the . . . . Tipogr. di Cupini.Guggisberg,A.M. et al. (2016) Whole-genome sequencing to evaluate the re-sistance landscape following antimalarial treatment failure with fosmido-mycin-clindamycin. J.Infect. Dis., 214, 1085–1091.Hume,J.C.C. et al. (2003) Human migration, mosquitoes and the evolution ofPlasmodium falciparum. Trends Parasitol., 19, 144–149.Kent,W.J. et al. (2002) The human genome browser at UCSC. Genome Res.,12, 996–1006.Kryazhimskiy,S. et al. (2014) Global epistasis makes adaptation predictabledespite sequence-level stochasticity. Science, 344, 1519–1522.Leichty,A.R., and Brisson,D. (2014) Selective whole genome amplification forresequencing target microbial species from complex natural samples.Genetics, 198, 473–481.Luikart,G. et al. (2003) The power and promise of population genomics: fromgenotyping to genome typing. Nat. Rev. Genet., 4, 981–994.Mardis,E.R. (2008) Next-generation DNA sequencing methods. Dx.doi.org,9, 387–402.Martin,M. (2011) Cutadapt removes adapter sequences from high-throughputsequencing reads. EMBnet J., 17, p 10–12.Mart ınez,F. et al. (2012) Ultradeep sequencing analysis of population dy-namics of virus escape mutants in RNAi-mediated resistant plants. Mol.Biol. Evol., 29, 3297–3307.Nelson,M.I. et al. (2008) Molecular Epidemiology of A/H3N2 and A/H1N1Influenza Virus during a Single Epidemic Season in the United States. PLoSPathogens, 4, e1000133.Niskanen,S., and €Ostergård,P.R.J. (2003). Cliquer User’s Guide, Version 1.0.Nunes,M.R.T. et al. (2012) Phylogeography of dengue virus serotype 4,Brazil, 2010-2011. Emerg. Infect. Dis., 18, 1858–1864.Oyola,S.O. et al. (2016) Whole genome sequencing of Plasmodium falciparumfrom dried blood spots using selective whole genome amplification.Malar. J., 15, 597.Ponstingl,H., and Ning,Z. (2010). SMALT. page GNU Public License v3.R Core Team (2015). R: A Language and Environment for Statistical Computing.Technical Report, R Foundation for Statistical Computing, Vienna.Rizk,G. et al. (2013) DSK: k-mer counting with very low memory usage.Bioinformatics (Oxford, England), 29, btt020–btt653.Schmeisser,C. et al. (2007) Metagenomics, biotechnology with non-culturablemicrobes. Appl. Microbiol. Biotechnol., 75, 955–962.Stack,J.C. et al. (2012) Inferring the inter-host transmission of influenza Avirus using patterns of intra-host genetic variation. Proc. R. Soc. Lond. B:Biol. Sci., 280, rspb20122173–20122173.Sundararaman,S.A. et al. (2016) Genomes of cryptic chimpanzee Plasmodiumspecies reveal key evolutionary events leading to human malaria. Nat.Commun., 7,Primer design for selective whole genome amplification 2077Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2071/3056525by gueston 07 January 2018
5028334186002	PMID28334186	5028334186	https://watermark.silverchair.com/btx114.pdf	supplemental file (.pdf)	main.pdf	Tue Jan  9 06:17:14 2018	application/pdf	/hive/users/max/projects/florian/ismb2017/files/28334186.main.pdf	Bioinformatics, 33(14), 2017, 2089–2096doi: 10.1093/bioinformatics/btx114Advance Access Publication Date: 27 February 2017Original PaperSequence analysisRNAscClust: clustering RNA sequences usingstructure conservation and graph based motifsMilad Miladi1,†, Alexander Junge2,3,†, Fabrizio Costa1,Stefan E. Seemann2,3, Jakob Hull Havgaard2,3, Jan Gorodkin2,3,* andRolf Backofen1,2,4,*1Bioinformatics Group, Department of Computer Science, University of Freiburg, Freiburg im Breisgau, Germany,Center for Non-coding RNA in Technology and Health, University of Copenhagen, Frederiksberg, Denmark,3Department of Veterinary and Animal Sciences, University of Copenhagen, Frederiksberg, Denmark and 4Centerfor Biological Signalling Studies (BIOSS), Cluster of Excellence, University of Freiburg, Freiburg im Breisgau, Germany2*To whom correspondence should be addressed.†The authors wish it to be known that, in their opinion, the ﬁrst two authors should be regarded as Joint First Authors.Associate Editor: Cenk SahinalpReceived on June 6, 2016; revised on December 22, 2016; editorial decision on February 21, 2017; accepted on February 23, 2017AbstractMotivation: Clustering RNA sequences with common secondary structure is an essential step towards studying RNA function. Whereas structural RNA alignment strategies typically identify common structure for orthologous structured RNAs, clustering seeks to group paralogous RNAs basedon structural similarities. However, existing approaches for clustering paralogous RNAs, do nottake the compensatory base pair changes obtained from structure conservation in orthologous sequences into account.Results: Here, we present RNAscClust, the implementation of a new algorithm to cluster a set ofstructured RNAs taking their respective structural conservation into account. For a set of multiplestructural alignments of RNA sequences, each containing a paralog sequence included in a structural alignment of its orthologs, RNAscClust computes minimum free-energy structures for eachsequence using conserved base pairs as prior information for the folding. The paralogs are thenclustered using a graph kernel-based strategy, which identiﬁes common structural features. Weshow that the clustering accuracy clearly beneﬁts from an increasing degree of compensatory basepair changes in the alignments.Availability and Implementation: RNAscClust is available at http://www.bioinf.uni-freiburg.de/Software/RNAscClust.Contact: gorodkin@rth.dk or backofen@informatik.uni-freiburg.deSupplementary information: Supplementary data are available at Bioinformatics online.1 IntroductionThe structure of an RNA molecule or non-coding RNA (ncRNA) isoften crucial to its function. A main characteristic is that evolutionary changes in the primary sequence are often compensatory suchthat, e.g. an A-U base pair in human may correspond to a G-C basepair in mouse, thus preserving a functional RNA structure while(partly) erasing sequence similarity.In silico genome-wide screens for structured RNAs have therefore focused on finding RNAs with evolutionarily conserved secondary structure (see Backofen and Hess, 2010; Gorodkin et al., 2010;for reviews). A main reason is that it is not feasible to search forstructured RNAs on single sequences only, as their secondary structure is not significantly more stable compared to that of random sequences (Rivas and Eddy, 2000). Although all screens take outset inCV The Author 2017. Published by Oxford University Press.2089This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/),which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contactjournals.permissions@oup.comDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2089/3056002by gueston 07 January 20182090corresponding or syntenic sequences, two lines of strategies havebeen employed, one searching for structured RNAs in sequencebased alignments and one conducting structural alignments.Whereas the former has the advantage of faster screenings, the latteris able to handle sequence identities below about 60 to 70%. In thisidentity range sequence based alignments are no longer accurateenough to represent RNA structure conservation (Gardner andGiegerich, 2004; Washietl and Hofacker, 2004). Examples of methods working on sequence based alignments include RNAz (Gruberet al., 2010) and EvoFold (Pedersen et al., 2006). Programs forstructural alignment applied to genomic screens includesFoldalign, Dynalign, LocaRNA and CMfinder (Havgaardet al., 2007; Fu et al., 2014; Will et al., 2013a; Yao et al., 2006).Corresponding screens for structure RNAs range from prokaryotes(Uzilov et al., 2006; Weinberg et al., 2010) to fly (Will et al., 2013b)to vertebrates (Smith et al., 2013; Torarinsson et al., 2006, 2008).The output of each screen for conserved RNA secondary structures is a set of multiple alignments containing orthologous RNAspredicted to adapt a common secondary structure. These sets arelargely unannotated and the road to obtain functional evidence forthese putative ncRNAs is tedious. One of the most promising annotation strategies would be to detect paralogs in form of RNA families or classes. Whereas members of RNA families originate from acommon ancestor, members of an RNA class share the same functional structure without evolutionary relationship (Stadler, 2014). Aprominent example for such an RNA class are microRNAs.An attractive strategy to detect RNA families and classes in computational ncRNA screens is to cluster the RNA candidates basedon sequence and structure. Early approaches directly clustered RNAsequences based on their sequence-structure alignment scores(Havgaard et al., 2007; Will et al., 2007), despite the high complexity of at least Oðn4 Þ for aligning two sequences. Albeit recentsequence-structure alignment tools are able to compute the alignment in time quadratic in sequence length (Otto et al., 2014; Willet al., 2015), the overall approach still does not scale to large datasets since it remains quadratic in the number of sequences clustered.For this reason, alignment-free RNA clustering approaches havebeen introduced (Heyne et al., 2012; Middleton and Kim, 2014).In this paper, we boost the alignment-free clustering pipelineGraphClust (Heyne et al., 2012) by employing information aboutcovariation contained in the alignments. The GraphClust pipelineworks on single sequences and clusters paralogs. Work extendingover single sequence clustering has been introduced by EvoFam tocluster EvoFold predictions (Parker et al., 2011). However, thesepredictions are grounded in sequence based alignments with limiteddegree of sequence variation. Here, we are interested in uncoveringthe full potential to search for paralogs including less sequentiallyconserved structured RNAs that may only be found through thestructural alignment strategy. Thus, in contrast to previous work,we here focus on measuring the clustering performance as a functionof the degree of compensatory base changes, or equivalently the degree of sequence similarity, in the structural alignments.We develop RNAscClust, which clusters sequences from an organism of interest that are aligned to their orthologs found in different species. Firstly, RNAscClust represents the sequence stemmingfrom the species of interest in each input alignment as a secondarystructure that is obtained by constraining highly conserved basepairs. The pipeline then compares these structures using a graph kernel (Costa and De Grave, 2010). The graph kernel decomposes eachstructure into several substructures and can be regarded as an extension of k-mer decompositions from sequences to graphs. Comparingthese substructures finally induces a similarity measure used toM.Miladi et al.cluster the structures. The usage of locality sensitive hashing techniques (Broder, 1997) enables a complexity linear in the size of thedataset, considerably lower than the quadratic time performance ofclustering approaches relying on all-vs-all comparisons.We compare the performance of RNAscClust to GraphClustusing benchmark datasets derived from the Rfam database(Nawrocki et al., 2014). RNAscClust is benchmarked with sets ofRNA sequence alignments restricted to specific ranges of sequenceidentity. Each RNAscClust clustering is compared to a corresponding GraphClust result obtained by clustering human sequencescontained in each alignment.We demonstrate a considerable positive effect of incorporatingstructure conservation in alignments of orthologous sequences whenclustering paralogous RNA sequences from an organism of interest.This results in a beneficial accuracy compared to clustering of singlesequences alone, especially for datasets with low to medium sequence identity.2 Materials and methods2.1 Clustering approachThis section describes the RNAscClust pipeline and analyzes itscomputational complexity. RNAscClust accepts a set of multiplealignments as input where each alignment contains a sequence fromthe organism of interest structurally aligned to its orthologs. Our approach first predicts the secondary structure for the sequence fromthe organism of interest in each alignment using information aboutconserved base pairs. The secondary structure is then encoded as asparse feature vector. Candidate clusters are iteratively selected inlinear time and refined in a final post-processing step. Figure 1 compares this structure conservation-aware clustering to single sequenceclustering. We furthermore introduce classification and clusteringperformance measures used in this work.2.1.1 Representing a multiple sequence alignment as anRNA secondary structureLet M be the set of structural alignments of RNA sequences to beclustered by RNAscClust. In the first step, we predict the consensusstructure Sm of each alignment m 2 M to identify conserved basepairs. We chose PETfold (Seemann et al., 2008) in this step as it isshown to perform well for predicting the consensus structure from aset of aligned sequences (Puton et al., 2013). PETfold predicts aconsensus structure by taking evolutionary and thermodynamic information into account and assigns a reliability r 2 ½0; 1  to eachbase pair. For a given alignment m and reliability threshold s, a basepair (i, j) is considered conserved if its reliability rij ! s. Conservedbase pairs are used as constraints for predicting the secondary structure of the sequence from the species of interest using RNAfold(Lorenz et al., 2011). This allows to project conserved base pairsfrom the alignments onto the sequence, while tolerating variationsin less structurally conserved alignment columns.Additionally, we use the recently proposed R-scape (Rivas et al.,2016) to identify base pairs with statistically significant(E-value < 0.05) covariation. R-scape assesses the statistical significance of the observed covariation by simulating alignments under thenull hypothesis that nucleotide substitutions appear independently ineach column under a phylogenetic model. This allows to put furtheremphasis on covarying base pairs, independent of sequence information, in the clustering process by adding decorated graphs (see Section2.1.2) whenever at least 20% of the base pairs are supported by covariation. The outcome of this first step, illustrated in Figure 2a, is aDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2089/3056002by gueston 07 January 2018RNAscClust2091(a)humancovariation(b)CCAUCGGACACAGUACUCAGUGACCCAGUGACACAGUCCAUCGhumanUAGCCUCG-CGUCGchimppigCAGUAUUGUUGCAAmouseA-AACUUUCGAACGconsensus structure ( . ( . . . ) )structure predictionfrom single sequence5' GAC3'UGwrong structureAGCA5'Gcorrect U U G G ACUG Astructure 3' U G AACsingle sequenceclusteringCCAUCGGACCCAGUC-GCCUCGC-GCACUGA-AACGUUstructure predictionbased on consensus structureACACUCAGUGUGAUACUGCA-UAGCGA-GCACCCAGACGNNCG ACUG ANN A3'AU5'Uemphasizecovaryingbase pairsclustering usingstructure conservationGACACAGUCCAUCGACUCAGUGACCCAGUGACACAGUGACCCAGUwrong clusteringACUCAGUcorrect clusteringFig. 1. Hypothetical example to illustrate the difference between single sequence clustering and clustering using conserved structure. Assume the indicated G-Ubase pair between the ﬁrst and last nucleotide in the left-most blue human sequence is part of its correct secondary structure. While single sequence structureprediction (a) fails to predict the G-U pair, information about covariation contained in the alignment (b) yields the correct secondary structure for the human sequence and allows to emphasize covarying base pairs. Taking covariation and conserved structures into account may thus yield an improved clustering(a)(b)Fig. 2. Representing the constrained folded secondary structure as a graph and feature extraction. (a) Base pairs with a reliability greater than t are set as structureconstraints (blue boxes) derived from the alignment consensus structure. A constrained secondary structure prediction is performed for the human sequence,the organism of interest in this example. Plain and, if enough covarying base pairs are found, decorated secondary structure are represented as graphs.(b) Auxiliary vertices (gray) are added to the secondary structure graph to emphasize stacked base pairs. The secondary structure is decomposed into substrucvtures using a graph kernel. Here, only neighborhood subgraphs for N1 and v ¼ 1; . . . ; 6 are shown and d ¼ 0 which results in the extraction of single root verticesinstead of root vertex pairs. The hashing function H encodes each subgraph as an integer which in turn becomes the index of the subgraph in the sparse feature46vector counting subgraph occurrences. Since N1 ¼ N1 , the feature is counted twice while the other neighborhood subgraphs are unique. The feature extractionfor N-N decorated structures is implemented the same way (Color version of this ﬁgure is available at Bioinformatics online.)secondary structure representing the alignment m in the remainingpart of the RNAscClust pipeline.2.1.2 Efficient encoding of the RNA secondary structureRNAscClust follows the approach implemented by Heyne et al.(2012) and represents each secondary structure as a graph wherenucleotides are encoded as vertices with discrete labels A, C, G, Uwhile the backbone and the base pair relations are encoded asedges. Auxiliary vertices adjacent to four nucleotides formingstacked base pairs are added (see Fig. 2b, top) to emphasize basepair stacks. We define the graph Gm as the secondary structuregraph associated with the alignment m 2 M. Our frameworkallows to add path graphs to Gm to include sequence information.Path graphs are graphs that only contain the backbone (i.e. theribose-phosphate bond) as edges. Adding a path graph to Gm allowsto consider sequence similarities in addition to similarities at the secondary structure level. Decorated graphs, according to R-scape, arecreated by representing significantly covarying base pairs as genericN-N pairs. Thus sequence information for these base pairs is removedallowing to match corresponding features between alignments without requiring the exact base pairs to be matched.In RNAscClust sparse feature vectors are extracted from Gmusing the Neighborhood Subgraph Pairwise Distance Kernel(NSPDK) (Costa and De Grave, 2010), a convolutional graph kernel. A graph kernel allows to compute the similarity of two graphsusing the dot product in the induced feature space. While graphDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2089/3056002by gueston 07 January 20182092kernels commonly define a feature space only implicitly and compute directly the resulting dot product, NSPDK explicitly enumerates the features and stores them in a sparse feature vector thatremains of manageable size. More precisely, the NSPDK defines asfeature all small subgraph-pairs at short distance from each other asdefined in the following.NSPDK considers neighborhood subgraphs: a neighborhoodvsubgraph Nr ðGm Þ is defined as the subgraph induced by all verticesthat are reachable from a given root vertex v in not more than rhops along the edges of Gm. The distance d between a pair of neighborhood subgraphs is defined as the distance between the respectuive root vertices. Finally, a feature in NSPDK is a pair Nr ðGm Þ andvmNr ðG Þ with root vertices u, v that are at distance d. The completefeature set is generated by considering all possible pairs of neighborhood subgraphs for all values of the parameters r and d such thatr 2 f0; . . . ; rmax g and d 2 f0; . . . ; dmax g. Each pair of neighborhoodsubgraphs is then encoded as an integer using a fast hashing procedure (see Fig. 2b; Costa and De Grave, 2010 for details) that yields alow number of hash collisions. One crucial advantage of NSPDK isthat given a graph G ¼ ðV; EÞ with vertex set V and edge set E, thesize of the associated sparse feature vector (i.e. number of non-zerofeatures) is bounded to a factor of jVj, allowing fast computations insubsequent steps. While other graph kernels commonly yield a number of features (i.e. subgraphs) that is exponential in the size of V,NSPDK generates a number of subgraphs that is linear in jVj (Costaand De Grave, 2010).2.1.3 Similarity notion between RNA alignmentsThe similarity between two alignments is defined as the dot productof the corresponding sparse feature vectors. As larger values of radius r and distance d tend to generate a larger number of highly specific features, the feature vectors are normalized such that eachcombination of r and d contributes equally to the final vector encoding. That is, each feature vector /r;d ðGÞ, generated by neighborhoodsubgraph pairs of radius r at distance d, is normalized to unit length:b/ r;d ðGÞ ¼ /r;d ðGÞ=jj/r;d ðGÞjj and then assembled into the final feaPbture vector /ðGÞ ¼ r2R;d2D / r;d ðGÞ:2.1.4 Clustering secondary structuresTo avoid the quadratic complexity arising from an all-vs-all comparisons of all secondary structures, RNAscClust performs approximatenearest neighbor queries to identify candidate clusters. More precisely, we build an inverse index based on a compact signature (obtained using the min-hash approach (Broder, 1997)) of the featurevectors which can be used to retrieve similar instances with a lookupoperation in constant time. See Heyne et al. (2012) for further details.Running the approximate nearest neighbor query on each instanceyields candidate clusters each consisting of a set of sequences. All candidate clusters are ranked by their mean pairwise similarity and areaccepted or rejected, in rank order, using a greedy procedure. Theprocedure discards a cluster if it does not contain at least fraction q ofunseen sequences, i.e. if the candidate cluster overlaps too much withthe union of all previously accepted clusters. To further improve theconsistency of the retrieved clusters, we post-process each cluster bycomputing the sequence-structure alignment tree of the clustered sequences using LocARNA (Will et al., 2007, 2012). Sequences belonging to the subtree with the highest average pairwise alignment scoreare then used to fit a covariance model using Infernal (Nawrockiand Eddy, 2013). The covariance model ultimately decides clustermembership by scanning the entire dataset and populating the clusterwith all the instances that score above a bit-score threshold.M.Miladi et al.2.1.5 Runtime complexity of RNAscClustFor the input set of alignments M of size N ¼ jMj, let L denote themaximum sequence length in the alignments, let S ¼ maxm2M ðjmjÞ denote an upper bound on the number of sequences per alignment. Theinitial consensus structure prediction using PETfold and constrainedfolding using RNAfold have complexity OðS Á L3 Þ per alignment therefore OðN Á S Á L3 Þ for the complete dataset.Let m be an alignment with the maximal number of vertices andedges. Generating its encoded graph Gm ¼ ðVm ; Em Þ has complexityOðjVm j þ jEm jÞ and complexity OðN Á ðjVm j þ jEm jÞÞ for the wholedataset. As outlined in Section 2.1.2, generating the sparse featurevectors from Gm has complexity OðjVm jÞ; OðN Á jVm jÞ for the wholedataset M, by hashing feature vectors to integer codes. Finally, bothclustering steps using approximate nearest neighbors queries andpost-processing have complexity OðNÞ (see Costa and De Grave,2010; Heyne et al., 2012 for further details). Since in realistic scenarios N ) L and N ) S, the overall runtime of RNAscClust isOðNÞ. The runtime of RNAscClust is thus linear in the number ofinput alignments. Figure 3 depicts the complete RNAscClust pipeline and indicates pipeline steps that are executed in parallel.2.2 Evaluation metricsClassification: Consider a binary classification problem. A true positive (TP) is an object correctly classified as positive, a false positive(FP) is an object wrongly classified as positive. Similarly, we definetrue and false negatives (TN and FN). We use the following measures to assess the performance of a binary classifier:Precision (also known as positive predictive value) is the fractionof correctly classified positives out of all objects classified as positive, i.e. TP=(TP þ FP).Recall (also known as sensitivity) is the fraction of correctlyclassified positives out of all positives, i.e. TP=(TP þ FN). Finally,F1-Score (van Rijsbergen, 1979) is the harmonic mean of Precisionand Recall:F1 À Score ¼ 2 ÁPrecision Á RecallPrecision þ RecallIn a multi-class scenario, as presented below, the F1-Score is themean of the class-wise F1-Score weighted by the class size.Clustering: The Rand Index (Rand, 1971) measures the fractionof object pairs that are grouped in the same way in a predicted clustering and the true class assignment. Let a be the number of objectpairs that are in the same class and in the same cluster and let b bethe number of pairs that are in different classes and in different clusters, then the Rand Index is defined as (a þ b)=(jMjÁ (jMj- 1)=2). TheAdjusted Rand Index (Hubert and Arabie, 1985), a version of theRand Index adjusted for chance, is defined as:Adjusted Rand Index ¼Rand Index À E½ Rand Index  1 À E½ Rand Index  Here, the E½ Rand Index   is the expected Rand Index. The AdjustedRand Index has an upper bound of 1 and higher values indicatea better agreement between the clustering and the true classassignment.2.3 MaterialsTo the best of our knowledge no dataset is available that can be directly used to benchmark RNAscClust. We thus created benchmarkdatasets following two different approaches to assess the performance of RNAscClust. These benchmarks are named the Rfam-omeand Rfam-cliques datasets. All benchmark sets were derived fromDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2089/3056002by gueston 07 January 2018RNAscClust2093RNAscClust speciﬁc input and methodologyInput multiplealignmentsConservation awarescondary structureSparse featurevectorizationLocal sensitivityhashingCandidate clusters12345GraphClust methodologyCandidate clustersof representativesCluster reﬁnementand extensionReportclusters786Iterate until no candiate leftFig. 3. An overview of RNAscClust with steps executed in parallel shown as stacks. The sequence of interest in each alignment is constraint folded to generate aconservation aware secondary structure (steps 1 & 2). The resulting secondary structure graph is transformed into a feature vector in step 3. Using local sensitivity hashing (step 4), candidate clusters are extracted in the ﬁfth step. Then a series of post-processing steps as implemented in GraphClust are invoked (steps 6–8): sequences of each cluster are aligned using LocARNA and only well aligning sequences are retained. A covariance model is generated with Infernal to extend clusters with sequences matching the model. Steps 6–8 are repeated until all candidate clusters are processedthe Rfam database (Nawrocki et al., 2014). The central design ideais to split each Rfam family seed alignment into subalignments andassess how well the clustering pipeline retrieves the Rfam families.Here a subalignment is considered to be a subset of an Rfam seedalignment. Human is the organism of interest in our benchmark.Each subalignment must hence contain a human sequence. The quality of a cluster assignment is measured by rating how well it agreeswith the true Rfam family assignment.alignment, the Rfam-cliques sets control the mean PSI and hence theamount of covariation captured in each alignment.To generate the Rfam-cliques benchmark dataset, each Rfam family seed alignment is processed separately and depicted as a graph.Each sequence in the alignment is a vertex. Two vertices are connectedby an edge if they originate from different species. More precisely, foran Rfam family F an undirected graph G is defined such that2.3.1 Rfam-ome benchmark datasetThe Rfam-ome dataset was designed to collect orthologs of a particularhuman RNA in one subalignment. On the other hand, human paralogsof the same Rfam family are assigned to different subalignments. TheRfam-ome benchmark is generated by processing each Rfam family individually. In the first step, human sequences are extracted from thefamily seed alignment. The genomic locations of these human sequences are then identified by a sequence search against the humangenome using BLAST (Altschul et al., 1997) while only accepting exactsequence matches. To extend these genomic locations to their genomicneighborhood, context of the same length as the hit is appended in upand downstream of each hit. Regions syntenic to these extended hitsare identified in 26 other species using LiftOver (Kent et al., 2003).For each species other than human, exact matches of the organism’s sequences contained in the input Rfam seed alignment are searched inthe regions orthologous to the human neighborhood. This step yieldssequences trusted to be orthologous to the original human sequencehit. Finally a subalignment containing each human (paralog) sequencealong with its orthologous sequences is built.Collecting the subalignments for all Rfam families yields the complete dataset named Rfam-ome. Note that all alignments in the Rfamome benchmark set are created by extracting the respective rows fromRfam seed alignments, while LiftOver is solely used to assignorthologous sequences to each human paralog. All genomes as well aschain files used by LiftOver were downloaded from the UCSC genome browser (Rosenbloom et al., 2015). Information about the genomes used are listed in the Supplementary Section S1 along withfurther details about the Rfam-ome pipeline.V ¼ fsi jsi is a sequence in the seed alignment of Fg;2.3.2 Rfam-cliques benchmark datasetsThe Rfam-ome dataset contains only few alignments with meanpairwise sequence identity (PSI) below 70% (Supplementary SectionS1.2). Using constraints on the PSI of sequences added to the sameG ¼ ðV; EÞ;E ¼ ffsi ; sj gjsi and sj belong to different species g:We then generate subgraphs of G where vertices are connectedonly if their PSI is in a specific range. For PSI thresholds l 2 ½0; 1 and h 2 ½0; 1  such that l < h, we define Gh , a subgraph of G:lGh ¼ ðV; Eh Þ;llEh ¼ ffsi ; sj gjfsi ; sj g 2 E and l < PSI ðsi ; sj Þlhg;where PSI(si, sj) is the PSI of the sequences si and sj. Gh contains thelsame vertices as G but only those edges whose corresponding pairsof sequences have a PSI in the range ½l; h .The Algorithm generating the Rfam-cliques set for an individual family is outlined below. Subalignments are selected to be maximal cliqueswith maximum mean PSI in each iteration. A clique is a subset of thevertices of a graph in which each pair of vertices is connected by anedge. A clique is maximal if it is not a subset of a larger clique. Extractedcliques must have at least five vertices/sequences, one of human origin.The Algorithm considers different PSI ranges in descending order. Itstarts with a graph containing vertices in V and the edge set E0:95 . After0:9extracting subalignments as maximal cliques, additional edge sets0:80:5C ¼ fE0:9 ; E0:7 ; . . . ; E0:4 g0:8are added iteratively to G and additional subalignments extracted. Thisiterative approach extracts cliques with homogeneous similarities firstand allows remaining edges to form cliques in subsequent iterationsthus yielding a broader PSI distribution in the alignments. Note that Ccontains non-overlapping edge sets selected according to the PSI of theadjacent sequences. The described procedure is performed for eachRfam family separately and the resulting subalignments are combinedto create the dataset named Rfam-cliques High. Further details aboutthe dataset generation can be found in Supplementary Section S2.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2089/3056002by gueston 07 January 20182094M.Miladi et al.Algorithm generating the Rfam-cliques benchmark set for asingle family.1: G ¼ ðV; EÞ ¼ ðV; 1Þ2: Rfam-cliques ¼ 13: for ðh; lÞ 2 fð0:95; 0:9Þ; ð0:9; 0:8Þ; . . . ; ð0:5; 0:4Þg do4:E ¼ E [ Ehl5:while G has a maximal clique of size! 5 that containsa human sequence do:6:C¼argmaxmeanPSIðcÞc 2 maximal-cliquesðGÞ;c has human sequence ;kck ! 57:Rfam-cliques ¼ Rfam-cliques [ C8:V ¼ VnC" remove vertices in C from G9:end while10: end for11: return Rfam-cliques2.3.3 Rfam-cliques variantsBesides the Rfam-cliques High set, we generated two additionalvariants of the Rfam-cliques dataset. The Rfam-cliques Mediumbenchmark set was generated by modifying Line 3 of the Algorithmas follows:ðh; lÞ 2 fð0:8; 0:7Þ; . . . ; ð0:5; 0:4ÞgThe Rfam-cliques Low benchmark was generated by setting Line 3 as:ðh; lÞ 2 fð0:7; 0:6Þ; . . . ; ð0:5; 0:4ÞgThis means that each pair of sequences assigned to one subalignmentof the Rfam-cliques Medium dataset has a PSI of at most 0.80 whileeach sequence pair contained in a subalignment of the Rfam-cliquesLow dataset has a PSI of at most 0.70.Our motivation for creating the Rfam-cliques Medium andRfam-cliques Low datasets in addition to the Rfam-cliques Highbenchmark was to test RNAscClust on benchmark sets with varying degrees of mean PSI of the alignments. This in turn allows us toassess the clustering performance of RNAscClust for differentamounts of covariation (see Supplementary Section S3.3 for an Rscape covariation analysis). Table 1 lists the mean of thesubalignment-wise mean PSI, referred to as mean PSI from here on,in each dataset together with the respective number of subalignments and Rfam families. All families comprising less than threealignments were removed from the datasets prior to benchmarking.2.3.4 Single-sequence datasetsBy design, each subalignment in the Rfam-ome and Rfam-cliquesbenchmarks contains a human sequence. This enables the comparison of RNAscClust and GraphClust by measuring the degree towhich Rfam families are reconstructed using human sequences aloneand comparing the outcome to an RNAscClust result harnessingcovariance information contained in the structural alignments.3 Results3.1 Similarity metric evaluation through classificationFirst, we assess the quality of the similarity metric, based on dotproducts of sparse feature vectors, induced by RNAscClust withoutperforming a clustering. An established approach (Videm et al.,Table 1. Benchmark dataset statistics: Mean of the subalignmentwise mean PSI (mean PSI), number of subalignments and Rfamfamilies in the benchmark datasets. Only Rfam families with atleast three subalignments are countedDatasetRfam-omeRfam-cliques HighRfam-cliques MediumRfam-cliques LowMean PSISubalignmentsFamilies0.780.730.630.5011823416692284826102014) is to test the performance of a classifier only depending on thepairwise similarities of all objects in the dataset. Here, pairwise similarities based on RNAscClust sparse feature vectors are comparedto those similarities generated by GraphClust using a k-NearestNeighbor (k-NN) classifier. RNAscClust default parameters areused in all subsequent analyses (RNAscClust’s pipeline default values are: s ¼ 0:9 (see Supplementary Section S3.4), rmax ¼ dmax ¼ 3;q ¼ 50%; / ¼ 20 bits and the size of the feature space is 230.GraphClust was run with default parameters except that nosequence windowing was performed to obtain a clustering of fulllength sequences. Up to 15 rounds of iterative clustering was performed for both tools.).The evaluation was performed by computing sparse feature vectors for the Rfam-ome and Rfam-cliques benchmark datasets. Thesimilarity of each pair of alignments was then computed as detailed inSection 2.1.3. A k-Nearest Neighbor classifier combined with 3-foldstratified cross-validation was used to rate the accuracy of the pairwise similarities for the benchmark sets. Stratified cross-validation ensures that each fold contains roughly the same distribution of classlabels as the entire dataset. The classifier’s parameter k was fixed to 1and cross-validation was solely used to measure the classification performance. Precision, recall and F1-Score obtained by the k-NN classifier after cross-validation are depicted in Table 2 for k ¼ 1. The k-NNclassifier based on RNAscClust similarities outperformed the classifier based on GraphClust similarities under all metrics and benchmarks considered. This indicates that the structure conservationbased similarities generated by RNAscClust reflect the Rfam familystructure in the Rfam-ome and Rfam-cliques datasets more accuratelythan sequence-based similarities produced by GraphClust. We obtained similar results for the 3-NN classifier of both RNAscClustand GraphClust (Supplementary Table S1).Note that both RNAscClust and GraphClust use sparse feature vectors to iteratively extract clusters from the dataset. Theseclustering and post-processing steps were not taken into account inthe above evaluation and are thus considered next.3.2 Clustering evaluationWe compared the clustering accuracy of RNAscClust andGraphClust for all benchmark datasets. Both RNAscClust andGraphClust use an iterative clustering procedure, howeverRNAscClust has the advantage of generating more accuratefeature vectors as demonstrated in the previous section. We henceaddressed the question to which extent these more accurate feature vectors translate into beneficial clusterings. All RNAscClustand GraphClust clusterings were compared to the Rfam familylabels of each benchmark dataset serving as the ground truth clustering. Instances that were not assigned to a cluster by GraphClust orRNAscClust were assigned to singleton clusters.Figure 4A depicts the Adjusted Rand Index of RNAscClust andGraphClust for the Rfam-cliques datasets, Figure 4B shows clusteringDownloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2089/3056002by gueston 07 January 2018RNAscClust2095Table 2. 1-Nearest-Neighbor classiﬁcation performance based on pairwise similarities computed by RNAscClust and GraphClustDatasetPrecisionRNAscClustGraphClustRfam-cliques LowRfam-cliques MediumRfam-cliques HighRfam-ome0.93 6 0.040.92 6 0.030.92 6 0.010.96 6 0.030.79 6 0.060.83 6 0.020.88 6 0.020.88 6 0.03RecallRNAscClustGraphClust0.95 6 0.010.93 6 0.010.91 6 0.000.97 6 0.020.78 6 0.090.85 6 0.010.87 6 0.000.92 6 0.02F1-ScoreRNAscClustGraphClust0.93 6 0.030.92 6 0.020.90 6 0.000.96 6 0.030.76 6 0.090.83 6 0.020.86 6 0.000.90 6 0.03Mean 6 standard deviation of Recall, Precision and F1-Score for 3-fold stratiﬁed cross validation are depicted.ABFig. 4. RNAscClust and GraphClust clustering performances, measured by the Adjusted Rand Index, depending on the mean of the alignment-wise mean pairwise sequence identity (mean PSI) of the Rfam-cliques Low, Medium and High (A) as well as Rfam-ome (B) benchmark setsresults for Rfam-ome set. The Rand Index is depicted in SupplementaryFigure S1. Three alternative configurations of the graph encoder arealso proposed in Supplementary Section S3 with an overall evaluationdepicted in Supplementary Figure S8. These experiments confirmed thatRNAscClust yields better clustering results than GraphClust for allbenchmarks. Furthermore, RNAscClust performed best for the Rfamcliques Low set while the performance decreased for Rfam-cliquesMedium and Rfam-cliques High sets. Recall that the mean PSI of theRfam-cliques Low dataset is lower than the mean PSI in the Rfam-cliques Medium set while the Rfam-cliques High has the highest meanPSI. We hypothesize that the performance increase achieved byRNAscClust is a result of the larger covariation captured in the Rfamcliques Medium and, even larger, in the Rfam-cliques Low set, whencompared to the Rfam-cliques High set. Additional covariation mayyield more accurate structure predictions in each alignment and hencean improved clustering performance.An example for the largely improved performance ofRNAscClust compared to GraphClust is the SECIS-1(RF00031) Rfam family in the Rfam-cliques Medium set with amean PSI of 39%. RNAscClust correctly clusters all sevenhuman sequences into one cluster consisting only of SECIS-1 sequences; GraphClust wrongly places them into multiple clustersmixed with sequences from other families. The same difference isobserved in the Rfam-cliques High set. For the Rfam-cliques Lowset, RNAscClust outperforms GraphClust by, for example,predicting more homogeneous and complete clusters for the wellknown structurally conserved tRNA family (RF00005) with amean PSI of 43%.4 DiscussionWe presented RNAscClust, a pipeline for clustering a set of multiple alignments of structured RNAs each containing a sequencefrom an organism of interest that is aligned to orthologous sequences. RNAscClust is geared towards clustering RNA structuresby taking structural conservation into account. RNAscClust harnesses evolutionarily conserved secondary structure in the clusteringprocess by maintaining conserved base pairs in a constrained folding. This emphasizes the core secondary structure of each alignmentwhile allowing flexibility in the structure arising due to insertions,deletions and non-compensatory mutations. RNA structures areencoded as graphs and a graph kernel is used to generate sparse feature vectors inducing a pairwise similarity notion. RNAscClust hasa runtime linear in the number of input alignments making it amenable to cluster large datasets.Employing structure conservation yielded a more accurate pairwise similarity measure and improved the clustering performance.The largest improvements in clustering accuracy were observed forbenchmark datasets with low to medium sequence identities. We hypothesize this happens for two reasons: Firstly, evolutionary information contained in the alignments can yield better secondary structurepredictions than single sequence folding, explaining the increasedclustering performance. Secondly, since RNAscClust focuses on evolutionarily conserved base pairs when comparing secondary structuresbetween alignments, identifying these conserved base pairs enables abetter estimation of the ncRNA transcript boundaries within thealignment. This helps further improving the secondary structure prediction accuracy in comparison with single sequence clustering.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2089/3056002by gueston 07 January 20182096RNAscClust could be extended by an improved post-processingstep. For instance, a novel post-processing step based on CMcompare(Honer zu Siederdissen and Hofacker, 2010) could be used to improve¨the clustering performance. The approach would be based on covariance models trained for each alignment which are afterwards compared using the Link score as computed by CMcompare. The graphkernel could be extended to allow for vectors of real numbers as nodeand edge labels. This way, both nucleotide and base pair distributionsin the input alignments could be encoded after defining an appropriate similarity function for subgraphs.RNAscClust produces accurate clusterings while running in lineartime. This will facilitate the interpretation of currently available and future large scale genomic screens for structured RNAs potentially containing millions of instances to be clustered (e.g. Smith et al., 2013).Pipeline availability: RNAscClust is available as source codeand as a Docker container (Merkel, 2014) making it possible to runthe pipeline without the need to install individual dependencies.Furthermore the container allows to reproduce all Figures andTables shown in Section 3.AcknowledgementsWe would like to thank Sita J. Saunders and Steffen Heyne for providinglibraries to encode secondary structures as graphs and assistance with runningGraphClust. We thank Christian Anthon for helpful discussions and providing the mapping of Rfam seed sequences to the respective genomes used forconstructing the Rfam-ome dataset.FundingThis work was supported by Innovation Fund Denmark, the Danish Centerfor Scientiﬁc Computing (DCSC/DeiC), the Danish Cancer Society and by theDeutsche Forschungsgemeinschaft (DFG, MO 2402/1-1, BA 2168/4-2, BA2168/3-3 A to R.B.).Conﬂict of Interest: none declared.ReferencesAltschul,S.F. et al. (1997) Gapped BLAST and PSI-BLAST: a new generationof protein database search programs. Nucleic Acids Res., 25, 3389–3402.Backofen,R. and Hess,W.R. (2010) Computational prediction of sRNAs andtheir targets in bacteria. RNA Biol., 7, 33–42.Broder,A.Z. (1997). On the resemblance and containment of documents. In:Compression and Complexity of Sequences 1997 (Proceedings), pp. 21–29.Costa,F. and De Grave,K. (2010). Fast neighborhood subgraph pairwise distance kernel. In: Proceedings of the 27th International Conference onMachine Learning (ICML-10), Haifa, Israel, pp. 255–262. Omnipress.Fu,Y. et al. (2014) Dynalign II: common secondary structure prediction for RNAhomologs with domain insertions. Nucleic Acids Res., 42, 13939–13948.Gardner,P.P. and Giegerich,R. (2004) A comprehensive comparison of comparative RNA structure prediction approaches. BMC Bioinformatics, 5, 18.Gorodkin,J. et al. (2010) De novo prediction of structured RNAs from genomic sequences. Trends Biotechnol., 28, 9–19.Gruber,A.R. et al. (2010). RNAz 2.0: Impoved noncoding RNA detection. In:Proceedings of the Paciﬁc Symposium on Biocomputing 2010, pp. 69–79.Havgaard,J.H. et al. (2007) Fast pairwise structural RNA alignments by pruning of the dynamical programming matrix. PLoS Comput. Biol., 3,1896–1908.Heyne,S. et al. (2012) GraphClust: alignment-free structural clustering of localRNA secondary structures. Bioinformatics, 28, i224–i232.Honer zu Siederdissen,C. and Hofacker,I.L. (2010) Discriminatory power of¨RNA family models. Bioinformatics, 26, i453–i459.Hubert,L. and Arabie,P. (1985) Comparing partitions. J. Class., 2, 193–218.M.Miladi et al.Kent,W.J. et al. (2003) Evolution’s cauldron: Duplication, deletion, and rearrangement in the mouse and human genomes. Proc. Natl. Acad. Sci. U. S. A.,100, 11484–11489.Lorenz,R. et al. (2011) ViennaRNA package 2.0. Algorithms Mol. Biol., 6, 1–14.Merkel,D. (2014) Docker: lightweight linux containers for consistent development and deployment. Linux J., 2014, 2.Middleton,S.A. and Kim,J. (2014) NoFold: RNA structure clustering withoutfolding or alignment. RNA, 20, 1671–1683.Nawrocki,E.P. and Eddy,S.R. (2013) Infernal 1.1: 100-fold faster RNA homology searches. Bioinformatics, 29, 2933–2935.Nawrocki,E.P. et al. (2014) Rfam 12.0: updates to the RNA families database.Nucleic Acids Res., 43, D130–D137.Otto,C. et al. (2014) ExpaRNA-P: simultaneous exact pattern matching andfolding of RNAs. BMC Bioinformatics, 15, 6602.Parker,B.J. et al. (2011) New families of human regulatory RNA structuresidentiﬁed by comparative analysis of vertebrate genomes. Genome Res., 21,1929–1943.Pedersen,J.S. et al. (2006) Identiﬁcation and classiﬁcation of conserved RNAsecondary structures in the human genome. PLoS Comput. Biol., 2, e33.Puton,T. et al. (2013) CompaRNA: a server for continuous benchmarking ofautomated methods for RNA secondary structure prediction. Nucleic AcidsRes, 41, 4307.Rand,W.M. (1971) Objective criteria for the evaluation of clustering methods.J. Am. Stat. Assoc., 66, 846–850.Rivas,E. and Eddy,S.R. (2000) Secondary structure alone is generally not statistically signiﬁcant for the detection of noncoding RNAs. Bioinformatics,16, 583–605.Rivas,E. et al. (2016) A statistical test for conserved RNA structure shows lackof evidence for structure in lncRNAs. Nat. Methods.Rosenbloom,K.R. et al. (2015) The UCSC Genome Browser database: 2015update. Nucleic Acids Res., 43, D670–D681.Seemann,S.E. et al. (2008) Unifying evolutionary and thermodynamic information for RNA folding of multiple alignments. Nucleic Acids Res., 36,6355–6362.Smith,M.A. et al. (2013) Widespread purifying selection on RNA structure inmammals. Nucleic Acids Res., 41, 8220–8236.Stadler,P.F. (2014) Class-speciﬁc prediction of ncRNAs. Methods Mol. Biol.,1097, 199–213.Torarinsson,E. et al. (2006) Thousands of corresponding human and mousegenomic regions unalignable in primary sequence contain common RNAstructure. Genome Res., 16, 885–889.Torarinsson,E. et al. (2008) Comparative genomics beyond sequence-basedalignments: RNA structures in the ENCODE regions. Genome Res., 18,242–251.Uzilov,A.V. et al. (2006) Detection of non-coding RNAs on the basis of predictedsecondary structure formation free energy change. BMC Bioinformatics, 7, 173.van Rijsbergen,C.J. (1979). Information Retrieval. 2nd edn. Butterworth, London.Videm,P. et al. (2014) BlockClust: efﬁcient clustering and classiﬁcation of noncoding RNAs from short read RNA-seq proﬁles. Bioinformatics, 30, i274–i282.Washietl,S. and Hofacker,I.L. (2004) Consensus folding of aligned sequencesas a new measure for the detection of functional RNAs by comparative genomics. J. Mol. Biol., 342, 19–30.Weinberg,Z. et al. (2010) Comparative genomics reveals 104 candidate structuredRNAs from bacteria, archaea, and their metagenomes. Genome Biol., 11, R31.Will,S. et al. (2007) Inferring non-coding RNA families and classes by meansof genome-scale structure-based clustering. PLoS Comput Biol., 3, e65.Will,S. et al. (2012) LocARNA-P: Accurate boundary prediction and improveddetection of structural RNAs. RNA. 18, 900–914.Will,S. et al. (2013a) LocARNAscan: Incorporating thermodynamic stabilityin sequence and structure-based RNA homology search. Algorithms Mol.Biol., 8, 14.Will,S. et al. (2013b) Structure-based whole-genome realignment revealsmany novel noncoding RNAs. Genome Res., 23, 1018–1027.Will,S. et al. (2015) SPARSE: quadratic time simultaneous alignment and foldingof RNAs without sequence-based heuristics. Bioinformatics, 31, 2489–2496.Yao,Z. et al. (2006) CMﬁnder–a covariance model based RNA motif ﬁndingalgorithm. Bioinformatics, 22, 445–452.Downloaded from https://academic.oup.com/bioinformatics/article-abstract/33/14/2089/3056002by gueston 07 January 2018
